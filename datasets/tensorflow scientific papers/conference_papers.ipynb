{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["\n","import json\n","import os\n","import re\n","import random\n","import pickle\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","data_path = 'source'\n",""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["## Reading arxiv metadata for filtering to DS / CS papers\n","\n","with open(os.path.join('output','arxiv_metadata.pkl'), \"rb\") as open_file:\n","    arxiv_metadata = pickle.load(open_file)\n",""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["203037it [00:52, 3837.54it/s]\n"]}],"source":["## Create a set for the CS paper arxiv_ids for filtering to them on semanticscholar\n","\n","cs_papers = set()\n","\n","with open(os.path.join('arxiv-dataset',\"train.txt\"),'r') as scipapers:\n","    for art in tqdm(scipapers): #203037 papers in total\n","        article = json.loads(art)\n","        id = article['article_id']\n","        cat = ' '+arxiv_metadata[id]['category']+' '\n","        if re.search(r'[ ](stat\\.ML|math\\.ST|cs\\.[A-Z]{2})',cat):\n","            cs_papers.add(id)\n",""]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["20548890it [07:18, 46836.55it/s]\n"]}],"source":["## Filter semanticscholar dataset\n","\n","pattern = r\"arxiv\\.org/(?:pdf|abs)/(.+)\"\n","\n","## Init papers dictionary\n","papers = {}\n","\n","with open('../papers-2017-10-30.json','r') as articles:\n","    ## Go through each line (article)\n","    for article in tqdm(articles):\n","        ## Load values from json\n","        article_json = json.loads(article)\n","        venue = article_json['venue']\n","        journal = article_json['journalName']\n","        urls = article_json['pdfUrls']\n","        ## Go through the list of urls\n","        for url in urls:\n","            ## If there's an arxiv url, get the arxiv_id from it\n","            if 'arxiv' in url:\n","                arxiv_search = re.search(pattern, url)\n","                ## If arxiv_id is found, check whether it's in the \"TF Scientific paper CS paper subset\"\n","                ## then break the loop as there's no need to go through more links for the same paper\n","                if arxiv_search:\n","                    arxiv_id = arxiv_search.group(1)\n","                    arxiv_id = re.sub(r\"\\.pdf.*?$\",\"\",arxiv_id)\n","                    arxiv_id = re.sub(r\"v[0-9]{1,2}$\",\"\",arxiv_id)\n","                    arxiv_id = arxiv_id.replace('/','')\n","                    ## If arxiv_id is in the TF CS subset, add the semanticscholar article to the dictionary\n","                    if arxiv_id in cs_papers:\n","                        papers[arxiv_id] = article_json\n","                    break\n",""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["\n","paperssubset = {}\n","for key, value in papers.items():\n","    paperssubset[key] = {k: value[k] for k in ('title','journalName','journalPages','journalVolume','venue','pdfUrls','id','authors','s2Url')}\n",""]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["## Write the CS paper subset to a pickle\n","\n","with open(os.path.join('output','cs_papers_semanticscholar.pkl'),'wb') as openfile:\n","    pickle.dump(paperssubset, openfile)\n",""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["9019"]},"metadata":{},"execution_count":10}],"source":["## Number of papers found from the TF scientific papers dataset CS subset\n","\n","len(papers)\n",""]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["## Create a set for the arxiv_ids, that are conference papers based on url / venue / journalname / etc\n","\n","conference_url = {'aclweb.org','iclr.cc','nips.cc','aaai.org','ieeecomputersociety','ieeexplore','ieee'}\n","conference_venue = {'ACL','ICLR','NIPS','CVPR','AAAI','IEEE'} # ICLM not found\n",""]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["\n","with open(os.path.join('output','conf_names.pkl'),'rb') as openfile:\n","    conf_names = pickle.load(openfile)\n",""]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["\n","conf_names = set(conf_names)\n",""]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["\n","conf_names.remove('WWW')\n","conf_names.remove('FG')\n","conf_names.remove('MM')\n","conf_names.remove('ALT')\n","conf_names.add('ACM')\n",""]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["2700\n"]}],"source":["\n","a = {}\n","\n","for key, value in papers.items():\n","    urls = value['pdfUrls']\n","    venue = ' '+value['venue']+' '\n","    journalname = value['journalName']\n","    journalvolume = value['journalVolume']\n","    journalpages = value['journalPages']\n","    if any(' '+cv+' ' in venue for cv in conf_names):\n","        a[key] = value\n","    for url in urls:\n","        url = url.lower()\n","        urlsearch = re.search(r\"(?:https://|http://)?(?:www\\.)?([a-z0-9.\\-]+?\\.(?:com|org|eu|co\\.uk|us|cc|edu\\.au|ac\\.il|gov|edu|ca|net|ac\\.uk|press|info|ie|ch|de|fr))[^a-z]\",url)\n","        if urlsearch:\n","            urlfound = urlsearch.group(1)\n","            if any(v.lower() in urlfound for v in conf_names):\n","                a[key] = value\n","    if journalname != '' and journalname != 'CoRR':#and journalname != 'CoRR':# and journalvolume != '' and journalpages != '':\n","        a[key] = value\n","\n","print(len(a))\n","\n",""]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["\n","with open(os.path.join('output','conference_papers.pkl'),'wb') as openfile:\n","    pickle.dump(a, openfile)\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}