{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["## Importing packages\n","\n","import json\n","import os\n","import re\n","import random\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import time\n","from tqdm import tqdm\n","from matplotlib import pyplot as plt\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["\n","with open(os.path.join('output','conference_papers.pkl'),'rb') as openfile:\n","    conference_papers = pickle.load(openfile)\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["## Defining regular expression filters for the sentences\n","\n","import re\n","\n","def pattern_search(sentence_list, whitespace_separated = False):\n","\n","    clean_sentence_list = []\n","    reason_list = []\n","\n","    p_basic = r\"(@x)|(\\[\\s)|(\\{)\" # contains @x or [ or {\n","    p_char = r\"[^a-zA-Z\\s.\\-,()!?\\\"':;]\" # contains anything else than a-z, A-Z, whitespace, - ,()!?\"'+:;=\n","    p_cit = r\"\\s\\w[.]\\s\" # contains whitespace+letter+period+whitespace\n","    p_etal = r\"et al\" # contains et al\n","    p_link = r\"(http)|(www)|([.]com)\" # contains http or www or .com\n","    p_punct = r\"[a-zA-Z][.!?\\-]\"\n","    p_ieeg = r\"\\si\\.e\\.|\\se\\.g\\.|\\sfig\\s\"\n","\n","    for i, sentence in enumerate(sentence_list):\n","        var = False\n","        # can't contain @x or [ or {\n","        reason = 'p_basic'\n","        if not re.search(p_basic, sentence):\n","            reason = 'p_char'\n","            # can't contain anything else than a-z and few exceptions\n","            if not re.search(p_char, sentence):\n","                reason = 'p_cit'\n","                # can't contain j. citations and other single letter before period\n","                if not re.search(p_cit, sentence):\n","                    reason = 'p_etal'\n","                    # can't contain et al\n","                    if not re.search(p_etal, sentence):\n","                        reason = 'p_link'\n","                        # can't contain link\n","                        if not re.search(p_link, sentence):\n","                            reason = 'p_ieg / p_punct'\n","                            # Filter out letter followed by period\n","                            if whitespace_separated == False:\n","                                if not re.search(p_ieeg,sentence):\n","                                    reason = 'None'\n","                                    var = True\n","                            else:\n","                                if not re.search(p_punct, sentence):\n","                                    var = True\n","                                    reason = 'None'\n","        if var == True:\n","            clean_sentence_list.append(sentence)\n","        reason_list.append(reason)\n","\n","    return clean_sentence_list, reason_list\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["203037it [01:13, 2749.05it/s]From 2700 CS/ML/mathstat papers, the loop collected 153055 sentences in 73.85962152481079 seconds\n","\n"]}],"source":["## Creating sentences dataset\n","\n","sentence_list = []\n","reason_list = []\n","counter = 0\n","\n","start_time = time.time()\n","with open(os.path.join('arxiv-dataset',\"train.txt\"),'r') as scipapers:\n","    for art in tqdm(scipapers): #203037 papers in total\n","        article = json.loads(art)\n","        id = article['article_id']\n","        text = article['article_text']\n","        article_sentence_list = []\n","        if id in conference_papers:\n","            counter = counter + 1\n","            article_sentence_list, article_reason_list = pattern_search(text, whitespace_separated = True) # applying regular expressions filters\n","            sentence_list = sentence_list + article_sentence_list\n","            reason_list = reason_list + article_reason_list\n","\n","print(f\"From {counter} CS/ML/mathstat papers, the loop collected {len(sentence_list)} sentences in {time.time() - start_time} seconds\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["## Removing double whitespaces and unnecessary whitespaces in front and at the end of sentences\n","\n","def sentence_filter(sentence_list):\n","    clean_sentence_list = []\n","    for i, sent in enumerate(sentence_list):\n","        # Multiple whitespaces to one\n","        sent = re.sub(r\"\\s+\",r\" \",sent)\n","        # Clean second part of sentece if letter+whitespace+period is followed by any text\n","        sent = re.sub(r\"(\\w\\s[.]).+\",r\"\\1\",sent)\n","        # Clean whitespaces before punctuations\n","        sent = re.sub(r\"\\s([.,!?:;])\",r\"\\1\",sent)\n","        # Clean start of sentence\n","        sent = re.sub(r\"^[^\\w]+\",r\"\",sent)\n","        # Clean whitespaces before and after hyphen and parentheses\n","        sent = re.sub(r\"([(\\-])\\s(\\w)\",r\"\\1\\2\",sent)\n","        sent = re.sub(r\"(\\w)\\s([)\\-])\",r\"\\1\\2\",sent)\n","        clean_sentence_list.append(sent)\n","    return clean_sentence_list\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["153055\n","['in the last years wireless communication systems coped with the problem of delivering reliable information while granting high throughput.', 'several works addressed the parallelization of turbo decoder architectures to achieve higher throughput.', 'although throughput and area have been the dominant metrics driving the optimization of turbo decoders, recently, the need for flexible systems able to support different operative modes, or even different standards, has changed the perspective.', 'multi-asip is an effective solution.', 'thus, together with flexible and high throughput processing elements, a multi-asip architecture must feature also a flexible and high throughput interconnection backbone.', 'in this work a general framework to design network on chip based turbo decoder architectures has been presented.', 'the proposed framework can be adapted to explore different topologies, degrees of parallelism, message injection rates and routing algorithms.', 'experimental results show that generalized de-bruijn and generalized kautz topologies achieve high throughput with a limited complexity overhead.', 'moreover, depending on the target throughput requirements different parallelism degrees, message injection rates and routing algorithms can be used to minimize the network area overhead.', 'we present a detailed study on the nature of biases in network sampling strategies to shed light on how best to sample from networks.']\n"]}],"source":["## Applying cleaner function on dataset\n","\n","clean_sentence_list = sentence_filter(sentence_list)\n","\n","print(len(clean_sentence_list))\n","print(clean_sentence_list[0:10])\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["## 90% mean by token count\n","\n","from nltk.tokenize import word_tokenize\n","\n","def keep_mean(clean_sentence_list):\n","    sentence_len = []\n","    for i, sentence in enumerate(clean_sentence_list):\n","        sent = word_tokenize(sentence)\n","        sentence_len.append([sentence,len(sent)])\n","\n","    sentence_len = pd.DataFrame(sentence_len, columns = ['sentence','len'])\n","\n","    sentence_len = sentence_len[(sentence_len.len < sentence_len.len.quantile(0.95)) \\\n","        & (sentence_len.len > sentence_len.len.quantile(0.05))]\n","\n","    mean_sentence_list = sentence_len['sentence'].tolist()\n","\n","    return mean_sentence_list\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["134001\n"]}],"source":["## Applying mean function to dataset\n","\n","cs_sentence_list = keep_mean(clean_sentence_list)\n","print(len(cs_sentence_list))\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["## Writing dataset to pickle\n","\n","with open(os.path.join('output','cs_conf_sentence_list.pkl'), 'wb') as open_file:\n","    pickle.dump(cs_sentence_list, open_file)\n"]}],"metadata":{"interpreter":{"hash":"e593b6b48f4d831142fd9a6dbb7deba9940f0775846a9f84aecc70739ae3211c"},"kernelspec":{"display_name":"Python 3.9.5 64-bit ('writingassistant': pyenv)","name":"python3"},"language_info":{"name":"python","version":""},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}