{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"T5_Fine-tuning_Paraphrasing_M2.ipynb","provenance":[{"file_id":"1LpapHXGWp-qsQzH5oQ5xlKZtIE1lOkuq","timestamp":1618570053589}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"59adc2455ef94917b5017cc043025620":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_63fd02c642164005bfb9d4233dccd9f4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5ceb625ea43f49fc9e0a4264de6952cb","IPY_MODEL_81b8393ed7e342b9a8864434e1780bbd"]}},"63fd02c642164005bfb9d4233dccd9f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5ceb625ea43f49fc9e0a4264de6952cb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_caf29b1ef48d4e249787c0f991d57ffc","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":791656,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":791656,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_682bf84af5d94355888384612e669e53"}},"81b8393ed7e342b9a8864434e1780bbd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_247166e7fc9e4cb8bf21c3fbeb27f626","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 792k/792k [00:01&lt;00:00, 715kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3fd8f09ed4c64f8da850047774b1aa55"}},"caf29b1ef48d4e249787c0f991d57ffc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"682bf84af5d94355888384612e669e53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"247166e7fc9e4cb8bf21c3fbeb27f626":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3fd8f09ed4c64f8da850047774b1aa55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9cf1e5c01968468680a3c416b66dc608":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d7e803c36c244b0a1bb01a60982f8d8","IPY_MODEL_1beea8b125454a2eba29293709d24f74"],"layout":"IPY_MODEL_3508e462ffa14fd3a47baba62592bb58"}},"6d7e803c36c244b0a1bb01a60982f8d8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_3c44611633c342ff8e0f23d9c1ce1b2a","max":1200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1bb939a638da4dcb994fd5d9291b6396","value":1200}},"1beea8b125454a2eba29293709d24f74":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb4c1260b6624303b4f25cb8d68172fa","placeholder":"​","style":"IPY_MODEL_520720468be8427886094f411f94c381","value":" 1.20k/1.20k [00:00&lt;00:00, 1.79kB/s]"}},"3508e462ffa14fd3a47baba62592bb58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c44611633c342ff8e0f23d9c1ce1b2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bb939a638da4dcb994fd5d9291b6396":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"bb4c1260b6624303b4f25cb8d68172fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"520720468be8427886094f411f94c381":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7db2d22531274b2f84e824b8ea15e20f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_092d2d7096814866a061ba9bbb773d25","IPY_MODEL_8a486bb5a7304f53b6a67320a54c1afc"],"layout":"IPY_MODEL_b222ef0042ac4565b547d144044df755"}},"092d2d7096814866a061ba9bbb773d25":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_62987d6ac17541fb8aaf626dc4551f85","max":2950825948,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6773dcf4e59462aa1ac948bafa780d5","value":2950825948}},"8a486bb5a7304f53b6a67320a54c1afc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf4fff25efa345969b20edfb8f81a26a","placeholder":"​","style":"IPY_MODEL_29bd097becaf4eada00d2d372ba0b4c9","value":" 2.95G/2.95G [01:29&lt;00:00, 33.1MB/s]"}},"b222ef0042ac4565b547d144044df755":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62987d6ac17541fb8aaf626dc4551f85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6773dcf4e59462aa1ac948bafa780d5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"bf4fff25efa345969b20edfb8f81a26a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29bd097becaf4eada00d2d372ba0b4c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"612031c950b84b6ea0eb1ec3f1869da1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba352efa580843cca223324d5f40f3b7","IPY_MODEL_04e44786cc4045a2963e24930647f3b7"],"layout":"IPY_MODEL_080fafced0c54c6794abc5c0cb35b437"}},"ba352efa580843cca223324d5f40f3b7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"Validation sanity check: 100%","description_tooltip":null,"layout":"IPY_MODEL_b80a87ea93504a459526a930395f9f1d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1db1692773c748089c381e99e1283428","value":1}},"04e44786cc4045a2963e24930647f3b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e17bdc9ea13437882366308a78f509d","placeholder":"​","style":"IPY_MODEL_aa1ccb026a76412aae9ad46207c4487b","value":" 5/5 [00:01&lt;00:00,  3.92it/s]"}},"080fafced0c54c6794abc5c0cb35b437":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"b80a87ea93504a459526a930395f9f1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1db1692773c748089c381e99e1283428":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"2e17bdc9ea13437882366308a78f509d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa1ccb026a76412aae9ad46207c4487b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf1c9738c00c4d359c08b0c80146bcc3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2c354eea4c1d452fb407693dc8abe017","IPY_MODEL_01b04479c6f94363a0b774c13744ef28"],"layout":"IPY_MODEL_1e69479bf4834d4ca805352b7028ef61"}},"2c354eea4c1d452fb407693dc8abe017":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Epoch 2: 100%","description_tooltip":null,"layout":"IPY_MODEL_d1dabae081064bfaa63a301a90fd967b","max":75000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1bbc7cfd59e14c6ab30b262fb1fd8de7","value":75000}},"01b04479c6f94363a0b774c13744ef28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65967ec9ea5341cdafb5cbc986342526","placeholder":"​","style":"IPY_MODEL_a709c507b47242e59eca625c0dd746f1","value":" 75000/75000 [6:11:31&lt;00:00,  3.36it/s, loss=0.079, v_num=14, val_loss=1.62]"}},"1e69479bf4834d4ca805352b7028ef61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"d1dabae081064bfaa63a301a90fd967b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bbc7cfd59e14c6ab30b262fb1fd8de7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"65967ec9ea5341cdafb5cbc986342526":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a709c507b47242e59eca625c0dd746f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58cd98deeeab41bfbc57d00a47a36a69":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9f665d732293435b989680cf17003e59","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7f394a657b074286971bfe100b9e821e","IPY_MODEL_abe2af20249f4a7195b9de77d72469d1"]}},"9f665d732293435b989680cf17003e59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"row wrap","width":"100%","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"inline-flex","left":null}},"7f394a657b074286971bfe100b9e821e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d530acedd7f04ee4a3105d3221abc73a","_dom_classes":[],"description":"Validating: 100%","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2fb01918543b4d76a73ea4a942a05b80"}},"abe2af20249f4a7195b9de77d72469d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6971d3680a8a433ea604db1216d7fbd4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1938/1938 [02:53&lt;00:00, 11.20it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_58136f6e39604c9eba249e90e1f94d58"}},"d530acedd7f04ee4a3105d3221abc73a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2fb01918543b4d76a73ea4a942a05b80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":"2","_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6971d3680a8a433ea604db1216d7fbd4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"58136f6e39604c9eba249e90e1f94d58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"PlNF-_DjfgtK"},"source":["\n","# **Install libraries**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"50WuQcwFS2V8","executionInfo":{"status":"ok","timestamp":1624124306005,"user_tz":-120,"elapsed":18029,"user":{"displayName":"antoine magdi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01891798323931301668"}},"outputId":"3e4cd6b1-14ea-4c7b-be14-0ad775fdc9e5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9fnyZQ_0UKZ3","executionInfo":{"status":"ok","timestamp":1624124306006,"user_tz":-120,"elapsed":12,"user":{"displayName":"antoine magdi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01891798323931301668"}},"outputId":"c3b624a8-756f-47af-f50f-eeab432a583d"},"source":["%cd \"/content/drive/MyDrive/T5_paraphrasing_reference_code\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/T5_paraphrasing_reference_code\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FITSyyasU2OV","executionInfo":{"status":"ok","timestamp":1624124310849,"user_tz":-120,"elapsed":4850,"user":{"displayName":"antoine magdi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01891798323931301668"}},"outputId":"31064230-27e2-4c82-be5b-e14d00e664b4"},"source":["!pip install sentencepiece"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n","\r\u001b[K     |▎                               | 10kB 19.5MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 26.2MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 23.0MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 17.6MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 15.4MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 17.5MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 13.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 15.0MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92kB 15.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 14.3MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 14.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 14.3MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 14.3MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 14.3MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 14.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163kB 14.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 14.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 307kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 14.3MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y4jFFe1QV7fs","executionInfo":{"status":"ok","timestamp":1624124417305,"user_tz":-120,"elapsed":106468,"user":{"displayName":"antoine magdi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01891798323931301668"}},"outputId":"0d3a0f9c-bd32-4208-ce7e-236186feb9a9"},"source":["!pip install torch==1.4.0\n","!pip install transformers==2.9.0\n","!pip install pytorch_lightning==0.7.5"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting torch==1.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/3b/fa92ece1e58a6a48ec598bab327f39d69808133e5b2fb33002ca754e381e/torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4MB)\n","\u001b[K     |████████████████████████████████| 753.4MB 23kB/s \n","\u001b[31mERROR: torchvision 0.10.0+cu102 has requirement torch==1.9.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n","\u001b[?25hInstalling collected packages: torch\n","  Found existing installation: torch 1.9.0+cu102\n","    Uninstalling torch-1.9.0+cu102:\n","      Successfully uninstalled torch-1.9.0+cu102\n","Successfully installed torch-1.4.0\n","Collecting transformers==2.9.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n","\u001b[K     |████████████████████████████████| 645kB 11.8MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (1.19.5)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 26.0MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (2.23.0)\n","Collecting tokenizers==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/59/bb06dd5ca53547d523422d32735585493e0103c992a52a97ba3aa3be33bf/tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n","\u001b[K     |████████████████████████████████| 5.6MB 26.4MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (2019.12.20)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (0.1.96)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (1.0.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (2.10)\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.45 tokenizers-0.7.0 transformers-2.9.0\n","Collecting pytorch_lightning==0.7.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ac/ac03f1f3fa950d96ca52f07d33fdbf5add05f164c1ac4eae179231dfa93d/pytorch_lightning-0.7.5-py3-none-any.whl (233kB)\n","\u001b[K     |████████████████████████████████| 235kB 12.9MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.7.5) (1.4.0)\n","Collecting future>=0.17.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n","\u001b[K     |████████████████████████████████| 829kB 21.6MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.7.5) (4.41.1)\n","Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.7.5) (2.5.0)\n","Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.7.5) (1.19.5)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.34.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (0.12.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (3.3.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (2.23.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (3.12.4)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (57.0.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.8.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (0.6.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.0.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.31.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (0.4.4)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (0.36.2)\n","Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.24.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (1.15.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.7.5) (4.5.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.0.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (4.2.2)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (4.7.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.7.5) (1.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.4.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.1.1)\n","Building wheels for collected packages: future\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491070 sha256=037b9286497d418119fe34838ce6d19c9428abeda363838341c6d86849d02733\n","  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n","Successfully built future\n","Installing collected packages: future, pytorch-lightning\n","  Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","Successfully installed future-0.18.2 pytorch-lightning-0.7.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pn-G_eUwFovO","executionInfo":{"status":"ok","timestamp":1624124417307,"user_tz":-120,"elapsed":22,"user":{"displayName":"antoine magdi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01891798323931301668"}},"outputId":"61ee5148-24d8-404d-e171-0bac1439e0b6"},"source":["# Check we have a GPU and check the memory size of the GUP\n","!nvidia-smi -L"],"execution_count":null,"outputs":[{"output_type":"stream","text":["GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-80f044c5-7c07-d362-d641-b48394c364c2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H0n55Ex1Bl2k"},"source":["# **Import packages**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m5WiQS6FEEsL","executionInfo":{"status":"ok","timestamp":1624124421255,"user_tz":-120,"elapsed":3955,"user":{"displayName":"antoine magdi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01891798323931301668"}},"outputId":"b1d5f941-a6b7-4b5f-e515-de25a40cc163"},"source":["import argparse\n","import glob\n","import os\n","import json\n","import time\n","import logging\n","import random\n","import re\n","from itertools import chain\n","from string import punctuation\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pytorch_lightning as pl\n","\n","from transformers import (\n","    AdamW,\n","    T5ForConditionalGeneration,\n","    T5Tokenizer,\n","    get_linear_schedule_with_warmup\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.file_utils:PyTorch version 1.4.0 available.\n","INFO:transformers.file_utils:TensorFlow version 2.5.0 available.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Ykds8V47B1XT"},"source":["# **Set a seed**"]},{"cell_type":"code","metadata":{"id":"CyrYjMFREUCn"},"source":["def set_seed(seed):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  torch.manual_seed(seed)\n","\n","set_seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8uSCxnPmCALw"},"source":["# **T5FineTuner**"]},{"cell_type":"code","metadata":{"id":"Zr7mnuYEEhxn"},"source":["class T5FineTuner(pl.LightningModule):\n","    def __init__(self, hparams):\n","        super(T5FineTuner, self).__init__()\n","        self.hparams = hparams\n","\n","        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n","        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n","\n","    def is_logger(self):\n","        return True #self.trainer.proc_rank <= 0\n","\n","    def forward(\n","            self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n","    ):\n","        return self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            decoder_attention_mask=decoder_attention_mask,\n","            lm_labels=lm_labels,\n","        )\n","\n","    def _step(self, batch):\n","        lm_labels = batch[\"target_ids\"]\n","        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n","\n","        outputs = self(\n","            input_ids=batch[\"source_ids\"],\n","            attention_mask=batch[\"source_mask\"],\n","            lm_labels=lm_labels,\n","            decoder_attention_mask=batch['target_mask']\n","        )\n","\n","        loss = outputs[0]\n","\n","        return loss\n","\n","    def training_step(self, batch, batch_idx):\n","        loss = self._step(batch)\n","\n","        tensorboard_logs = {\"train_loss\": loss}\n","        return {\"loss\": loss, \"log\": tensorboard_logs}\n","\n","    def training_epoch_end(self, outputs):\n","        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n","        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n","        return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n","\n","    def validation_step(self, batch, batch_idx):\n","        loss = self._step(batch)\n","        return {\"val_loss\": loss}\n","\n","    def validation_epoch_end(self, outputs):\n","        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n","        tensorboard_logs = {\"val_loss\": avg_loss}\n","        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n","\n","    def configure_optimizers(self):\n","        \"Prepare optimizer and schedule (linear warmup and decay)\"\n","\n","        model = self.model\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.hparams.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n","        self.opt = optimizer\n","        return [optimizer]\n","\n","    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None, on_tpu=False, using_native_amp=False, using_lbfgs=False):\n","        if self.trainer.use_tpu:\n","            xm.optimizer_step(optimizer)\n","        else:\n","            optimizer.step()\n","        optimizer.zero_grad()\n","        self.lr_scheduler.step()\n","\n","    def get_tqdm_dict(self):\n","        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n","\n","        return tqdm_dict\n","\n","    def train_dataloader(self):\n","        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n","        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True,\n","                                num_workers=4)\n","        t_total = (\n","                (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n","                // self.hparams.gradient_accumulation_steps\n","                * float(self.hparams.num_train_epochs)\n","        )\n","        scheduler = get_linear_schedule_with_warmup(\n","            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n","        )\n","        self.lr_scheduler = scheduler\n","        return dataloader\n","\n","    def val_dataloader(self):\n","        val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"dev\", args=self.hparams)\n","        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)\n","\n","logger = logging.getLogger(__name__)\n","\n","class LoggingCallback(pl.Callback):\n","  def on_validation_end(self, trainer, pl_module):\n","    logger.info(\"***** Validation results *****\")\n","    if pl_module.is_logger():\n","      metrics = trainer.callback_metrics\n","      # Log results\n","      for key in sorted(metrics):\n","        if key not in [\"log\", \"progress_bar\"]:\n","          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n","\n","  def on_test_end(self, trainer, pl_module):\n","    logger.info(\"***** Test results *****\")\n","\n","    if pl_module.is_logger():\n","      metrics = trainer.callback_metrics\n","\n","      # Log and save results to file\n","      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n","      with open(output_test_results_file, \"w\") as writer:\n","        for key in sorted(metrics):\n","          if key not in [\"log\", \"progress_bar\"]:\n","            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n","            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2nsjLzviCG_A"},"source":["# **Load datasets**"]},{"cell_type":"code","metadata":{"id":"X-Dee6AzyIr0"},"source":["from sklearn.model_selection import train_test_split\n","#Use your dataset for the fine tuning in this section.\n","data_df = pd.read_pickle(\"/content/drive/MyDrive/T5_paraphrasing_reference_code/data_paraphrased_Z-filtered_75K.pkl\")\n","data_train, data_dev = train_test_split(data_df, test_size=0.02584, shuffle=False)\n","data_train.to_csv (\"/content/drive/MyDrive/T5_paraphrasing_reference_code/data_M2/train.tsv\", sep='\\t',index = False)\n","data_dev.to_csv('/content/drive/MyDrive/T5_paraphrasing_reference_code/data_M2/dev.tsv', sep='\\t',index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O38qYPeyOJbA","executionInfo":{"status":"ok","timestamp":1624015353475,"user_tz":-120,"elapsed":30,"user":{"displayName":"antoine magdi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01891798323931301668"}},"outputId":"9205ffb8-6d00-46d5-b2ea-61a905768c0f"},"source":["print('Training data: ', data_train.shape)\n","print('Validation data: ', data_dev.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training data:  (73062, 6)\n","Validation data:  (1938, 6)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t4eRy5oVCUny"},"source":["# **Set arguments**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":188,"referenced_widgets":["59adc2455ef94917b5017cc043025620","63fd02c642164005bfb9d4233dccd9f4","5ceb625ea43f49fc9e0a4264de6952cb","81b8393ed7e342b9a8864434e1780bbd","caf29b1ef48d4e249787c0f991d57ffc","682bf84af5d94355888384612e669e53","247166e7fc9e4cb8bf21c3fbeb27f626","3fd8f09ed4c64f8da850047774b1aa55"]},"id":"7UZzosIjEr8I","executionInfo":{"status":"ok","timestamp":1624015356765,"user_tz":-120,"elapsed":3314,"user":{"displayName":"antoine magdi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01891798323931301668"}},"outputId":"435a0a2f-5b83-440c-f7a7-37e820d68d8b"},"source":["args_dict = dict(\n","    data_dir=\"data_M2\", # path for data files\n","    output_dir=\"t5_paraphrase_M2\", # path to save the checkpoints\n","    model_name_or_path='t5-large',\n","    tokenizer_name_or_path='t5-large',\n","    max_seq_length=256,\n","    learning_rate=3e-4,\n","    weight_decay=0.0,\n","    adam_epsilon=1e-8,\n","    warmup_steps=0,\n","    train_batch_size=1,\n","    eval_batch_size=1,\n","    num_train_epochs=2,\n","    gradient_accumulation_steps=16,\n","    n_gpu=1,\n","    early_stop_callback=False,\n","    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n","    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n","    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n","    seed=42,\n",")\n","\n","train_path = \"data_M2/train.tsv\"\n","val_path = \"data_M2/dev.tsv\"\n","\n","# train = pd.read_csv(train_path, sep=\"\\t\").astype(str)\n","# print(train.head())\n","\n","tokenizer = T5Tokenizer.from_pretrained('t5-large')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:filelock:Lock 140085867655056 acquired on /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f.lock\n","INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpoo4jyfmt\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59adc2455ef94917b5017cc043025620","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model in cache at /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n","INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n","INFO:filelock:Lock 140085867655056 released on /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f.lock\n","INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model from cache at /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_p5OZ7pRCel5"},"source":["# **ParaphraseDataset()**"]},{"cell_type":"code","metadata":{"id":"E5UwYTUAGAo2"},"source":["class ParaphraseDataset(Dataset):\n","    def __init__(self, tokenizer, data_dir, type_path, max_len=512):\n","        self.path = os.path.join(data_dir, type_path + '.tsv')\n","\n","        self.source_column = \"paraphrase\"\n","        self.target_column = \"original\"\n","        self.data = pd.read_csv(self.path, sep=\"\\t\").astype(str)\n","\n","        self.max_len = max_len\n","        self.tokenizer = tokenizer\n","        self.inputs = []\n","        self.targets = []\n","\n","        self._build()\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, index):\n","        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n","        target_ids = self.targets[index][\"input_ids\"].squeeze()\n","\n","        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n","        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n","\n","        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n","\n","    def _build(self):\n","        for idx in range(len(self.data)):\n","            input_, target = self.data.loc[idx, self.source_column], self.data.loc[idx, self.target_column]\n","\n","            input_ = \"paraphrase: \"+ input_ + ' </s>'\n","            target = target + \" </s>\"\n","\n","            # tokenize inputs\n","            tokenized_inputs = self.tokenizer.batch_encode_plus(\n","                [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\", truncation='longest_first'\n","            )\n","            # tokenize targets\n","            tokenized_targets = self.tokenizer.batch_encode_plus(\n","                [target], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\", truncation='longest_first'\n","            )\n","\n","            self.inputs.append(tokenized_inputs)\n","            self.targets.append(tokenized_targets)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gkVUkHFMClrH"},"source":["# **Start training**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["9cf1e5c01968468680a3c416b66dc608","6d7e803c36c244b0a1bb01a60982f8d8","1beea8b125454a2eba29293709d24f74","3508e462ffa14fd3a47baba62592bb58","3c44611633c342ff8e0f23d9c1ce1b2a","1bb939a638da4dcb994fd5d9291b6396","bb4c1260b6624303b4f25cb8d68172fa","520720468be8427886094f411f94c381","7db2d22531274b2f84e824b8ea15e20f","092d2d7096814866a061ba9bbb773d25","8a486bb5a7304f53b6a67320a54c1afc","b222ef0042ac4565b547d144044df755","62987d6ac17541fb8aaf626dc4551f85","b6773dcf4e59462aa1ac948bafa780d5","bf4fff25efa345969b20edfb8f81a26a","29bd097becaf4eada00d2d372ba0b4c9","612031c950b84b6ea0eb1ec3f1869da1","ba352efa580843cca223324d5f40f3b7","04e44786cc4045a2963e24930647f3b7","080fafced0c54c6794abc5c0cb35b437","b80a87ea93504a459526a930395f9f1d","1db1692773c748089c381e99e1283428","2e17bdc9ea13437882366308a78f509d","aa1ccb026a76412aae9ad46207c4487b","bf1c9738c00c4d359c08b0c80146bcc3","2c354eea4c1d452fb407693dc8abe017","01b04479c6f94363a0b774c13744ef28","1e69479bf4834d4ca805352b7028ef61","d1dabae081064bfaa63a301a90fd967b","1bbc7cfd59e14c6ab30b262fb1fd8de7","65967ec9ea5341cdafb5cbc986342526","a709c507b47242e59eca625c0dd746f1","b3f7aedaa7f342aba684aa2df1ff3625","58cd98deeeab41bfbc57d00a47a36a69","9f665d732293435b989680cf17003e59","7f394a657b074286971bfe100b9e821e","abe2af20249f4a7195b9de77d72469d1","d530acedd7f04ee4a3105d3221abc73a","2fb01918543b4d76a73ea4a942a05b80","6971d3680a8a433ea604db1216d7fbd4","58136f6e39604c9eba249e90e1f94d58"]},"id":"n41vU8IbGhIM","executionInfo":{"status":"ok","timestamp":1623974192538,"user_tz":-120,"elapsed":1011637,"user":{"displayName":"antoine magdi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01891798323931301668"}},"outputId":"f1ae03c4-0fd8-4526-8c7f-c05b291fd004"},"source":["dataset = ParaphraseDataset(tokenizer, 'data_M2', 'dev', 256)\n","print(\"Val dataset: \",len(dataset))\n","\n","data = dataset[61]\n","print(tokenizer.decode(data['source_ids']))\n","print(tokenizer.decode(data['target_ids']))\n","\n","if not os.path.exists('t5_paraphrase_M2'):\n","    os.makedirs('t5_paraphrase_M2')\n","\n","args_dict.update({'data_dir': 'data_M2', 'output_dir': 't5_paraphrase_M2', 'num_train_epochs':2,'max_seq_length':256})\n","args = argparse.Namespace(**args_dict)\n","print(args_dict)\n","\n","checkpoint_callback = pl.callbacks.ModelCheckpoint(\n","    filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n",")\n","\n","train_params = dict(\n","    accumulate_grad_batches=args.gradient_accumulation_steps,\n","    gpus=args.n_gpu,\n","    max_epochs=args.num_train_epochs,\n"," #   early_stop_callback=False,\n","    precision= 16 if args.fp_16 else 32,\n","    amp_level=args.opt_level,\n","    gradient_clip_val=args.max_grad_norm,\n","    checkpoint_callback=checkpoint_callback,\n","    callbacks=[LoggingCallback()],\n",")\n","\n","def get_dataset(tokenizer, type_path, args):\n","  return ParaphraseDataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length)\n","\n","print (\"Initialize model\")\n","model = T5FineTuner(args)\n","\n","trainer = pl.Trainer(**train_params)\n","\n","print (\" Training model\")\n","trainer.fit(model)\n","\n","print (\"training finished\")\n","\n","print (\"Saving model\")\n","model.model.save_pretrained('t5_paraphrase_M2')\n","\n","print (\"Model saved\")\n","\n","!cp \"/content/t5_paraphrase_M2/\" -a \"/content/drive/My Drive/\"\n","!cp \"/content/lightning_logs/\" -a \"/content/drive/My Drive/\"\n","print (\"Copied the final folder to Google Drive\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: Checkpoint directory t5_paraphrase_M2 exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n","  warnings.warn(*args, **kwargs)\n","INFO:filelock:Lock 140684174908944 acquired on /root/.cache/torch/transformers/0e9978f992c9b90cd05d080648b1b1c8aabc3f931f62781fa8fcbc281eba168d.ba29edd8b0c069c672abbe0f807c1cd7cac52350f14d193ba0a0ef5cdb9a255e.lock\n","INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpbjy1ygxh\n"],"name":"stderr"},{"output_type":"stream","text":["Val dataset:  1938\n","paraphrase: when the job is finished, the solver will generate an assignment once.\n","when replanning occurs at the task completion level, an assignment is generated once by the solver.\n","{'data_dir': 'data_M2', 'output_dir': 't5_paraphrase_M2', 'model_name_or_path': 't5-large', 'tokenizer_name_or_path': 't5-large', 'max_seq_length': 256, 'learning_rate': 0.0003, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'train_batch_size': 1, 'eval_batch_size': 1, 'num_train_epochs': 2, 'gradient_accumulation_steps': 16, 'n_gpu': 1, 'early_stop_callback': False, 'fp_16': False, 'opt_level': 'O1', 'max_grad_norm': 1.0, 'seed': 42}\n","Initialize model\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9cf1e5c01968468680a3c416b66dc608","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1200.0, style=ProgressStyle(description…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-config.json in cache at /root/.cache/torch/transformers/0e9978f992c9b90cd05d080648b1b1c8aabc3f931f62781fa8fcbc281eba168d.ba29edd8b0c069c672abbe0f807c1cd7cac52350f14d193ba0a0ef5cdb9a255e\n","INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/0e9978f992c9b90cd05d080648b1b1c8aabc3f931f62781fa8fcbc281eba168d.ba29edd8b0c069c672abbe0f807c1cd7cac52350f14d193ba0a0ef5cdb9a255e\n","INFO:filelock:Lock 140684174908944 released on /root/.cache/torch/transformers/0e9978f992c9b90cd05d080648b1b1c8aabc3f931f62781fa8fcbc281eba168d.ba29edd8b0c069c672abbe0f807c1cd7cac52350f14d193ba0a0ef5cdb9a255e.lock\n","INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-config.json from cache at /root/.cache/torch/transformers/0e9978f992c9b90cd05d080648b1b1c8aabc3f931f62781fa8fcbc281eba168d.ba29edd8b0c069c672abbe0f807c1cd7cac52350f14d193ba0a0ef5cdb9a255e\n","INFO:transformers.configuration_utils:Model config T5Config {\n","  \"architectures\": [\n","    \"T5WithLMHeadModel\"\n","  ],\n","  \"d_ff\": 4096,\n","  \"d_kv\": 64,\n","  \"d_model\": 1024,\n","  \"decoder_start_token_id\": 0,\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_heads\": 16,\n","  \"num_layers\": 24,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 200,\n","      \"min_length\": 30,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4,\n","      \"prefix\": \"summarize: \"\n","    },\n","    \"translation_en_to_de\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to German: \"\n","    },\n","    \"translation_en_to_fr\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to French: \"\n","    },\n","    \"translation_en_to_ro\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to Romanian: \"\n","    }\n","  },\n","  \"vocab_size\": 32128\n","}\n","\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:filelock:Lock 140684174960912 acquired on /root/.cache/torch/transformers/e47fdf946478fcd76239a89ab1db1545af6261da0f9be758eb538a22de9553fc.f7406fdda08cdd666e1b81685deafd24a40ba2d5579384751f9f7023254ffb5b.lock\n","INFO:transformers.file_utils:https://cdn.huggingface.co/t5-large-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp8m1_9w7n\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7db2d22531274b2f84e824b8ea15e20f","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2950825948.0, style=ProgressStyle(descr…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:transformers.file_utils:storing https://cdn.huggingface.co/t5-large-pytorch_model.bin in cache at /root/.cache/torch/transformers/e47fdf946478fcd76239a89ab1db1545af6261da0f9be758eb538a22de9553fc.f7406fdda08cdd666e1b81685deafd24a40ba2d5579384751f9f7023254ffb5b\n","INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/e47fdf946478fcd76239a89ab1db1545af6261da0f9be758eb538a22de9553fc.f7406fdda08cdd666e1b81685deafd24a40ba2d5579384751f9f7023254ffb5b\n","INFO:filelock:Lock 140684174960912 released on /root/.cache/torch/transformers/e47fdf946478fcd76239a89ab1db1545af6261da0f9be758eb538a22de9553fc.f7406fdda08cdd666e1b81685deafd24a40ba2d5579384751f9f7023254ffb5b.lock\n","INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/t5-large-pytorch_model.bin from cache at /root/.cache/torch/transformers/e47fdf946478fcd76239a89ab1db1545af6261da0f9be758eb538a22de9553fc.f7406fdda08cdd666e1b81685deafd24a40ba2d5579384751f9f7023254ffb5b\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.modeling_utils:Weights of T5ForConditionalGeneration not initialized from pretrained model: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n","INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model from cache at /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n","INFO:lightning:GPU available: True, used: True\n","INFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n"],"name":"stderr"},{"output_type":"stream","text":[" Training model\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:lightning:\n","     | Name                                                                  | Type                       | Params\n","-----------------------------------------------------------------------------------------------------------------\n","0    | model                                                                 | T5ForConditionalGeneration | 737 M \n","1    | model.shared                                                          | Embedding                  | 32 M  \n","2    | model.encoder                                                         | T5Stack                    | 334 M \n","3    | model.encoder.block                                                   | ModuleList                 | 302 M \n","4    | model.encoder.block.0                                                 | T5Block                    | 12 M  \n","5    | model.encoder.block.0.layer                                           | ModuleList                 | 12 M  \n","6    | model.encoder.block.0.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","7    | model.encoder.block.0.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","8    | model.encoder.block.0.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","9    | model.encoder.block.0.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","10   | model.encoder.block.0.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","11   | model.encoder.block.0.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","12   | model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias   | Embedding                  | 512   \n","13   | model.encoder.block.0.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","14   | model.encoder.block.0.layer.0.dropout                                 | Dropout                    | 0     \n","15   | model.encoder.block.0.layer.1                                         | T5LayerFF                  | 8 M   \n","16   | model.encoder.block.0.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","17   | model.encoder.block.0.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n","18   | model.encoder.block.0.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n","19   | model.encoder.block.0.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n","20   | model.encoder.block.0.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","21   | model.encoder.block.0.layer.1.dropout                                 | Dropout                    | 0     \n","22   | model.encoder.block.1                                                 | T5Block                    | 12 M  \n","23   | model.encoder.block.1.layer                                           | ModuleList                 | 12 M  \n","24   | model.encoder.block.1.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","25   | model.encoder.block.1.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","26   | model.encoder.block.1.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","27   | model.encoder.block.1.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","28   | model.encoder.block.1.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","29   | model.encoder.block.1.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","30   | model.encoder.block.1.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","31   | model.encoder.block.1.layer.0.dropout                                 | Dropout                    | 0     \n","32   | model.encoder.block.1.layer.1                                         | T5LayerFF                  | 8 M   \n","33   | model.encoder.block.1.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","34   | model.encoder.block.1.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n","35   | model.encoder.block.1.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n","36   | model.encoder.block.1.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n","37   | model.encoder.block.1.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","38   | model.encoder.block.1.layer.1.dropout                                 | Dropout                    | 0     \n","39   | model.encoder.block.2                                                 | T5Block                    | 12 M  \n","40   | model.encoder.block.2.layer                                           | ModuleList                 | 12 M  \n","41   | model.encoder.block.2.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","42   | model.encoder.block.2.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","43   | model.encoder.block.2.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","44   | model.encoder.block.2.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","45   | model.encoder.block.2.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","46   | model.encoder.block.2.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","47   | model.encoder.block.2.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","48   | model.encoder.block.2.layer.0.dropout                                 | Dropout                    | 0     \n","49   | model.encoder.block.2.layer.1                                         | T5LayerFF                  | 8 M   \n","50   | model.encoder.block.2.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","51   | model.encoder.block.2.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n","52   | model.encoder.block.2.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n","53   | model.encoder.block.2.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n","54   | model.encoder.block.2.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","55   | model.encoder.block.2.layer.1.dropout                                 | Dropout                    | 0     \n","56   | model.encoder.block.3                                                 | T5Block                    | 12 M  \n","57   | model.encoder.block.3.layer                                           | ModuleList                 | 12 M  \n","58   | model.encoder.block.3.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","59   | model.encoder.block.3.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","60   | model.encoder.block.3.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","61   | model.encoder.block.3.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","62   | model.encoder.block.3.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","63   | model.encoder.block.3.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","64   | model.encoder.block.3.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","65   | model.encoder.block.3.layer.0.dropout                                 | Dropout                    | 0     \n","66   | model.encoder.block.3.layer.1                                         | T5LayerFF                  | 8 M   \n","67   | model.encoder.block.3.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","68   | model.encoder.block.3.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n","69   | model.encoder.block.3.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n","70   | model.encoder.block.3.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n","71   | model.encoder.block.3.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","72   | model.encoder.block.3.layer.1.dropout                                 | Dropout                    | 0     \n","73   | model.encoder.block.4                                                 | T5Block                    | 12 M  \n","74   | model.encoder.block.4.layer                                           | ModuleList                 | 12 M  \n","75   | model.encoder.block.4.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","76   | model.encoder.block.4.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","77   | model.encoder.block.4.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","78   | model.encoder.block.4.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","79   | model.encoder.block.4.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","80   | model.encoder.block.4.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","81   | model.encoder.block.4.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","82   | model.encoder.block.4.layer.0.dropout                                 | Dropout                    | 0     \n","83   | model.encoder.block.4.layer.1                                         | T5LayerFF                  | 8 M   \n","84   | model.encoder.block.4.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","85   | model.encoder.block.4.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n","86   | model.encoder.block.4.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n","87   | model.encoder.block.4.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n","88   | model.encoder.block.4.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","89   | model.encoder.block.4.layer.1.dropout                                 | Dropout                    | 0     \n","90   | model.encoder.block.5                                                 | T5Block                    | 12 M  \n","91   | model.encoder.block.5.layer                                           | ModuleList                 | 12 M  \n","92   | model.encoder.block.5.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","93   | model.encoder.block.5.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","94   | model.encoder.block.5.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","95   | model.encoder.block.5.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","96   | model.encoder.block.5.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","97   | model.encoder.block.5.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","98   | model.encoder.block.5.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","99   | model.encoder.block.5.layer.0.dropout                                 | Dropout                    | 0     \n","100  | model.encoder.block.5.layer.1                                         | T5LayerFF                  | 8 M   \n","101  | model.encoder.block.5.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","102  | model.encoder.block.5.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n","103  | model.encoder.block.5.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n","104  | model.encoder.block.5.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n","105  | model.encoder.block.5.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","106  | model.encoder.block.5.layer.1.dropout                                 | Dropout                    | 0     \n","107  | model.encoder.block.6                                                 | T5Block                    | 12 M  \n","108  | model.encoder.block.6.layer                                           | ModuleList                 | 12 M  \n","109  | model.encoder.block.6.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","110  | model.encoder.block.6.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","111  | model.encoder.block.6.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","112  | model.encoder.block.6.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","113  | model.encoder.block.6.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","114  | model.encoder.block.6.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","115  | model.encoder.block.6.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","116  | model.encoder.block.6.layer.0.dropout                                 | Dropout                    | 0     \n","117  | model.encoder.block.6.layer.1                                         | T5LayerFF                  | 8 M   \n","118  | model.encoder.block.6.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","119  | model.encoder.block.6.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n","120  | model.encoder.block.6.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n","121  | model.encoder.block.6.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n","122  | model.encoder.block.6.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","123  | model.encoder.block.6.layer.1.dropout                                 | Dropout                    | 0     \n","124  | model.encoder.block.7                                                 | T5Block                    | 12 M  \n","125  | model.encoder.block.7.layer                                           | ModuleList                 | 12 M  \n","126  | model.encoder.block.7.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","127  | model.encoder.block.7.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","128  | model.encoder.block.7.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","129  | model.encoder.block.7.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","130  | model.encoder.block.7.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","131  | model.encoder.block.7.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","132  | model.encoder.block.7.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","133  | model.encoder.block.7.layer.0.dropout                                 | Dropout                    | 0     \n","134  | model.encoder.block.7.layer.1                                         | T5LayerFF                  | 8 M   \n","135  | model.encoder.block.7.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","136  | model.encoder.block.7.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n","137  | model.encoder.block.7.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n","138  | model.encoder.block.7.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n","139  | model.encoder.block.7.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","140  | model.encoder.block.7.layer.1.dropout                                 | Dropout                    | 0     \n","141  | model.encoder.block.8                                                 | T5Block                    | 12 M  \n","142  | model.encoder.block.8.layer                                           | ModuleList                 | 12 M  \n","143  | model.encoder.block.8.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","144  | model.encoder.block.8.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","145  | model.encoder.block.8.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","146  | model.encoder.block.8.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","147  | model.encoder.block.8.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","148  | model.encoder.block.8.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","149  | model.encoder.block.8.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","150  | model.encoder.block.8.layer.0.dropout                                 | Dropout                    | 0     \n","151  | model.encoder.block.8.layer.1                                         | T5LayerFF                  | 8 M   \n","152  | model.encoder.block.8.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","153  | model.encoder.block.8.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n","154  | model.encoder.block.8.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n","155  | model.encoder.block.8.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n","156  | model.encoder.block.8.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","157  | model.encoder.block.8.layer.1.dropout                                 | Dropout                    | 0     \n","158  | model.encoder.block.9                                                 | T5Block                    | 12 M  \n","159  | model.encoder.block.9.layer                                           | ModuleList                 | 12 M  \n","160  | model.encoder.block.9.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","161  | model.encoder.block.9.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","162  | model.encoder.block.9.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","163  | model.encoder.block.9.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","164  | model.encoder.block.9.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","165  | model.encoder.block.9.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","166  | model.encoder.block.9.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","167  | model.encoder.block.9.layer.0.dropout                                 | Dropout                    | 0     \n","168  | model.encoder.block.9.layer.1                                         | T5LayerFF                  | 8 M   \n","169  | model.encoder.block.9.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","170  | model.encoder.block.9.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n","171  | model.encoder.block.9.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n","172  | model.encoder.block.9.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n","173  | model.encoder.block.9.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","174  | model.encoder.block.9.layer.1.dropout                                 | Dropout                    | 0     \n","175  | model.encoder.block.10                                                | T5Block                    | 12 M  \n","176  | model.encoder.block.10.layer                                          | ModuleList                 | 12 M  \n","177  | model.encoder.block.10.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","178  | model.encoder.block.10.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","179  | model.encoder.block.10.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","180  | model.encoder.block.10.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","181  | model.encoder.block.10.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","182  | model.encoder.block.10.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","183  | model.encoder.block.10.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","184  | model.encoder.block.10.layer.0.dropout                                | Dropout                    | 0     \n","185  | model.encoder.block.10.layer.1                                        | T5LayerFF                  | 8 M   \n","186  | model.encoder.block.10.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","187  | model.encoder.block.10.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n","188  | model.encoder.block.10.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n","189  | model.encoder.block.10.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n","190  | model.encoder.block.10.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","191  | model.encoder.block.10.layer.1.dropout                                | Dropout                    | 0     \n","192  | model.encoder.block.11                                                | T5Block                    | 12 M  \n","193  | model.encoder.block.11.layer                                          | ModuleList                 | 12 M  \n","194  | model.encoder.block.11.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","195  | model.encoder.block.11.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","196  | model.encoder.block.11.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","197  | model.encoder.block.11.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","198  | model.encoder.block.11.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","199  | model.encoder.block.11.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","200  | model.encoder.block.11.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","201  | model.encoder.block.11.layer.0.dropout                                | Dropout                    | 0     \n","202  | model.encoder.block.11.layer.1                                        | T5LayerFF                  | 8 M   \n","203  | model.encoder.block.11.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","204  | model.encoder.block.11.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n","205  | model.encoder.block.11.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n","206  | model.encoder.block.11.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n","207  | model.encoder.block.11.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","208  | model.encoder.block.11.layer.1.dropout                                | Dropout                    | 0     \n","209  | model.encoder.block.12                                                | T5Block                    | 12 M  \n","210  | model.encoder.block.12.layer                                          | ModuleList                 | 12 M  \n","211  | model.encoder.block.12.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","212  | model.encoder.block.12.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","213  | model.encoder.block.12.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","214  | model.encoder.block.12.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","215  | model.encoder.block.12.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","216  | model.encoder.block.12.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","217  | model.encoder.block.12.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","218  | model.encoder.block.12.layer.0.dropout                                | Dropout                    | 0     \n","219  | model.encoder.block.12.layer.1                                        | T5LayerFF                  | 8 M   \n","220  | model.encoder.block.12.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","221  | model.encoder.block.12.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n","222  | model.encoder.block.12.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n","223  | model.encoder.block.12.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n","224  | model.encoder.block.12.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","225  | model.encoder.block.12.layer.1.dropout                                | Dropout                    | 0     \n","226  | model.encoder.block.13                                                | T5Block                    | 12 M  \n","227  | model.encoder.block.13.layer                                          | ModuleList                 | 12 M  \n","228  | model.encoder.block.13.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","229  | model.encoder.block.13.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","230  | model.encoder.block.13.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","231  | model.encoder.block.13.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","232  | model.encoder.block.13.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","233  | model.encoder.block.13.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","234  | model.encoder.block.13.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","235  | model.encoder.block.13.layer.0.dropout                                | Dropout                    | 0     \n","236  | model.encoder.block.13.layer.1                                        | T5LayerFF                  | 8 M   \n","237  | model.encoder.block.13.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","238  | model.encoder.block.13.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n","239  | model.encoder.block.13.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n","240  | model.encoder.block.13.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n","241  | model.encoder.block.13.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","242  | model.encoder.block.13.layer.1.dropout                                | Dropout                    | 0     \n","243  | model.encoder.block.14                                                | T5Block                    | 12 M  \n","244  | model.encoder.block.14.layer                                          | ModuleList                 | 12 M  \n","245  | model.encoder.block.14.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","246  | model.encoder.block.14.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","247  | model.encoder.block.14.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","248  | model.encoder.block.14.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","249  | model.encoder.block.14.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","250  | model.encoder.block.14.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","251  | model.encoder.block.14.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","252  | model.encoder.block.14.layer.0.dropout                                | Dropout                    | 0     \n","253  | model.encoder.block.14.layer.1                                        | T5LayerFF                  | 8 M   \n","254  | model.encoder.block.14.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","255  | model.encoder.block.14.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n","256  | model.encoder.block.14.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n","257  | model.encoder.block.14.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n","258  | model.encoder.block.14.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","259  | model.encoder.block.14.layer.1.dropout                                | Dropout                    | 0     \n","260  | model.encoder.block.15                                                | T5Block                    | 12 M  \n","261  | model.encoder.block.15.layer                                          | ModuleList                 | 12 M  \n","262  | model.encoder.block.15.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","263  | model.encoder.block.15.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","264  | model.encoder.block.15.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","265  | model.encoder.block.15.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","266  | model.encoder.block.15.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","267  | model.encoder.block.15.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","268  | model.encoder.block.15.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","269  | model.encoder.block.15.layer.0.dropout                                | Dropout                    | 0     \n","270  | model.encoder.block.15.layer.1                                        | T5LayerFF                  | 8 M   \n","271  | model.encoder.block.15.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","272  | model.encoder.block.15.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n","273  | model.encoder.block.15.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n","274  | model.encoder.block.15.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n","275  | model.encoder.block.15.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","276  | model.encoder.block.15.layer.1.dropout                                | Dropout                    | 0     \n","277  | model.encoder.block.16                                                | T5Block                    | 12 M  \n","278  | model.encoder.block.16.layer                                          | ModuleList                 | 12 M  \n","279  | model.encoder.block.16.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","280  | model.encoder.block.16.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","281  | model.encoder.block.16.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","282  | model.encoder.block.16.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","283  | model.encoder.block.16.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","284  | model.encoder.block.16.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","285  | model.encoder.block.16.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","286  | model.encoder.block.16.layer.0.dropout                                | Dropout                    | 0     \n","287  | model.encoder.block.16.layer.1                                        | T5LayerFF                  | 8 M   \n","288  | model.encoder.block.16.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","289  | model.encoder.block.16.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n","290  | model.encoder.block.16.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n","291  | model.encoder.block.16.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n","292  | model.encoder.block.16.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","293  | model.encoder.block.16.layer.1.dropout                                | Dropout                    | 0     \n","294  | model.encoder.block.17                                                | T5Block                    | 12 M  \n","295  | model.encoder.block.17.layer                                          | ModuleList                 | 12 M  \n","296  | model.encoder.block.17.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","297  | model.encoder.block.17.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","298  | model.encoder.block.17.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","299  | model.encoder.block.17.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","300  | model.encoder.block.17.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","301  | model.encoder.block.17.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","302  | model.encoder.block.17.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","303  | model.encoder.block.17.layer.0.dropout                                | Dropout                    | 0     \n","304  | model.encoder.block.17.layer.1                                        | T5LayerFF                  | 8 M   \n","305  | model.encoder.block.17.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","306  | model.encoder.block.17.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n","307  | model.encoder.block.17.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n","308  | model.encoder.block.17.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n","309  | model.encoder.block.17.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","310  | model.encoder.block.17.layer.1.dropout                                | Dropout                    | 0     \n","311  | model.encoder.block.18                                                | T5Block                    | 12 M  \n","312  | model.encoder.block.18.layer                                          | ModuleList                 | 12 M  \n","313  | model.encoder.block.18.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","314  | model.encoder.block.18.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","315  | model.encoder.block.18.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","316  | model.encoder.block.18.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","317  | model.encoder.block.18.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","318  | model.encoder.block.18.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","319  | model.encoder.block.18.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","320  | model.encoder.block.18.layer.0.dropout                                | Dropout                    | 0     \n","321  | model.encoder.block.18.layer.1                                        | T5LayerFF                  | 8 M   \n","322  | model.encoder.block.18.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","323  | model.encoder.block.18.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n","324  | model.encoder.block.18.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n","325  | model.encoder.block.18.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n","326  | model.encoder.block.18.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","327  | model.encoder.block.18.layer.1.dropout                                | Dropout                    | 0     \n","328  | model.encoder.block.19                                                | T5Block                    | 12 M  \n","329  | model.encoder.block.19.layer                                          | ModuleList                 | 12 M  \n","330  | model.encoder.block.19.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","331  | model.encoder.block.19.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","332  | model.encoder.block.19.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","333  | model.encoder.block.19.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","334  | model.encoder.block.19.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","335  | model.encoder.block.19.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","336  | model.encoder.block.19.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","337  | model.encoder.block.19.layer.0.dropout                                | Dropout                    | 0     \n","338  | model.encoder.block.19.layer.1                                        | T5LayerFF                  | 8 M   \n","339  | model.encoder.block.19.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","340  | model.encoder.block.19.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n","341  | model.encoder.block.19.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n","342  | model.encoder.block.19.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n","343  | model.encoder.block.19.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","344  | model.encoder.block.19.layer.1.dropout                                | Dropout                    | 0     \n","345  | model.encoder.block.20                                                | T5Block                    | 12 M  \n","346  | model.encoder.block.20.layer                                          | ModuleList                 | 12 M  \n","347  | model.encoder.block.20.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","348  | model.encoder.block.20.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","349  | model.encoder.block.20.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","350  | model.encoder.block.20.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","351  | model.encoder.block.20.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","352  | model.encoder.block.20.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","353  | model.encoder.block.20.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","354  | model.encoder.block.20.layer.0.dropout                                | Dropout                    | 0     \n","355  | model.encoder.block.20.layer.1                                        | T5LayerFF                  | 8 M   \n","356  | model.encoder.block.20.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","357  | model.encoder.block.20.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n","358  | model.encoder.block.20.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n","359  | model.encoder.block.20.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n","360  | model.encoder.block.20.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","361  | model.encoder.block.20.layer.1.dropout                                | Dropout                    | 0     \n","362  | model.encoder.block.21                                                | T5Block                    | 12 M  \n","363  | model.encoder.block.21.layer                                          | ModuleList                 | 12 M  \n","364  | model.encoder.block.21.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","365  | model.encoder.block.21.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","366  | model.encoder.block.21.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","367  | model.encoder.block.21.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","368  | model.encoder.block.21.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","369  | model.encoder.block.21.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","370  | model.encoder.block.21.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","371  | model.encoder.block.21.layer.0.dropout                                | Dropout                    | 0     \n","372  | model.encoder.block.21.layer.1                                        | T5LayerFF                  | 8 M   \n","373  | model.encoder.block.21.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","374  | model.encoder.block.21.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n","375  | model.encoder.block.21.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n","376  | model.encoder.block.21.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n","377  | model.encoder.block.21.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","378  | model.encoder.block.21.layer.1.dropout                                | Dropout                    | 0     \n","379  | model.encoder.block.22                                                | T5Block                    | 12 M  \n","380  | model.encoder.block.22.layer                                          | ModuleList                 | 12 M  \n","381  | model.encoder.block.22.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","382  | model.encoder.block.22.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","383  | model.encoder.block.22.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","384  | model.encoder.block.22.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","385  | model.encoder.block.22.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","386  | model.encoder.block.22.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","387  | model.encoder.block.22.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","388  | model.encoder.block.22.layer.0.dropout                                | Dropout                    | 0     \n","389  | model.encoder.block.22.layer.1                                        | T5LayerFF                  | 8 M   \n","390  | model.encoder.block.22.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","391  | model.encoder.block.22.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n","392  | model.encoder.block.22.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n","393  | model.encoder.block.22.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n","394  | model.encoder.block.22.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","395  | model.encoder.block.22.layer.1.dropout                                | Dropout                    | 0     \n","396  | model.encoder.block.23                                                | T5Block                    | 12 M  \n","397  | model.encoder.block.23.layer                                          | ModuleList                 | 12 M  \n","398  | model.encoder.block.23.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","399  | model.encoder.block.23.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","400  | model.encoder.block.23.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","401  | model.encoder.block.23.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","402  | model.encoder.block.23.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","403  | model.encoder.block.23.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","404  | model.encoder.block.23.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","405  | model.encoder.block.23.layer.0.dropout                                | Dropout                    | 0     \n","406  | model.encoder.block.23.layer.1                                        | T5LayerFF                  | 8 M   \n","407  | model.encoder.block.23.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","408  | model.encoder.block.23.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n","409  | model.encoder.block.23.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n","410  | model.encoder.block.23.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n","411  | model.encoder.block.23.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","412  | model.encoder.block.23.layer.1.dropout                                | Dropout                    | 0     \n","413  | model.encoder.final_layer_norm                                        | T5LayerNorm                | 1 K   \n","414  | model.encoder.dropout                                                 | Dropout                    | 0     \n","415  | model.decoder                                                         | T5Stack                    | 435 M \n","416  | model.decoder.block                                                   | ModuleList                 | 402 M \n","417  | model.decoder.block.0                                                 | T5Block                    | 16 M  \n","418  | model.decoder.block.0.layer                                           | ModuleList                 | 16 M  \n","419  | model.decoder.block.0.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","420  | model.decoder.block.0.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","421  | model.decoder.block.0.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","422  | model.decoder.block.0.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","423  | model.decoder.block.0.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","424  | model.decoder.block.0.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","425  | model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias   | Embedding                  | 512   \n","426  | model.decoder.block.0.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","427  | model.decoder.block.0.layer.0.dropout                                 | Dropout                    | 0     \n","428  | model.decoder.block.0.layer.1                                         | T5LayerCrossAttention      | 4 M   \n","429  | model.decoder.block.0.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n","430  | model.decoder.block.0.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n","431  | model.decoder.block.0.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n","432  | model.decoder.block.0.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n","433  | model.decoder.block.0.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n","434  | model.decoder.block.0.layer.1.EncDecAttention.relative_attention_bias | Embedding                  | 512   \n","435  | model.decoder.block.0.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","436  | model.decoder.block.0.layer.1.dropout                                 | Dropout                    | 0     \n","437  | model.decoder.block.0.layer.2                                         | T5LayerFF                  | 8 M   \n","438  | model.decoder.block.0.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","439  | model.decoder.block.0.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n","440  | model.decoder.block.0.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n","441  | model.decoder.block.0.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n","442  | model.decoder.block.0.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n","443  | model.decoder.block.0.layer.2.dropout                                 | Dropout                    | 0     \n","444  | model.decoder.block.1                                                 | T5Block                    | 16 M  \n","445  | model.decoder.block.1.layer                                           | ModuleList                 | 16 M  \n","446  | model.decoder.block.1.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","447  | model.decoder.block.1.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","448  | model.decoder.block.1.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","449  | model.decoder.block.1.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","450  | model.decoder.block.1.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","451  | model.decoder.block.1.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","452  | model.decoder.block.1.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","453  | model.decoder.block.1.layer.0.dropout                                 | Dropout                    | 0     \n","454  | model.decoder.block.1.layer.1                                         | T5LayerCrossAttention      | 4 M   \n","455  | model.decoder.block.1.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n","456  | model.decoder.block.1.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n","457  | model.decoder.block.1.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n","458  | model.decoder.block.1.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n","459  | model.decoder.block.1.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n","460  | model.decoder.block.1.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","461  | model.decoder.block.1.layer.1.dropout                                 | Dropout                    | 0     \n","462  | model.decoder.block.1.layer.2                                         | T5LayerFF                  | 8 M   \n","463  | model.decoder.block.1.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","464  | model.decoder.block.1.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n","465  | model.decoder.block.1.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n","466  | model.decoder.block.1.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n","467  | model.decoder.block.1.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n","468  | model.decoder.block.1.layer.2.dropout                                 | Dropout                    | 0     \n","469  | model.decoder.block.2                                                 | T5Block                    | 16 M  \n","470  | model.decoder.block.2.layer                                           | ModuleList                 | 16 M  \n","471  | model.decoder.block.2.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","472  | model.decoder.block.2.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","473  | model.decoder.block.2.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","474  | model.decoder.block.2.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","475  | model.decoder.block.2.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","476  | model.decoder.block.2.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","477  | model.decoder.block.2.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","478  | model.decoder.block.2.layer.0.dropout                                 | Dropout                    | 0     \n","479  | model.decoder.block.2.layer.1                                         | T5LayerCrossAttention      | 4 M   \n","480  | model.decoder.block.2.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n","481  | model.decoder.block.2.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n","482  | model.decoder.block.2.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n","483  | model.decoder.block.2.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n","484  | model.decoder.block.2.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n","485  | model.decoder.block.2.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","486  | model.decoder.block.2.layer.1.dropout                                 | Dropout                    | 0     \n","487  | model.decoder.block.2.layer.2                                         | T5LayerFF                  | 8 M   \n","488  | model.decoder.block.2.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","489  | model.decoder.block.2.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n","490  | model.decoder.block.2.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n","491  | model.decoder.block.2.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n","492  | model.decoder.block.2.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n","493  | model.decoder.block.2.layer.2.dropout                                 | Dropout                    | 0     \n","494  | model.decoder.block.3                                                 | T5Block                    | 16 M  \n","495  | model.decoder.block.3.layer                                           | ModuleList                 | 16 M  \n","496  | model.decoder.block.3.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","497  | model.decoder.block.3.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","498  | model.decoder.block.3.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","499  | model.decoder.block.3.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","500  | model.decoder.block.3.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","501  | model.decoder.block.3.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","502  | model.decoder.block.3.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","503  | model.decoder.block.3.layer.0.dropout                                 | Dropout                    | 0     \n","504  | model.decoder.block.3.layer.1                                         | T5LayerCrossAttention      | 4 M   \n","505  | model.decoder.block.3.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n","506  | model.decoder.block.3.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n","507  | model.decoder.block.3.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n","508  | model.decoder.block.3.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n","509  | model.decoder.block.3.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n","510  | model.decoder.block.3.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","511  | model.decoder.block.3.layer.1.dropout                                 | Dropout                    | 0     \n","512  | model.decoder.block.3.layer.2                                         | T5LayerFF                  | 8 M   \n","513  | model.decoder.block.3.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","514  | model.decoder.block.3.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n","515  | model.decoder.block.3.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n","516  | model.decoder.block.3.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n","517  | model.decoder.block.3.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n","518  | model.decoder.block.3.layer.2.dropout                                 | Dropout                    | 0     \n","519  | model.decoder.block.4                                                 | T5Block                    | 16 M  \n","520  | model.decoder.block.4.layer                                           | ModuleList                 | 16 M  \n","521  | model.decoder.block.4.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","522  | model.decoder.block.4.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","523  | model.decoder.block.4.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","524  | model.decoder.block.4.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","525  | model.decoder.block.4.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","526  | model.decoder.block.4.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","527  | model.decoder.block.4.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","528  | model.decoder.block.4.layer.0.dropout                                 | Dropout                    | 0     \n","529  | model.decoder.block.4.layer.1                                         | T5LayerCrossAttention      | 4 M   \n","530  | model.decoder.block.4.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n","531  | model.decoder.block.4.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n","532  | model.decoder.block.4.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n","533  | model.decoder.block.4.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n","534  | model.decoder.block.4.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n","535  | model.decoder.block.4.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","536  | model.decoder.block.4.layer.1.dropout                                 | Dropout                    | 0     \n","537  | model.decoder.block.4.layer.2                                         | T5LayerFF                  | 8 M   \n","538  | model.decoder.block.4.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","539  | model.decoder.block.4.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n","540  | model.decoder.block.4.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n","541  | model.decoder.block.4.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n","542  | model.decoder.block.4.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n","543  | model.decoder.block.4.layer.2.dropout                                 | Dropout                    | 0     \n","544  | model.decoder.block.5                                                 | T5Block                    | 16 M  \n","545  | model.decoder.block.5.layer                                           | ModuleList                 | 16 M  \n","546  | model.decoder.block.5.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","547  | model.decoder.block.5.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","548  | model.decoder.block.5.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","549  | model.decoder.block.5.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","550  | model.decoder.block.5.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","551  | model.decoder.block.5.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","552  | model.decoder.block.5.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","553  | model.decoder.block.5.layer.0.dropout                                 | Dropout                    | 0     \n","554  | model.decoder.block.5.layer.1                                         | T5LayerCrossAttention      | 4 M   \n","555  | model.decoder.block.5.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n","556  | model.decoder.block.5.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n","557  | model.decoder.block.5.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n","558  | model.decoder.block.5.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n","559  | model.decoder.block.5.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n","560  | model.decoder.block.5.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","561  | model.decoder.block.5.layer.1.dropout                                 | Dropout                    | 0     \n","562  | model.decoder.block.5.layer.2                                         | T5LayerFF                  | 8 M   \n","563  | model.decoder.block.5.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","564  | model.decoder.block.5.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n","565  | model.decoder.block.5.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n","566  | model.decoder.block.5.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n","567  | model.decoder.block.5.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n","568  | model.decoder.block.5.layer.2.dropout                                 | Dropout                    | 0     \n","569  | model.decoder.block.6                                                 | T5Block                    | 16 M  \n","570  | model.decoder.block.6.layer                                           | ModuleList                 | 16 M  \n","571  | model.decoder.block.6.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","572  | model.decoder.block.6.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","573  | model.decoder.block.6.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","574  | model.decoder.block.6.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","575  | model.decoder.block.6.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","576  | model.decoder.block.6.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","577  | model.decoder.block.6.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","578  | model.decoder.block.6.layer.0.dropout                                 | Dropout                    | 0     \n","579  | model.decoder.block.6.layer.1                                         | T5LayerCrossAttention      | 4 M   \n","580  | model.decoder.block.6.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n","581  | model.decoder.block.6.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n","582  | model.decoder.block.6.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n","583  | model.decoder.block.6.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n","584  | model.decoder.block.6.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n","585  | model.decoder.block.6.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","586  | model.decoder.block.6.layer.1.dropout                                 | Dropout                    | 0     \n","587  | model.decoder.block.6.layer.2                                         | T5LayerFF                  | 8 M   \n","588  | model.decoder.block.6.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","589  | model.decoder.block.6.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n","590  | model.decoder.block.6.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n","591  | model.decoder.block.6.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n","592  | model.decoder.block.6.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n","593  | model.decoder.block.6.layer.2.dropout                                 | Dropout                    | 0     \n","594  | model.decoder.block.7                                                 | T5Block                    | 16 M  \n","595  | model.decoder.block.7.layer                                           | ModuleList                 | 16 M  \n","596  | model.decoder.block.7.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","597  | model.decoder.block.7.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","598  | model.decoder.block.7.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","599  | model.decoder.block.7.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","600  | model.decoder.block.7.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","601  | model.decoder.block.7.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","602  | model.decoder.block.7.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","603  | model.decoder.block.7.layer.0.dropout                                 | Dropout                    | 0     \n","604  | model.decoder.block.7.layer.1                                         | T5LayerCrossAttention      | 4 M   \n","605  | model.decoder.block.7.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n","606  | model.decoder.block.7.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n","607  | model.decoder.block.7.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n","608  | model.decoder.block.7.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n","609  | model.decoder.block.7.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n","610  | model.decoder.block.7.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","611  | model.decoder.block.7.layer.1.dropout                                 | Dropout                    | 0     \n","612  | model.decoder.block.7.layer.2                                         | T5LayerFF                  | 8 M   \n","613  | model.decoder.block.7.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","614  | model.decoder.block.7.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n","615  | model.decoder.block.7.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n","616  | model.decoder.block.7.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n","617  | model.decoder.block.7.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n","618  | model.decoder.block.7.layer.2.dropout                                 | Dropout                    | 0     \n","619  | model.decoder.block.8                                                 | T5Block                    | 16 M  \n","620  | model.decoder.block.8.layer                                           | ModuleList                 | 16 M  \n","621  | model.decoder.block.8.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","622  | model.decoder.block.8.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","623  | model.decoder.block.8.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","624  | model.decoder.block.8.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","625  | model.decoder.block.8.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","626  | model.decoder.block.8.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","627  | model.decoder.block.8.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","628  | model.decoder.block.8.layer.0.dropout                                 | Dropout                    | 0     \n","629  | model.decoder.block.8.layer.1                                         | T5LayerCrossAttention      | 4 M   \n","630  | model.decoder.block.8.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n","631  | model.decoder.block.8.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n","632  | model.decoder.block.8.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n","633  | model.decoder.block.8.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n","634  | model.decoder.block.8.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n","635  | model.decoder.block.8.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","636  | model.decoder.block.8.layer.1.dropout                                 | Dropout                    | 0     \n","637  | model.decoder.block.8.layer.2                                         | T5LayerFF                  | 8 M   \n","638  | model.decoder.block.8.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","639  | model.decoder.block.8.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n","640  | model.decoder.block.8.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n","641  | model.decoder.block.8.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n","642  | model.decoder.block.8.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n","643  | model.decoder.block.8.layer.2.dropout                                 | Dropout                    | 0     \n","644  | model.decoder.block.9                                                 | T5Block                    | 16 M  \n","645  | model.decoder.block.9.layer                                           | ModuleList                 | 16 M  \n","646  | model.decoder.block.9.layer.0                                         | T5LayerSelfAttention       | 4 M   \n","647  | model.decoder.block.9.layer.0.SelfAttention                           | T5Attention                | 4 M   \n","648  | model.decoder.block.9.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n","649  | model.decoder.block.9.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n","650  | model.decoder.block.9.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n","651  | model.decoder.block.9.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n","652  | model.decoder.block.9.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n","653  | model.decoder.block.9.layer.0.dropout                                 | Dropout                    | 0     \n","654  | model.decoder.block.9.layer.1                                         | T5LayerCrossAttention      | 4 M   \n","655  | model.decoder.block.9.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n","656  | model.decoder.block.9.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n","657  | model.decoder.block.9.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n","658  | model.decoder.block.9.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n","659  | model.decoder.block.9.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n","660  | model.decoder.block.9.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n","661  | model.decoder.block.9.layer.1.dropout                                 | Dropout                    | 0     \n","662  | model.decoder.block.9.layer.2                                         | T5LayerFF                  | 8 M   \n","663  | model.decoder.block.9.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n","664  | model.decoder.block.9.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n","665  | model.decoder.block.9.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n","666  | model.decoder.block.9.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n","667  | model.decoder.block.9.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n","668  | model.decoder.block.9.layer.2.dropout                                 | Dropout                    | 0     \n","669  | model.decoder.block.10                                                | T5Block                    | 16 M  \n","670  | model.decoder.block.10.layer                                          | ModuleList                 | 16 M  \n","671  | model.decoder.block.10.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","672  | model.decoder.block.10.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","673  | model.decoder.block.10.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","674  | model.decoder.block.10.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","675  | model.decoder.block.10.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","676  | model.decoder.block.10.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","677  | model.decoder.block.10.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","678  | model.decoder.block.10.layer.0.dropout                                | Dropout                    | 0     \n","679  | model.decoder.block.10.layer.1                                        | T5LayerCrossAttention      | 4 M   \n","680  | model.decoder.block.10.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n","681  | model.decoder.block.10.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n","682  | model.decoder.block.10.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n","683  | model.decoder.block.10.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n","684  | model.decoder.block.10.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n","685  | model.decoder.block.10.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","686  | model.decoder.block.10.layer.1.dropout                                | Dropout                    | 0     \n","687  | model.decoder.block.10.layer.2                                        | T5LayerFF                  | 8 M   \n","688  | model.decoder.block.10.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","689  | model.decoder.block.10.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n","690  | model.decoder.block.10.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n","691  | model.decoder.block.10.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n","692  | model.decoder.block.10.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n","693  | model.decoder.block.10.layer.2.dropout                                | Dropout                    | 0     \n","694  | model.decoder.block.11                                                | T5Block                    | 16 M  \n","695  | model.decoder.block.11.layer                                          | ModuleList                 | 16 M  \n","696  | model.decoder.block.11.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","697  | model.decoder.block.11.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","698  | model.decoder.block.11.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","699  | model.decoder.block.11.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","700  | model.decoder.block.11.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","701  | model.decoder.block.11.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","702  | model.decoder.block.11.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","703  | model.decoder.block.11.layer.0.dropout                                | Dropout                    | 0     \n","704  | model.decoder.block.11.layer.1                                        | T5LayerCrossAttention      | 4 M   \n","705  | model.decoder.block.11.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n","706  | model.decoder.block.11.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n","707  | model.decoder.block.11.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n","708  | model.decoder.block.11.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n","709  | model.decoder.block.11.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n","710  | model.decoder.block.11.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","711  | model.decoder.block.11.layer.1.dropout                                | Dropout                    | 0     \n","712  | model.decoder.block.11.layer.2                                        | T5LayerFF                  | 8 M   \n","713  | model.decoder.block.11.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","714  | model.decoder.block.11.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n","715  | model.decoder.block.11.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n","716  | model.decoder.block.11.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n","717  | model.decoder.block.11.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n","718  | model.decoder.block.11.layer.2.dropout                                | Dropout                    | 0     \n","719  | model.decoder.block.12                                                | T5Block                    | 16 M  \n","720  | model.decoder.block.12.layer                                          | ModuleList                 | 16 M  \n","721  | model.decoder.block.12.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","722  | model.decoder.block.12.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","723  | model.decoder.block.12.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","724  | model.decoder.block.12.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","725  | model.decoder.block.12.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","726  | model.decoder.block.12.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","727  | model.decoder.block.12.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","728  | model.decoder.block.12.layer.0.dropout                                | Dropout                    | 0     \n","729  | model.decoder.block.12.layer.1                                        | T5LayerCrossAttention      | 4 M   \n","730  | model.decoder.block.12.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n","731  | model.decoder.block.12.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n","732  | model.decoder.block.12.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n","733  | model.decoder.block.12.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n","734  | model.decoder.block.12.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n","735  | model.decoder.block.12.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","736  | model.decoder.block.12.layer.1.dropout                                | Dropout                    | 0     \n","737  | model.decoder.block.12.layer.2                                        | T5LayerFF                  | 8 M   \n","738  | model.decoder.block.12.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","739  | model.decoder.block.12.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n","740  | model.decoder.block.12.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n","741  | model.decoder.block.12.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n","742  | model.decoder.block.12.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n","743  | model.decoder.block.12.layer.2.dropout                                | Dropout                    | 0     \n","744  | model.decoder.block.13                                                | T5Block                    | 16 M  \n","745  | model.decoder.block.13.layer                                          | ModuleList                 | 16 M  \n","746  | model.decoder.block.13.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","747  | model.decoder.block.13.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","748  | model.decoder.block.13.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","749  | model.decoder.block.13.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","750  | model.decoder.block.13.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","751  | model.decoder.block.13.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","752  | model.decoder.block.13.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","753  | model.decoder.block.13.layer.0.dropout                                | Dropout                    | 0     \n","754  | model.decoder.block.13.layer.1                                        | T5LayerCrossAttention      | 4 M   \n","755  | model.decoder.block.13.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n","756  | model.decoder.block.13.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n","757  | model.decoder.block.13.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n","758  | model.decoder.block.13.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n","759  | model.decoder.block.13.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n","760  | model.decoder.block.13.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","761  | model.decoder.block.13.layer.1.dropout                                | Dropout                    | 0     \n","762  | model.decoder.block.13.layer.2                                        | T5LayerFF                  | 8 M   \n","763  | model.decoder.block.13.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","764  | model.decoder.block.13.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n","765  | model.decoder.block.13.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n","766  | model.decoder.block.13.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n","767  | model.decoder.block.13.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n","768  | model.decoder.block.13.layer.2.dropout                                | Dropout                    | 0     \n","769  | model.decoder.block.14                                                | T5Block                    | 16 M  \n","770  | model.decoder.block.14.layer                                          | ModuleList                 | 16 M  \n","771  | model.decoder.block.14.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","772  | model.decoder.block.14.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","773  | model.decoder.block.14.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","774  | model.decoder.block.14.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","775  | model.decoder.block.14.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","776  | model.decoder.block.14.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","777  | model.decoder.block.14.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","778  | model.decoder.block.14.layer.0.dropout                                | Dropout                    | 0     \n","779  | model.decoder.block.14.layer.1                                        | T5LayerCrossAttention      | 4 M   \n","780  | model.decoder.block.14.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n","781  | model.decoder.block.14.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n","782  | model.decoder.block.14.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n","783  | model.decoder.block.14.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n","784  | model.decoder.block.14.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n","785  | model.decoder.block.14.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","786  | model.decoder.block.14.layer.1.dropout                                | Dropout                    | 0     \n","787  | model.decoder.block.14.layer.2                                        | T5LayerFF                  | 8 M   \n","788  | model.decoder.block.14.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","789  | model.decoder.block.14.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n","790  | model.decoder.block.14.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n","791  | model.decoder.block.14.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n","792  | model.decoder.block.14.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n","793  | model.decoder.block.14.layer.2.dropout                                | Dropout                    | 0     \n","794  | model.decoder.block.15                                                | T5Block                    | 16 M  \n","795  | model.decoder.block.15.layer                                          | ModuleList                 | 16 M  \n","796  | model.decoder.block.15.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","797  | model.decoder.block.15.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","798  | model.decoder.block.15.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","799  | model.decoder.block.15.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","800  | model.decoder.block.15.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","801  | model.decoder.block.15.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","802  | model.decoder.block.15.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","803  | model.decoder.block.15.layer.0.dropout                                | Dropout                    | 0     \n","804  | model.decoder.block.15.layer.1                                        | T5LayerCrossAttention      | 4 M   \n","805  | model.decoder.block.15.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n","806  | model.decoder.block.15.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n","807  | model.decoder.block.15.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n","808  | model.decoder.block.15.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n","809  | model.decoder.block.15.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n","810  | model.decoder.block.15.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","811  | model.decoder.block.15.layer.1.dropout                                | Dropout                    | 0     \n","812  | model.decoder.block.15.layer.2                                        | T5LayerFF                  | 8 M   \n","813  | model.decoder.block.15.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","814  | model.decoder.block.15.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n","815  | model.decoder.block.15.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n","816  | model.decoder.block.15.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n","817  | model.decoder.block.15.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n","818  | model.decoder.block.15.layer.2.dropout                                | Dropout                    | 0     \n","819  | model.decoder.block.16                                                | T5Block                    | 16 M  \n","820  | model.decoder.block.16.layer                                          | ModuleList                 | 16 M  \n","821  | model.decoder.block.16.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","822  | model.decoder.block.16.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","823  | model.decoder.block.16.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","824  | model.decoder.block.16.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","825  | model.decoder.block.16.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","826  | model.decoder.block.16.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","827  | model.decoder.block.16.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","828  | model.decoder.block.16.layer.0.dropout                                | Dropout                    | 0     \n","829  | model.decoder.block.16.layer.1                                        | T5LayerCrossAttention      | 4 M   \n","830  | model.decoder.block.16.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n","831  | model.decoder.block.16.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n","832  | model.decoder.block.16.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n","833  | model.decoder.block.16.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n","834  | model.decoder.block.16.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n","835  | model.decoder.block.16.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","836  | model.decoder.block.16.layer.1.dropout                                | Dropout                    | 0     \n","837  | model.decoder.block.16.layer.2                                        | T5LayerFF                  | 8 M   \n","838  | model.decoder.block.16.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","839  | model.decoder.block.16.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n","840  | model.decoder.block.16.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n","841  | model.decoder.block.16.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n","842  | model.decoder.block.16.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n","843  | model.decoder.block.16.layer.2.dropout                                | Dropout                    | 0     \n","844  | model.decoder.block.17                                                | T5Block                    | 16 M  \n","845  | model.decoder.block.17.layer                                          | ModuleList                 | 16 M  \n","846  | model.decoder.block.17.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","847  | model.decoder.block.17.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","848  | model.decoder.block.17.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","849  | model.decoder.block.17.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","850  | model.decoder.block.17.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","851  | model.decoder.block.17.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","852  | model.decoder.block.17.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","853  | model.decoder.block.17.layer.0.dropout                                | Dropout                    | 0     \n","854  | model.decoder.block.17.layer.1                                        | T5LayerCrossAttention      | 4 M   \n","855  | model.decoder.block.17.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n","856  | model.decoder.block.17.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n","857  | model.decoder.block.17.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n","858  | model.decoder.block.17.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n","859  | model.decoder.block.17.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n","860  | model.decoder.block.17.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","861  | model.decoder.block.17.layer.1.dropout                                | Dropout                    | 0     \n","862  | model.decoder.block.17.layer.2                                        | T5LayerFF                  | 8 M   \n","863  | model.decoder.block.17.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","864  | model.decoder.block.17.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n","865  | model.decoder.block.17.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n","866  | model.decoder.block.17.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n","867  | model.decoder.block.17.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n","868  | model.decoder.block.17.layer.2.dropout                                | Dropout                    | 0     \n","869  | model.decoder.block.18                                                | T5Block                    | 16 M  \n","870  | model.decoder.block.18.layer                                          | ModuleList                 | 16 M  \n","871  | model.decoder.block.18.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","872  | model.decoder.block.18.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","873  | model.decoder.block.18.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","874  | model.decoder.block.18.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","875  | model.decoder.block.18.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","876  | model.decoder.block.18.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","877  | model.decoder.block.18.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","878  | model.decoder.block.18.layer.0.dropout                                | Dropout                    | 0     \n","879  | model.decoder.block.18.layer.1                                        | T5LayerCrossAttention      | 4 M   \n","880  | model.decoder.block.18.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n","881  | model.decoder.block.18.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n","882  | model.decoder.block.18.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n","883  | model.decoder.block.18.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n","884  | model.decoder.block.18.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n","885  | model.decoder.block.18.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","886  | model.decoder.block.18.layer.1.dropout                                | Dropout                    | 0     \n","887  | model.decoder.block.18.layer.2                                        | T5LayerFF                  | 8 M   \n","888  | model.decoder.block.18.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","889  | model.decoder.block.18.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n","890  | model.decoder.block.18.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n","891  | model.decoder.block.18.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n","892  | model.decoder.block.18.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n","893  | model.decoder.block.18.layer.2.dropout                                | Dropout                    | 0     \n","894  | model.decoder.block.19                                                | T5Block                    | 16 M  \n","895  | model.decoder.block.19.layer                                          | ModuleList                 | 16 M  \n","896  | model.decoder.block.19.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","897  | model.decoder.block.19.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","898  | model.decoder.block.19.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","899  | model.decoder.block.19.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","900  | model.decoder.block.19.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","901  | model.decoder.block.19.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","902  | model.decoder.block.19.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","903  | model.decoder.block.19.layer.0.dropout                                | Dropout                    | 0     \n","904  | model.decoder.block.19.layer.1                                        | T5LayerCrossAttention      | 4 M   \n","905  | model.decoder.block.19.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n","906  | model.decoder.block.19.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n","907  | model.decoder.block.19.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n","908  | model.decoder.block.19.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n","909  | model.decoder.block.19.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n","910  | model.decoder.block.19.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","911  | model.decoder.block.19.layer.1.dropout                                | Dropout                    | 0     \n","912  | model.decoder.block.19.layer.2                                        | T5LayerFF                  | 8 M   \n","913  | model.decoder.block.19.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","914  | model.decoder.block.19.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n","915  | model.decoder.block.19.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n","916  | model.decoder.block.19.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n","917  | model.decoder.block.19.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n","918  | model.decoder.block.19.layer.2.dropout                                | Dropout                    | 0     \n","919  | model.decoder.block.20                                                | T5Block                    | 16 M  \n","920  | model.decoder.block.20.layer                                          | ModuleList                 | 16 M  \n","921  | model.decoder.block.20.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","922  | model.decoder.block.20.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","923  | model.decoder.block.20.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","924  | model.decoder.block.20.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","925  | model.decoder.block.20.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","926  | model.decoder.block.20.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","927  | model.decoder.block.20.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","928  | model.decoder.block.20.layer.0.dropout                                | Dropout                    | 0     \n","929  | model.decoder.block.20.layer.1                                        | T5LayerCrossAttention      | 4 M   \n","930  | model.decoder.block.20.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n","931  | model.decoder.block.20.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n","932  | model.decoder.block.20.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n","933  | model.decoder.block.20.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n","934  | model.decoder.block.20.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n","935  | model.decoder.block.20.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","936  | model.decoder.block.20.layer.1.dropout                                | Dropout                    | 0     \n","937  | model.decoder.block.20.layer.2                                        | T5LayerFF                  | 8 M   \n","938  | model.decoder.block.20.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","939  | model.decoder.block.20.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n","940  | model.decoder.block.20.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n","941  | model.decoder.block.20.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n","942  | model.decoder.block.20.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n","943  | model.decoder.block.20.layer.2.dropout                                | Dropout                    | 0     \n","944  | model.decoder.block.21                                                | T5Block                    | 16 M  \n","945  | model.decoder.block.21.layer                                          | ModuleList                 | 16 M  \n","946  | model.decoder.block.21.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","947  | model.decoder.block.21.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","948  | model.decoder.block.21.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","949  | model.decoder.block.21.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","950  | model.decoder.block.21.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","951  | model.decoder.block.21.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","952  | model.decoder.block.21.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","953  | model.decoder.block.21.layer.0.dropout                                | Dropout                    | 0     \n","954  | model.decoder.block.21.layer.1                                        | T5LayerCrossAttention      | 4 M   \n","955  | model.decoder.block.21.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n","956  | model.decoder.block.21.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n","957  | model.decoder.block.21.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n","958  | model.decoder.block.21.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n","959  | model.decoder.block.21.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n","960  | model.decoder.block.21.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","961  | model.decoder.block.21.layer.1.dropout                                | Dropout                    | 0     \n","962  | model.decoder.block.21.layer.2                                        | T5LayerFF                  | 8 M   \n","963  | model.decoder.block.21.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","964  | model.decoder.block.21.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n","965  | model.decoder.block.21.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n","966  | model.decoder.block.21.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n","967  | model.decoder.block.21.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n","968  | model.decoder.block.21.layer.2.dropout                                | Dropout                    | 0     \n","969  | model.decoder.block.22                                                | T5Block                    | 16 M  \n","970  | model.decoder.block.22.layer                                          | ModuleList                 | 16 M  \n","971  | model.decoder.block.22.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","972  | model.decoder.block.22.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","973  | model.decoder.block.22.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","974  | model.decoder.block.22.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","975  | model.decoder.block.22.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","976  | model.decoder.block.22.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","977  | model.decoder.block.22.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","978  | model.decoder.block.22.layer.0.dropout                                | Dropout                    | 0     \n","979  | model.decoder.block.22.layer.1                                        | T5LayerCrossAttention      | 4 M   \n","980  | model.decoder.block.22.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n","981  | model.decoder.block.22.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n","982  | model.decoder.block.22.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n","983  | model.decoder.block.22.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n","984  | model.decoder.block.22.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n","985  | model.decoder.block.22.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","986  | model.decoder.block.22.layer.1.dropout                                | Dropout                    | 0     \n","987  | model.decoder.block.22.layer.2                                        | T5LayerFF                  | 8 M   \n","988  | model.decoder.block.22.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","989  | model.decoder.block.22.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n","990  | model.decoder.block.22.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n","991  | model.decoder.block.22.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n","992  | model.decoder.block.22.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n","993  | model.decoder.block.22.layer.2.dropout                                | Dropout                    | 0     \n","994  | model.decoder.block.23                                                | T5Block                    | 16 M  \n","995  | model.decoder.block.23.layer                                          | ModuleList                 | 16 M  \n","996  | model.decoder.block.23.layer.0                                        | T5LayerSelfAttention       | 4 M   \n","997  | model.decoder.block.23.layer.0.SelfAttention                          | T5Attention                | 4 M   \n","998  | model.decoder.block.23.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n","999  | model.decoder.block.23.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n","1000 | model.decoder.block.23.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n","1001 | model.decoder.block.23.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n","1002 | model.decoder.block.23.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n","1003 | model.decoder.block.23.layer.0.dropout                                | Dropout                    | 0     \n","1004 | model.decoder.block.23.layer.1                                        | T5LayerCrossAttention      | 4 M   \n","1005 | model.decoder.block.23.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n","1006 | model.decoder.block.23.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n","1007 | model.decoder.block.23.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n","1008 | model.decoder.block.23.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n","1009 | model.decoder.block.23.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n","1010 | model.decoder.block.23.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n","1011 | model.decoder.block.23.layer.1.dropout                                | Dropout                    | 0     \n","1012 | model.decoder.block.23.layer.2                                        | T5LayerFF                  | 8 M   \n","1013 | model.decoder.block.23.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n","1014 | model.decoder.block.23.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n","1015 | model.decoder.block.23.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n","1016 | model.decoder.block.23.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n","1017 | model.decoder.block.23.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n","1018 | model.decoder.block.23.layer.2.dropout                                | Dropout                    | 0     \n","1019 | model.decoder.final_layer_norm                                        | T5LayerNorm                | 1 K   \n","1020 | model.decoder.dropout                                                 | Dropout                    | 0     \n","1021 | model.lm_head                                                         | Linear                     | 32 M  \n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"612031c950b84b6ea0eb1ec3f1869da1","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf1c9738c00c4d359c08b0c80146bcc3","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3f7aedaa7f342aba684aa2df1ff3625","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:__main__:***** Validation results *****\n","INFO:__main__:avg_val_loss = tensor(1.6279, device='cuda:0')\n","\n","INFO:__main__:loss = tensor(0.8171, device='cuda:0')\n","\n","INFO:__main__:train_loss = tensor(0.8171, device='cuda:0')\n","\n","INFO:__main__:val_loss = tensor(1.6279, device='cuda:0')\n","\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"58cd98deeeab41bfbc57d00a47a36a69","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:__main__:***** Validation results *****\n","INFO:__main__:avg_train_loss = tensor(1.5839, device='cuda:0')\n","\n","INFO:__main__:avg_val_loss = tensor(1.6156, device='cuda:0')\n","\n","INFO:__main__:epoch = 0\n","\n","INFO:__main__:loss = tensor(1.7229, device='cuda:0')\n","\n","INFO:__main__:train_loss = tensor(1.7229, device='cuda:0')\n","\n","INFO:__main__:val_loss = tensor(1.6156, device='cuda:0')\n","\n","INFO:transformers.configuration_utils:Configuration saved in t5_paraphrase_M2/config.json\n"],"name":"stderr"},{"output_type":"stream","text":["\n","training finished\n","Saving model\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.modeling_utils:Model weights saved in t5_paraphrase_M2/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["Model saved\n","cp: cannot stat '/content/t5_paraphrase_M2/': No such file or directory\n","cp: cannot stat '/content/lightning_logs/': No such file or directory\n","Copied the final folder to Google Drive\n"],"name":"stdout"}]}]}