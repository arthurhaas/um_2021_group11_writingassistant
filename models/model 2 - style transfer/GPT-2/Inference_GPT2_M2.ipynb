{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inference_GPT2_M2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSnIPO5H4Lfd"
      },
      "source": [
        "# GPT-2 style transfer model\n",
        "\n",
        "1. Run all cells\n",
        "2. Scroll down to the interactive part to explore GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43xZG8-z4hM6"
      },
      "source": [
        "! echo \"Prepare download of GPT-2 model...\"\n",
        "! mkdir gpt2_model_2\n",
        "%cd gpt2_model_2\n",
        "! echo \"download model checkpoint\"\n",
        "! gdown \"https://drive.google.com/uc?id=1UpXWO-JAkcjxp3V6vn2Yda_QkDc9LpVF\"\n",
        "! gdown \"https://drive.google.com/uc?id=1R7cXi7Sl3zCPrVtlfsEAAMKSzU2kd0nc\"\n",
        "! gdown \"https://drive.google.com/uc?id=17U7K4t2pJUPaS93lmk6Gs8eBDz4cF4ld\"\n",
        "! gdown \"https://drive.google.com/uc?id=1yo82NL3EXGSRBDiul69PiF1s2YxzAhMn\"\n",
        "! gdown \"https://drive.google.com/uc?id=1TDQuZB9eRz2Zge-OLgDwDch8pSUK5hEV\"\n",
        "! gdown \"https://drive.google.com/uc?id=1d7jC7QWDVrh-dJzLdRWZ77o_hXz0W-Ds\"\n",
        "! gdown \"https://drive.google.com/uc?id=1cwZgYBeO5v67U9Vx3NkIXFgWXDTg4Ykj\"\n",
        "! gdown \"https://drive.google.com/uc?id=11nvySJwrcZNAMLdIvtY0TkwoL-egucBj\"\n",
        "! gdown \"https://drive.google.com/uc?id=13P8u_SliPx2aKJgaT8mfEjEeShgWVKA2\"\n",
        "! gdown \"https://drive.google.com/uc?id=1Q6gOZ-_6ls4F4OHz9M5lsyy8Vrnkm2Mj\"\n",
        "! gdown \"https://drive.google.com/uc?id=13HEi95_4w2CYNilliUNimC4n8qJ1g5Eh\"\n",
        "! gdown \"https://drive.google.com/uc?id=1RJ7njs1nRnGSf2l9x5hHEImDKhbyCvJT\"\n",
        "%cd ..\n",
        "\n",
        "checkpoint = \"/content/gpt2_model_2\"\n",
        "\n",
        "! echo \"install libraries...\"\n",
        "!pip install -q git+https://github.com/huggingface/transformers\n",
        "!pip install -q transformers torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWjrTUlATqPA"
      },
      "source": [
        "from transformers import pipeline, TextGenerationPipeline, GPT2LMHeadModel, AutoTokenizer\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, )\n",
        "science_generator = TextGenerationPipeline(model=model, tokenizer=tokenizer, return_full_text=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIxvgomcTqHv"
      },
      "source": [
        "def processor(input):\n",
        "  input_prompt = \"<BOS>\"+input+\"<GENERATE_SCIENCE>\"\n",
        "\n",
        "  max_length= len(input_prompt)\n",
        "  science = science_generator(input_prompt, min_length = int(max_length/2), max_length = max_length, do_sample=True,\n",
        "                repetition_penalty=1.1, temperature=1.2,\n",
        "                top_p=0.55, top_k=50)\n",
        "\n",
        "  x = science[0]['generated_text']\n",
        "  output = ''\n",
        "  for ch in x:\n",
        "    if ch == '.':\n",
        "      output += '.'\n",
        "      break\n",
        "    else:\n",
        "      output += ch\n",
        "\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2l5Lyv4fBgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ebe2bf9-0f33-4792-a8a8-a2d2ffb3362a"
      },
      "source": [
        "#@title Interactive\n",
        "\n",
        "#@markdown Please set your sentences, that you would like to be transferred.\n",
        "text = \"I am very happy for this model.\" #@param {type: \"string\"}\n",
        "\n",
        "print(processor(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "thanks to the model, I'm very happy.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Eu7KwFj78om"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}