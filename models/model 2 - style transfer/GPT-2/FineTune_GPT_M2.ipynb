{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FineTune_GPT_M2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2xz8yAttvNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3607040d-4e92-47b9-d14f-731ae87bd0cc"
      },
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKJmnI943ajz",
        "outputId": "ad4f6e0c-6ab0-4591-ddb4-d68d6bc56cf0"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "\n",
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9.0+cu102\n",
            "4.8.0.dev0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Rb1BSLugcZn"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install transformers torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltEwIIMDg9ex"
      },
      "source": [
        "import logging\n",
        "import math\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    MODEL_WITH_LM_HEAD_MAPPING,\n",
        "    AutoConfig,\n",
        "    GPT2LMHeadModel,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    LineByLineTextDataset,\n",
        "    PreTrainedTokenizer,\n",
        "    TextDataset,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "# Setup logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Get access to model types and model configs to select GPT2 model and config\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL0YR9WMhG0v"
      },
      "source": [
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\"\n",
        "        },\n",
        "    )\n",
        "    model_type: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"If training from scratch, pass a model type from the list: \"\n",
        "            + \", \".join(MODEL_TYPES)\n",
        "        },\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"Where do you want to store the pretrained models downloaded from s3\"\n",
        "        },\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX9aWWf1hL4X"
      },
      "source": [
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "\n",
        "    train_data_file: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The input training data file (a text file).\"}\n",
        "    )\n",
        "    eval_data_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"\n",
        "        },\n",
        "    )\n",
        "    line_by_line: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    mlm: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"Train with masked-language modeling loss instead of language modeling.\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    block_size: int = field(\n",
        "        default=-1,\n",
        "        metadata={\n",
        "            \"help\": \"Optional input sequence length after tokenization.\"\n",
        "            \"The training dataset will be truncated in block of this size for training.\"\n",
        "            \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
        "        },\n",
        "    )\n",
        "    \n",
        "    overwrite_cache: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Overwrite the cached training and evaluation sets\"},\n",
        "    )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od_Gxsz1hZvV"
      },
      "source": [
        "# Create LineByLineDataset \n",
        "def get_dataset(\n",
        "    args: DataTrainingArguments, tokenizer: PreTrainedTokenizer, evaluate=False\n",
        "):\n",
        "    file_path = args.eval_data_file if evaluate else args.train_data_file\n",
        "    if args.line_by_line:\n",
        "        return LineByLineTextDataset(\n",
        "            tokenizer=tokenizer, file_path=file_path, block_size=args.block_size\n",
        "        )\n",
        "    else:\n",
        "        return TextDataset(\n",
        "            tokenizer=tokenizer,\n",
        "            file_path=file_path,\n",
        "            block_size=args.block_size,\n",
        "            overwrite_cache=args.overwrite_cache,\n",
        "        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeEGwWDf0SS1",
        "outputId": "0d49bdc2-0102-4d92-cd93-aa217e4fd3b6"
      },
      "source": [
        "customTokenList = ['<GENERATE_SCIENCE>']\n",
        "print(len(customTokenList))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXE6Zq8-hikF"
      },
      "source": [
        "def main():\n",
        "    model_args = ModelArguments(\n",
        "        model_name_or_path=\"gpt2\", model_type=\"gpt2\"\n",
        "    )\n",
        "    data_args = DataTrainingArguments(\n",
        "        train_data_file=\"/content/gdrive/MyDrive/rpg1/train_m2_gpt_l2l.txt\",\n",
        "        eval_data_file=\"/content/gdrive/MyDrive/rpg1/eval_m2_gpt_l2l.txt\",\n",
        "        line_by_line=True,\n",
        "        block_size=512,\n",
        "        overwrite_cache=True,\n",
        "    )\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"/content/gdrive/MyDrive/rpg1/M2_GPT\",\n",
        "        overwrite_output_dir=True,\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        logging_steps=500,\n",
        "        per_device_train_batch_size=8,\n",
        "        num_train_epochs=6,\n",
        "        save_total_limit=1,\n",
        "        save_steps=3000,\n",
        "    )\n",
        "\n",
        "    if data_args.eval_data_file is None and training_args.do_eval:\n",
        "        raise ValueError(\n",
        "            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
        "            \"or remove the --do_eval argument.\"\n",
        "        )\n",
        "\n",
        "    if (\n",
        "        os.path.exists(training_args.output_dir)\n",
        "        and os.listdir(training_args.output_dir)\n",
        "        and training_args.do_train\n",
        "        and not training_args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
        "        )\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "        training_args.local_rank,\n",
        "        training_args.device,\n",
        "        training_args.n_gpu,\n",
        "        bool(training_args.local_rank != -1),\n",
        "        training_args.fp16,\n",
        "    )\n",
        "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
        "\n",
        "    # Set seed for deterministic training runs\n",
        "    set_seed(training_args.seed)\n",
        "\n",
        "\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_args.model_name_or_path, cache_dir=model_args.cache_dir\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.model_name_or_path, cache_dir=model_args.cache_dir\n",
        "    )\n",
        "\n",
        "    model = GPT2LMHeadModel.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
        "        config=config,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "    )\n",
        "    \n",
        "    special_tokens_dict = {\n",
        "        \"bos_token\": \"<BOS>\",\n",
        "        \"eos_token\": \"<EOS>\",\n",
        "        \"pad_token\": \"<PAD>\",\n",
        "        \"additional_special_tokens\": customTokenList,\n",
        "    }\n",
        "\n",
        "\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if data_args.block_size <= 0:\n",
        "      # If block_size <= 0, set it to max. possible value allowed by model\n",
        "        data_args.block_size = tokenizer.model_max_length\n",
        "    else:\n",
        "        data_args.block_size = min(data_args.block_size, tokenizer.model_max_length)\n",
        "\n",
        "    # Get datasets\n",
        "\n",
        "    train_dataset = (\n",
        "        get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\n",
        "    )\n",
        "\n",
        "    print('train_dataset: \\n' + str(len(train_dataset)))\n",
        "    eval_dataset = (\n",
        "        get_dataset(data_args, tokenizer=tokenizer, evaluate=True)\n",
        "        if training_args.do_eval\n",
        "        else None\n",
        "    )\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=data_args.mlm,\n",
        "    )\n",
        "\n",
        "    # Initialize our Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    try:\n",
        "      if training_args.do_train:\n",
        "          model_path = (\n",
        "              model_args.model_name_or_path\n",
        "              if model_args.model_name_or_path is not None\n",
        "              and os.path.isdir(model_args.model_name_or_path)\n",
        "              else None\n",
        "          )\n",
        "          trainer.train(model_path=model_path)\n",
        "          trainer.save_model()\n",
        "          tokenizer.save_pretrained(training_args.output_dir)\n",
        "    except KeyboardInterrupt:\n",
        "      print(\"Saving model that was in the middle of training\")\n",
        "      trainer.save_model()\n",
        "      tokenizer.save_pretrained(training_args.output_dir)\n",
        "      return\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if training_args.do_eval:\n",
        "        logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "        eval_output = trainer.evaluate()\n",
        "\n",
        "        perplexity = math.exp(eval_output[\"eval_loss\"])\n",
        "        result = {\"perplexity\": perplexity}\n",
        "\n",
        "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results_lm.txt\")\n",
        "        if trainer.is_world_process_zero():\n",
        "            with open(output_eval_file, \"w\") as writer:\n",
        "                logger.info(\"***** Eval results *****\")\n",
        "                for key in sorted(result.keys()):\n",
        "                    logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "                    writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "        results.update(result)\n",
        "\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdzdlR4ciaZ6"
      },
      "source": [
        "# Press the Run Cell button to the left to start training\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# To stop training and save model, press the same Run Cell button (now, it is the Interrupt Execution button)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FdTXrrl-m4uM",
        "outputId": "c4fb472d-77d5-4893-dd29-e650113d38ca"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUoTHvmHHz43"
      },
      "source": [
        "from transformers import pipeline, TextGenerationPipeline, GPT2LMHeadModel, AutoTokenizer\n",
        "\n",
        "checkpoint = \"/content/gdrive/MyDrive/rpg1/M2_GPT/checkpoint-54000\"\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, )\n",
        "science_generator = TextGenerationPipeline(model=model, tokenizer=tokenizer, return_full_text=False)\n",
        "# The format for input_prompt: \"<BOS>Less scientific sentence...<GENERATE_SCIENCE>\"\n",
        "# Supported genres: superhero, sci_fi, horror, thriller, action, drama"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clXP7bNrPxja"
      },
      "source": [
        "# <GENERATE_SCIENCE>\n",
        "# <BOS>\n",
        "sent = \"doing this over and over to distill and refine and distill your code is refactoring in essence.\"\n",
        "# sent = \"Without discounting the immense success of pseudolabeling techniques, self-supervised representations offer two unique advantages.\"\n",
        "input_prompt = \"<BOS>\"+sent+\"<GENERATE_SCIENCE>\"\n",
        "\n",
        "max_length= len(input_prompt)\n",
        "print(str(max_length)+'\\n')\n",
        "science = science_generator(input_prompt, min_length = int(max_length/2), max_length = max_length, do_sample=True,\n",
        "               repetition_penalty=1.1, temperature=1.2,\n",
        "               top_p=0.55, top_k=50)\n",
        "\n",
        "print(science)\n",
        "x = science[0]['generated_text']\n",
        "output = ''\n",
        "for ch in x:\n",
        "  if ch == '.':\n",
        "    output += '.'\n",
        "    break\n",
        "  else:\n",
        "    output += ch\n",
        "\n",
        "print('\\n'+output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyKFX60YPDJ9"
      },
      "source": [
        "def processor(input):\n",
        "  input_prompt = \"<BOS>\"+input+\"<GENERATE_SCIENCE>\"\n",
        "\n",
        "  max_length= len(input_prompt)\n",
        "  science = science_generator(input_prompt, min_length = int(max_length/2), max_length = max_length, do_sample=True,\n",
        "                repetition_penalty=1.1, temperature=1.2,\n",
        "                top_p=0.55, top_k=50)\n",
        "\n",
        "  x = science[0]['generated_text']\n",
        "  output = ''\n",
        "  for ch in x:\n",
        "    if ch == '.':\n",
        "      output += '.'\n",
        "      break\n",
        "    else:\n",
        "      output += ch\n",
        "\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}