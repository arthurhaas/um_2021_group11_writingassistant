{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "T5_Fine-tuning_Paraphrasing_M1_github_version.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "t4eRy5oVCUny",
        "_p5OZ7pRCel5"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b7915c3393144333bdd7790043764ae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e167ff73d20244979a49f4d5cbb4fd9f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cbddbbf73942495aadcd5d7ed2aa905d",
              "IPY_MODEL_6c24a93a388e46e388d38822151a6e06"
            ]
          }
        },
        "e167ff73d20244979a49f4d5cbb4fd9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cbddbbf73942495aadcd5d7ed2aa905d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c55883e103da4c5f9181efd5d3256197",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 791656,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 791656,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ffd961778399467c9e22992a30d0fddf"
          }
        },
        "6c24a93a388e46e388d38822151a6e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_395f06762e394b958450ed2d0c6c33b7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 792k/792k [01:12&lt;00:00, 10.9kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_00a5559c57cc4803a4d2aaa03a0367c6"
          }
        },
        "c55883e103da4c5f9181efd5d3256197": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ffd961778399467c9e22992a30d0fddf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "395f06762e394b958450ed2d0c6c33b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "00a5559c57cc4803a4d2aaa03a0367c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aed05c5c315740368f83e6eca532f9f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fdd0e3c41d0b4bc19f7d314e26ef0f48",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f3f31364a895473c9ab849ac42937efe",
              "IPY_MODEL_4e0e9d57c434447a9c816835add9640f"
            ]
          }
        },
        "fdd0e3c41d0b4bc19f7d314e26ef0f48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f3f31364a895473c9ab849ac42937efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2fd4e7e83993403bb641e7ad66470b5c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1200,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1200,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2640a61685994e35b0e0e5c395f3dfb2"
          }
        },
        "4e0e9d57c434447a9c816835add9640f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a76ac0af599543fa87b7eb1a4ac0ea64",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.20k/1.20k [00:51&lt;00:00, 23.4B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_50b25b8a3303423ca8bf5f17b21be399"
          }
        },
        "2fd4e7e83993403bb641e7ad66470b5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2640a61685994e35b0e0e5c395f3dfb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a76ac0af599543fa87b7eb1a4ac0ea64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "50b25b8a3303423ca8bf5f17b21be399": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2bbe2aef2b814e42b4315ccf8f8f2a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_242f9feb9e5a4b7b954f02e53ad4328b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_36db1105ef7c4bd380eddede92112af3",
              "IPY_MODEL_e856286cad1441d180294dd5d29208e5"
            ]
          }
        },
        "242f9feb9e5a4b7b954f02e53ad4328b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "36db1105ef7c4bd380eddede92112af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b0619d17d04d4c5ca883ee7f635685d2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2950825948,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2950825948,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_21660c8cfb7140f08abbe5310cb75b7c"
          }
        },
        "e856286cad1441d180294dd5d29208e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bfc85d822cb74aeea767cb868d23ca5b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.95G/2.95G [00:50&lt;00:00, 57.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_78923b3fad624df3b964c2c2d9c0d5ef"
          }
        },
        "b0619d17d04d4c5ca883ee7f635685d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "21660c8cfb7140f08abbe5310cb75b7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bfc85d822cb74aeea767cb868d23ca5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "78923b3fad624df3b964c2c2d9c0d5ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd4baf5b8268439f879e8400ac60cdad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_60710ed13bb04ef5851d78d364118757",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a8d1da893a74448fbb175e3fc8a3f789",
              "IPY_MODEL_076c25cf95324b759d3edec9d605c113"
            ]
          }
        },
        "60710ed13bb04ef5851d78d364118757": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "a8d1da893a74448fbb175e3fc8a3f789": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4e3a84d8fe00411f939ea69c5718c686",
            "_dom_classes": [],
            "description": "Validation sanity check: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_867b63776c804d44842216cd4a3d0173"
          }
        },
        "076c25cf95324b759d3edec9d605c113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4ae87ed87631488aae873038f787c16d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5/5 [00:00&lt;00:00,  3.46it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e75f7b8c1b5e4eb09a056429a1c9c8d2"
          }
        },
        "4e3a84d8fe00411f939ea69c5718c686": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "867b63776c804d44842216cd4a3d0173": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4ae87ed87631488aae873038f787c16d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e75f7b8c1b5e4eb09a056429a1c9c8d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9532733e4f4841bf8ac7b05c9135d6fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8a833ff8b705488ea96dc9b33a9f4ae3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_215f7588e14e44c6830b5aa70e03b7cb",
              "IPY_MODEL_97b5a61f64594942a855d2ed696aaf90"
            ]
          }
        },
        "8a833ff8b705488ea96dc9b33a9f4ae3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "215f7588e14e44c6830b5aa70e03b7cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_961bfae912b54192b6652735b70004c8",
            "_dom_classes": [],
            "description": "Epoch 3: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 74554,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 74554,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_005b10eec76c49e9b71a2eb2cbbb66c9"
          }
        },
        "97b5a61f64594942a855d2ed696aaf90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_190a4ae6d30648198756ae0c2e3a9e11",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 74554/74554 [4:15:03&lt;00:00,  4.87it/s, loss=0.048, v_num=4, val_loss=1.44]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ddc6a4f9ecbc44ebb4c159c0cb77876b"
          }
        },
        "961bfae912b54192b6652735b70004c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "005b10eec76c49e9b71a2eb2cbbb66c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "190a4ae6d30648198756ae0c2e3a9e11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ddc6a4f9ecbc44ebb4c159c0cb77876b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "00e754584ca14e14af2b9b43513ec7f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7a441311c74044e8bba66e96bbe18896",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c868cd6bbe5e44608daade6b6d403471",
              "IPY_MODEL_c7ac1c2536014399aade316a891be30d"
            ]
          }
        },
        "7a441311c74044e8bba66e96bbe18896": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "c868cd6bbe5e44608daade6b6d403471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_df7b38dc32164badbb0062939603387e",
            "_dom_classes": [],
            "description": "Validating: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8516740de6104b26855b2b727d260d60"
          }
        },
        "c7ac1c2536014399aade316a891be30d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_054fb5b919394620a271a968bea719ff",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1492/1492 [01:42&lt;00:00, 14.79it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_faeb144a7b244d7495c759259a1eb2bd"
          }
        },
        "df7b38dc32164badbb0062939603387e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8516740de6104b26855b2b727d260d60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "054fb5b919394620a271a968bea719ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "faeb144a7b244d7495c759259a1eb2bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e62fe18fc384995a1aa94c1491fa8a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4d5eed1db37b4f0d9d84524809c4545f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b1ef05eadbaf4327af8bf59197777fb4",
              "IPY_MODEL_e0a22f484d584dfd833eba41afa8f073"
            ]
          }
        },
        "4d5eed1db37b4f0d9d84524809c4545f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "b1ef05eadbaf4327af8bf59197777fb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_14c0ca72da7e4fcebd3fedf8bb1e8a97",
            "_dom_classes": [],
            "description": "Validating: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2f934076c9bf4193948b060df408d062"
          }
        },
        "e0a22f484d584dfd833eba41afa8f073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1c2bc891bc3d4593b52179e3f133d2b0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1492/1492 [01:41&lt;00:00, 14.91it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dd74512c9fe944ffa2874ab4e42104c8"
          }
        },
        "14c0ca72da7e4fcebd3fedf8bb1e8a97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2f934076c9bf4193948b060df408d062": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c2bc891bc3d4593b52179e3f133d2b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dd74512c9fe944ffa2874ab4e42104c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlNF-_DjfgtK"
      },
      "source": [
        "\n",
        "# **Install libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50WuQcwFS2V8",
        "outputId": "17b84575-65ce-41d4-8cce-484537a385aa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fnyZQ_0UKZ3",
        "outputId": "1d5cdc61-f270-47c0-929b-7c18c31606fa"
      },
      "source": [
        "%cd \"/content/drive/MyDrive/T5_paraphrasing_reference_code\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/T5_paraphrasing_reference_code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FITSyyasU2OV",
        "outputId": "c3c17d0c-6113-414f-813e-a6b3917a7003"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 23.9MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 25.7MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 23.5MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 17.5MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 14.9MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 16.9MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 13.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 13.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 13.8MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 13.7MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 13.7MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 13.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 13.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 13.7MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 13.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 13.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 13.7MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4jFFe1QV7fs",
        "outputId": "54c19207-e476-42af-f0da-a65dfd5859ea"
      },
      "source": [
        "!pip install torch==1.4.0\n",
        "!pip install transformers==2.9.0\n",
        "!pip install pytorch_lightning==0.7.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/3b/fa92ece1e58a6a48ec598bab327f39d69808133e5b2fb33002ca754e381e/torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 16kB/s \n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "Successfully installed torch-1.4.0\n",
            "Collecting transformers==2.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 14.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 29.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (2.23.0)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/59/bb06dd5ca53547d523422d32735585493e0103c992a52a97ba3aa3be33bf/tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 27.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (0.1.95)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (8.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (1.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (2020.12.5)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.7.0 transformers-2.9.0\n",
            "Collecting pytorch_lightning==0.7.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ac/ac03f1f3fa950d96ca52f07d33fdbf5add05f164c1ac4eae179231dfa93d/pytorch_lightning-0.7.5-py3-none-any.whl (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 14.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.7.5) (1.4.0)\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 26.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.7.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.7.5) (1.19.5)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.7.5) (2.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.30.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (3.3.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (3.12.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (2.0.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (0.36.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (56.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.7.5) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.7.5) (4.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (0.4.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.4.1)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=6a8acf0a443632db50e14df50728e676f032260ca160d893e94b7b5769429854\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built future\n",
            "Installing collected packages: future, pytorch-lightning\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed future-0.18.2 pytorch-lightning-0.7.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn-G_eUwFovO",
        "outputId": "72658528-ad9b-429e-a5b2-633e1df5a18d"
      },
      "source": [
        "# Check we have a GPU and check the memory size of the GUP\n",
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-88849243-7e8b-c203-2354-c0f39881bc63)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0n55Ex1Bl2k"
      },
      "source": [
        "# **Import packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5WiQS6FEEsL",
        "outputId": "ea3f38d2-78d6-486a-b257-53c998216304"
      },
      "source": [
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import re\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.file_utils:PyTorch version 1.4.0 available.\n",
            "INFO:transformers.file_utils:TensorFlow version 2.4.1 available.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykds8V47B1XT"
      },
      "source": [
        "# **Set a seed**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyrYjMFREUCn"
      },
      "source": [
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uSCxnPmCALw"
      },
      "source": [
        "# **T5FineTuner**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr7mnuYEEhxn"
      },
      "source": [
        "class T5FineTuner(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(T5FineTuner, self).__init__()\n",
        "        self.hparams = hparams\n",
        "\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
        "\n",
        "    def is_logger(self):\n",
        "        return True #self.trainer.proc_rank <= 0\n",
        "\n",
        "    def forward(\n",
        "            self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n",
        "    ):\n",
        "        return self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            lm_labels=lm_labels,\n",
        "        )\n",
        "\n",
        "    def _step(self, batch):\n",
        "        lm_labels = batch[\"target_ids\"]\n",
        "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        outputs = self(\n",
        "            input_ids=batch[\"source_ids\"],\n",
        "            attention_mask=batch[\"source_mask\"],\n",
        "            lm_labels=lm_labels,\n",
        "            decoder_attention_mask=batch['target_mask']\n",
        "        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self._step(batch)\n",
        "\n",
        "        tensorboard_logs = {\"train_loss\": loss}\n",
        "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
        "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
        "        return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self._step(batch)\n",
        "        return {\"val_loss\": loss}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "        tensorboard_logs = {\"val_loss\": avg_loss}\n",
        "        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
        "\n",
        "        model = self.model\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": self.hparams.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
        "        self.opt = optimizer\n",
        "        return [optimizer]\n",
        "\n",
        "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None, on_tpu=False, using_native_amp=False, using_lbfgs=False):\n",
        "        if self.trainer.use_tpu:\n",
        "            xm.optimizer_step(optimizer)\n",
        "        else:\n",
        "            optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        self.lr_scheduler.step()\n",
        "\n",
        "    def get_tqdm_dict(self):\n",
        "        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
        "\n",
        "        return tqdm_dict\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n",
        "        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True,\n",
        "                                num_workers=4)\n",
        "        t_total = (\n",
        "                (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
        "                // self.hparams.gradient_accumulation_steps\n",
        "                * float(self.hparams.num_train_epochs)\n",
        "        )\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
        "        )\n",
        "        self.lr_scheduler = scheduler\n",
        "        return dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"dev\", args=self.hparams)\n",
        "        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class LoggingCallback(pl.Callback):\n",
        "  def on_validation_end(self, trainer, pl_module):\n",
        "    logger.info(\"***** Validation results *****\")\n",
        "    if pl_module.is_logger():\n",
        "      metrics = trainer.callback_metrics\n",
        "      # Log results\n",
        "      for key in sorted(metrics):\n",
        "        if key not in [\"log\", \"progress_bar\"]:\n",
        "          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "\n",
        "  def on_test_end(self, trainer, pl_module):\n",
        "    logger.info(\"***** Test results *****\")\n",
        "\n",
        "    if pl_module.is_logger():\n",
        "      metrics = trainer.callback_metrics\n",
        "\n",
        "      # Log and save results to file\n",
        "      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
        "      with open(output_test_results_file, \"w\") as writer:\n",
        "        for key in sorted(metrics):\n",
        "          if key not in [\"log\", \"progress_bar\"]:\n",
        "            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nsjLzviCG_A"
      },
      "source": [
        "# **Load datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "ggbl8IwjHQMi",
        "outputId": "b46db874-b0f5-415b-ee50-801c98b360be"
      },
      "source": [
        "data_train = pd.read_csv(\"data/train.tsv\", sep=\"\\t\")#.astype(str)\n",
        "data_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>e</th>\n",
              "      <th>f</th>\n",
              "      <th>g</th>\n",
              "      <th>h</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>S &gt; NP VP .</td>\n",
              "      <td>S &gt; SBAR , NP VP .</td>\n",
              "      <td>False</td>\n",
              "      <td>Mr. Whetstone is goingto speak to you after I ...</td>\n",
              "      <td>after I'm done, Mr. Whetstone will be speaking.</td>\n",
              "      <td>(0.3, 0.375, 0.33333333333333326)</td>\n",
              "      <td>(-0.33333333333333337, False)</td>\n",
              "      <td>2</td>\n",
              "      <td>('en', 'en')</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>S &gt; NP VP .</td>\n",
              "      <td>S &gt; RB S VP .</td>\n",
              "      <td>False</td>\n",
              "      <td>I guess it's up to me to save this family, then.</td>\n",
              "      <td>so saving my family is on me.</td>\n",
              "      <td>(0.18181818181818182, 0.2857142857142857, 0.22...</td>\n",
              "      <td>(-1.0, False)</td>\n",
              "      <td>2</td>\n",
              "      <td>('en', 'en')</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>S &gt; NP VP .</td>\n",
              "      <td>S &gt; NP VP .</td>\n",
              "      <td>True</td>\n",
              "      <td>It'll speed distribution when we get the lines...</td>\n",
              "      <td>we would expedite such distributions when we f...</td>\n",
              "      <td>(0.2222222222222222, 0.2222222222222222, 0.222...</td>\n",
              "      <td>(-1.0, False)</td>\n",
              "      <td>0</td>\n",
              "      <td>('en', 'en')</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>S &gt; NP VP .</td>\n",
              "      <td>S &gt; NP VP .</td>\n",
              "      <td>True</td>\n",
              "      <td>I'm less than 15 minutes away from Matobo's ho...</td>\n",
              "      <td>I'll be with Matoby in fifteen minutes.</td>\n",
              "      <td>(0.1111111111111111, 0.14285714285714285, 0.125)</td>\n",
              "      <td>(-1.0, True)</td>\n",
              "      <td>0</td>\n",
              "      <td>('en', 'en')</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>S &gt; SBAR , NP VP .</td>\n",
              "      <td>S &gt; SBAR , NP VP .</td>\n",
              "      <td>True</td>\n",
              "      <td>If it'd exist, I'd free it.</td>\n",
              "      <td>if he really existed, I would have freed him.</td>\n",
              "      <td>(0.16666666666666666, 0.1111111111111111, 0.13...</td>\n",
              "      <td>(-1.0, True)</td>\n",
              "      <td>0</td>\n",
              "      <td>('en', 'en')</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                   a  ...  g             h\n",
              "0           0         S > NP VP .  ...  2  ('en', 'en')\n",
              "1           1         S > NP VP .  ...  2  ('en', 'en')\n",
              "2           2         S > NP VP .  ...  0  ('en', 'en')\n",
              "3           3         S > NP VP .  ...  0  ('en', 'en')\n",
              "4           4  S > SBAR , NP VP .  ...  0  ('en', 'en')\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgWCflrKOuBY"
      },
      "source": [
        "data_dev = pd.read_csv(\"data/dev.tsv\", sep=\"\\t\")#.astype(str)\n",
        "#data_dev_1 = data_dev[data_dev['label']==1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O38qYPeyOJbA",
        "outputId": "1b0339b1-8728-404f-b89a-81cd1f3c91ac"
      },
      "source": [
        "print('Training data: ', data_train.shape)\n",
        "print('Validation data: ', data_dev.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data:  (73062, 10)\n",
            "Validation data:  (1492, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4eRy5oVCUny"
      },
      "source": [
        "# **Set arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321,
          "referenced_widgets": [
            "b7915c3393144333bdd7790043764ae3",
            "e167ff73d20244979a49f4d5cbb4fd9f",
            "cbddbbf73942495aadcd5d7ed2aa905d",
            "6c24a93a388e46e388d38822151a6e06",
            "c55883e103da4c5f9181efd5d3256197",
            "ffd961778399467c9e22992a30d0fddf",
            "395f06762e394b958450ed2d0c6c33b7",
            "00a5559c57cc4803a4d2aaa03a0367c6"
          ]
        },
        "id": "7UZzosIjEr8I",
        "outputId": "ff1b7127-926b-44df-a136-02bd0a25477a"
      },
      "source": [
        "args_dict = dict(\n",
        "    data_dir=\"data\", # path for data files\n",
        "    output_dir=\"t5_paraphrase_M2\", # path to save the checkpoints\n",
        "    model_name_or_path='t5-large',\n",
        "    tokenizer_name_or_path='t5-large',\n",
        "    max_seq_length=256,\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=0.0,\n",
        "    adam_epsilon=1e-8,\n",
        "    warmup_steps=0,\n",
        "    train_batch_size=1,\n",
        "    eval_batch_size=1,\n",
        "    num_train_epochs=3,\n",
        "    gradient_accumulation_steps=16,\n",
        "    n_gpu=1,\n",
        "    early_stop_callback=False,\n",
        "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
        "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
        "    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "train_path = \"data/train.tsv\"\n",
        "val_path = \"data/dev.tsv\"\n",
        "\n",
        "train = pd.read_csv(train_path, sep=\"\\t\").astype(str)\n",
        "print(train.head())\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-large')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Unnamed: 0                   a  ...  g             h\n",
            "0          0         S > NP VP .  ...  2  ('en', 'en')\n",
            "1          1         S > NP VP .  ...  2  ('en', 'en')\n",
            "2          2         S > NP VP .  ...  0  ('en', 'en')\n",
            "3          3         S > NP VP .  ...  0  ('en', 'en')\n",
            "4          4  S > SBAR , NP VP .  ...  0  ('en', 'en')\n",
            "\n",
            "[5 rows x 10 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 140473723411792 acquired on /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f.lock\n",
            "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpkj0pio49\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7915c3393144333bdd7790043764ae3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model in cache at /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n",
            "INFO:filelock:Lock 140473723411792 released on /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f.lock\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model from cache at /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p5OZ7pRCel5"
      },
      "source": [
        "# **ParaphraseDataset()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5UwYTUAGAo2"
      },
      "source": [
        "class ParaphraseDataset(Dataset):\n",
        "    def __init__(self, tokenizer, data_dir, type_path, max_len=512):\n",
        "        self.path = os.path.join(data_dir, type_path + '.tsv')\n",
        "\n",
        "        self.source_column = \"sentence1\"\n",
        "        self.target_column = \"sentence2\"\n",
        "        self.data = pd.read_csv(self.path, sep=\"\\t\").astype(str)\n",
        "\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
        "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
        "\n",
        "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
        "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
        "\n",
        "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
        "\n",
        "    def _build(self):\n",
        "        for idx in range(len(self.data)):\n",
        "            input_, target = self.data.loc[idx, self.source_column], self.data.loc[idx, self.target_column]\n",
        "\n",
        "            input_ = \"paraphrase: \"+ input_ + ' </s>'\n",
        "            target = target + \" </s>\"\n",
        "\n",
        "            # tokenize inputs\n",
        "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "                [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\", truncation='longest_first'\n",
        "            )\n",
        "            # tokenize targets\n",
        "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "                [target], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\", truncation='longest_first'\n",
        "            )\n",
        "\n",
        "            self.inputs.append(tokenized_inputs)\n",
        "            self.targets.append(tokenized_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkVUkHFMClrH"
      },
      "source": [
        "# **Start training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "aed05c5c315740368f83e6eca532f9f9",
            "fdd0e3c41d0b4bc19f7d314e26ef0f48",
            "f3f31364a895473c9ab849ac42937efe",
            "4e0e9d57c434447a9c816835add9640f",
            "2fd4e7e83993403bb641e7ad66470b5c",
            "2640a61685994e35b0e0e5c395f3dfb2",
            "a76ac0af599543fa87b7eb1a4ac0ea64",
            "50b25b8a3303423ca8bf5f17b21be399",
            "2bbe2aef2b814e42b4315ccf8f8f2a32",
            "242f9feb9e5a4b7b954f02e53ad4328b",
            "36db1105ef7c4bd380eddede92112af3",
            "e856286cad1441d180294dd5d29208e5",
            "b0619d17d04d4c5ca883ee7f635685d2",
            "21660c8cfb7140f08abbe5310cb75b7c",
            "bfc85d822cb74aeea767cb868d23ca5b",
            "78923b3fad624df3b964c2c2d9c0d5ef",
            "dd4baf5b8268439f879e8400ac60cdad",
            "60710ed13bb04ef5851d78d364118757",
            "a8d1da893a74448fbb175e3fc8a3f789",
            "076c25cf95324b759d3edec9d605c113",
            "4e3a84d8fe00411f939ea69c5718c686",
            "867b63776c804d44842216cd4a3d0173",
            "4ae87ed87631488aae873038f787c16d",
            "e75f7b8c1b5e4eb09a056429a1c9c8d2",
            "9532733e4f4841bf8ac7b05c9135d6fd",
            "8a833ff8b705488ea96dc9b33a9f4ae3",
            "215f7588e14e44c6830b5aa70e03b7cb",
            "97b5a61f64594942a855d2ed696aaf90",
            "961bfae912b54192b6652735b70004c8",
            "005b10eec76c49e9b71a2eb2cbbb66c9",
            "190a4ae6d30648198756ae0c2e3a9e11",
            "ddc6a4f9ecbc44ebb4c159c0cb77876b",
            "00e754584ca14e14af2b9b43513ec7f4",
            "7a441311c74044e8bba66e96bbe18896",
            "c868cd6bbe5e44608daade6b6d403471",
            "c7ac1c2536014399aade316a891be30d",
            "df7b38dc32164badbb0062939603387e",
            "8516740de6104b26855b2b727d260d60",
            "054fb5b919394620a271a968bea719ff",
            "faeb144a7b244d7495c759259a1eb2bd",
            "9e62fe18fc384995a1aa94c1491fa8a8",
            "4d5eed1db37b4f0d9d84524809c4545f",
            "b1ef05eadbaf4327af8bf59197777fb4",
            "e0a22f484d584dfd833eba41afa8f073",
            "14c0ca72da7e4fcebd3fedf8bb1e8a97",
            "2f934076c9bf4193948b060df408d062",
            "1c2bc891bc3d4593b52179e3f133d2b0",
            "dd74512c9fe944ffa2874ab4e42104c8",
            "e502d6697c1c49719aa55826a81f53f2"
          ]
        },
        "id": "n41vU8IbGhIM",
        "outputId": "0ae84cfe-efd2-4b0c-c497-edc483979644"
      },
      "source": [
        "dataset = ParaphraseDataset(tokenizer, 'data', 'dev', 256)\n",
        "print(\"Val dataset: \",len(dataset))\n",
        "\n",
        "data = dataset[61]\n",
        "print(tokenizer.decode(data['source_ids']))\n",
        "print(tokenizer.decode(data['target_ids']))\n",
        "\n",
        "if not os.path.exists('t5_paraphrase_M2'):\n",
        "    os.makedirs('t5_paraphrase_M2')\n",
        "\n",
        "args_dict.update({'data_dir': 'data', 'output_dir': 't5_paraphrase_M2', 'num_train_epochs':3,'max_seq_length':256})\n",
        "args = argparse.Namespace(**args_dict)\n",
        "print(args_dict)\n",
        "\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n",
        ")\n",
        "\n",
        "train_params = dict(\n",
        "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
        "    gpus=args.n_gpu,\n",
        "    max_epochs=args.num_train_epochs,\n",
        " #   early_stop_callback=False,\n",
        "    precision= 16 if args.fp_16 else 32,\n",
        "    amp_level=args.opt_level,\n",
        "    gradient_clip_val=args.max_grad_norm,\n",
        "    checkpoint_callback=checkpoint_callback,\n",
        "    callbacks=[LoggingCallback()],\n",
        ")\n",
        "\n",
        "def get_dataset(tokenizer, type_path, args):\n",
        "  return ParaphraseDataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length)\n",
        "\n",
        "print (\"Initialize model\")\n",
        "model = T5FineTuner(args)\n",
        "\n",
        "trainer = pl.Trainer(**train_params)\n",
        "\n",
        "print (\" Training model\")\n",
        "trainer.fit(model)\n",
        "\n",
        "print (\"training finished\")\n",
        "\n",
        "print (\"Saving model\")\n",
        "model.model.save_pretrained('t5_paraphrase_M2')\n",
        "\n",
        "print (\"Model saved\")\n",
        "\n",
        "!cp \"/content/t5_paraphrase_M2/\" -a \"/content/drive/My Drive/\"\n",
        "!cp \"/content/lightning_logs/\" -a \"/content/drive/My Drive/\"\n",
        "print (\"Copied the final folder to Google Drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Val dataset:  1492\n",
            "paraphrase: It made a hard, sickening sound when it struck the floor.\n",
            "when he hit the ground, he made an unpleasant, dull sound.\n",
            "{'data_dir': 'data', 'output_dir': 't5_paraphrase', 'model_name_or_path': 't5-large', 'tokenizer_name_or_path': 't5-large', 'max_seq_length': 256, 'learning_rate': 0.0003, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'train_batch_size': 1, 'eval_batch_size': 1, 'num_train_epochs': 3, 'gradient_accumulation_steps': 16, 'n_gpu': 1, 'early_stop_callback': False, 'fp_16': False, 'opt_level': 'O1', 'max_grad_norm': 1.0, 'seed': 42}\n",
            "Initialize model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 140473702933776 acquired on /root/.cache/torch/transformers/0e9978f992c9b90cd05d080648b1b1c8aabc3f931f62781fa8fcbc281eba168d.ba29edd8b0c069c672abbe0f807c1cd7cac52350f14d193ba0a0ef5cdb9a255e.lock\n",
            "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpuu7icmcz\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aed05c5c315740368f83e6eca532f9f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1200.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-config.json in cache at /root/.cache/torch/transformers/0e9978f992c9b90cd05d080648b1b1c8aabc3f931f62781fa8fcbc281eba168d.ba29edd8b0c069c672abbe0f807c1cd7cac52350f14d193ba0a0ef5cdb9a255e\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/0e9978f992c9b90cd05d080648b1b1c8aabc3f931f62781fa8fcbc281eba168d.ba29edd8b0c069c672abbe0f807c1cd7cac52350f14d193ba0a0ef5cdb9a255e\n",
            "INFO:filelock:Lock 140473702933776 released on /root/.cache/torch/transformers/0e9978f992c9b90cd05d080648b1b1c8aabc3f931f62781fa8fcbc281eba168d.ba29edd8b0c069c672abbe0f807c1cd7cac52350f14d193ba0a0ef5cdb9a255e.lock\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-config.json from cache at /root/.cache/torch/transformers/0e9978f992c9b90cd05d080648b1b1c8aabc3f931f62781fa8fcbc281eba168d.ba29edd8b0c069c672abbe0f807c1cd7cac52350f14d193ba0a0ef5cdb9a255e\n",
            "INFO:transformers.configuration_utils:Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 4096,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_heads\": 16,\n",
            "  \"num_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 140473703328016 acquired on /root/.cache/torch/transformers/e47fdf946478fcd76239a89ab1db1545af6261da0f9be758eb538a22de9553fc.f7406fdda08cdd666e1b81685deafd24a40ba2d5579384751f9f7023254ffb5b.lock\n",
            "INFO:transformers.file_utils:https://cdn.huggingface.co/t5-large-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp3v1evqtj\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2bbe2aef2b814e42b4315ccf8f8f2a32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2950825948.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.file_utils:storing https://cdn.huggingface.co/t5-large-pytorch_model.bin in cache at /root/.cache/torch/transformers/e47fdf946478fcd76239a89ab1db1545af6261da0f9be758eb538a22de9553fc.f7406fdda08cdd666e1b81685deafd24a40ba2d5579384751f9f7023254ffb5b\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/e47fdf946478fcd76239a89ab1db1545af6261da0f9be758eb538a22de9553fc.f7406fdda08cdd666e1b81685deafd24a40ba2d5579384751f9f7023254ffb5b\n",
            "INFO:filelock:Lock 140473703328016 released on /root/.cache/torch/transformers/e47fdf946478fcd76239a89ab1db1545af6261da0f9be758eb538a22de9553fc.f7406fdda08cdd666e1b81685deafd24a40ba2d5579384751f9f7023254ffb5b.lock\n",
            "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/t5-large-pytorch_model.bin from cache at /root/.cache/torch/transformers/e47fdf946478fcd76239a89ab1db1545af6261da0f9be758eb538a22de9553fc.f7406fdda08cdd666e1b81685deafd24a40ba2d5579384751f9f7023254ffb5b\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.modeling_utils:Weights of T5ForConditionalGeneration not initialized from pretrained model: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model from cache at /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n",
            "INFO:lightning:GPU available: True, used: True\n",
            "INFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Training model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:lightning:\n",
            "     | Name                                                                  | Type                       | Params\n",
            "-----------------------------------------------------------------------------------------------------------------\n",
            "0    | model                                                                 | T5ForConditionalGeneration | 737 M \n",
            "1    | model.shared                                                          | Embedding                  | 32 M  \n",
            "2    | model.encoder                                                         | T5Stack                    | 334 M \n",
            "3    | model.encoder.block                                                   | ModuleList                 | 302 M \n",
            "4    | model.encoder.block.0                                                 | T5Block                    | 12 M  \n",
            "5    | model.encoder.block.0.layer                                           | ModuleList                 | 12 M  \n",
            "6    | model.encoder.block.0.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "7    | model.encoder.block.0.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "8    | model.encoder.block.0.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "9    | model.encoder.block.0.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "10   | model.encoder.block.0.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "11   | model.encoder.block.0.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "12   | model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias   | Embedding                  | 512   \n",
            "13   | model.encoder.block.0.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "14   | model.encoder.block.0.layer.0.dropout                                 | Dropout                    | 0     \n",
            "15   | model.encoder.block.0.layer.1                                         | T5LayerFF                  | 8 M   \n",
            "16   | model.encoder.block.0.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "17   | model.encoder.block.0.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "18   | model.encoder.block.0.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "19   | model.encoder.block.0.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "20   | model.encoder.block.0.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "21   | model.encoder.block.0.layer.1.dropout                                 | Dropout                    | 0     \n",
            "22   | model.encoder.block.1                                                 | T5Block                    | 12 M  \n",
            "23   | model.encoder.block.1.layer                                           | ModuleList                 | 12 M  \n",
            "24   | model.encoder.block.1.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "25   | model.encoder.block.1.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "26   | model.encoder.block.1.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "27   | model.encoder.block.1.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "28   | model.encoder.block.1.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "29   | model.encoder.block.1.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "30   | model.encoder.block.1.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "31   | model.encoder.block.1.layer.0.dropout                                 | Dropout                    | 0     \n",
            "32   | model.encoder.block.1.layer.1                                         | T5LayerFF                  | 8 M   \n",
            "33   | model.encoder.block.1.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "34   | model.encoder.block.1.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "35   | model.encoder.block.1.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "36   | model.encoder.block.1.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "37   | model.encoder.block.1.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "38   | model.encoder.block.1.layer.1.dropout                                 | Dropout                    | 0     \n",
            "39   | model.encoder.block.2                                                 | T5Block                    | 12 M  \n",
            "40   | model.encoder.block.2.layer                                           | ModuleList                 | 12 M  \n",
            "41   | model.encoder.block.2.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "42   | model.encoder.block.2.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "43   | model.encoder.block.2.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "44   | model.encoder.block.2.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "45   | model.encoder.block.2.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "46   | model.encoder.block.2.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "47   | model.encoder.block.2.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "48   | model.encoder.block.2.layer.0.dropout                                 | Dropout                    | 0     \n",
            "49   | model.encoder.block.2.layer.1                                         | T5LayerFF                  | 8 M   \n",
            "50   | model.encoder.block.2.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "51   | model.encoder.block.2.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "52   | model.encoder.block.2.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "53   | model.encoder.block.2.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "54   | model.encoder.block.2.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "55   | model.encoder.block.2.layer.1.dropout                                 | Dropout                    | 0     \n",
            "56   | model.encoder.block.3                                                 | T5Block                    | 12 M  \n",
            "57   | model.encoder.block.3.layer                                           | ModuleList                 | 12 M  \n",
            "58   | model.encoder.block.3.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "59   | model.encoder.block.3.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "60   | model.encoder.block.3.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "61   | model.encoder.block.3.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "62   | model.encoder.block.3.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "63   | model.encoder.block.3.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "64   | model.encoder.block.3.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "65   | model.encoder.block.3.layer.0.dropout                                 | Dropout                    | 0     \n",
            "66   | model.encoder.block.3.layer.1                                         | T5LayerFF                  | 8 M   \n",
            "67   | model.encoder.block.3.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "68   | model.encoder.block.3.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "69   | model.encoder.block.3.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "70   | model.encoder.block.3.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "71   | model.encoder.block.3.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "72   | model.encoder.block.3.layer.1.dropout                                 | Dropout                    | 0     \n",
            "73   | model.encoder.block.4                                                 | T5Block                    | 12 M  \n",
            "74   | model.encoder.block.4.layer                                           | ModuleList                 | 12 M  \n",
            "75   | model.encoder.block.4.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "76   | model.encoder.block.4.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "77   | model.encoder.block.4.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "78   | model.encoder.block.4.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "79   | model.encoder.block.4.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "80   | model.encoder.block.4.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "81   | model.encoder.block.4.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "82   | model.encoder.block.4.layer.0.dropout                                 | Dropout                    | 0     \n",
            "83   | model.encoder.block.4.layer.1                                         | T5LayerFF                  | 8 M   \n",
            "84   | model.encoder.block.4.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "85   | model.encoder.block.4.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "86   | model.encoder.block.4.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "87   | model.encoder.block.4.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "88   | model.encoder.block.4.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "89   | model.encoder.block.4.layer.1.dropout                                 | Dropout                    | 0     \n",
            "90   | model.encoder.block.5                                                 | T5Block                    | 12 M  \n",
            "91   | model.encoder.block.5.layer                                           | ModuleList                 | 12 M  \n",
            "92   | model.encoder.block.5.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "93   | model.encoder.block.5.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "94   | model.encoder.block.5.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "95   | model.encoder.block.5.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "96   | model.encoder.block.5.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "97   | model.encoder.block.5.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "98   | model.encoder.block.5.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "99   | model.encoder.block.5.layer.0.dropout                                 | Dropout                    | 0     \n",
            "100  | model.encoder.block.5.layer.1                                         | T5LayerFF                  | 8 M   \n",
            "101  | model.encoder.block.5.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "102  | model.encoder.block.5.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "103  | model.encoder.block.5.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "104  | model.encoder.block.5.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "105  | model.encoder.block.5.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "106  | model.encoder.block.5.layer.1.dropout                                 | Dropout                    | 0     \n",
            "107  | model.encoder.block.6                                                 | T5Block                    | 12 M  \n",
            "108  | model.encoder.block.6.layer                                           | ModuleList                 | 12 M  \n",
            "109  | model.encoder.block.6.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "110  | model.encoder.block.6.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "111  | model.encoder.block.6.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "112  | model.encoder.block.6.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "113  | model.encoder.block.6.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "114  | model.encoder.block.6.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "115  | model.encoder.block.6.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "116  | model.encoder.block.6.layer.0.dropout                                 | Dropout                    | 0     \n",
            "117  | model.encoder.block.6.layer.1                                         | T5LayerFF                  | 8 M   \n",
            "118  | model.encoder.block.6.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "119  | model.encoder.block.6.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "120  | model.encoder.block.6.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "121  | model.encoder.block.6.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "122  | model.encoder.block.6.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "123  | model.encoder.block.6.layer.1.dropout                                 | Dropout                    | 0     \n",
            "124  | model.encoder.block.7                                                 | T5Block                    | 12 M  \n",
            "125  | model.encoder.block.7.layer                                           | ModuleList                 | 12 M  \n",
            "126  | model.encoder.block.7.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "127  | model.encoder.block.7.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "128  | model.encoder.block.7.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "129  | model.encoder.block.7.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "130  | model.encoder.block.7.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "131  | model.encoder.block.7.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "132  | model.encoder.block.7.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "133  | model.encoder.block.7.layer.0.dropout                                 | Dropout                    | 0     \n",
            "134  | model.encoder.block.7.layer.1                                         | T5LayerFF                  | 8 M   \n",
            "135  | model.encoder.block.7.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "136  | model.encoder.block.7.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "137  | model.encoder.block.7.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "138  | model.encoder.block.7.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "139  | model.encoder.block.7.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "140  | model.encoder.block.7.layer.1.dropout                                 | Dropout                    | 0     \n",
            "141  | model.encoder.block.8                                                 | T5Block                    | 12 M  \n",
            "142  | model.encoder.block.8.layer                                           | ModuleList                 | 12 M  \n",
            "143  | model.encoder.block.8.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "144  | model.encoder.block.8.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "145  | model.encoder.block.8.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "146  | model.encoder.block.8.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "147  | model.encoder.block.8.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "148  | model.encoder.block.8.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "149  | model.encoder.block.8.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "150  | model.encoder.block.8.layer.0.dropout                                 | Dropout                    | 0     \n",
            "151  | model.encoder.block.8.layer.1                                         | T5LayerFF                  | 8 M   \n",
            "152  | model.encoder.block.8.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "153  | model.encoder.block.8.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "154  | model.encoder.block.8.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "155  | model.encoder.block.8.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "156  | model.encoder.block.8.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "157  | model.encoder.block.8.layer.1.dropout                                 | Dropout                    | 0     \n",
            "158  | model.encoder.block.9                                                 | T5Block                    | 12 M  \n",
            "159  | model.encoder.block.9.layer                                           | ModuleList                 | 12 M  \n",
            "160  | model.encoder.block.9.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "161  | model.encoder.block.9.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "162  | model.encoder.block.9.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "163  | model.encoder.block.9.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "164  | model.encoder.block.9.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "165  | model.encoder.block.9.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "166  | model.encoder.block.9.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "167  | model.encoder.block.9.layer.0.dropout                                 | Dropout                    | 0     \n",
            "168  | model.encoder.block.9.layer.1                                         | T5LayerFF                  | 8 M   \n",
            "169  | model.encoder.block.9.layer.1.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "170  | model.encoder.block.9.layer.1.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "171  | model.encoder.block.9.layer.1.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "172  | model.encoder.block.9.layer.1.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "173  | model.encoder.block.9.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "174  | model.encoder.block.9.layer.1.dropout                                 | Dropout                    | 0     \n",
            "175  | model.encoder.block.10                                                | T5Block                    | 12 M  \n",
            "176  | model.encoder.block.10.layer                                          | ModuleList                 | 12 M  \n",
            "177  | model.encoder.block.10.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "178  | model.encoder.block.10.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "179  | model.encoder.block.10.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "180  | model.encoder.block.10.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "181  | model.encoder.block.10.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "182  | model.encoder.block.10.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "183  | model.encoder.block.10.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "184  | model.encoder.block.10.layer.0.dropout                                | Dropout                    | 0     \n",
            "185  | model.encoder.block.10.layer.1                                        | T5LayerFF                  | 8 M   \n",
            "186  | model.encoder.block.10.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "187  | model.encoder.block.10.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "188  | model.encoder.block.10.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "189  | model.encoder.block.10.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "190  | model.encoder.block.10.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "191  | model.encoder.block.10.layer.1.dropout                                | Dropout                    | 0     \n",
            "192  | model.encoder.block.11                                                | T5Block                    | 12 M  \n",
            "193  | model.encoder.block.11.layer                                          | ModuleList                 | 12 M  \n",
            "194  | model.encoder.block.11.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "195  | model.encoder.block.11.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "196  | model.encoder.block.11.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "197  | model.encoder.block.11.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "198  | model.encoder.block.11.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "199  | model.encoder.block.11.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "200  | model.encoder.block.11.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "201  | model.encoder.block.11.layer.0.dropout                                | Dropout                    | 0     \n",
            "202  | model.encoder.block.11.layer.1                                        | T5LayerFF                  | 8 M   \n",
            "203  | model.encoder.block.11.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "204  | model.encoder.block.11.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "205  | model.encoder.block.11.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "206  | model.encoder.block.11.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "207  | model.encoder.block.11.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "208  | model.encoder.block.11.layer.1.dropout                                | Dropout                    | 0     \n",
            "209  | model.encoder.block.12                                                | T5Block                    | 12 M  \n",
            "210  | model.encoder.block.12.layer                                          | ModuleList                 | 12 M  \n",
            "211  | model.encoder.block.12.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "212  | model.encoder.block.12.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "213  | model.encoder.block.12.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "214  | model.encoder.block.12.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "215  | model.encoder.block.12.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "216  | model.encoder.block.12.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "217  | model.encoder.block.12.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "218  | model.encoder.block.12.layer.0.dropout                                | Dropout                    | 0     \n",
            "219  | model.encoder.block.12.layer.1                                        | T5LayerFF                  | 8 M   \n",
            "220  | model.encoder.block.12.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "221  | model.encoder.block.12.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "222  | model.encoder.block.12.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "223  | model.encoder.block.12.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "224  | model.encoder.block.12.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "225  | model.encoder.block.12.layer.1.dropout                                | Dropout                    | 0     \n",
            "226  | model.encoder.block.13                                                | T5Block                    | 12 M  \n",
            "227  | model.encoder.block.13.layer                                          | ModuleList                 | 12 M  \n",
            "228  | model.encoder.block.13.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "229  | model.encoder.block.13.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "230  | model.encoder.block.13.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "231  | model.encoder.block.13.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "232  | model.encoder.block.13.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "233  | model.encoder.block.13.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "234  | model.encoder.block.13.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "235  | model.encoder.block.13.layer.0.dropout                                | Dropout                    | 0     \n",
            "236  | model.encoder.block.13.layer.1                                        | T5LayerFF                  | 8 M   \n",
            "237  | model.encoder.block.13.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "238  | model.encoder.block.13.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "239  | model.encoder.block.13.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "240  | model.encoder.block.13.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "241  | model.encoder.block.13.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "242  | model.encoder.block.13.layer.1.dropout                                | Dropout                    | 0     \n",
            "243  | model.encoder.block.14                                                | T5Block                    | 12 M  \n",
            "244  | model.encoder.block.14.layer                                          | ModuleList                 | 12 M  \n",
            "245  | model.encoder.block.14.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "246  | model.encoder.block.14.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "247  | model.encoder.block.14.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "248  | model.encoder.block.14.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "249  | model.encoder.block.14.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "250  | model.encoder.block.14.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "251  | model.encoder.block.14.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "252  | model.encoder.block.14.layer.0.dropout                                | Dropout                    | 0     \n",
            "253  | model.encoder.block.14.layer.1                                        | T5LayerFF                  | 8 M   \n",
            "254  | model.encoder.block.14.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "255  | model.encoder.block.14.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "256  | model.encoder.block.14.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "257  | model.encoder.block.14.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "258  | model.encoder.block.14.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "259  | model.encoder.block.14.layer.1.dropout                                | Dropout                    | 0     \n",
            "260  | model.encoder.block.15                                                | T5Block                    | 12 M  \n",
            "261  | model.encoder.block.15.layer                                          | ModuleList                 | 12 M  \n",
            "262  | model.encoder.block.15.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "263  | model.encoder.block.15.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "264  | model.encoder.block.15.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "265  | model.encoder.block.15.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "266  | model.encoder.block.15.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "267  | model.encoder.block.15.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "268  | model.encoder.block.15.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "269  | model.encoder.block.15.layer.0.dropout                                | Dropout                    | 0     \n",
            "270  | model.encoder.block.15.layer.1                                        | T5LayerFF                  | 8 M   \n",
            "271  | model.encoder.block.15.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "272  | model.encoder.block.15.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "273  | model.encoder.block.15.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "274  | model.encoder.block.15.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "275  | model.encoder.block.15.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "276  | model.encoder.block.15.layer.1.dropout                                | Dropout                    | 0     \n",
            "277  | model.encoder.block.16                                                | T5Block                    | 12 M  \n",
            "278  | model.encoder.block.16.layer                                          | ModuleList                 | 12 M  \n",
            "279  | model.encoder.block.16.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "280  | model.encoder.block.16.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "281  | model.encoder.block.16.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "282  | model.encoder.block.16.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "283  | model.encoder.block.16.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "284  | model.encoder.block.16.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "285  | model.encoder.block.16.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "286  | model.encoder.block.16.layer.0.dropout                                | Dropout                    | 0     \n",
            "287  | model.encoder.block.16.layer.1                                        | T5LayerFF                  | 8 M   \n",
            "288  | model.encoder.block.16.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "289  | model.encoder.block.16.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "290  | model.encoder.block.16.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "291  | model.encoder.block.16.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "292  | model.encoder.block.16.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "293  | model.encoder.block.16.layer.1.dropout                                | Dropout                    | 0     \n",
            "294  | model.encoder.block.17                                                | T5Block                    | 12 M  \n",
            "295  | model.encoder.block.17.layer                                          | ModuleList                 | 12 M  \n",
            "296  | model.encoder.block.17.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "297  | model.encoder.block.17.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "298  | model.encoder.block.17.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "299  | model.encoder.block.17.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "300  | model.encoder.block.17.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "301  | model.encoder.block.17.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "302  | model.encoder.block.17.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "303  | model.encoder.block.17.layer.0.dropout                                | Dropout                    | 0     \n",
            "304  | model.encoder.block.17.layer.1                                        | T5LayerFF                  | 8 M   \n",
            "305  | model.encoder.block.17.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "306  | model.encoder.block.17.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "307  | model.encoder.block.17.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "308  | model.encoder.block.17.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "309  | model.encoder.block.17.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "310  | model.encoder.block.17.layer.1.dropout                                | Dropout                    | 0     \n",
            "311  | model.encoder.block.18                                                | T5Block                    | 12 M  \n",
            "312  | model.encoder.block.18.layer                                          | ModuleList                 | 12 M  \n",
            "313  | model.encoder.block.18.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "314  | model.encoder.block.18.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "315  | model.encoder.block.18.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "316  | model.encoder.block.18.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "317  | model.encoder.block.18.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "318  | model.encoder.block.18.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "319  | model.encoder.block.18.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "320  | model.encoder.block.18.layer.0.dropout                                | Dropout                    | 0     \n",
            "321  | model.encoder.block.18.layer.1                                        | T5LayerFF                  | 8 M   \n",
            "322  | model.encoder.block.18.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "323  | model.encoder.block.18.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "324  | model.encoder.block.18.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "325  | model.encoder.block.18.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "326  | model.encoder.block.18.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "327  | model.encoder.block.18.layer.1.dropout                                | Dropout                    | 0     \n",
            "328  | model.encoder.block.19                                                | T5Block                    | 12 M  \n",
            "329  | model.encoder.block.19.layer                                          | ModuleList                 | 12 M  \n",
            "330  | model.encoder.block.19.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "331  | model.encoder.block.19.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "332  | model.encoder.block.19.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "333  | model.encoder.block.19.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "334  | model.encoder.block.19.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "335  | model.encoder.block.19.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "336  | model.encoder.block.19.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "337  | model.encoder.block.19.layer.0.dropout                                | Dropout                    | 0     \n",
            "338  | model.encoder.block.19.layer.1                                        | T5LayerFF                  | 8 M   \n",
            "339  | model.encoder.block.19.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "340  | model.encoder.block.19.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "341  | model.encoder.block.19.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "342  | model.encoder.block.19.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "343  | model.encoder.block.19.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "344  | model.encoder.block.19.layer.1.dropout                                | Dropout                    | 0     \n",
            "345  | model.encoder.block.20                                                | T5Block                    | 12 M  \n",
            "346  | model.encoder.block.20.layer                                          | ModuleList                 | 12 M  \n",
            "347  | model.encoder.block.20.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "348  | model.encoder.block.20.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "349  | model.encoder.block.20.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "350  | model.encoder.block.20.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "351  | model.encoder.block.20.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "352  | model.encoder.block.20.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "353  | model.encoder.block.20.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "354  | model.encoder.block.20.layer.0.dropout                                | Dropout                    | 0     \n",
            "355  | model.encoder.block.20.layer.1                                        | T5LayerFF                  | 8 M   \n",
            "356  | model.encoder.block.20.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "357  | model.encoder.block.20.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "358  | model.encoder.block.20.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "359  | model.encoder.block.20.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "360  | model.encoder.block.20.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "361  | model.encoder.block.20.layer.1.dropout                                | Dropout                    | 0     \n",
            "362  | model.encoder.block.21                                                | T5Block                    | 12 M  \n",
            "363  | model.encoder.block.21.layer                                          | ModuleList                 | 12 M  \n",
            "364  | model.encoder.block.21.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "365  | model.encoder.block.21.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "366  | model.encoder.block.21.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "367  | model.encoder.block.21.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "368  | model.encoder.block.21.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "369  | model.encoder.block.21.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "370  | model.encoder.block.21.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "371  | model.encoder.block.21.layer.0.dropout                                | Dropout                    | 0     \n",
            "372  | model.encoder.block.21.layer.1                                        | T5LayerFF                  | 8 M   \n",
            "373  | model.encoder.block.21.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "374  | model.encoder.block.21.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "375  | model.encoder.block.21.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "376  | model.encoder.block.21.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "377  | model.encoder.block.21.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "378  | model.encoder.block.21.layer.1.dropout                                | Dropout                    | 0     \n",
            "379  | model.encoder.block.22                                                | T5Block                    | 12 M  \n",
            "380  | model.encoder.block.22.layer                                          | ModuleList                 | 12 M  \n",
            "381  | model.encoder.block.22.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "382  | model.encoder.block.22.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "383  | model.encoder.block.22.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "384  | model.encoder.block.22.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "385  | model.encoder.block.22.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "386  | model.encoder.block.22.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "387  | model.encoder.block.22.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "388  | model.encoder.block.22.layer.0.dropout                                | Dropout                    | 0     \n",
            "389  | model.encoder.block.22.layer.1                                        | T5LayerFF                  | 8 M   \n",
            "390  | model.encoder.block.22.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "391  | model.encoder.block.22.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "392  | model.encoder.block.22.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "393  | model.encoder.block.22.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "394  | model.encoder.block.22.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "395  | model.encoder.block.22.layer.1.dropout                                | Dropout                    | 0     \n",
            "396  | model.encoder.block.23                                                | T5Block                    | 12 M  \n",
            "397  | model.encoder.block.23.layer                                          | ModuleList                 | 12 M  \n",
            "398  | model.encoder.block.23.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "399  | model.encoder.block.23.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "400  | model.encoder.block.23.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "401  | model.encoder.block.23.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "402  | model.encoder.block.23.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "403  | model.encoder.block.23.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "404  | model.encoder.block.23.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "405  | model.encoder.block.23.layer.0.dropout                                | Dropout                    | 0     \n",
            "406  | model.encoder.block.23.layer.1                                        | T5LayerFF                  | 8 M   \n",
            "407  | model.encoder.block.23.layer.1.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "408  | model.encoder.block.23.layer.1.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "409  | model.encoder.block.23.layer.1.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "410  | model.encoder.block.23.layer.1.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "411  | model.encoder.block.23.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "412  | model.encoder.block.23.layer.1.dropout                                | Dropout                    | 0     \n",
            "413  | model.encoder.final_layer_norm                                        | T5LayerNorm                | 1 K   \n",
            "414  | model.encoder.dropout                                                 | Dropout                    | 0     \n",
            "415  | model.decoder                                                         | T5Stack                    | 435 M \n",
            "416  | model.decoder.block                                                   | ModuleList                 | 402 M \n",
            "417  | model.decoder.block.0                                                 | T5Block                    | 16 M  \n",
            "418  | model.decoder.block.0.layer                                           | ModuleList                 | 16 M  \n",
            "419  | model.decoder.block.0.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "420  | model.decoder.block.0.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "421  | model.decoder.block.0.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "422  | model.decoder.block.0.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "423  | model.decoder.block.0.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "424  | model.decoder.block.0.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "425  | model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias   | Embedding                  | 512   \n",
            "426  | model.decoder.block.0.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "427  | model.decoder.block.0.layer.0.dropout                                 | Dropout                    | 0     \n",
            "428  | model.decoder.block.0.layer.1                                         | T5LayerCrossAttention      | 4 M   \n",
            "429  | model.decoder.block.0.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n",
            "430  | model.decoder.block.0.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n",
            "431  | model.decoder.block.0.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n",
            "432  | model.decoder.block.0.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n",
            "433  | model.decoder.block.0.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n",
            "434  | model.decoder.block.0.layer.1.EncDecAttention.relative_attention_bias | Embedding                  | 512   \n",
            "435  | model.decoder.block.0.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "436  | model.decoder.block.0.layer.1.dropout                                 | Dropout                    | 0     \n",
            "437  | model.decoder.block.0.layer.2                                         | T5LayerFF                  | 8 M   \n",
            "438  | model.decoder.block.0.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "439  | model.decoder.block.0.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "440  | model.decoder.block.0.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "441  | model.decoder.block.0.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "442  | model.decoder.block.0.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "443  | model.decoder.block.0.layer.2.dropout                                 | Dropout                    | 0     \n",
            "444  | model.decoder.block.1                                                 | T5Block                    | 16 M  \n",
            "445  | model.decoder.block.1.layer                                           | ModuleList                 | 16 M  \n",
            "446  | model.decoder.block.1.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "447  | model.decoder.block.1.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "448  | model.decoder.block.1.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "449  | model.decoder.block.1.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "450  | model.decoder.block.1.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "451  | model.decoder.block.1.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "452  | model.decoder.block.1.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "453  | model.decoder.block.1.layer.0.dropout                                 | Dropout                    | 0     \n",
            "454  | model.decoder.block.1.layer.1                                         | T5LayerCrossAttention      | 4 M   \n",
            "455  | model.decoder.block.1.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n",
            "456  | model.decoder.block.1.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n",
            "457  | model.decoder.block.1.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n",
            "458  | model.decoder.block.1.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n",
            "459  | model.decoder.block.1.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n",
            "460  | model.decoder.block.1.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "461  | model.decoder.block.1.layer.1.dropout                                 | Dropout                    | 0     \n",
            "462  | model.decoder.block.1.layer.2                                         | T5LayerFF                  | 8 M   \n",
            "463  | model.decoder.block.1.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "464  | model.decoder.block.1.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "465  | model.decoder.block.1.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "466  | model.decoder.block.1.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "467  | model.decoder.block.1.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "468  | model.decoder.block.1.layer.2.dropout                                 | Dropout                    | 0     \n",
            "469  | model.decoder.block.2                                                 | T5Block                    | 16 M  \n",
            "470  | model.decoder.block.2.layer                                           | ModuleList                 | 16 M  \n",
            "471  | model.decoder.block.2.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "472  | model.decoder.block.2.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "473  | model.decoder.block.2.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "474  | model.decoder.block.2.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "475  | model.decoder.block.2.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "476  | model.decoder.block.2.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "477  | model.decoder.block.2.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "478  | model.decoder.block.2.layer.0.dropout                                 | Dropout                    | 0     \n",
            "479  | model.decoder.block.2.layer.1                                         | T5LayerCrossAttention      | 4 M   \n",
            "480  | model.decoder.block.2.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n",
            "481  | model.decoder.block.2.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n",
            "482  | model.decoder.block.2.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n",
            "483  | model.decoder.block.2.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n",
            "484  | model.decoder.block.2.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n",
            "485  | model.decoder.block.2.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "486  | model.decoder.block.2.layer.1.dropout                                 | Dropout                    | 0     \n",
            "487  | model.decoder.block.2.layer.2                                         | T5LayerFF                  | 8 M   \n",
            "488  | model.decoder.block.2.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "489  | model.decoder.block.2.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "490  | model.decoder.block.2.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "491  | model.decoder.block.2.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "492  | model.decoder.block.2.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "493  | model.decoder.block.2.layer.2.dropout                                 | Dropout                    | 0     \n",
            "494  | model.decoder.block.3                                                 | T5Block                    | 16 M  \n",
            "495  | model.decoder.block.3.layer                                           | ModuleList                 | 16 M  \n",
            "496  | model.decoder.block.3.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "497  | model.decoder.block.3.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "498  | model.decoder.block.3.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "499  | model.decoder.block.3.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "500  | model.decoder.block.3.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "501  | model.decoder.block.3.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "502  | model.decoder.block.3.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "503  | model.decoder.block.3.layer.0.dropout                                 | Dropout                    | 0     \n",
            "504  | model.decoder.block.3.layer.1                                         | T5LayerCrossAttention      | 4 M   \n",
            "505  | model.decoder.block.3.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n",
            "506  | model.decoder.block.3.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n",
            "507  | model.decoder.block.3.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n",
            "508  | model.decoder.block.3.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n",
            "509  | model.decoder.block.3.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n",
            "510  | model.decoder.block.3.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "511  | model.decoder.block.3.layer.1.dropout                                 | Dropout                    | 0     \n",
            "512  | model.decoder.block.3.layer.2                                         | T5LayerFF                  | 8 M   \n",
            "513  | model.decoder.block.3.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "514  | model.decoder.block.3.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "515  | model.decoder.block.3.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "516  | model.decoder.block.3.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "517  | model.decoder.block.3.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "518  | model.decoder.block.3.layer.2.dropout                                 | Dropout                    | 0     \n",
            "519  | model.decoder.block.4                                                 | T5Block                    | 16 M  \n",
            "520  | model.decoder.block.4.layer                                           | ModuleList                 | 16 M  \n",
            "521  | model.decoder.block.4.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "522  | model.decoder.block.4.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "523  | model.decoder.block.4.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "524  | model.decoder.block.4.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "525  | model.decoder.block.4.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "526  | model.decoder.block.4.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "527  | model.decoder.block.4.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "528  | model.decoder.block.4.layer.0.dropout                                 | Dropout                    | 0     \n",
            "529  | model.decoder.block.4.layer.1                                         | T5LayerCrossAttention      | 4 M   \n",
            "530  | model.decoder.block.4.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n",
            "531  | model.decoder.block.4.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n",
            "532  | model.decoder.block.4.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n",
            "533  | model.decoder.block.4.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n",
            "534  | model.decoder.block.4.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n",
            "535  | model.decoder.block.4.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "536  | model.decoder.block.4.layer.1.dropout                                 | Dropout                    | 0     \n",
            "537  | model.decoder.block.4.layer.2                                         | T5LayerFF                  | 8 M   \n",
            "538  | model.decoder.block.4.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "539  | model.decoder.block.4.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "540  | model.decoder.block.4.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "541  | model.decoder.block.4.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "542  | model.decoder.block.4.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "543  | model.decoder.block.4.layer.2.dropout                                 | Dropout                    | 0     \n",
            "544  | model.decoder.block.5                                                 | T5Block                    | 16 M  \n",
            "545  | model.decoder.block.5.layer                                           | ModuleList                 | 16 M  \n",
            "546  | model.decoder.block.5.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "547  | model.decoder.block.5.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "548  | model.decoder.block.5.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "549  | model.decoder.block.5.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "550  | model.decoder.block.5.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "551  | model.decoder.block.5.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "552  | model.decoder.block.5.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "553  | model.decoder.block.5.layer.0.dropout                                 | Dropout                    | 0     \n",
            "554  | model.decoder.block.5.layer.1                                         | T5LayerCrossAttention      | 4 M   \n",
            "555  | model.decoder.block.5.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n",
            "556  | model.decoder.block.5.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n",
            "557  | model.decoder.block.5.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n",
            "558  | model.decoder.block.5.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n",
            "559  | model.decoder.block.5.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n",
            "560  | model.decoder.block.5.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "561  | model.decoder.block.5.layer.1.dropout                                 | Dropout                    | 0     \n",
            "562  | model.decoder.block.5.layer.2                                         | T5LayerFF                  | 8 M   \n",
            "563  | model.decoder.block.5.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "564  | model.decoder.block.5.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "565  | model.decoder.block.5.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "566  | model.decoder.block.5.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "567  | model.decoder.block.5.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "568  | model.decoder.block.5.layer.2.dropout                                 | Dropout                    | 0     \n",
            "569  | model.decoder.block.6                                                 | T5Block                    | 16 M  \n",
            "570  | model.decoder.block.6.layer                                           | ModuleList                 | 16 M  \n",
            "571  | model.decoder.block.6.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "572  | model.decoder.block.6.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "573  | model.decoder.block.6.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "574  | model.decoder.block.6.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "575  | model.decoder.block.6.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "576  | model.decoder.block.6.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "577  | model.decoder.block.6.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "578  | model.decoder.block.6.layer.0.dropout                                 | Dropout                    | 0     \n",
            "579  | model.decoder.block.6.layer.1                                         | T5LayerCrossAttention      | 4 M   \n",
            "580  | model.decoder.block.6.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n",
            "581  | model.decoder.block.6.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n",
            "582  | model.decoder.block.6.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n",
            "583  | model.decoder.block.6.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n",
            "584  | model.decoder.block.6.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n",
            "585  | model.decoder.block.6.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "586  | model.decoder.block.6.layer.1.dropout                                 | Dropout                    | 0     \n",
            "587  | model.decoder.block.6.layer.2                                         | T5LayerFF                  | 8 M   \n",
            "588  | model.decoder.block.6.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "589  | model.decoder.block.6.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "590  | model.decoder.block.6.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "591  | model.decoder.block.6.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "592  | model.decoder.block.6.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "593  | model.decoder.block.6.layer.2.dropout                                 | Dropout                    | 0     \n",
            "594  | model.decoder.block.7                                                 | T5Block                    | 16 M  \n",
            "595  | model.decoder.block.7.layer                                           | ModuleList                 | 16 M  \n",
            "596  | model.decoder.block.7.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "597  | model.decoder.block.7.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "598  | model.decoder.block.7.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "599  | model.decoder.block.7.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "600  | model.decoder.block.7.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "601  | model.decoder.block.7.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "602  | model.decoder.block.7.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "603  | model.decoder.block.7.layer.0.dropout                                 | Dropout                    | 0     \n",
            "604  | model.decoder.block.7.layer.1                                         | T5LayerCrossAttention      | 4 M   \n",
            "605  | model.decoder.block.7.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n",
            "606  | model.decoder.block.7.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n",
            "607  | model.decoder.block.7.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n",
            "608  | model.decoder.block.7.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n",
            "609  | model.decoder.block.7.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n",
            "610  | model.decoder.block.7.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "611  | model.decoder.block.7.layer.1.dropout                                 | Dropout                    | 0     \n",
            "612  | model.decoder.block.7.layer.2                                         | T5LayerFF                  | 8 M   \n",
            "613  | model.decoder.block.7.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "614  | model.decoder.block.7.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "615  | model.decoder.block.7.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "616  | model.decoder.block.7.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "617  | model.decoder.block.7.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "618  | model.decoder.block.7.layer.2.dropout                                 | Dropout                    | 0     \n",
            "619  | model.decoder.block.8                                                 | T5Block                    | 16 M  \n",
            "620  | model.decoder.block.8.layer                                           | ModuleList                 | 16 M  \n",
            "621  | model.decoder.block.8.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "622  | model.decoder.block.8.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "623  | model.decoder.block.8.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "624  | model.decoder.block.8.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "625  | model.decoder.block.8.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "626  | model.decoder.block.8.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "627  | model.decoder.block.8.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "628  | model.decoder.block.8.layer.0.dropout                                 | Dropout                    | 0     \n",
            "629  | model.decoder.block.8.layer.1                                         | T5LayerCrossAttention      | 4 M   \n",
            "630  | model.decoder.block.8.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n",
            "631  | model.decoder.block.8.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n",
            "632  | model.decoder.block.8.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n",
            "633  | model.decoder.block.8.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n",
            "634  | model.decoder.block.8.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n",
            "635  | model.decoder.block.8.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "636  | model.decoder.block.8.layer.1.dropout                                 | Dropout                    | 0     \n",
            "637  | model.decoder.block.8.layer.2                                         | T5LayerFF                  | 8 M   \n",
            "638  | model.decoder.block.8.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "639  | model.decoder.block.8.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "640  | model.decoder.block.8.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "641  | model.decoder.block.8.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "642  | model.decoder.block.8.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "643  | model.decoder.block.8.layer.2.dropout                                 | Dropout                    | 0     \n",
            "644  | model.decoder.block.9                                                 | T5Block                    | 16 M  \n",
            "645  | model.decoder.block.9.layer                                           | ModuleList                 | 16 M  \n",
            "646  | model.decoder.block.9.layer.0                                         | T5LayerSelfAttention       | 4 M   \n",
            "647  | model.decoder.block.9.layer.0.SelfAttention                           | T5Attention                | 4 M   \n",
            "648  | model.decoder.block.9.layer.0.SelfAttention.q                         | Linear                     | 1 M   \n",
            "649  | model.decoder.block.9.layer.0.SelfAttention.k                         | Linear                     | 1 M   \n",
            "650  | model.decoder.block.9.layer.0.SelfAttention.v                         | Linear                     | 1 M   \n",
            "651  | model.decoder.block.9.layer.0.SelfAttention.o                         | Linear                     | 1 M   \n",
            "652  | model.decoder.block.9.layer.0.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "653  | model.decoder.block.9.layer.0.dropout                                 | Dropout                    | 0     \n",
            "654  | model.decoder.block.9.layer.1                                         | T5LayerCrossAttention      | 4 M   \n",
            "655  | model.decoder.block.9.layer.1.EncDecAttention                         | T5Attention                | 4 M   \n",
            "656  | model.decoder.block.9.layer.1.EncDecAttention.q                       | Linear                     | 1 M   \n",
            "657  | model.decoder.block.9.layer.1.EncDecAttention.k                       | Linear                     | 1 M   \n",
            "658  | model.decoder.block.9.layer.1.EncDecAttention.v                       | Linear                     | 1 M   \n",
            "659  | model.decoder.block.9.layer.1.EncDecAttention.o                       | Linear                     | 1 M   \n",
            "660  | model.decoder.block.9.layer.1.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "661  | model.decoder.block.9.layer.1.dropout                                 | Dropout                    | 0     \n",
            "662  | model.decoder.block.9.layer.2                                         | T5LayerFF                  | 8 M   \n",
            "663  | model.decoder.block.9.layer.2.DenseReluDense                          | T5DenseReluDense           | 8 M   \n",
            "664  | model.decoder.block.9.layer.2.DenseReluDense.wi                       | Linear                     | 4 M   \n",
            "665  | model.decoder.block.9.layer.2.DenseReluDense.wo                       | Linear                     | 4 M   \n",
            "666  | model.decoder.block.9.layer.2.DenseReluDense.dropout                  | Dropout                    | 0     \n",
            "667  | model.decoder.block.9.layer.2.layer_norm                              | T5LayerNorm                | 1 K   \n",
            "668  | model.decoder.block.9.layer.2.dropout                                 | Dropout                    | 0     \n",
            "669  | model.decoder.block.10                                                | T5Block                    | 16 M  \n",
            "670  | model.decoder.block.10.layer                                          | ModuleList                 | 16 M  \n",
            "671  | model.decoder.block.10.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "672  | model.decoder.block.10.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "673  | model.decoder.block.10.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "674  | model.decoder.block.10.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "675  | model.decoder.block.10.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "676  | model.decoder.block.10.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "677  | model.decoder.block.10.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "678  | model.decoder.block.10.layer.0.dropout                                | Dropout                    | 0     \n",
            "679  | model.decoder.block.10.layer.1                                        | T5LayerCrossAttention      | 4 M   \n",
            "680  | model.decoder.block.10.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n",
            "681  | model.decoder.block.10.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n",
            "682  | model.decoder.block.10.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n",
            "683  | model.decoder.block.10.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n",
            "684  | model.decoder.block.10.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n",
            "685  | model.decoder.block.10.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "686  | model.decoder.block.10.layer.1.dropout                                | Dropout                    | 0     \n",
            "687  | model.decoder.block.10.layer.2                                        | T5LayerFF                  | 8 M   \n",
            "688  | model.decoder.block.10.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "689  | model.decoder.block.10.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "690  | model.decoder.block.10.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "691  | model.decoder.block.10.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "692  | model.decoder.block.10.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "693  | model.decoder.block.10.layer.2.dropout                                | Dropout                    | 0     \n",
            "694  | model.decoder.block.11                                                | T5Block                    | 16 M  \n",
            "695  | model.decoder.block.11.layer                                          | ModuleList                 | 16 M  \n",
            "696  | model.decoder.block.11.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "697  | model.decoder.block.11.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "698  | model.decoder.block.11.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "699  | model.decoder.block.11.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "700  | model.decoder.block.11.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "701  | model.decoder.block.11.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "702  | model.decoder.block.11.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "703  | model.decoder.block.11.layer.0.dropout                                | Dropout                    | 0     \n",
            "704  | model.decoder.block.11.layer.1                                        | T5LayerCrossAttention      | 4 M   \n",
            "705  | model.decoder.block.11.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n",
            "706  | model.decoder.block.11.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n",
            "707  | model.decoder.block.11.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n",
            "708  | model.decoder.block.11.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n",
            "709  | model.decoder.block.11.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n",
            "710  | model.decoder.block.11.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "711  | model.decoder.block.11.layer.1.dropout                                | Dropout                    | 0     \n",
            "712  | model.decoder.block.11.layer.2                                        | T5LayerFF                  | 8 M   \n",
            "713  | model.decoder.block.11.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "714  | model.decoder.block.11.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "715  | model.decoder.block.11.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "716  | model.decoder.block.11.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "717  | model.decoder.block.11.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "718  | model.decoder.block.11.layer.2.dropout                                | Dropout                    | 0     \n",
            "719  | model.decoder.block.12                                                | T5Block                    | 16 M  \n",
            "720  | model.decoder.block.12.layer                                          | ModuleList                 | 16 M  \n",
            "721  | model.decoder.block.12.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "722  | model.decoder.block.12.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "723  | model.decoder.block.12.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "724  | model.decoder.block.12.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "725  | model.decoder.block.12.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "726  | model.decoder.block.12.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "727  | model.decoder.block.12.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "728  | model.decoder.block.12.layer.0.dropout                                | Dropout                    | 0     \n",
            "729  | model.decoder.block.12.layer.1                                        | T5LayerCrossAttention      | 4 M   \n",
            "730  | model.decoder.block.12.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n",
            "731  | model.decoder.block.12.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n",
            "732  | model.decoder.block.12.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n",
            "733  | model.decoder.block.12.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n",
            "734  | model.decoder.block.12.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n",
            "735  | model.decoder.block.12.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "736  | model.decoder.block.12.layer.1.dropout                                | Dropout                    | 0     \n",
            "737  | model.decoder.block.12.layer.2                                        | T5LayerFF                  | 8 M   \n",
            "738  | model.decoder.block.12.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "739  | model.decoder.block.12.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "740  | model.decoder.block.12.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "741  | model.decoder.block.12.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "742  | model.decoder.block.12.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "743  | model.decoder.block.12.layer.2.dropout                                | Dropout                    | 0     \n",
            "744  | model.decoder.block.13                                                | T5Block                    | 16 M  \n",
            "745  | model.decoder.block.13.layer                                          | ModuleList                 | 16 M  \n",
            "746  | model.decoder.block.13.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "747  | model.decoder.block.13.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "748  | model.decoder.block.13.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "749  | model.decoder.block.13.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "750  | model.decoder.block.13.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "751  | model.decoder.block.13.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "752  | model.decoder.block.13.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "753  | model.decoder.block.13.layer.0.dropout                                | Dropout                    | 0     \n",
            "754  | model.decoder.block.13.layer.1                                        | T5LayerCrossAttention      | 4 M   \n",
            "755  | model.decoder.block.13.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n",
            "756  | model.decoder.block.13.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n",
            "757  | model.decoder.block.13.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n",
            "758  | model.decoder.block.13.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n",
            "759  | model.decoder.block.13.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n",
            "760  | model.decoder.block.13.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "761  | model.decoder.block.13.layer.1.dropout                                | Dropout                    | 0     \n",
            "762  | model.decoder.block.13.layer.2                                        | T5LayerFF                  | 8 M   \n",
            "763  | model.decoder.block.13.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "764  | model.decoder.block.13.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "765  | model.decoder.block.13.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "766  | model.decoder.block.13.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "767  | model.decoder.block.13.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "768  | model.decoder.block.13.layer.2.dropout                                | Dropout                    | 0     \n",
            "769  | model.decoder.block.14                                                | T5Block                    | 16 M  \n",
            "770  | model.decoder.block.14.layer                                          | ModuleList                 | 16 M  \n",
            "771  | model.decoder.block.14.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "772  | model.decoder.block.14.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "773  | model.decoder.block.14.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "774  | model.decoder.block.14.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "775  | model.decoder.block.14.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "776  | model.decoder.block.14.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "777  | model.decoder.block.14.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "778  | model.decoder.block.14.layer.0.dropout                                | Dropout                    | 0     \n",
            "779  | model.decoder.block.14.layer.1                                        | T5LayerCrossAttention      | 4 M   \n",
            "780  | model.decoder.block.14.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n",
            "781  | model.decoder.block.14.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n",
            "782  | model.decoder.block.14.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n",
            "783  | model.decoder.block.14.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n",
            "784  | model.decoder.block.14.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n",
            "785  | model.decoder.block.14.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "786  | model.decoder.block.14.layer.1.dropout                                | Dropout                    | 0     \n",
            "787  | model.decoder.block.14.layer.2                                        | T5LayerFF                  | 8 M   \n",
            "788  | model.decoder.block.14.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "789  | model.decoder.block.14.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "790  | model.decoder.block.14.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "791  | model.decoder.block.14.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "792  | model.decoder.block.14.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "793  | model.decoder.block.14.layer.2.dropout                                | Dropout                    | 0     \n",
            "794  | model.decoder.block.15                                                | T5Block                    | 16 M  \n",
            "795  | model.decoder.block.15.layer                                          | ModuleList                 | 16 M  \n",
            "796  | model.decoder.block.15.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "797  | model.decoder.block.15.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "798  | model.decoder.block.15.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "799  | model.decoder.block.15.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "800  | model.decoder.block.15.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "801  | model.decoder.block.15.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "802  | model.decoder.block.15.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "803  | model.decoder.block.15.layer.0.dropout                                | Dropout                    | 0     \n",
            "804  | model.decoder.block.15.layer.1                                        | T5LayerCrossAttention      | 4 M   \n",
            "805  | model.decoder.block.15.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n",
            "806  | model.decoder.block.15.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n",
            "807  | model.decoder.block.15.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n",
            "808  | model.decoder.block.15.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n",
            "809  | model.decoder.block.15.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n",
            "810  | model.decoder.block.15.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "811  | model.decoder.block.15.layer.1.dropout                                | Dropout                    | 0     \n",
            "812  | model.decoder.block.15.layer.2                                        | T5LayerFF                  | 8 M   \n",
            "813  | model.decoder.block.15.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "814  | model.decoder.block.15.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "815  | model.decoder.block.15.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "816  | model.decoder.block.15.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "817  | model.decoder.block.15.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "818  | model.decoder.block.15.layer.2.dropout                                | Dropout                    | 0     \n",
            "819  | model.decoder.block.16                                                | T5Block                    | 16 M  \n",
            "820  | model.decoder.block.16.layer                                          | ModuleList                 | 16 M  \n",
            "821  | model.decoder.block.16.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "822  | model.decoder.block.16.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "823  | model.decoder.block.16.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "824  | model.decoder.block.16.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "825  | model.decoder.block.16.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "826  | model.decoder.block.16.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "827  | model.decoder.block.16.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "828  | model.decoder.block.16.layer.0.dropout                                | Dropout                    | 0     \n",
            "829  | model.decoder.block.16.layer.1                                        | T5LayerCrossAttention      | 4 M   \n",
            "830  | model.decoder.block.16.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n",
            "831  | model.decoder.block.16.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n",
            "832  | model.decoder.block.16.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n",
            "833  | model.decoder.block.16.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n",
            "834  | model.decoder.block.16.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n",
            "835  | model.decoder.block.16.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "836  | model.decoder.block.16.layer.1.dropout                                | Dropout                    | 0     \n",
            "837  | model.decoder.block.16.layer.2                                        | T5LayerFF                  | 8 M   \n",
            "838  | model.decoder.block.16.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "839  | model.decoder.block.16.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "840  | model.decoder.block.16.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "841  | model.decoder.block.16.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "842  | model.decoder.block.16.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "843  | model.decoder.block.16.layer.2.dropout                                | Dropout                    | 0     \n",
            "844  | model.decoder.block.17                                                | T5Block                    | 16 M  \n",
            "845  | model.decoder.block.17.layer                                          | ModuleList                 | 16 M  \n",
            "846  | model.decoder.block.17.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "847  | model.decoder.block.17.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "848  | model.decoder.block.17.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "849  | model.decoder.block.17.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "850  | model.decoder.block.17.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "851  | model.decoder.block.17.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "852  | model.decoder.block.17.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "853  | model.decoder.block.17.layer.0.dropout                                | Dropout                    | 0     \n",
            "854  | model.decoder.block.17.layer.1                                        | T5LayerCrossAttention      | 4 M   \n",
            "855  | model.decoder.block.17.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n",
            "856  | model.decoder.block.17.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n",
            "857  | model.decoder.block.17.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n",
            "858  | model.decoder.block.17.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n",
            "859  | model.decoder.block.17.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n",
            "860  | model.decoder.block.17.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "861  | model.decoder.block.17.layer.1.dropout                                | Dropout                    | 0     \n",
            "862  | model.decoder.block.17.layer.2                                        | T5LayerFF                  | 8 M   \n",
            "863  | model.decoder.block.17.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "864  | model.decoder.block.17.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "865  | model.decoder.block.17.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "866  | model.decoder.block.17.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "867  | model.decoder.block.17.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "868  | model.decoder.block.17.layer.2.dropout                                | Dropout                    | 0     \n",
            "869  | model.decoder.block.18                                                | T5Block                    | 16 M  \n",
            "870  | model.decoder.block.18.layer                                          | ModuleList                 | 16 M  \n",
            "871  | model.decoder.block.18.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "872  | model.decoder.block.18.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "873  | model.decoder.block.18.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "874  | model.decoder.block.18.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "875  | model.decoder.block.18.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "876  | model.decoder.block.18.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "877  | model.decoder.block.18.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "878  | model.decoder.block.18.layer.0.dropout                                | Dropout                    | 0     \n",
            "879  | model.decoder.block.18.layer.1                                        | T5LayerCrossAttention      | 4 M   \n",
            "880  | model.decoder.block.18.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n",
            "881  | model.decoder.block.18.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n",
            "882  | model.decoder.block.18.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n",
            "883  | model.decoder.block.18.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n",
            "884  | model.decoder.block.18.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n",
            "885  | model.decoder.block.18.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "886  | model.decoder.block.18.layer.1.dropout                                | Dropout                    | 0     \n",
            "887  | model.decoder.block.18.layer.2                                        | T5LayerFF                  | 8 M   \n",
            "888  | model.decoder.block.18.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "889  | model.decoder.block.18.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "890  | model.decoder.block.18.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "891  | model.decoder.block.18.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "892  | model.decoder.block.18.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "893  | model.decoder.block.18.layer.2.dropout                                | Dropout                    | 0     \n",
            "894  | model.decoder.block.19                                                | T5Block                    | 16 M  \n",
            "895  | model.decoder.block.19.layer                                          | ModuleList                 | 16 M  \n",
            "896  | model.decoder.block.19.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "897  | model.decoder.block.19.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "898  | model.decoder.block.19.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "899  | model.decoder.block.19.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "900  | model.decoder.block.19.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "901  | model.decoder.block.19.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "902  | model.decoder.block.19.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "903  | model.decoder.block.19.layer.0.dropout                                | Dropout                    | 0     \n",
            "904  | model.decoder.block.19.layer.1                                        | T5LayerCrossAttention      | 4 M   \n",
            "905  | model.decoder.block.19.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n",
            "906  | model.decoder.block.19.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n",
            "907  | model.decoder.block.19.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n",
            "908  | model.decoder.block.19.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n",
            "909  | model.decoder.block.19.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n",
            "910  | model.decoder.block.19.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "911  | model.decoder.block.19.layer.1.dropout                                | Dropout                    | 0     \n",
            "912  | model.decoder.block.19.layer.2                                        | T5LayerFF                  | 8 M   \n",
            "913  | model.decoder.block.19.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "914  | model.decoder.block.19.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "915  | model.decoder.block.19.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "916  | model.decoder.block.19.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "917  | model.decoder.block.19.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "918  | model.decoder.block.19.layer.2.dropout                                | Dropout                    | 0     \n",
            "919  | model.decoder.block.20                                                | T5Block                    | 16 M  \n",
            "920  | model.decoder.block.20.layer                                          | ModuleList                 | 16 M  \n",
            "921  | model.decoder.block.20.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "922  | model.decoder.block.20.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "923  | model.decoder.block.20.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "924  | model.decoder.block.20.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "925  | model.decoder.block.20.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "926  | model.decoder.block.20.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "927  | model.decoder.block.20.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "928  | model.decoder.block.20.layer.0.dropout                                | Dropout                    | 0     \n",
            "929  | model.decoder.block.20.layer.1                                        | T5LayerCrossAttention      | 4 M   \n",
            "930  | model.decoder.block.20.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n",
            "931  | model.decoder.block.20.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n",
            "932  | model.decoder.block.20.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n",
            "933  | model.decoder.block.20.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n",
            "934  | model.decoder.block.20.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n",
            "935  | model.decoder.block.20.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "936  | model.decoder.block.20.layer.1.dropout                                | Dropout                    | 0     \n",
            "937  | model.decoder.block.20.layer.2                                        | T5LayerFF                  | 8 M   \n",
            "938  | model.decoder.block.20.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "939  | model.decoder.block.20.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "940  | model.decoder.block.20.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "941  | model.decoder.block.20.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "942  | model.decoder.block.20.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "943  | model.decoder.block.20.layer.2.dropout                                | Dropout                    | 0     \n",
            "944  | model.decoder.block.21                                                | T5Block                    | 16 M  \n",
            "945  | model.decoder.block.21.layer                                          | ModuleList                 | 16 M  \n",
            "946  | model.decoder.block.21.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "947  | model.decoder.block.21.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "948  | model.decoder.block.21.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "949  | model.decoder.block.21.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "950  | model.decoder.block.21.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "951  | model.decoder.block.21.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "952  | model.decoder.block.21.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "953  | model.decoder.block.21.layer.0.dropout                                | Dropout                    | 0     \n",
            "954  | model.decoder.block.21.layer.1                                        | T5LayerCrossAttention      | 4 M   \n",
            "955  | model.decoder.block.21.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n",
            "956  | model.decoder.block.21.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n",
            "957  | model.decoder.block.21.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n",
            "958  | model.decoder.block.21.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n",
            "959  | model.decoder.block.21.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n",
            "960  | model.decoder.block.21.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "961  | model.decoder.block.21.layer.1.dropout                                | Dropout                    | 0     \n",
            "962  | model.decoder.block.21.layer.2                                        | T5LayerFF                  | 8 M   \n",
            "963  | model.decoder.block.21.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "964  | model.decoder.block.21.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "965  | model.decoder.block.21.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "966  | model.decoder.block.21.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "967  | model.decoder.block.21.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "968  | model.decoder.block.21.layer.2.dropout                                | Dropout                    | 0     \n",
            "969  | model.decoder.block.22                                                | T5Block                    | 16 M  \n",
            "970  | model.decoder.block.22.layer                                          | ModuleList                 | 16 M  \n",
            "971  | model.decoder.block.22.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "972  | model.decoder.block.22.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "973  | model.decoder.block.22.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "974  | model.decoder.block.22.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "975  | model.decoder.block.22.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "976  | model.decoder.block.22.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "977  | model.decoder.block.22.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "978  | model.decoder.block.22.layer.0.dropout                                | Dropout                    | 0     \n",
            "979  | model.decoder.block.22.layer.1                                        | T5LayerCrossAttention      | 4 M   \n",
            "980  | model.decoder.block.22.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n",
            "981  | model.decoder.block.22.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n",
            "982  | model.decoder.block.22.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n",
            "983  | model.decoder.block.22.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n",
            "984  | model.decoder.block.22.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n",
            "985  | model.decoder.block.22.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "986  | model.decoder.block.22.layer.1.dropout                                | Dropout                    | 0     \n",
            "987  | model.decoder.block.22.layer.2                                        | T5LayerFF                  | 8 M   \n",
            "988  | model.decoder.block.22.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "989  | model.decoder.block.22.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "990  | model.decoder.block.22.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "991  | model.decoder.block.22.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "992  | model.decoder.block.22.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "993  | model.decoder.block.22.layer.2.dropout                                | Dropout                    | 0     \n",
            "994  | model.decoder.block.23                                                | T5Block                    | 16 M  \n",
            "995  | model.decoder.block.23.layer                                          | ModuleList                 | 16 M  \n",
            "996  | model.decoder.block.23.layer.0                                        | T5LayerSelfAttention       | 4 M   \n",
            "997  | model.decoder.block.23.layer.0.SelfAttention                          | T5Attention                | 4 M   \n",
            "998  | model.decoder.block.23.layer.0.SelfAttention.q                        | Linear                     | 1 M   \n",
            "999  | model.decoder.block.23.layer.0.SelfAttention.k                        | Linear                     | 1 M   \n",
            "1000 | model.decoder.block.23.layer.0.SelfAttention.v                        | Linear                     | 1 M   \n",
            "1001 | model.decoder.block.23.layer.0.SelfAttention.o                        | Linear                     | 1 M   \n",
            "1002 | model.decoder.block.23.layer.0.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "1003 | model.decoder.block.23.layer.0.dropout                                | Dropout                    | 0     \n",
            "1004 | model.decoder.block.23.layer.1                                        | T5LayerCrossAttention      | 4 M   \n",
            "1005 | model.decoder.block.23.layer.1.EncDecAttention                        | T5Attention                | 4 M   \n",
            "1006 | model.decoder.block.23.layer.1.EncDecAttention.q                      | Linear                     | 1 M   \n",
            "1007 | model.decoder.block.23.layer.1.EncDecAttention.k                      | Linear                     | 1 M   \n",
            "1008 | model.decoder.block.23.layer.1.EncDecAttention.v                      | Linear                     | 1 M   \n",
            "1009 | model.decoder.block.23.layer.1.EncDecAttention.o                      | Linear                     | 1 M   \n",
            "1010 | model.decoder.block.23.layer.1.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "1011 | model.decoder.block.23.layer.1.dropout                                | Dropout                    | 0     \n",
            "1012 | model.decoder.block.23.layer.2                                        | T5LayerFF                  | 8 M   \n",
            "1013 | model.decoder.block.23.layer.2.DenseReluDense                         | T5DenseReluDense           | 8 M   \n",
            "1014 | model.decoder.block.23.layer.2.DenseReluDense.wi                      | Linear                     | 4 M   \n",
            "1015 | model.decoder.block.23.layer.2.DenseReluDense.wo                      | Linear                     | 4 M   \n",
            "1016 | model.decoder.block.23.layer.2.DenseReluDense.dropout                 | Dropout                    | 0     \n",
            "1017 | model.decoder.block.23.layer.2.layer_norm                             | T5LayerNorm                | 1 K   \n",
            "1018 | model.decoder.block.23.layer.2.dropout                                | Dropout                    | 0     \n",
            "1019 | model.decoder.final_layer_norm                                        | T5LayerNorm                | 1 K   \n",
            "1020 | model.decoder.dropout                                                 | Dropout                    | 0     \n",
            "1021 | model.lm_head                                                         | Linear                     | 32 M  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd4baf5b8268439f879e8400ac60cdad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9532733e4f4841bf8ac7b05c9135d6fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00e754584ca14e14af2b9b43513ec7f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:***** Validation results *****\n",
            "INFO:__main__:avg_val_loss = tensor(1.3903, device='cuda:0')\n",
            "\n",
            "INFO:__main__:loss = tensor(1.6659, device='cuda:0')\n",
            "\n",
            "INFO:__main__:train_loss = tensor(1.6659, device='cuda:0')\n",
            "\n",
            "INFO:__main__:val_loss = tensor(1.3903, device='cuda:0')\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e62fe18fc384995a1aa94c1491fa8a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:***** Validation results *****\n",
            "INFO:__main__:avg_train_loss = tensor(1.5628, device='cuda:0')\n",
            "\n",
            "INFO:__main__:avg_val_loss = tensor(1.3654, device='cuda:0')\n",
            "\n",
            "INFO:__main__:epoch = 0\n",
            "\n",
            "INFO:__main__:loss = tensor(0.7336, device='cuda:0')\n",
            "\n",
            "INFO:__main__:train_loss = tensor(0.7336, device='cuda:0')\n",
            "\n",
            "INFO:__main__:val_loss = tensor(1.3654, device='cuda:0')\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e502d6697c1c49719aa55826a81f53f2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:***** Validation results *****\n",
            "INFO:__main__:avg_train_loss = tensor(1.1044, device='cuda:0')\n",
            "\n",
            "INFO:__main__:avg_val_loss = tensor(1.4444, device='cuda:0')\n",
            "\n",
            "INFO:__main__:epoch = 1\n",
            "\n",
            "INFO:__main__:loss = tensor(0.2213, device='cuda:0')\n",
            "\n",
            "INFO:__main__:train_loss = tensor(0.2213, device='cuda:0')\n",
            "\n",
            "INFO:__main__:val_loss = tensor(1.4444, device='cuda:0')\n",
            "\n",
            "INFO:transformers.configuration_utils:Configuration saved in t5_paraphrase/config.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training finished\n",
            "Saving model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.modeling_utils:Model weights saved in t5_paraphrase/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved\n",
            "cp: cannot stat '/content/t5_paraphrase/': No such file or directory\n",
            "cp: cannot stat '/content/lightning_logs/': No such file or directory\n",
            "Copied the final folder to Google Drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}