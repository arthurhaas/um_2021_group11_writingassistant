paper,review_count,clarity_weighted,substance_weighted,recommendation_weighted,accepted,cat,split,abstract,text_clean
660,2,3.43,4.43,3.43,True,acl_2017,dev,"We propose two novel methodologies for the automatic generation of rhythmic poetry in a variety of forms. The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration. The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes. A large-scale extrinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54% of the time. In addition, participants rated a machinegenerated poem to be the best amongst all evaluated.","The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration. The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes. In addition, participants rated a machinegenerated poem to be the best amongst all evaluated. Poetry is an advanced form of linguistic communication, in which a message is conveyed that satisfies both aesthetic and semantic constraints. As poetry is one of the most expressive forms of language, the automatic creation of texts recognisable as poetry is difficult. In addition to requiring an understanding of many aspects of language including phonetic patterns such as rhyme, rhythm and alliteration, poetry composition also requires a deep understanding of the meaning of language. These rules may describe aspects of the literary devices used, and are usually highly prescriptive. Examples of different forms of poetry are limericks, ballads and sonnets. Limericks, for example, are characterised by their strict rhyme scheme (AABBA), their rhythm (two unstressed syllables followed by one stressed syllable) and their shorter third and fourth lines. Creating such poetry requires not only an understanding of the language itself, but also of how it sounds when spoken aloud. Statistical text generation usually requires the construction of a generative language model that explicitly learns the probability of any given word given previous context. Poetry generation is an interesting application, since performing this task automatically requires the creation of models that not only focus on what is being written (content), but also on how it is being written (form). We experiment with two novel methodologies for solving this task. The first involves training a model to learn an implicit representation of content and form through the use of a phonological encoding. The second involves training a generative language model to represent content, which is then constrained by a discriminative pronunciation model, representing form. This second model is of particular interest because poetry with arbitrary rhyme, rhythm, repetition and themes can be generated by tuning the pronunciation model. Most systems that have been proposed can loosely be categorised as rule-based expert systems, or statistical approaches. These approaches are often inspired by how humans might generate poetry. Statistical approaches, conversely, make no assumptions about the creative process. For the task of automatic generation of classical Chinese poetry, they were able to outperform all other Chinese poetry generation systems with both manual and automatic evaluation. Our first model is a pure neural language model, trained on a phonetic encoding of poetry in order to represent both form and content. Training on phonetic symbols allows the model to learn effective representations of pronunciation, including rhyme and rhythm. However, just training on a large corpus of poetry data is not enough. Specifically, two problems need to be overcome. This means that we require an additional probabilistic model in order to determine the most likely word given a sequence of phonemes. It is therefore important to train the model on poetry which has its own internal consistency. Thus, the model comprises three steps: transliterating an ortographic sequence to its phonetic representation, training a neural language model on the phonetic encoding, and decoding the generated sequence back from phonemes to orthographic symbols. The number of letters and number of phones in a word are rarely a one-to-one match: letters may match with up to three phones. In addition to transliterating words into phoneme sequences, we also represent word break characters as a specific symbol. This makes decipherment, when converting back into an orthographic representation, much easier. The model is trained using stochastic gradient descent to predict the next phoneme given a sequence of phonemes. Although no experiments were carried out on corpora of early-modern English, it is likely that this accuracy would be significantly lower. This allows the network to learn features like rhyme even when spread over multiple lines. Orthographic decoding When decoding from phonemes back to orthographic symbols, the goal is to compute the most likely word corresponding to a sequence of phonemes. We calculate the transition probabilities (using the n-gram model) and the emission matrix (using the CMU pronunciation dictionary) to determine pronunciations that correspond to a single word. All pronunciations are naively considered equiprobable. We perform Viterbi decoding to find the most likely sequence of words. Output A popular form of poetry with strict internal structure is the sonnet. Not only is it mostly in strict Iambic Pentameter, but the grammar of the output is mostly correct and the poetry contains rhyme. As the example shows, phonetic-level language models are effective at learning poetic form, despite small training sets and relatively few parameters. However, the fact that they require training data with internal poetic consistency implies that they do not generalise to other forms of poetry. That is, in order to generate poetry in Dactylic Hexameter (for example), a phonetic model must be trained on a corpus of Dactylic poetry. Not only is this impractical, but in many cases no corpus of adequate size even exists. Even when such poetic corpora are available, a new model must be trained for each type of poetry. This precludes tweaking the form of the output, which is important when generating poetry automatically. We now explore an alternative approach. This allows us to represent the problem of creating poetry as a constraint satisfaction problem, where we can modify constraints to restrict the types of poetry we generate. This approach offers several benefits over the word-level models that are prevalent in the literature. Namely, their more compact vocabulary allows for more efficient training; they can learn common prefixes and suffixes to allow us to sample words that are not present in the training corpus and can learn effective language representations from relatively small corpora; and they can handle archaic and incorrect spellings of words. As we no longer need the model to explicitly represent the form of generated poetry, we can loosen our constraints when choosing a training corpus. Instead of relying on poetry only in sonnet form, we can instead construct a generic corpus of poetry taken from online sources. The increase in corpus size facilitates a corresponding increase in the number of permissible model parameters. The model was trained to predict the next character given a sequence of characters, using stochastic gradient descent. Rhythm Modeling Although a character-level language model trained on a corpus of generic poetry allows us to generate interesting text, internal irregularities and noise in the training data prevent the model from learning important features such as rhythm. Hence, we require an additional classifier to constrain our model by either accepting or rejecting sampled lines based on the presence or absence of these features. As the presence of meter (rhythm) is the most characteristic feature of poetry, it therefore must be our primary focus. Furthermore, they are constructed from American English, meaning that British English may be misclassified. These issues are circumvented by applying lightly supervised learning to determine the contextual stress pattern of any word. That is, we exploit the latent structure in our corpus of sonnet poetry, namely, the fact that sonnets are composed of lines in rigid Iambic Pentameter, and are therefore exactly ten syllables long with alternating syllabic stress. This allows us to derive a syllablestress distribution. Although we use the sonnets corpus for this, it is important to note that any corpus with such a latent structure could be used. By representing a line as a cascade of Weighted Finite State Transducers (WFST), we can perform Expectation Maximisation over the poetry corpus to obtain a probabilistic classifier which enables us to determine the most likely stress patterns for each word. Every word is represented by a single transducer. Since weights can be assigned to state transitions, we can model the probability that a given input string maps to a particular output. We then iterate over each line of the sonnet corpus, using Expectation Maximisation to train the cascades. We train the rhythm classifier by converging the cascades to whatever output is the most likely given the line. Constraining the model To generate poetry using this model, we sample sequences of characters from the character-level language model. tokens in an intermediary buffer. We then apply the separately trained word-level WFSTs to construct a cascade of this buffer and perform Viterbi decoding over the cascade. This defines the distribution of stress-patterns over our word tokens. We can represent this cascade as a probabilistic classifier, and accept or reject the buffered output based on how closely it conforms to the desired meter. While sampling sequences of words from this model, the entire generated sequence is passed to the classifier each time a new word is sampled. The pronunciation model then returns the probability that the entire line is within the specified meter. If a new word is rejected by the classifier, the state of the network is rolled back to the state of the last formulaically acceptable line, removing the rejected word from memory. The constraint on rhythm can be controlled by adjusting the acceptability threshold of the classifier. By increasing the threshold, output focuses on form over content. Conversely, decreasing the criterion puts greater emphasis on content. Generic poetry Sonnet poetry LSTM WFST Rhythmic Output Trained Trained Buffer It is important for any generative poetry model to include themes and poetic devices. One way to achieve this would be by constructing a corpus that exhibits the desired themes and devices. we would aggregate love poetry to train the model, which would thus learn an implicit representation of love. However, this forces us to generate poetry according to discrete themes and styles from pretrained models, requiring a new training corpus for each model. In other words, we would suffer from similar limitations as with the phonetic-level model, in that we require a dedicated corpus. This approach is more robust, and provides us with more control over the final output, including the capacity to vary the inclusion of poetic devices in the output. Themes In order to introduce thematic content, we heuristically boost the probability of sampling words that are semantically related to a theme word from the language model. For example, the theme winter might include thematic words frozen, cold, snow and frosty. We represent these semantic neighbours at the character level, and heuristically boost their probability by multiplying the sampling probability of such character strings by a function of their cosine similarity to the key word. Thus, the likelihood of sampling a thematically related word is artificially increased, while still constraining the model rhythmically. Poetic devices A similar method may be used for poetic devices such as assonance, consonance and alliteration. same heuristic to boost the probability of sampling character strings that have previously been sampled. That is, to sample a line with many instances of alliteration (multiple words with the same initial sound) we record the historical frequencies of characters sampled at the beginning of each previous word. After a word break character, we boost the probability that those characters will be sampled again in the softmax. We only keep track of frequencies for a fixed number of time steps. By increasing or decreasing the size of this window, we can manipulate the prevalence of alliteration. Variations of this approach are applied to invoke consonance (by boosting intra-word consonants) and assonance (by boosting intra-word vowels). In order to examine how effective our methodologies for generating poetry are, we evaluate the proposed models in two ways. First, we perform an intrinsic evaluation where we examine the quality of the models and the generated poetry. Second, we perform an extrinsic evaluation where we evaluate the generated output using human annotators, and compare it to human-generated poetry. To evaluate the ability of both models to generate formulaic poetry that adheres to rhythmic rules, we compared sets of fifty sampled lines from each model. The first set was sampled from the phonetic-level model trained on Iambic poetry. The second set was sampled from the characterlevel model, constrained to Iambic form. For comparison, and to act as a baseline, we also sampled from the unconstrained character model. We created gold-standard syllabic classifications by recording each line spoken-aloud, and marking each syllable as either stressed or unstressed. We then compared these observations to loose Iambic Pentameter (containing all four variants), to determine how many syllabic misclassifications existed on each line. This was done by speaking each line aloud, and noting where the speaker put stresses. We can see that the phonetic model performed similarly, but produced more subtle mistakes than the constrained character model: many of the errors were single mistakes in an otherwise correct line of poetry. In order to investigate this further, we examined to what extent these errors are due to transliteration (i.e., the phonetic encoding and orthographic decoding steps). Coverage captures the percentage of in-vocabulary items. The relatively low per-word accuracy achieved on the Wikipedia corpus is likely due to the high frequency of out-ofvocabulary words. The results show that a significant number of errors in the phonetic-level model are likely to be caused by transliteration mistakes. We conducted an indistinguishability study with a selection of automatically generated poetry and human poetry. his radiant ribs girdled empty and very-least beautiful as dignified to see. c) Man with the broken blood blue glass and gold. Cheap chatter chants to be a lover do. e) The son still streams and strength and spirit. The ridden souls of which the fills of. b) Is that people like things (are the way we to figure it out) and I thought of you reading and then is your show or you know we will finish along will you play. we only evaluate on the constrained characterlevel model. The aim of the study was to determine whether participants could distinguish between human and generated poetry, and if so to what extent. Our participants were not financially incentivised, perceiving the evaluation as an intellectual challenge. We naively consider the overall quality of a poem to be the mean of these three measures. Based on human judgments, we can determine whether the models presented in this work can produce poetry of a similar quality to humans. This formula represents readability as a function of the complexity of the input words. We selected nine machine-generated poems with a high readability score. eight were selected for the final comparison based on their comparable readability score. The selected poems were segmented into passages of between four and six lines, to match the length of the generated poetry segments. This indicates that our participants were unable to tell the difference between human and generated poetry in any significant way. Although our participants generally considered the human poems to be of marginally higher quality than our generated poetry, they were unable to effectively distinguish between them. Furthermore, the poem with the highest overall quality rating is a machine generated one. This shows that our approach was effective at generating high-quality rhythmic verse. Generally the set of poetry produced by the neural character model was slightly less readable and emotive than the human poetry, but had above average form. All generated poems included in this evaluation can be found in the supplementary material. Our contributions are twofold. First, we developed a neural language model trained on a phonetic transliteration of poetic form and content. Although example output looked promising, this model was limited by its inability to generalise to novel forms of verse. We then proposed a more robust model trained on unformed poetic text, whose output form is constrained at sample time. This approach offers greater control over the style of the generated poetry than the earlier method, and facilitates themes and poetic devices."
94,1,5.0,4.0,4.0,True,acl_2017,dev,"Restricted non-monotonicity has been shown beneficial for the projective arceager dependency parser in previous research, as posterior decisions can repair mistakes made in previous states due to the lack of information. In this paper, we propose a novel, fully non-monotonic transition system based on the non-projective Covington algorithm. As a non-monotonic system requires exploration of erroneous actions during the training process, we develop several non-monotonic variants of the recently defined dynamic oracle for the Covington parser, based on tight approximations of the loss. Experiments on datasets from the CoNLL-X and CoNLL-XI shared tasks show that a non-monotonic dynamic oracle outperforms the monotonic version in the majority of languages.","They parse a sentence from left to right by greedily choosing the highestscoring transition to go from the current parser configuration or state to the next. The resulting sequence of transitions incrementally builds a parse for the input sentence. The scoring of the transitions is provided by a statistical model, previously trained to approximate an oracle, a function that selects the needed transitions to parse a gold tree. Unfortunately, the greedy nature that grants these parsers their efficiency also represents their main limitation. In particular, they present a modified arc-eager transition system where the Left-Arc and Reduce transitions are non-monotonic: the former is used to repair invalid attachments made in previous states by replacing them with a leftward arc, and the latter allows the parser to link two words with a rightward arc that were previously left unattached due to an erroneous decision. Since the Right-Arc transition is still monotonic and leftward arcs can never be repaired because their dependent is removed from the stack by the arc-eager parser and rendered inaccessible, this approach can only repair certain kinds of mistakes: namely, it can fix erroneous rightward arcs by replacing them with a leftward arc, and connect a limited set of unattached words with rightward arcs. In addition, they argue that non-monotonicity in the training oracle can be harmful for the final accuracy and, therefore, they suggest to apply it only as a fallback component for a monotonic oracle, which is given priority over the non-monotonic one. Thus, this strategy will follow the path dictated by the monotonic oracle the majority of the time. Furthermore, both versions of the algorithm are limited to projective trees. In this paper, we propose a non-monotonic transition system based on the non-projective Covington parser, together with a dynamic oracle to train it with erroneous examples that will need to be repaired. To our knowledge, the presented system is the first nonmonotonic parser that can produce non-projective dependency analyses. For this reason, a leftward arc cannot be repaired with a rightward arc, because this would imply going back in the sentence. The other three combinations (replacing leftward with leftward, rightward with leftward or rightward with rightward arcs) are possible.     However, the two transitions that create arcs will be disallowed in configurations where this would cause a violation of the single-head constraint (a node can have at most one incoming arc) or the acyclicity constraint (the dependency graph cannot have cycles). The resulting parser can generate any possible dependency tree for the input, including arbitrary non-projective trees. While it runs in quadratic worst-case time, in theory worse than lineartime transition-based parsers (e.g. When we refer to parser outputs as trees, we assume that this transformation is being implicitly made. A dynamic oracle is a function that maps a configuration c and a gold tree tG to the set of transitions that can be applied in c and lead to some parse tree t minimizing the Hamming loss with respect to tG (the amount of nodes whose head is different in t and tG). If this holds, then the loss of a configuration c equals the number of gold arcs that are not individually reachable from c, which is easy to compute in most parsers. In spite of this, they prove that a dynamic oracle for the Covington parser can be efficiently built by counting individually unreachable arcs, and correcting for the presence of such cycles. We now define a non-monotonic variant of the Covington non-projective parser. To do so, we allow the Right-Arc and Left-Arc transitions to create arcs between any pair of nodes without restriction. If the node attached as dependent already had a previous head, the existing attachment is discarded in favor of the new one. This allows the parser to correct erroneous attachments made in the past by assigning new heads, while still enforcing the single-head constraint, as only the most recent head assigned to each node is kept. To enforce acyclicity, one possibility would be to keep the logic of the monotonic algorithm, forbidding the creation of arcs that would create cycles. However, this greatly complicates the definition of the set of individually unreachable arcs, which is needed to compute the loss bounds that will be used by the dynamic oracle. We do not know of a way to characterize the conditions under which such a transition sequence exists, and thus cannot estimate the loss efficiently. Instead, we enforce the acyclicity constraint in a similar way to the single-head constraint: Right-Arc and Left-Arc transitions are always allowed, even if the prospective arc would create a cycle in A. This not only enforces the acyclicity constraint while keeping the computation of U(c, tG) simple and efficient, but also produces a straightforward, coherent algorithm (arc transitions are always allowed, and both constraints are enforced by deleting a previous arc) and allows us to exploit non-monotonicity to the maximum (we can not only recover from assigning a node the wrong head, but also from situations where previous errors together with the acyclicity constraint prevent us from building a gold arc, keeping with the principle that later decisions override earlier ones). To successfully train a non-monotonic system, we need a dynamic oracle with error exploration, so that the parser will be put in erroneous states and need to apply non-monotonic transitions in order to repair them. Our modification is an approximate dynamic oracle: due to the extra flexibility added to the algorithm by non-monotonicity, we do not know of an efficient way of obtaining an exact calculation of the loss of a given configuration. First of all, we adapt the computation of the set of individually unreachable arcs U(c, tG) to the new algorithm. Note that, since the head of a node can change during the parsing process and arcs that produce cycles in A can be built, the two last conditions present in the monotonic scenario for computing U(c, tG) are not needed when we use nonmonotonicity and, as a consequence, the set of individually reachable arcs I(c, tG) is larger: due to the greater flexibility provided by nonmonotonicity, we can reach arcs that would be unreachable for the monotonic version. It is worth noting that there always exists at least one transition sequence that builds every arc in I(c, tG) at some point (although not all of them necessarily appear in the final tree, due to non-monotonicity). This can be easily shown based on the fact that the non-monotonic parser does not forbid transitions at any configuration. Characterizing the situations where such an alternative exists is the main difficulty for an exact calculation of the loss. However, replacing a wrong attachment with another wrong attachment need not increase loss. For each language, we show in boldface the average value and relative difference of the bound that is closer to the loss. This implies that the calculation of the two non-monotonic upper bounds is less efficient than the linear loss computation in the monotonic scenario. However, a non-monotonic algorithm that uses the lower bound as loss expression is the fastest option (even faster than the monotonic approach) as the oracle does not need to compute cycles at all, speeding up the training process. This is feasible because the lower and upper bounds allow us to prune the search space: if an upper and a lower bound coincide for a configuration we already know the loss and need not keep searching, and if we can branch to two configurations such that the lower bound of one is greater or equal than an upper bound of the other, we can discard the former as it will never lead to smaller loss than the latter.     haustive search with pruning guarantees to find the exact loss. Thus, although we do not know an algorithm to obtain the exact loss which is fast enough to be practical, any of the three studied loss bounds can be used to obtain a feasible approximate dynamic oracle with full non-monotonicity. For the non-monotonic algorithm, we test the three different loss expressions defined in the previous section. For the non-monotonic dynamic oracle, three variants are shown, one for each loss expression implemented. This could be explained by the fact that identifying problematic cycles is a difficult task to learn for the parser, and for this reason a more straightforward approach, which tries to avoid all kinds of cycles (regardless of whether they will cost gold arcs or not), can perform better. For each language, we run five experiments with the same setup but different seeds and report the averaged accuracy. Best results for each language are shown in boldface. cing a wrong arc with another indirectly helps due to breaking prospective cycles. Finally, note that, despite this remarkable performance, the non-monotonic version (regardless of the loss expression implemented) has an inexplicable drop in accuracy in Basque in comparison to the other two oracles. We presented a novel, fully non-monotonic variant of the well-known non-projective Covington parser, trained with a dynamic oracle. Due to the unpredictability of a non-monotonic scenario, the real loss of each configuration cannot be computed. To overcome this, we proposed three different loss expressions that closely bound the loss and enable us to implement a practical non-monotonic dynamic oracle. On average, our non-monotonic algorithm obtains better perfomance than the monotonic version, regardless of the loss calculation used. In particular, one of the loss expressions developed proved very promising by providing the best average accuracy, in spite of being the farthest approximation from the actual loss. On the other hand, the proposed lower bound makes the nonmonotonic system the fastest one among all dynamic oracles developed for the non-projective Covington algorithm. To our knowledge, this is the first implementation of non-monotonicity for a nonprojective parsing algorithm, and the first approximate dynamic oracle that uses close, efficientlycomputable approximations of the loss, showing this to be a feasible alternative when it is not practical to compute the actual loss."
37,1,4.0,4.0,4.0,True,acl_2017,dev,"We study response selection for multi-turn conversation in retrieval based chatbots. Existing work either concatenates utterances in context or matches a response with a highly abstract context vector finally, which may lose relationships among the utterances or important information in the context. We propose a sequential matching network (SMN) to address both problems. SMN first matches a response with each utterance in the context on multiple levels of granularity, and distills important matching information from each pair as a vector with convolution and pooling operations. The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models relationships among the utterances. The final matching score is calculated with the hidden states of the RNN. Empirical study on two public data sets shows that SMN can significantly outperform state-of-the-art methods for response selection in multi-turn conversation.","Existing work on building chatbots includes generation based methods and retrieval based methods. Retrieval based chatbots enjoy the advantage of informative and fluent responses, because they select a proper response for the current conversation from a repository with re-sponse selection algorithms. In a chatbot, multi-turn response selection takes a message and utterances in its previous turns as input and selects a response that is natural and relevant to the whole context. The key to response selection lies in inputresponse matching. Different from single-turn conversation, multi-turn conversation requires matching between a response and a conversation context in which one needs to consider not only the matching between the response and the input message but also the matching between the response and the utterances in previous turns. We propose a sequential matching network (SMN), a new context based matching model that can tackle both challenges in an end-to-end way. The reason that existing models lose important information in the context is that they first represent the whole context as a vector and then match the context vector with a response vector. Thus responses in these models cannot meet the context until the final step in matching. To avoid information loss, SMN matches a response with each utterance in the context at the beginning and encodes important information in each pair into a matching vector. The matching vectors are then accumulated in the temporal order of the utterances to model their relationships. The final matching degree is computed with the accumulation of the matching vectors. The two matrices capture important matching information in the pair on a word level and a segment level respectively, and the information is distilled and fused as a matching vector through an alternation of convolution and pooling operations on the matrices. By this means, important information from multiple levels of granularity in the context is recognized under sufficient supervision from the response and carried into matching with minimal loss. The matching vectors are then uploaded to another GRU to form a matching score for the context and the response. The GRU accumulates the pair matching in its hidden states in the chronological order of the utterances in the context. It models relationships and dependencies among the utterances in a matching fashion and has the utterance order supervise the accumulation of pair matching. The match-ing degree of the context and the response is computed by a logit model with the hidden states of the GRU. In addition to the Ubuntu corpus, we create a human labeled Chinese data set, namely Douban Conversation Corpus, and test our model on it. Different from the Ubuntu corpus in which data is collected from a specific domain and negative candidates are randomly sampled, conversations in this data come from open domain, and response candidates in this data set are collected from a retrieval engine and labeled by three human judges. As far as we know, Douban Conversation Corpus is the first human labeled data set for multi-turn response selection and could be a good complement to the Ubuntu corpus. We release Douban Conversation Corups and our source code at an anonymous url for blind review. We have uploaded code and data with this paper. Our work belongs to retrieval based methods, and we study context based response selection. Recently, researchers begin to pay attention to multi-turn conversation. Our model is different in that it matches a response with each utterance at first and accumulates matching information instead of sentences by a GRU, thus useful information for matching can be sufficiently retained.   SMN first decomposes context-response matching into several utterance-response pair matching and then all pair matching is accumulated as a context based matching through a recurrent neural network. SMN consists of three layers. The first layer matches a response candidate with each utterance in the context on a word level and a segment level, and important matching information from the two levels is distilled by convolution and pooling and encoded in a matching vector. The matching vectors are then fed into the second layer where they are accumulated in the hidden states of a recurrent neural network with GRU following the chronological order of the utterances in the context. The third layer calculates the final matching score with the hidden states of the second layer. SMN enjoys several advantages over the existing models. First, a response candidate can meet each utterance in the context at the very beginning, thus matching information in every utteranceresponse pair can be sufficiently extracted and carried to the final matching score with minimal loss. Second, information extraction from each utterance is conducted on different levels of granularity and under sufficient supervision from the response, thus semantic structures that are useful to response selection in each utterance can be well identified and extracted. Third, matching and utterance relationships are coupled rather than separately modeled, thus utterance relationships (e.g., order), as a kind of knowledge, can supervise the formation of the matching score.         These areas will be transformed and selected by convolution and pooling operations and carry the important information in the utterance to the matching vector. This is how our model identifies important information in context and leverage it in matching under the supervision of the response. We consider multiple channels because we want to capture important matching information on multiple levels of granularity of text.                         While candidate retrieval is not the focus of the paper, it is an important step in a real system. In this work, we exploit a heuristic method to obtain response candidates from the index.     Then we send the expanded message to the index and retrieve response candidates using the inline retrieval algorithm of the index. Finally, we use g(s, r) to re-rank the candidates and return the top one as a response to the context. We tested our model on a public English data set and a Chinese data set we publish with this paper. Positive responses are true responses from human, and negative ones are randomly sampled. Ubuntu Corpus is a domain specific data set, and response candidates are obtained from negative sampling without human judgment. To further verify the efficacy of our model, we created a new data set with open domain conversations, namely Douban Conversation Corpus. Douban Conversation Corpus simulates the real scenario of a retrieval based chatbot, and we publish it to research communities to facilitate the research of multi-turn response selection. We recruited three labelers to judge if a candidate is a proper response to the context. A proper response means the response can naturally reply to the message given the whole context. Each pair received three labels and the majority of the labels were taken as the final decision. Multi-Channel is a simple version of our model without considering utterance relationships. For baseline models, if their results are available in the existing literatures (e.g., those on the Ubuntu corpus), we just copied the numbers, otherwise we implemented the models following the settings in the literatures. Numbers in bold mean that the improvement is statistically significant compared with the best baseline. We employed early-stopping as a regularization strategy. Even the state-of-the-art singleturn matching models perform much worse than our models. The results demonstrate that one cannot neglects utterance relationships and simply perform multi-turn response selection by concatenating utterances together. SMNdynamic is only slightly better than SMNstatic and SMNlast. The reason might be that GRU can select useful signals from the matching sequence and accumulate them in the final state with its gate mechanism, thus the efficacy of attention mechanism is not obvious for the task. It is from the test set and our model successfully ranked the correct response to the top position. Other pieces of our model are shown in the supplementary material. Darker areas mean larger value. mation is extracted from it. This is because NTN only matches a pair by an utterance vector and a response vector and loses important information in the pair. Second, the performance slightly drops when replacing the GRU for matching accumulation with a multi-layer perceptron (denoted as ReplaceA). This indicates that utterance relationships are useful. Context length: we study how our model (SMNlast) performs across the length of contexts. Our model consistently performs better than the baselines, and when contexts become longer, the gap becomes larger. The results demonstrate that our model can well capture the dependencies, espe-cially long dependencies, among utterances in contexts. We give the comparisons on other metrics in our supplementary material. We conducted a side-by-side human comparison on the top one responses of the two models for each context in the test set. This indicates that a retrieval based chatbot with SMN can provide a better experience than the state-of-the-art generation model in practice. We present a new context based model for multiturn response selection in retrieval-based chatbots. Experiment results on public data sets show that the model can significantly outperform the stateof-the-art methods. Besides, we publish the first human labeled multi-turn response selection data set to research communities. In the future, we are going to study how to model logical consistency of responses and improve candidate retrieval (see supplementary material)."
352,2,4.0,4.6,4.0,True,acl_2017,dev,"Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks.","However, most existing work on multi-task learning attempts to divide the features of different tasks into private and shared spaces, merely based on whether parameters of some components should be shared. The major limitation of this framework is that the shared feature space could contain some unnecessary task-specific features, while some sharable features could also be mixed in private space, suffering from feature redundancy. Taking the following two sentences as examples, which are extracted from two different sentiment classification tasks: Movie reviews and Baby products reviews. The infantile cart is simple and easy to use. This kind of humour is infantile and boring. Additionally, the capacity of shared space could also be wasted by some unnecessary features. To address this problem, in this paper we propose an adversarial multi-task framework, in which the shared and private feature spaces are inherently disjoint by introducing orthogonality constraints. To prevent the shared and private latent feature spaces from interfering with each other, we introduce two strategies: adversarial training and orthogonality constraints. The adversarial training is used to ensure that the shared feature space simply contains common and task-invariant information, while the orthogonality constraint is used to eliminate redundant features from the private and shared spaces. The contributions of this paper can be summarized as follows. Proposed model divides the task-specific and shared space in a more precise way, rather than roughly sharing parameters. We extend the original binary adversarial training to multi-class, which not only enables multiple tasks to be jointly trained, but allows us to utilize unlabeled data. We can condense the shared knowledge among multiple tasks into an off-the-shelf neural layer, which can be easily transferred to new tasks. Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks. We define the LSTM units at each time step t to be a collection of vectors in Rd: an input gate it, a forget gate ft, an output gate ot, a memory cell ct and a hidden state ht. d is the number of the LSTM units. The LSTM is precisely specified as follows. The output at the last moment hT can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes. Given a corpus with N training samples (xi, yi), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions. Yellow and gray boxes represent shared and private LSTM layers respectively. classification by learning tasks in parallel. To facilitate this, we give some explanation for notations used in this paper. In neural network based model, the latent features can be regarded as the states of hidden neurons. Specific to text classification, the latent features are the hidden states of LSTM at the end of a sentence. Therefore, the sharing schemes are different in how to group the shared features. Here, we first introduce two sharing schemes with multi-task learning: fully-shared scheme and shared-private scheme. Fully-SharedModel (FS-MTL) In fully-shared model, we use a single shared LSTM layer to extract features for all the tasks. For example, given two tasks m and n, it takes the view that the features of taskm can be totally shared by task n and vice versa. This model ignores the fact that some features are task-dependent. Accordingly, we can see each task is assigned a private LSTM layer and shared LSTM layer. The final features are concatenation of the features from private space and shared space. For a sentence in task k, its feature h(k), emitted by the deep muti-task architectures, is ultimately fed into the corresponding task-specific softmax layer for classification or other tasks. The parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions on all the tasks. Although the shared-private model separates the feature space into the shared and private spaces, there is no guarantee that sharable features can not exist in private feature space, or vice versa. Thus, some useful sharable features could be ignored in shared-private model, and the shared feature space is also vulnerable to contamination by some taskspecific information. Therefore, a simple principle can be applied into multi-task learning that a good shared feature space should contain more common information and no task-specific information. Yellow and gray boxes represent shared and private LSTM layers respectively. The goal is to learn a generative distribution pG(x) that matches the real data distribution Pdata(x) Specifically, GAN learns a generative network G and discriminative model D, in which G generates samples from the generator distribution pG(x). and D learns to determine whether a sample is from pG(x) or Pdata(x). This adversarial training encourages shared space to be more pure and ensure the shared representation not be contaminated by task-specific features. Task Discriminator Discriminator is used to map the shared representation of sentences into a probability distribution, estimating what kinds of tasks the encoded sentence comes from. Adversarial Loss Different with most existing multi-task learning algorithm, we add an extra task adversarial loss LAdv to prevent task-specific feature from creeping in to shared space. The task adversarial loss is used to train a model to produce shared features such that a classifier cannot reliably predict the task based on these features. The original loss of adversarial network is limited since it can only be used in binary situation. Here, there is a minmax optimization and the basic idea is that, given a sentence, the shared LSTM generates a representation to mislead the task discriminator. At the same time, the discriminator tries its best to make a correct classification on the type of task. After the training phase, the shared feature extractor and task discriminator reach a point at which both cannot improve and the discriminator is unable to differentiate among all the tasks. Semi-supervised Learning Multi-task Learning We notice that the LAdv requires only the input sentence x and does not require the corresponding label y, which makes it possible to combine our model with semi-supervised learning. Finally, in this semi-supervised multi-task learning framework, our model can not only utilize the data from related tasks, but can employ abundant unlabeled corpora. We notice that there is a potential drawback of the above model. That is, the task-invariant features can appear both in shared space and private space. Test Unlab. Avg. L Vocab. The last two columns represent the average length and vocabulary size of corresponding dataset. on shared-private latent space analysis, we introduce orthogonality constraints, which penalize redundant latent representations and encourages the shared and private extractors to encode different aspects of the inputs. The goal is to classify a product review as either positive or negative. The remaining two datasets are about movie reviews. One key aspect of this dataset is that each movie review has several sentences. The multi-task frameworks proposed by previous works are various while not all can be applied to the tasks we focused. Nevertheless, we chose two most related neural models for multi-task learning and implement them as competitor methods. The numbers in brackets represent the improvements relative to the average performance (Avg.) of three single task baselines. From this table, we can see that the performance of most tasks can be improved with a large margin with the help of multi-task learning, in which our model achieves the lowest error rates. It is noteworthy that for FS-MTL, the performances of some tasks are degraded, since this model puts all private and shared information into a unified space. With the help of adversarial learning, the shared feature extractor Es can generate more pure taskinvariant representations, which can be considered as off-the-shelf knowledge and then be used for unseen new tasks. The parameters of transferred layer are kept frozen, and the rest of parameters of the network MT are randomly initialized. More formally, we investigate two mechanisms towards the transferred shared extractor. The first one Single Channel (SC) model consists of one shared feature extractor Es from MS, then the extracted representation will be sent to an output layer. By contrast, the BiChannel (BC) model introduces an extra LSTM layer to encode more task-specific information. sarial training framework, we also make a comparison with vanilla multi-task learning method. Besides, for the two kinds of transfer strategies, the Bi-Channel model performs better. The reason is that the task-specific layer introduced in the BiChannel model can store some private features. Overall, the results indicate that we can save the existing knowledge into a shared recurrent layer using adversarial multi-task learning, which is quite useful for a new task. To get an intuitive understanding of how the introduced orthogonality constraints worked compared with vanilla shared-private model, we design an experiment to examine the behaviors of neurons from private layer and shared layer.     By visualizing the hidden state hj and analyzing the maximum activation, we can find what kinds of patterns the current neuron focuses on. Here, we randomly sample a sentence from the validation set of Baby task and analyze the changes of the predicted sentiment score at different time steps, which are obtained by SP-MTL and our proposed model. Additionally, to get more insights into how neurons in shared layer behave diversely towards different input word, we visualize the activation of two typical neurons. This indicates the importance of introducing adversarial learning to prevent the shared layer from being contaminated by task-specific features. Y-axis represents the sentiment score, while X-axis represents the input words in chronological order. The darker grey horizontal line gives a border between the positive and negative sentiments. Clearly, suppose we have many tasks to be trained jointly, the shared layer bear much pressure and must sacrifice substantial amount of capacity to capture the patterns they actually do not need. Furthermore, some typical taskinvariant features also go into task-specific layer. There are two threads of related work. One thread is multi-task learning with neural network. In most of these models, the lower layers are shared across all tasks, while top layers are task-specific. These work has potential limitation of just learning a shared space solely on sharing parameters, while our model introduce two strategies to learn the clear and non-redundant shared-private space. Another thread of work is adversarial network. Adversarial networks have recently surfaced as a general tool measure equivalence between distributions and it has proven to be effective in a variety of tasks. Different from these models, our model aims to find task-invariant sharable information for multiple related tasks using adversarial training strategy. Moreover, we extend binary adversarial training to multi-class, which enable multiple tasks to be jointly trained. In this paper, we have proposed an adversarial multi-task learning framework, in which the taskspecific and task-invariant features are learned non-redundantly, therefore capturing the sharedprivate separation of different tasks. We also perform extensive qualitative analysis, deriving insights and indirectly explaining the quantitative improvements in the overall performance."
489,2,3.5,4.5,4.0,True,acl_2017,dev,"We compare three recent models of referential word meaning that link visual object representations to lexical representations in a distributional vector space, either directly through cross-modal mapping or indirectly through visual predictors for individual words. We use these models to predict object names as they could be used in naturalistic referring expressions. We find that cross-modal mapping generally produces semantically appropriate and mutually highly similar object names in its topn list, but sometimes fails to make desired distinctions. Visual word predictors, on the other hand, can react to more subtle visual distinctions and select specific terms, but sometimes stray taxonomically very far from the correct one. Combination of the approaches improves over the individual predictions in a standard naming task. All approaches can be extended to the zero-shot naming case, where the correct name is one for which no instances were seen during training; again they show complementary strengths and weaknesses, depending on the setup and the lexical relation of the unattested object name to known ones.","For a long time, however, research on REG mostly assumed the availability of symbolic representations of referent and scene, and sidestepped questions about how speakers actually choose these names, due to the lack of models capable of capturing what a word like house refers to in the real world. Recent advances in image processing promise to fill this gap, with state-of-the-art computer vision systems being able to classify images into thousands of different categories (eg. In contrast, humans seem to be more flexible as to the chosen level of generality. Depending on the prototypicality of the object to name, and possibly other visual properties, a general name might be more or less appropriate. Other work at the intersection of image and language processing has investigated models that learn to directly associate visual objects with a representation of word meaning, for example through cross-modal transfer into distributional vector spaces. Under the assumption that such semantic spaces represent, in some form at least, taxonomic knowledge, this makes labels on different levels of specificity available for a given object. Moreover, if the mapping is sufficiently general, it should be able to map objects to an appropriate label, even if during training of the mapping this label has not been seen (zero-shot learning). This paper pursues the hypothesis that an accurate model of referential word meaning does not need to fully integrate visual and lexical knowledge (e.g. as expressed in a distributional vector space), but at the same time, has to go beyond treating words as independent labels. We explore different ways of linking these predictors to distributional knowledge, during application and during training. We find that these improve over direct cross-modal mapping and direct visual classification in a standard and a zero-shot setup of an object naming task, as they allow for a more flexible combination of lexical and visual information when modeling referential meaning. In this paper, we focus on a particular problem posed by REG on real-world images, namely generating the appropriate head noun for a given object. Their approach focusses links abstract object categories in ImageNet to actual words via various translation procedures. We are interested in learning referential appropriateness and extensional word meanings directly from actual human referring expressions (REs) paired with objects in images, using an existing object recognizer for feature extraction. When tested on standard object recognition tasks, transfer, however, comes at a price. To the best of our knowledge, this pattern has not been systematically investigated any further. In this work, we present a model that exploits distributional knowledge for learning referential word meaning as well, but explore and compare different ways of combining visual and lexical aspects of referential word meaning. Cf. This is similar to picture naming setups used in psycholinguistic research (cf. Names For most of our experiments, we only use a subset of this vocabulary, namely the set of object names. As the REs contain nouns that cannot be considered to be names (background, bottom, etc. Nevertheless, the data is challenging, as the corpus contains references to objects that fall outside of the object labeling scheme that available object recognition systems are typically optimized for, cf. We pair each image region from the test set with its corresponding names from the remaining REs. This model will be called TRANSFER below. During training, we pair each object with the distributional embedding of its name, and use standard Ridge regression for learning the transformation. For ease of comparison with other models, we stick with simple Ridge Regression in this work. For decoding, we map an object into the distributional space, and retrieve the nearest neighbors of the predicted vector using cosine similarity. In theory, the model should generalize easily to words that it has not observed in a pair with an object during training as it can map an object anywhere in the distributional space. Predictions can then be mapped into distributional space during application time via the vectors of the predicted words. Thus, the classifiers learn to predict referential appropriateness for individual words based on the visual features of the objects they refer to, in isolation of other words. The model can be used to predict names directly, without links into a distributional space. Then, in parallel to cross-modal mapping, we retrieve the nearest neighbors of the combined embedding from the distributional space. Thus, with this model, we use two different modes of decoding: one that projects into distributional space, one that only applies the available word classifiers. Finally, we implement an approach that combines ideas from cross-modal mapping with the WAC model: we train individual predictors for each word in the vocabulary, but, during training, we exploit lexical similarity relations encoded in a distributional space. Instead of treating a word as a binary classifier, we annotate its training instances with a fine-grained similarity signal according to their object names. Thus, we task the model with jointly learning similarities and referential appropriateness, by training it with Ridge regression on a continuous output space. This is the SIM-WAP model, recently proposed in (Anonymous). When training a predictor for such a word w, we use all available objects from our corpus and annotate them with the expected lexical similarity between w and the actual object names v, which for all objects will be below the maximal value that marks genuine positive instances. During decoding, this model does not need to project its predictions into a distributional space, but it simply applies all available predictors to the object, and takes the argmax over the predicted referential appropriateness scores. This Section reports on experiments in a standard setup of the object naming task where all object names are paired with visual instances of their referents during training. This seems to suggest that lexical or at least distributional knowledge is detrimental when learning what a word refers to in the real world and that referential meaning should potentially be learned from visual object representation only. individual word classifiers trained on visual features only. This suggests that referential meanings for a word are learned less accurately when mapping from visual to distributional space, which replicates results reported in the literature on standard object recognition benchmarks. If the models are complementary, their combination should lead to more confident and accurate naming decisions. Setup We combine TRANSFER, SIM-WAP and WAC by aggregating the scores they predict for different object names for a given object. On their own, the distributionally informed models are less tuned to specific word occurrences than the visual word classifiers in the WAC model, but they can add useful information which leads to a clear overall improvement. We take this as a promising finding, supporting our initial hypothesis that knowledge on lexical distributional meaning should and can be exploited when learning how to use words for reference. Av. These examples give some interesting insight into why the models capture different aspects of referential word use and meaning. The transfer and sim-wap model rank words on top that are clearly more similar to each other than word pairs on average, whereas words ranked top by the wac model are more dissimilar. Another observation is that the mapping models have difficulties dealing with object names in singular and plural. Such specific phenomena at the intersection of visual and semantic similarity have found very little attention in the literature. We will investigate them further in our Experiments on zeroshot naming in the following Section. This could constitute an additional challenge for zero-shot learning, as distributional similarities might be misleading when the model has to fully rely on them for learning referential word meanings. Both relations are typically captured by distributional models of word meaning in terms of closeness in the vector space, but their visual and referential relation is clearly different. Random As in previous work on zero-shot learning, we consider zero-shot naming for words of varying degrees of similarity in our vocabulary. the model sees objects during training that are instances of vehicle for example, but never encounters actual uses of that name. From each pair, we randomly include the singular or plural noun in the set of zero-shot nouns. Thus, we make sure that the model encounters singular and plural names during training, but it never encounters both variants of a name. This setup factors out the generalization problem, and assesses to what extent a model is able to capture the referential meaning of a word that does not have instances in the training data. In particular, we find that the SIMWAP model which induces individual predictors for words that have not been observed in the training data is clearly more successful than TRANSFER or WAC that project predictions into the distributional space. An interesting exception is the good performance of the TRANSFER model on the hypernym test set, when evaluated with a disjoint vocabulary. Thus, combining several vectors predicted by a model of referential word meaning can provide additional information, as compared to mapping an object to a single vector in distributional space. More work is needed to establish how these approaches can be integrated more effectively. Previous cross-modal mapping models essentially force semantically similar objects to be mapped into the same area in the semantic space regardless of their actual visual similarity. We found that cross-modal mapping produces semantically appropriate and mutually highly similar object names in its top-n list, but does not preserve differences in referential word use (e.g. appropriatness of person vs. woman) especially within the same semantic field. We have shown that it is beneficial for performance in standard and zeroshot object naming to treat words as individual predictors that capture referential appropriateness and are only indirectly linked to a distributional space, either through lexical mapping during application or through cross-modal similarity mapping during training. Also from an REG perspective, various extensions of this approach are possible, such as the inclusion of contextual information during object naming and its combination with attribute selection."
173,2,3.5,3.0,4.0,True,acl_2017,dev,"Word embeddings have become widelyused in document analysis. A large number of models have been proposed, but the net gain these models can achieve expectably beyond the traditional bag-ofwords based approaches remains undetermined. Our empirical studies, conducted from a nonparametric unsupervised perspective, reveal where and how word embeddings can contribute to document analysis. Our approach is based on a recent algorithmic advance in nonparametric clustering for empirical measures, which neither invents nor relies on any document vector representations. The new document clustering approach proposed in this work is easy to use and stably outperforms other existing methodologies on a variety of document-clustering tasks.","A key appeal of word embedding methods is that they can be obtained from external large-scale corpus and then be easily utilized for different data. Moreover, they must also consider how to quantify that gain. Such a preliminary evaluation is often necessary before any further decisions can be made about the data. Based on word embeddings, high-level models are designed for various tasks. Examples include entity representations, similarity measures, data manifolds, hierarchical structures, language models, and neural architectures. Therefore, it is important to investigate whether the gain or loss found in practice should be credited to the extra assumptions associated with those high-level models or to the use of basic word embeddings. As our experiments demonstrate, introducing these extra assumptions will make individual methods effective only if certain constraints are met. We will address this issue from an unsupervised perspective. Our proposed clustering framework has several advantages. Hence, it excludes any vector representation of the documents and sidesteps extra high-level assumptions, which is crucial and beneficial to evaluating the gain from basic word embeddings. Our approach is intuitive and robust. the Wasserstein distance considers the cross-term relationship between different words in a principled fashion. Here, the travel cost of a path between two words is their (squared) Euclidean distance in the word embedding space. The main technical hurdle is to compute the Wasserstein barycenter, which is a discrete distribution, efficiently for a given set of discrete distributions. Obtaining highest quality clustering of unstructured text data merits consuming extra computational resources. Our contributions. Our work has two main contributions. First, we create a basic tool of document clustering with mere hyper-parameters at scale. Our tool leverages the state-of-the-art numerical toolbox developed for optimal transport to achieve computational feasibility. Second, with our tool, one can quantitatively inspect how well a word-embedding model can fit the data and how much gain or loss will be obtained compared to traditional bag-of-words models. Acquiring insights on these questions is valuable for document analysis beyond clustering. Such high magnitude of computations had prohibited it from many real-world applications until recently. Although the effectiveness of Wasserstein distance has been well recognized in the computer vision and multimedia literature, the property of Wasserstein barycenter has not been well understood. Our work differs from theirs in the methodology. We directly pursue a scalable clustering setting rather than construct a nearest neighbor graph based on calculated distances, because the calculation of the Wasserstein distances of all pairs is too expensive to be practical. On the other hand, our approach adopts the framework similar to K-means which is of complexity O(n) per iteration, and usually converges within tens of iterations.   Suppose we are to find K clusters. The assignment step finds each member distribution its nearest mean from K candidates.   K. Each mean is iteratively updated to minimize its total within cluster variation.   cK are Wasserstein barycenters. At the core of solving the above formulation is an optimization method that searches the Wasserstein barycenters of varying partitions. Therefore, we concentrate on the following problem.   The above Wasserstein barycenter formulation involves two levels of optimization: the outer level finding the minimizer of total variations, and the inner level solving Wasserstein distances. This constitutes the third level of optimization. We briefly sketch their algorithmic procedure of this optimization method here for the sake of completeness. To solve for Wasserstein barycenter defined in Eq.         In a data parallel implementation, only Eq. We make available our codes and pre-processed datasets for reproducing all experiments of our approach. We prepare six datasets to conduct a set of experiments. Two short-text datasets are created as follows. It also shows more realistic nature of real-world applications such as news event clustering. We also experiment with two long-text datasets and two domain-specific text datasets. Evaluating clustering results is known to be nontrivial. Generally speaking, more clusters leads to higher homogeneity by chance. The largest possible vocabulary used, excluding word embedding based approaches, is composed of words appearing in at least two documents. On each dataset, we select the same set of Ks, the number of clusters, for all methods. We prepare two versions of the TF-IDF vectors as the unigram model. The ensembled K-means methods are used to obtain clusters. The difference between the two methods highlights the sensitivity issue brought by the size of chosen vocabulary. We also compare our approach with the following seven additional baselines. Details on their experimental setups and hyper-parameter search strategies can be found in the Appendix. We report the runtime for our approach upon two largest datasets. The experiments regarding other smaller datasets all finish within minutes in a single machine, which we omit due to page constraints. Like K-means, the running time spent by our approach depends on the number of actual iterations before a termination criterion is met. Based on the results, our approach is much more practical as a basic document clustering tool. We summarize our numerical results in this section. Regular text datasets. We consider them to be regular and representative datasets encountered more frequently in applications. We report the clustering performances of the ten methods in Fig. The higher result at the same level of homogeneity is better, and the ability to achieve higher homogeneity is also welcomed. Specifically, it ranks first in three datasets, and second in the other one. Laplacian, LSI, and TfidfN can achieve comparably performance if their reduced dimensions are fine tuned, which unfortunately is not realistic in practice. NMF is a simple and effective method which always gives stable, though subpar, performance. Short texts vs. long texts. This outcome is somewhat expected, because the bag-ofwords method suffers from high sparsity for short texts, and word-embedding based methods in theory should have an edge here. As shown in Fig. Nevertheless, we find lifting from word embedding to document clustering is not a free lunch. Domain-specific text datasets. We are also interested in how word embedding can help group domain-specific texts into clusters. Our preliminary result indicates state-of-the-art word embeddings do not provide enough gain here to exceed the performance of existing methodologies. See next section for more discussions.) As we mentioned, the effectiveness of Wasserstein document clustering depends on how relevant the utilized word embeddings are with the tasks. In those general document clustering tasks, however, word embedding models trained on general corpus perform robustly well with acceptably small variations. This outcome reveals our framework as generally effective and not dependent on a specific word embedding model. Their results are not as good as those we reported (therefore detailed numbers are not included due to space limit). Inadequate embeddings may not be disastrous. The improvements by percentiles are also shown in the subscripts. Its performances across different datasets is highly correlated with the bag-of-words (Tfidf and TfidfN). Performance advantage. Therefore, for document clustering, users can expect to gain performance improvements by using our approach. Clustering sensitivity. They are always contracted around the top-right region of the whole population, revealing the predictive and robustly supreme performance of our approach. When bag-of-words suffices. Other unsupervised regularization over data is likely unnecessary, or even degrades the performance slightly. Toward better word embeddings. Our experiments on the Ohsumed dataset have been limited. The result shows that it could be highly desirable to incorporate certain domain knowledge to derive more effective vector embeddings of words and phrases to encode their domainspecific knowledge, such as jargons that have knowledge dependencies and hierarchies in educational data mining, and signal words that capture multi-dimensional aspects of emotions in sentiment analysis. Furthermore, this observation holds for varying lengths of documents and varying difficulty levels of clustering tasks. Our nonparametric framework benefits from both bag-of-words and word embeddings. This paper introduces a nonparametric clustering framework for document analysis. Its computational tractability, robustness and supreme performance, as a fundamental tool, are empirically validated. Its ease of use enables data scientists to use it for the pre-screening purpose of examining word embeddings in a specific task. Finally, the gains acquired from word embeddings are quantitatively measured from a nonparametric unsupervised perspective."
371,2,2.0,3.14,2.0,False,acl_2017,dev,"We propose a new, simple, yet effective framework, phrasal recurrent neural networks (pRNN), for language modeling and machine translation. Different from previous RNN-based language models, pRNNs store the sentential history as a set of candidate phrases with different lengths that precede the word to predict. To represent phrases as fix-length realvalued vectors, we build the RNN pyramid, which is composed of shifted parallel RNN sequences. When predicting the next word, pRNNs employ a soft attention mechanism to selective and combine the suggestions of candidate phrases. We test our model on language model and machine translation tasks. Our model leads to an improvement of over 10 points in perplexity both on standard Penn Treebank and FBIS English data set over a state-of-the-art LSTM language modeling baseline. We also apply pRNNs to encode the source sentence of machine translation besides a conventional bi-direction encoder, which improves over the Moses (phrase-based statistical model) and a strong sequence-to-sequence baseline in the Chinese-English machine translation task.","A somewhat opposite direction is to explore hyper-word structure. Both models, however, rely heavily on human labeled data on the language structures, which is extremely expensive and limited in scale. What different here are pRNNs permit phrases with arbitrary lengths instead of limiting them for the computational issue. The phrases in pRNNs are composed and selected in a way that is jointly learned in the language modeling, therefore requiring no human-labeled data or external model such as word alignment. In previous RNN-based language modeling, the hidden state of RNN before the word to predict summarizes the history of all previous words. Similarly, in pRNNs, we use the all state of all parallel RNNs (with the same parameters) to capture the history of all subsequence of words that precede the word to predict, with the starting word shifting from the first word the one right before the word to predict. This mechanism will be trained jointly with the composition models in pRNNs in optimizing a designed objective function, e.g, perplexity or likelihood. Which shows the potential to discover and utilize hidden structures of surface word sequences. Instead of stacking deeper and deeper layers of RNNs. Our model obtains significant better perplexities than state-of-the-art sequential Long-Short Term (LSTM) model on language modeling task, both on PTB and FBIS English data set. We also apply pRNNs to encode the source sentence of machine translation besides a conventional bi-direction encoder, which improves over the Moses (phrase-based statistical model) and a strong sequence-to-sequence baseline in the Chinese-English machine translation task. We assume that, in the task of language model and machine translation, selecting the appropriate hidden structures for one sentence is highly related to the performance of the task. It takes embed-dings (x?) Even when we choose RNNs to construct structure vectors, we are still facing a big problem. To represent all candidate phrases in a sentence with n words, we build a RNN pyramid (RNNP (Fig. RNNn indicates that it begins at the n-th word of the sentence. With RNN pyramid built on a sentence, we can map all potential phrases with varying lengths into real-valued fix-length vectors. These vectors are representations of candidate structures we plan to compare at next stage. With the candidate structures represented by a fixlength vector (Fig. Initial status are indicated by circles. Because hidden state is considered containing all history information. The set of all hidden status in the pyramid can be mapped one-to-one to the representation of all candidate phrases of the sentence. al. The Pyramid and Seq part share the same embedding in experiments, we draw them separately in the diagram just for clearance. All rest low-frequency words are replaced with UNK tag. This version of data is widely used among the language modeling community. Then we join two context vectors from two encoders into a larger one. Then we allow the decoder to choose which portion of the larger context (which source words or candidate phrases) is more relevant to the next generated word of the target sentence.   We evaluate all three models, PBSMT, RNNsearch, pRNN on the same data set. All rest low-frequency words are replaced with UNK tag. Phrase-based NMT We configure our model exactly as the baseline model, except adding an extra RNN pyramid as a secondary source encoder. As the limitation on the memory of GPU, we keep only phrases ended at eos into consideration. The above two lines of the table are results of open-source machine translation systems. Bold numbers indicate the best results on the data set (column). pRNNs are better than original RNNsearch model baseline (in-house reimplemented). We find it is interesting that results of src-pyr-all are only slightly better than src-pyr-last, we guess this is due to the limited discriminative power of simple attention mechanism when meeting large number of complex candidates. For a fair comparison, we run baseline system (RNNsearch) many times (not epoch) and report only the best one. We only run PBNMT once. The Same trend can be observed in neural network strand. In neural machine translation (NMT) area, the situation is more complex. The most important reason to dig into sub-word is to handle out-of-vocabulary word problem. This problem is rooted in the limited size of vocabulary, which utilized by NMT mapping symbols to real-valued dense vector. Another direction is to explore hyper-word structure. However, this method depends heavily on human-labeled data, which is always expensive and limited in scale. However, the HPB model and NMT model are trained separately and combined only when decoding. Deep Memory Network is an effective implementation of the neural turing machine. However, there are much more other methods in the location-based category. Cell-by-cell vs. Incremental If we consider the hidden of one time step inside pyramid RNN as memory, we can name the operations as incremental read and write. The intuition behind incremental addressing is, when we read little, we know little, we only have the ability to write little. But when we read more, we know more, we are gone to have the ability to write more. Our experiments clearly show that the proposed pRNN model is quite effective in language modeling and machine translation. The most significant question that remains is how well the quality of forest generated as a by-product of pRNN, will it get a better result than other supervised parsing model trained on human label data. We introduced phrasal recurrent neural network, an RNN model with all potential candidate phrases considered. Our model does not require any human labeled data to construct the structures. It outperforms the state-of-the-art LSTM language models. Our model does not require any external resources such as human labeled data or word align model to construct the phrases. It outperforms both state-of-the-art PBSMT and RNNsearch model."
768,3,2.82,2.27,2.36,False,acl_2017,test,"Detecting entailment between words is a key task for several NLP applications. Previous work has largely focused on entailment between words out of context. We propose, instead, to address lexical entailment in context, providing exemplar sentences to ground the meaning of words considered in the entailment relation. We show that contextualized word representations constructed from existing word embeddings, and word-context similarity features lead to significant improvements over context-agnostic models on two novel entailment test sets, and also improve the state-of-the-art on the related task of detecting semantic relations in context (Shwartz and Dagan, 2015).","Ignoring context is problematic since entailment might hold between some senses of the words, but not others. The championship game was played in NYC. The hunters were interested in the big game. In this paper, we investigate how to represent and compare the meaning of words in context for lexical entailment. We say that entailment holds if the meaning of wl in the context of cl is more specific than the meaning of wr in the context of cr. However, we limit entailment to the specificity relation in this work to better understand the impact of context. Note that lexical entailment in context is not textual entailment. How can we construct vector representations of the meaning of target words wl and wr in their respective exemplar contexts cl and cr? agnostic representations of words in cl and cr respectively. First, we construct fixed length representations for the contexts cl and cr by running convolutional filters over Cl and Cr. An alternative approach to contextualizing word representations is to directly compare the representations of words with representations of contexts. This model approaches the state-of-the-art on lexical substitution, sentence completion, and supervised word sense disambiguation. Given words and context representations described above, how can we predict entailment? We follow the same practice with our context-aware word representations. Intuitively, the classifier learns to weight the importance of each dimension of a word representation to detect entailment. We hypothesize that entailment relations hold between related words and introduce similarity features to capture this non-directional relation between words and contexts. The cosine similarity captures the difference in the two vectors in terms of the angle between then, the euclidean distance measures the difference in magnitude, and the dot product captures both magnitude and angle. We apply these measures to three types of representations. These word pairs are semiautomatically labeled with semantic relations outof-context. In addition to CONTEXT-PPDB, we would like a test set for controlled analysis of lexical entailment in context. We would like to directly assess the sensitivity of our models to contexts that signal different word senses, as well as quantify the extent to which our models detect asymmetric entailment relations rather than semantic similarity. We call this dataset CONTEXT-WN. WordNet groups synonyms into synsets. Most synsets are further accompanied by one or more short sentences illustrating the use of the members of the synset. The idea behind CONTEXT-WN is to use these example sentences as context for the words, and hypernymy relations to draw examples of lexical entailment relations. Also obtain c i and cih which are example sentences corresponding to wi and wih respectively- (wi, wih, c i, cih) serves as a positive example. Permute the positive examples to get negative examples. From (wi, wih, c i, cih) and (wj, wjh, c j, cjh), generate negative examples (wi, wjh, c i, cjh) and (w j, wih, c j, cih). Flip the positive examples to generate more negative examples. From (wi, wih, c i, cih) generate the negative example (wih, w i, cih, c i). CONTEXT-WN satisfies our desiderata. The dataset has a very specific focus since we only pick hypernym-hyponym pairs. Our final dataset takes a cross-lingual view of lexical entailment in context. Each word pair (wl, wr) consists of an English word and a French word. For each example, we assign the majority judgment as the correct label. We use Logistic Regression as our classifier for both tasks, to allow for direct comparisons with previous work. We do not tune parameters of the classifier, except for adding class weights in the CONTEXT-WN experiments to account for the unbalanced data. We evaluate our models on cross-lingual test set in a transfer setting by training on the training set of CONTEXT-PPDB. The classifier is again a Logistic Regression classifier with class weights. These features include scores for likelihood of context-agnostic entailment labels, distributional similarities, and probabilities of the word pair being paraphrases, among other scores. The trends on CONTEXT-WN are similar to those on CONTEXT-PPDB. The evaluation metric is weighted F score. the baseline. The strength of these results attest to the generalizable nature of our features, which helps capture correspondences beyond a single language. In this section, we aim to further test the assumptions underlying our proposed context representations. How sensitive are our models to changes in contexts? These examples are created by permuting the contexts of the positive examples, and thus, can directly help in answering our question. It is reassuring to observe that improved detection of context with our features does not come at the cost of detecting entailment direction. the masked representations calculated by using only the mean. In fact, on CONTEXT-WN we can see that our method also captures directionality best. The dataset we introduce in this work is inspired by the latter line of work, but instead of just extracting word pairs we also obtain exemplar contexts from WordNet. Modeling word meaning in context Several approaches have been proposed to model the meaning of a word in a given context to capture semantic equivalence in tasks such as lexical substitution, word sense disambiguation or paraphrase ranking (but not entailment). These models methods start with token representations for individual occurrences of a word and then choose a set of token vectors based on the current context. These models share the idea of using an element-wise multiplication to apply a context mask to word type representations. The use of convolution is motivated by success of similar models on sentence classification tasks. However, all these works only use convolved representations to predict properties of the sentence (e.g., sentiment). We use them, instead, to contextualize our target word representations. We proposed to address lexical entailment in context, providing exemplar sentences to ground the meaning of words being considered for entailment. We show that contextualized word representations constructed by transforming contextagnostic representations, combined with wordcontext similarity features, lead to large improvements over context-agnostic model, not only in English, but also between English and French words on two novel datasets. We also improve the state-of-the-art on a related task of detecting semantic relations in context. Our features are sensitive to changes in entailment based on context, and also capture the directionality of entailment. In future work, we aim to further improve performance on both monolingual and cross-lingual datasets."
355,3,4.0,4.0,4.0,True,acl_2017,test,"The accuracy of Japanese predicate argument structure (PAS) analysis has improved in recent years thanks to joint modeling of interactions between multiple predicates. However, this approach heavily relies on syntactic information predicted by parsers, and suffers from the error propagation. To remedy this problem, we introduce a model using grid-type recurrent neural networks (Grid-RNN), which automatically induces features sensitive to multi-predicate interactions from word sequence information of a sentence. The experiments on the NAIST Text Corpus show that our model exceeds the accuracy of the state-of-the-art Japanese PAS analyzer without syntactic information.","This approach is based on the linguistic intuition that the predicates in a sentence are semantically related to each other and the interaction information can be a clue for PAS analysis. However, to model such multi-predicate interactions, this ap-proach heavily relies on syntactic information predicted by parsers and suffers from the error propagation caused by the pipeline processing. To remedy this problem, we propose a neural model which automatically induces features sensitive to multi-predicate interactions from word sequence information of a sentence. This model takes as input all predicates and their argument candidates in a sentence at a time, and captures the interactions using grid-type recurrent neural networks (Grid-RNN) without syntactic information. In this paper, we firstly introduce a basic model using RNNs, which independently estimates arguments of each predicate without considering the multi-predicate interactions (Sec. Then, extending this model, we propose a neural model using Grid-RNNs (Sec. In particular, the neural model using GridRNNs achieves the best result, which suggests that our grid-type neural architecture effectively captures multi-predicate interactions. In Japanese PAS analysis, we identify arguments taking part in the three major case roles, nominative (NOM), accusative (ACC) and dative (DAT) cases, for each predicate. Zero: The arguments referred to by zero pronouns within the same sentence, which have no direct syntactic dependency with the predicate. Inter-Zero: The arguments referred to by zero pronouns out of the same sentence. Following this trend, we focus on intra-sentential argument analysis. Arguments are often omitted in Japanese sentences. In Japanese PAS analysis, when an argument of the target predicate is omitted, we have to identify the antecedent of the omitted argument (Zero argument). The analysis for such Zero arguments is much more difficult than that forDep arguments because of the lack of direct syntactic dependencies. For Dep arguments, the syntactic dependency between an argument and its predicate is a strong clue. This argument can easily be identified by relying on the syntactic dependency. As an solution to this problem, we exploit two kinds of information: (i) context in the entire sentence and (ii) multi-predicate interactions. For the former, we introduce single-sequence model, which induces context-sensitive representations from a sequence of argument candidates of a predicate. This model consists of three components: (i) Input Layer, (ii) RNN Layer and (iii) Output Layer. RNN Layer: Produce high-level feature vectors using Bi-RNNs. Output Layer: Compute the probability of each case label for each word using the softmax function. In the following subsections, we describe each of them in more detail. PRED: Word index of the target predicate and words around the predicate. MARK: Binary index that represents whether the word is the predicate or not. The underlined word is the target predicate. The extracted features are mapped to each vector, and all the vectors are concatenated into one feature vector. Similarly, as the PRED feature, we extract each word index xword of the C words taking the target predicate at the center, where C is the window size. Then, using feature indices, we extract feature vector representations from each embedding matrix. From each embedding matrix, we extract corresponding column vectors and concatenate them as a feature vector representation xt based on Eq. In the RNN layers, feature vectors are updated recurrently using Bi-RNNs. Bi-RNNs process an input sequence from the left-to-right manner in odd-numbered layers and the opposite in evennumbered layers. By stacking these layers, we can construct the deeper network structures. Using each vector h(L)t, we calculate the probability of case labels for each word in the output layer. Each element of yt is a probability value corresponding to each label. The label with the maximum probability among them is output as a result. In this task, there are five labels: NOM, ACC, DAT, PRED, null. The labels NOM, ACC and DAT indicate the nominative, accusative and dative case, respectively. PRED is the label for the predicate. null represents a word that does not play any case role. While the single-sequence model assumes the independence between predicates, the multisequence model assumes the multi-predicate interactions. Grid Layer: Update the hidden states over different sequences using Grid-RNNs. Output Layer: Compute the probability of each case label for each word using the softmax function. In the following subsections, we describe them in more detail. Then, using Eq. In the grid layers, we use Grid-RNNs to propagate the feature information over the different sequences (inter-sequence connections). Taking as input the hidden states of neighboring sequences, the network propagates feature information over multiple sequences (predicates). Existing approaches for Japanese PAS analysis are divided into two categories: (i) pointwise approach and (ii) joint approach. Compared with the pointwise approach, the joint approach has achieved better results. This is based on the linguistic intuition that the predicates in a sentence are semantically related to each other, and the interaction information can be a clue for PAS analysis. Taking a step further in this direction, we propose the neural architecture that effectively models the multi-predicate interactions. Both models exploited syntactic and selectional preference information as atomic features of neural networks. Using neural networks, the good performance was realized with mitigating the cost of manually designing combination features. In this work, we demonstrate that even without such syntactic information, our neural models realize the state-of-the-art performance by using word sequence information of a sentence. Some neural models achieved high performance without syntactic information in English SRL. Our models can be regarded as an extension from their model. In contrast, in Japanese dependency-based PAS analysis, since arguments are infrequently adjacent to each other, we replaced the CRF with the softmax function. We do not use any external resources. The window size C for the PRED feature (Sec. Single-Seq is the single-sequence model, and Multi-Seq is the multi-sequence model. We compare our models with the models in the previous works (Sec. In all the metrics, both of the single-sequence (Single-Seq) and multisequence model (Multi-Seq) outperformed the baseline models. This confirms that our neural sequence models realize high-performance even without syntactic information by learning contextual information effective for PAS analysis from a word sequence of the sentence. These results suggest that it is beneficial to Japanese PAS analysis, particularly to the zero argument identification, to model the context in the entire sentence using RNNs. This result demonstrates that the grid-type neural architecture can effectively capture the multipredicate interactions by connecting between the sequences of the argument candidates for all predicates in a sentence. NOM, ACC or DAT is the nominal, accusative or dative case, respectively. The performance tends to get better as the RNN or Grid layers get deeper with residual connections. This suggests that although dative arguments appear infrequently compared with the other two case arguments, the neural models can robustly learn it. In addition, for zero arguments (Zero), the neural models achieved better results than the baseline models. Therefore, the improvements of the results suggest that the neural models effectively capture long distance dependencies using RNNs that can encode the context in the entire sentence. In this work, we introduced neural sequence models that automatically induce effective feature representations from word sequence information of a sentence for Japanese PAS analysis. In particular, our multi-sequence model improved the performance for zero argument identification, one of the problematic issues in Japanese PAS analysis, by considering the multi-predicate interactions using Grid-RNNs. Since our neural models are applicable to SRL, applying our models for multilingual SRL tasks is an interesting line of the future research. In addition, in this work, the model parameters were learned without any external resources. For future work, we plan to explore effective methods for exploiting large-scale unlabeled data to learn the neural models."
323,2,4.0,3.5,3.5,True,acl_2017,test,"We propose a local coherence model based on a convolutional neural network that operates over the entity grid representation of a text. The model captures long range entity transitions along with entity-specific features without loosing generalization, thanks to the power of distributed representation. We present a pairwise ranking method to train the model in an end-to-end fashion on a task and learn task-specific high level features. Our evaluation on three different coherence assessment tasks demonstrates that our model achieves state of the art results outperforming existing models by a good margin.","Coherence models that can distinguish a coherent from incoherent texts have a wide range of applications in text generation, summarization, and coherence scoring. The grid is then converted into a feature vector containing probabilities of local entity transitions, which enables machine learning models to learn the degree of text coherence. While the entity grid and its extensions have been successful in many applications, they are limited in several ways. Secondly, feature vector computation in existing models is decoupled from the target task, which limits the models to learn task-specific features. In this paper, we propose a neural architecture for coherence assessment that can capture long range entity transitions along with arbitrary entityspecific features. Our model obtains generalization through distributed representations of entity transitions and entity features. We also present an end-to-end training method to learn task-specific high level features automatically in our model. Discrimination and insertion involve identifying the right order of the sentences in a text with different levels of difficulty. In summary coherence rating task, we compare the rankings, given by the model, against human pairwise judgments of coherence. Our model achieves state of the art results in all these tasks. We have released our code with the submission. The remainder of this paper is organized as follows. Their model represents a text by a two-dimensional array called entity grid that captures transitions of discourse entities across sentences. They consider noun phrases (NP) as entities, and employ a coreference resolver to detect mentions of the same entity (e.g., Obama, the president). Each entry Gi,j in the entity grid represents the syntactic role that entity ej plays in sentence si, which can be one of: subject (S), object (O), or other (X). In addition, entities not appearing in a sentence are marked by a special symbol (-). If an entity appears more than once with different grammatical roles in the same sentence, the role with the highest rank (S O X) is considered. To distinguish between transitions of important entities from unimportant ones, they consider the salience of the entities, which they quantify by their occurrence frequency in the document. Subsequent studies proposed to extend the basic entity grid model. These extensions led to the best results reported so far. Despite its success, existing entity grid models are limited in several ways. In particular, to model transitions of length k withR different grammatical roles, the basic entity grid model needs to computeRk transition probabilities from a grid. One can imagine that the estimated distribution becomes sparse as k increases. In other words, feature extraction is decoupled from the target downstream tasks. This can limit the models to learn task-specific features. Therefore, models that can be trained in an end-to-end fashion on different target tasks are desirable. In the following section, we present a neural architecture that allows us to capture long range entity transitions along with arbitrary entity-specific features without loosing generalization. We also present an end-to-end training method to learn task-specific features automatically. The second layer computes high-level features by going over each column (transitions) of the grid. The following layer selects the most important high-level features, which are in turn used for coherence scoring. The features computed at different layers of the network are automatically trained by backpropagation to be relevant to the task. In the following, we elaborate on the layers of the neural network model. I) Transforming grammatical roles into feature vectors: Grammatical roles are fed to our model as indices taken from a finite vocabulary V. We consider E a model parameter to be learned by backpropagation on a given task. We can initialize E randomly or using pretrained vectors trained on a general coherence task. II) Modeling entity transitions: The vectors produced by the lookup layer are combined by subsequent layers of the network to generate a coherence score for the document. In other words, columns in a grid are treated independently. This means, we aim to compose local patches of entity transitions into higher-level representations, while treating the patches independently of their position in the entity grid. Convolutional filters learn to compose local transition features of a grid into higher-level representations automatically. Moreover, unlike existing grid models that compute transition probabilities from a single document, embedding vectors and convolutional filters are learned from all training documents, which helps the neural framework to obtain better generalization and robustness. Pooling layer: After the convolution, we apply a max-pooling operation to each feature map. from each feature map in the convolutional layer. By applying max pooling on this feature map, the network then discovers that the transition appeared in the grid. Our model as described above neuralizes the basic entity grid model that considers only entity transitions without distinguishing between types of the entities. One simple way to incorporate entity-specific features into our model is to attach the feature value (e.g., named entity type) with the grammatical role in the grid. The training set comprises ordered pairs (di, dj), where document di exhibits a higher degree of coherence than document dj. We evaluate the effectiveness of our coherence models on two different evaluation tasks: sentence ordering and summary coherence rating. to locate the original position of a sentence previously removed from a document. To measure this, each sentence in the document is removed in turn, and an insertion place is located for which the model gives the highest coherence score to the document. The insertion score is then computed as the average fraction of sentences per document reinserted in their actual position. Discrimination can be easier for longer documents, since a random permutation is likely to be different than the original one. Insertion is a much more difficult task since the candidate documents differ only by the position of one sentence. We preferred WSJ corpus for several reasons. We compare our coherence model against a random baseline and several existing models. Random: The Random baseline makes a random decision for the evaluation tasks. This setting yielded best scores for this model. This model considers all nouns as entities. Grid-CNN: This is our proposed neural extension of the basic entity grid (all nouns), where we only consider entity transitions as input. The features are: (i) named entity type, (ii) salience as determined by occurrence frequency of the entity, and (iii) whether the entity has a proper mention. We were not sure what could go wrong, therefore, we excluded it from our table of results. We use rectified linear units (ReLU) as activations (f). Dropout Filter Win. Discr. Ins. The best model on DEV is then used for the final evaluation on the TEST set. We run each experiment five times, each time with a different random seed, and we report the average of the runs to avoid any randomness in results. Statistical significance tests are done using an approximate randomization test based on the accuracy. Among the existing models, the graph-based model gets the lowest scores, where the extended grid gets the highest scores on both tasks. Note that the Extended Grid-CNN yields these improvements considering only a subset of the Extended Grid features. This demonstrates the effectiveness of distributed representation and convolutional feature learning method. Compared to discrimination, gain in insertion is less verbose. There could be two reasons. First, as mentioned before, insertion is a harder task than discrimination. Second, our models were not trained specifically on the insertion task. The model that is trained to distinguish an original document from its random permutation may learn features that are not specific enough to distinguish documents when only one sentence differs. It will be interesting to see how the model performs when we train it on the insertion task directly. Therefore, we also present versions of our model, where we use pre-trained models from discrimination task on WSJ corpus (last two rows in the table). The pre-trained models are then fine-tuned on the summary rating task. They also introduced three tasks to evaluate the performance of coherence models: discrimination, summary coherence rating, and readability. A number of extensions of the basic entity grid model has been proposed. Their model learns not only from original document and its permutations but also from ranking preferences among the permutations themselves. To model local entity transition, the method constructs a directed projection graph representing the connection between adjacent sentences. Two sentences have a connected edge if they share at least one entity in common. The coherence score of the document is then computed as the average out-degree of sentence nodes. In addition, there are some approaches that model text coherence based on coreferences and discourse relations. In this work, they also estimate text coherence through pronoun coreference modeling. Instead of modeling entity transitions, they model discourse role transitions between sentences. Following this tradition, in this paper we propose to neuralize the popular entity grid models. First, they use a recurrent or a recursive neural network to compute the representation for each sentence in L from its words and their pre-trained embeddings. Then the concatenated vector is passed through a non-linear hidden layer, and finally the output layer decides if the window of sentences is a coherent text or not. Our approach is fundamentally different from their approach; our model operates over entity grids, and we use convolutional architecture to model sufficiently long entity transitions. We presented a local coherence model based on a convolutional neural network that operates over the distributed representation of entity transitions in the grid representation of a text. Our architecture can model sufficiently long entity transitions, and can incorporate entity-specific features without loosing generalization power. We described a pairwise ranking approach to train the model on a target task and learn task-specific features. Our evaluation on discrimination, insertion and summary coherence rating tasks demonstrates the effectiveness of our approach yielding the best results reported so far on these tasks. In future, we would like to include other sources of information in our model. We would also like to extend our model to other forms of discourse, especially, asynchronous conversations, where participants communicate with each other at different times (e.g., forum, email)."
148,1,2.0,4.0,4.0,True,acl_2017,test,"Knowing the quality of reading comprehension (RC) datasets is important for the development of natural language understanding systems. In this study, two classes of metrics were adopted for evaluating RC datasets: prerequisite skills and readability. We applied these classes to six existing datasets, including MCTest and SQuAD, and demonstrated the characteristics of the datasets according to each metric and the correlation between the two classes. Our dataset analysis suggested that the readability of RC datasets does not directly affect the question difficulty and that it is possible to create an RC dataset that is easy-to-read but difficult-to-answer.","Such an ability can be tested with a reading comprehension (RC) task that requires the agent to read open-domain documents and answer questions about them. Building the RC ability is challenging because RC comprises multiple processes including parsing, understanding cohesion, and inference with linguistic and general knowledge. Clarifying what a system achieves is important to the development of RC systems. To achieve robust improvement, systems need to be measured according to various metrics, not just simple accuracy. However, a current problem is that most RC datasets are presented only with superficial categories, such as question types (e.g., what, where, and who) and answer types (e.g., numeric, location, and person). In these situations, it is difficult to assess systems accurately. These two examples show that the readability of the text does not necessarily correlate with the difficulty of the questions. Therefore, we adopted two classes of evaluation metrics for RC datasets to analyze both the quality of datasets and the performance of systems. Our paper is divided into the following sections. The first class defines the difficulty of comprehending the context to answer questions. That study presented an important observation of the relation between the difficulty of an RC task and prerequisite skills: the more skills that are required to answer a question, the more difficult the question is. From this observation, we assume that the number of required skills corresponds to the difficulty of a question. This is because each skill corresponds to a function of a system, which has to be equipped with the system. Therefore, we reorganized the category of knowledge reasoning in terms of textual entailment and human text comprehension. We utilized these insights in order to develop a comprehensive but not overly specific classification of knowledge reasoning. The second class defines the difficulty of reading contents, readability, in documents considering syntactic and lexical complexity. In the annotation, annotators selected sentences needed for answering and then annotated them with prerequisite skills under the same condition for RC datasets with different task formulations. Therefore, our annotation was equivalent in that the datasets were annotated from the point of view of whether a context entails a hypothesis that can be made from a question and its answer. This means our methodology could not evaluate the competence of looking for sentences that need to be read and answer candidates from the context. Therefore, our methodology was used to evaluate the understanding of contextual entailments in a broader sense for RC. We adopt two classes of evaluation metrics to show the qualitative features of RC datasets. Through analyses of RC datasets, we demonstrate that there is only a weak correlation between the difficulty of questions and the readability of context texts in RC datasets. We revise the previous classification of prerequisite skills for RC. Specifically, skills of knowledge reasoning are organized in terms of entailment phenomena and human text comprehension in psychology. We annotate six existing RC datasets with our organized metrics for the comparison and make the results publicly available. We believe that our annotation results will help researchers to develop a method for the stepby-step construction of better RC datasets and a method to improve RC systems. In this section, we present a short chronicle of RC datasets. Thus, an important issue for designing RC datasets is their scalability. This dataset was created by crowdsourcing and is based on scalable methodology. Since then, more large-scale datasets have been proposed with the development of machine learning. These context texts and questions were automatically curated and generated from large corpora. This has highlighted the demand for more stable and robust sourcing methods regarding dataset quality. They were aimed at achieving large and good-quality content for machine learning models. In psychology, there is a rich tradition of research on human text comprehension. This model assumes connectional and computational architecture for text comprehension. Construction: Read sentences or clauses as inputs; form and elaborate concepts and propositions corresponding to the inputs. discourse, and coherence). The first is whether inferences are automatic or controlled. Therefore, this dichotomy is not suited for empirical evaluation, which we are working on attempting. The second is whether inferences are retrieved or generated. Retrieved means that the information used for inference is retrieved from context. In contrast, when inferences are generated, the reader uses external knowledge going beyond the context. Bridging inference connects current information to other information that was previously encountered. Elaboration connects current information to external knowledge that is not in a context. We use these two types of inferences in the classification of knowledge reasoning. This class covers the textbase and situation model, or understanding each fact and associating multiple facts in a text: relations of events, characters, topic of story, and so on. This skill is a renamed version of mathematical operations. Causal relation: Understanding of causality that is represented by explicit expressions of why, because, the reason... (only if they exist). For commonsense reasoning in the original classification, we defined the following four categories, which can be jointly required. Note that we excluded direct reference because it is coreference resolution (pronominalization) or elaboration (epithets). and What is the main subject of this article?). Although this skill can be regarded as part of elaboration, we defined an independent skill because this knowledge is specific to RC. The last two skills are intended to be performed on a single sentence. Schematic clause relation: Understanding of complex sentences that have coordination or subordination, including relative clauses. This skill is a renamed version of special sentence structure. We did the same with understanding of constructions, which was merged into idioms in bridging. Note that we did not construct this classification to be dependent on RC models. This is because our methodology is intended to be general and applicable to many kinds of architectures. In this study, we evaluated the readability of texts based on metrics in NLP. of characters per word (NumChar) -Ave. num. of syllables per word (NumSyll) -Ave. sentence length in words (MLS) -Proportion of words in AWL (AWL) -Modifier variation (ModVar) -Num. We applied these metrics only to sentences that needed to be read to answer questions. Nonetheless, because these metrics were proposed for human readability, they do not necessarily correlate with those for RC systems. However, we could not use their results because we did not have discourse annotations. We asked three annotators to annotate questions of RC datasets with the prerequisite skills that are required to answer each question. We allowed multiple labeling. For each task that was curated from the datasets, the annotators saw the context, question, and its answer jointly. When a dataset consisted of multiple choice questions, we showed all candidates and labeled the correct one with an asterisk. The annotators then selected sentences that needed to be read to answer the question and decided if each prerequisite skill was required. The annotators were allowed to select nonsense for an unsolvable question to distinguish it from a solvable question that required no skills. We explain the method of choosing questions for the annotation in Appendix A. There were other datasets we did not annotate in this study. We decided not to annotate those datasets because of the following reasons. Math. Coref. resol. Logical rsng. Causal rel. Sptemp rel. Clause rel. and all tokens are lowercased, which seemingly prevents inferences based on proper nouns. Thus, we decided that its texts are not suitable for human reading and annotation. We present the results from evaluating the RC datasets according to the two classes of metrics. MCTest achieved high scores in several skills (first in causal relation and meta-knowledge and second in coreference resolution and spatiotemporal relation) and lower score in punctuation. These scores seem to be because the MCTest dataset consists of narratives. Another dataset that achieved remarkable scores is Who-did-What. This dataset achieved the highest score for ellipsis. This is because the questions of Who-did-What are automatically generated from articles not used as context. This methodology can avoid textual overlap between a question and its context; therefore, the skills of ellipsis, bridging, and elaboration are frequently required. With regard to nonsense, MS MARCO and Who-did-What received relatively high scores. This appears to have been caused by the automated curation, which may generate separation between the contents of the context and question (i.e., web segments and a search query in MS MARCO, and a context article and question article in Who-didWhat). In stark contrast, NewsQA had no nonsense questions. Although this result was affected by our filtering described in Appendix A, it is important to note that the NewsQA dataset includes annotations of meta information whether or not a question makes sense (is question bad). Wikipedia articles and technical documents generally require a high grade level to understand. In contrast, MCTest had the lowest scores; its dataset consist of narratives for children. The first figure shows the trends of the datasets. Three separate domains can be seen. This leads to the following two insights. First, the readability of RC datasets does not directly affect the difficulty of their questions. That is, RC datasets that are difficult to read are not necessary difficult to answer. Second, it is possible to create difficult questions from context that is easy to read. MCTest is a good example. The context texts of MCTest dataset are easy to read, but the difficulty of the questions is comparable to that of the other datasets. MCTest is a good example of an RC dataset that is easy to read but difficult to answer. Who-did-What performed well in terms of its query sourcing method. Although its questions are automatically created, they are sophisticated in terms of knowledge reasoning. However, an issue with the automated sourcing method is excluding nonsense questions. MS MARCO was a relatively easy dataset in terms of prerequisite skills. A problem is that the dataset contained nonsense questions. Such information enabled us to avoid using nonsense questions, such as in the training of machine learning models. In this section, we discuss several matters regarding the construction of RC datasets and the development of RC systems using our methodology. How to utilize the two classes of metrics for system development: One example for the development of an RC system is that it should be built to solve an easy-to-read and easy-to-answer dataset. The next step is to improve the system so that it can solve an easy-to-read and difficult-toanswer dataset. Finally, only after it can solve such dataset should the system be applied to a difficultto-read and difficult-to-answer dataset. Appropriate datasets can be prepared for every step by measuring their properties using the metrics of this study. Corpus genre: Attention should be paid to the genre of corpus used to construct a dataset. Expository documents like news articles tend to require factorial understanding. Most existing RC datasets use such texts because of their availability. If we want to build agents that work in the real world, RC datasets may have to be constructed from narratives. Most questions that are prevalent now require only local coherence (e.g., referential relations and thematic roles) with a narrow context. Annotation issues: There were questions for which there were disagreements regarding the decision of nonsense. For example, some questions can be solved by external knowledge without seeing their context. As a brief analysis, we further investigated sentences in the context of the datasets that were highlighted in the annotation. Analyses were performed in two ways: for each question, we counted the number of required sentences and their distance (see Appendix B for the calculation method). Although the scores seemed to be approximately level, MCTest required multiple sentences the most frequently. The second row presents the average distance of required sentences. In contrast, SQuAD and MS MARCO showed lower scores: most of their questions seem only to require reading a single sentence to answer. Of course, the scores of distances should depend on the length of the context texts. In this study, we adopted evaluation metrics to analyze both the performance of a system and the quality of RC datasets. Next, we annotated six existing RC datasets with those defined metrics. Our annotation highlights the characteristics of the datasets and provides a valuable guide for the construction of new datasets and the development of RC systems. For future work, we plan to use the analysis in the present study to construct a system that can be applied to multiple datasets. Its questions are automatically created from an article that differs from one used for context. In addition, questions that can be solved by a simple baseline method are excluded from the dataset. If a question required only one sentence to be read, its distance was zero. If a question required two adjacent sentences to be read, its distance was one. If a question required more than two sentences to be read, its distance was the sum of distances of any two sentences."
49,2,5.0,4.0,4.0,True,acl_2017,test,"Chunks (or phrases) had once played a pivotal role in machine translation. By using a chunk rather than a word as the basic translation unit, local (intrachunk) and global (inter-chunk) word orders/dependencies can be easily modeled. The chunk structure, despite its importance, has not been considered in the decoders of neural machine translation (NMT). In this paper, we propose chunkbased decoders for NMT, each of which consists of a chunk-level decoder and a word-level decoder. The chunk-level decoder models global dependencies, while the word-level decoder decides the word orders in a chunk. To generate a target sentence, the chunk-level decoder generates a chunk representation containing global information, based on which, the wordlevel decoder predicts the words inside the chunk. Experimental results show that our method can significantly improve translation performance in a WAT ’16 Englishto-Japanese translation task.","Most of the NMT models, however, still rely on a sequential decoder based on recurrent neural network (RNN), due to the difficulty in capturing the structure of a target sentence that is unseen during translation. With the sequential decoder, however, there are two problems to be solved. This problem can become more serious when the target sequence become longer. In such a case, the decoder is required to capture the longer dependencies in a target sentence. Another problem with the sequential decoder is that it is expected to cover possible word orders simply by memorizing the local word sequences in the limited training data. This problem can be more serious in free word-order languages such as Czech, German, Japanese, and Turkish. This means that simply memorizing the word order in training data is not enough to train a model that can assign a high probability to a correct sentence regardless of its word order.  The aligned words are content words and the underlined words are function words. By using a chunk rather than a word as the basic translation unit, one can treat a sentence as a shorter sequence. This makes it easy to capture the longer dependencies in a target sentence. The order of words in a chunk is relatively fixed while that in a sentence is much more flexible. Thus, modeling intra-chunk (local) dependencies and inter-chunk (global) dependencies independently can help capture the difference of the flexibility between the word order and the chunk order in free word-order languages. In this paper, we refine the original RNN decoder to consider chunk information in NMT. Our focus is the hierarchical structure of a sentence: each sentence consists of chunks, and each chunk consists of words. In this section, we briefly introduce the architecture of the attention-based NMT model, which is the basis of our proposed models. An NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps a source sentence into a fixed-length vector, the decoder maps the vector into a target sentence. While various architectures are leveraged as an encoder to capture the structural information in the source language, most of the NMT models rely on a standard sequential network such as LSTM or GRU as its decoder. While the former gate allows the model to forget the previous states, the latter gate decides how much the model updates its content.  are trainable matrices or vectors. In a decoder using the attention mechanism, the obtained context vector cj in each timestep replaces cs in Eqs.  A chunk-level decoder generates a chunk representation for each chunk while a word-level decoder uses the representation to predict each word. Not assuming any structural information of the target language, the sequential decoder has to memorize long dependencies in a sequence. The first term represents the generation probability of a chunk and the second term indicates the probability of a word in the chunk. We model the former term as a chunk-level decoder and the latter term as a word-level decoder. In the above formulation, we model the information of the words and their orders in a chunk. Although our idea can be used in several languages, the optimal network architecture could be adapted depending on the word order of the target language. In this work, we design models for languages in which content words are followed by function words, such as Japanese and Korean. The details of our models are described in the following sections. The model described in this section is the basis of our proposed models. By using a standard sequential encoder, we need not perform any additional preprocessing on test data such as syntactic parsing. This design prevents our model from being affected by any errors that may occur as a result of additional preprocessing during test time. The chunk representation contains the information about the words that should be predicted by the word-level decoder. In other words, its hidden layers are required to memorize the long-term dependencies in the target language. In contrast, in our word-level decoder, the hidden state iterates only over the length of a chunk. Thus, our word-level decoder is released from the pressure of memorizing the long (interchunk) dependencies and can focus on learning the short (intra-chunk) dependencies. The second term in Eq. This may affect the word-level decoder because it cannot access any previous information at the first word of each chunk. The chunk-level decoder in Eq. This may affect the chunk-level decoder because it cannot memorize what kind of information has already been generated by the word-level decoder. The information about the words in a chunk should not be included in the representation of the next chunk; otherwise, it may generate the same chunks for multiple times, or forget to translate some words in the source sentence. Data To clarify the effectiveness of our decoders, we choose Japanese, a free word-order language, as the target language. The effect of chunking errors in training the decoder can be suppressed so we can evaluate the potential of our method. All the parameters are initialized randomly with Gaussian distribution. It takes about a week to train each model with an NVIDIA TITAN X (Pascal) GPU. We saved the trained models that performed best on the development set during training, and use them to test the systems with the test set. Each sequence of underlined words correspond to a chunk recognized by our decoder. The results show that capturing the chunk structure in the target language is more effective than capturing the syntax structure in the source language. The characterbased model has a great advantage in that it does not require a large vocabulary size. One possible reason for this is that using a character-based model rather than a word-based model makes it more difficult to capture long-distance dependencies because the length of a target sequence becomes much longer in the character-based model. There has been much work done on using chunk (or phrase) structure to improve machine translation quality. Our work is different from their works in that our models are based on NMT, but not SMT or EBMT. The decoders in the above works can model the chunk structure by storing chunk pairs in a large table. On the other hand, we do that by separately training a chunk generation model and a word prediction model with two RNNs. Given a syntactic tree of a source sentence, their tree-based encoder encodes words from the leaf nodes to the root nodes recursively. To prevent the tokenization errors from propagating to the whole NMT system, their lattice-based encoder can utilize multiple tokenization results. These works focus on the encoding process and propose better encoders that can exploit the structures of the source language. In contrast, our work focuses on the decoding process to capture the structure of the target language. The encoders described above and our proposed decoders are complementary so they can be combined into a single network. In contrast, we build a chunk-word level model to explicitly capture the syntactic structure based on chunk segmentation. Although these architectures look similar to each other, there is a fundamental difference between the directions of the connection between two layers. A stacked RNN consists of multiple RNN layers that are connected from the input side to the output side at every timestep. Before it generates a chunk, there is a feed-forward connection from the chunk-level decoder to the word-level decoder. However, after generating a chunk representation, the connection is to be reversed to feedback the information from the word-level decoder to the chunk-level decoder. By switching the connections between two layers, our model can capture the chunk structure explicitly. This is the first work that proposes the decoders for NMT that can capture plausible linguistic structures like chunk. In this paper, we propose chunk-based decoders for NMT. As the attention mechanism in NMT plays a similar role to the translation model in phrase-based SMT, our chunk-based decoders are intended to capture the notion of chunk in chunkbased (or phrase-based) SMT. We utilize the chunk structure to efficiently capture long-distance dependencies and cope with the problem of free word-order languages like Japanese. We design three models that have hierarchical RNN-like architectures, each of which consists of a word-level decoder and a chunk-level decoder. In future work, we will apply our method to other target languages and evaluate the effectiveness on different languages such as Czech, German or Turkish."
496,2,5.0,3.5,4.0,True,acl_2017,test,"Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure.","Training neural MT (NMT) models can be done in an end-to-end fashion, which is simpler and more elegant than traditional MT systems. However, little is known about what and how much these models learn about each language and its features. iii) what impact do different representations (character vs. word) have on learning? and (iv) what do different modules learn about the syntactic and semantic structure of a language? Answering such questions is imperative for fully understanding the NMT architecture. different layers or encoder vs. decoder)? To achieve this, we follow a simple but effective procedure with three steps: (i) train a neural MT system on a parallel corpus; (ii) use the trained model to extract feature representations for words in a language of interest; and (iii) train a classifier using extracted features to make predictions for another task. We then evaluate the quality of the trained classifier on the given task as a proxy to the quality of the extracted representations. We focus on the tasks of part-of-speech (POS) and full morphological tagging. We investigate how different neural MT systems capture POS and morphology through a series of experiments along several parameters. For instance, we contrast word-based and character-based representations, use different encoding layers, vary source and target languages, and compare extracting features from the encoder vs. the decoder. We experiment with several languages with varying degrees of morphological richness: French, German, Czech, Arabic, and Hebrew. This improvement is correlated with better BLEU scores. On the other hand, word-based models are sufficient for learning the structure of common words. Translating into morphologically-poorer languages leads to better source-side word representations. This is partly, but not completely, correlated with BLEU scores. The attention mechanism removes much of the burden of learning word representations from the decoder. After training the NMT system, we freeze the parameters of the encoder and use ENC as a feature extractor to generate vectors representing words in the sentence. Let ENCi(s) denote the encoded representation of word wi. For example, this may be the output of the LSTM after word wi. We feed ENCi(s) to a neural classifier that is trained to predict POS or morphological tags and evaluate the quality of the representation based on our ability to train a good classifier. By comparing the performance of classifiers trained with features from different instantiations of ENC, we can evaluate what MT encoders learn about word structure. We follow a similar procedure for analyzing representation learning in DEC. The classifier itself can be modeled in different ways. For example, it may be an LSTM over outputs of the encoder. However, as we are interested in assessing the quality of the representations learned by the MT system, we choose to model the classifier as a simple feed-forward network with one hidden layer and a ReLU non-linearity. The classifier is trained with a cross-entropy loss; more details on its architecture are in the supplementary material. Language pairs We experiment with several language pairs, including morphologically-rich languages, that have received relatively significant attention in the MT community. These include Arabic-, German-, French-, and Czech-English pairs. To broaden our analysis and study the effect of having morphologically-rich languages on both source and target sides, we also include ArabicHebrew, two languages with rich and similar morphological systems, and Arabic-German, two languages with rich but different morphologies. This allows for comparable and crosslinguistic analysis. We use official dev and test sets for tuning and testing. Reported figures are the averages over test sets. Annotated data We use two kinds of datasets to train POS and morphological classifiers: goldstandard and predicted tags. For predicted tags, we simply used freely available taggers to annotate the MT data. For gold tags, we use gold-annotated datasets. We train and test our classifiers on predicted annotations, and similarly on gold annotations, when we have them. We report both results wherever available. We then train a classifier that uses the features ENCi(s) to predict POS or morphological tags. In this section we compare different word representations extracted with different encoders. Our word-based model uses a word embedding matrix which is initialized randomly and learned with other NMT parameters. In both cases we run the encoder over these representations and use its output ENCi(s) as features for the classifier. Charbased models always generate better representations for POS tagging, especially in the case of morphologically-richer languages like Arabic and Czech. We observed a similar pattern in the full morphological tagging task. Impact of word frequency Let us look more closely at an example case: Arabic POS and morphological tagging. Clearly, the char-based model is superior to the word-based one. In other words, the more frequent the word, the less need there is for character information. These findings make intuitive sense: the char-based model is able to learn character n-gram patterns that are important for identifying word structure, but as the word becomes more frequent the word-based model has seen enough examples to make a decision. share similar misclassified tags. Much of the confusion comes from wrongly predicting nouns (NN, NNP). In the charbased case, this hardly happens. Tags closer to the upper-right corner occur more frequently in the training set and are better predicted by char-based compared to wordbased representations. ticles (CC, DT, WP). plural). The charbased model is thus especially good with frequent tags and infrequent words, which is understandable given that infrequent words typically belong to frequent open categories like nouns and verbs. We would like to understand what kind of information different layers capture. a trained model with multiple layers, we extract representations from the different layers in the encoder. Let ENCli(s) denote the encoded representation of word wi after the l-th layer. We vary l and train different classifiers to predict POS or morphological tags. The general trend is that passing word vectors through the encoder improves POS tagging, which can be explained by contextual information contained in the representations after one layer. Thus translation quality improves when adding layers but morphology quality degrades. Intuitively, it seems that lower layers of the network learn to represent word structure while higher layers focus more on word meaning. While translating from morphologically-rich languages is challenging, translating into such languages is even harder. How does the target language affect the learned source language representations? Does translating into a morphologically-rich language require more knowledge about source language morphology? In order to investigate these questions, we fix the source language and train NMT models on different target languages. These target languages represent a morphologically-poor language (English), a morphologically-rich language with similar morphology to the source language (Hebrew), and a morphologically-rich language with different morphology (German). To make a fair comparison, we train the models on the intersection of the training data based on the source language. In this way the experimental setup is completely identical: the models are trained on the same Arabic sentences with different translations. As expected, translating to English is easier than translating to the morphologically-richer Hebrew and German, resulting in higher BLEU. compared to German. POS and morphology accuracies share an intriguing pattern: the representations that are learned when translating to English are better for predicting POS or morphology than those learned when translating to German, which are in turn better than those learned when translating to Hebrew. This is remarkable given that English is a morphologically-poor language that does not display many of the morphological properties that are found in the Arabic source. In contrast, German and Hebrew have richer morphologies, so one could expect that translating into them would make the model learn more about morphology. To probe this more, we trained an Arabic-Arabic autoencoder on the same training data. This implies that higher BLEU does not necessarily entail better morphological representations. In other words, a better translation model learns more informative representations, but only when it is actually learning to translate rather than merely memorizing the data as in the autoencoder case. We found this to be consistently true also for charbased experiments, and in other language pairs. So far we only looked at the encoder. However, the decoder DEC is a crucial part in an MT system with access to both source and target sentences. In order to examine what the decoder learns about morphology, we first train an NMT system on the parallel corpus. Then, we use the trained model to encode a source sentence and extract features for words in the target sentence. However, we observed similar low POS tagging accuracy using decoder representations from high-quality models. As an alternative explanation for the poor quality of the decoder representations, consider the fundamental tasks of the two NMT modules: encoder and decoder. tags. ogy in order to create a good generic representation. In the following section we show that the attention mechanism also plays an important role in the division of labor between encoder and decoder. These two sources of information need to jointly point to the most relevant source word(s) and predict the next most likely word. Thus, the decoder puts significant emphasis on mapping back to the source sentence, which may come at the expense of obtaining a meaningful representation of the current word. We hypothesize that the attention mechanism hurts the quality of the target word representations learned by the decoder. To test this hypothesis, we train NMT models with and without attention and compare the quality of their learned representations. Without attention, the decoder is forced to learn more informative representations of the target language. We also conducted experiments to verify our findings regarding word-based versus character-based representations on the decoder side. By character representation we mean a character CNN on the input words. The decoder predictions are still done at the word-level, which enables us to use its hidden states as word representations. While char-based representations improve the encoder, they do not help the decoder. In practice, this can lead to generating unknown words. Analysis of neural models The opacity of neural networks has motivated researchers to analyze such models in different ways. While such visualizations illuminate the inner workings of the network, they are often qualitative in nature and somewhat anecdotal. A different approach tries to provide quantitative analysis by correlating parts of the neural network with linguistic properties, for example by training a classifier to predict features of interest. In contrast, we focus on representations in morphologically-rich languages and evaluate both source and target sides across several criteria. The latter approach has the advantage of keeping the original word boundaries without requiring pre-and post-processing. We evaluate the quality of different representations learned by an MT system augmented with a character CNN in terms of POS and morphological tagging, and contrast them with a purely word-based system. Neural nets have become ubiquitous in machine translation due to their elegant architecture and good performance. The representations they use for linguistic units are crucial for obtaining highquality translation. In this work, we investigated how neural MT models learn word structure. We evaluated their representation quality on POS and morphological tagging in a number of languages. This is partly correlated with BLEU. These insights can guide further development of neural MT systems. For instance, jointly learning translation and morphology can possibly lead to better representations and improved translation. Our analysis indicates that this kind of approach should take into account factors such as the encoding layer and the type of word representation. Another area for future work is to extend the analysis to other representations (e.g. byte-pair encoding), deeper networks, and more semanticallyoriented tasks such as semantic parsing."
435,2,4.5,4.0,2.5,False,acl_2017,test,"Causation is a psychological tool of humans to understand the world and it is projected in natural language. Causation relates two events, so in order to understand the causal relation of those events and the causal reasoning of humans, the study of causality classification is required. Herein, we propose a neural network architecture for the task of causality classification. We claim that the encoding of the meaning of a sentence is required for the disambiguation of its causal meaning. Our results show that our claim holds, and we outperform the state-of-the-art.","Causal reasoning is the process of relating two events, namely cause and its effect. There are different theories concerning how natural language approximates causation. Therefore, the syntaxgrounded construction of causality defined by those theories is far away from the human mental model of causation. Causation or causality has also been studied in computational linguistics. There are some semantic and discourse resources that take into account causality in the range of linguistic phenomena that they annotate. The type of causation annotated in PropBank is related to the syntax and semantics of the verbs. Causality is annotated in PDTB as either an explicit or an implicit relation between events. When causality is explicit, it is signalled by a lexical marker such as because or since between others. The existence of AltLex means that the expression of causality is not defined by a limited number of lexical constructions, so the coverage of the total number of possible causal expressions is restricted to the grade of coverage of the base corpus of PDTB. However, causality does not have a fixed list of lexical or syntactic constructions. They also proposed a classification method, which is based on the use of features from the corpus and features from lexical resources. The last set of features reduces the ability of the system to classify causality, because they restrict the system to the causal definition of the lexical resources. Causal meaning disambiguation is the task of identifying whether there is a causal relation between two events. We hypothesize that neural networks are able to encode the meaning of those events, and discover whether the underlying relation between them is causal. Empiric results on this corpus show that our claim indeed holds. The authors defined a set of linguistic rules for the identification of cause-effect relations, and developed a pattern matching system based on those rules. The errors of the system show the common errors of a full linguistically grounded classification system, which are related to the ambiguity of some causal links such as by or as or the lexical and syntactic ambiguity of some causative verbs. The type of relations studied by the author are those conveyed by a verb and two noun phrases. The clas-sification method uses as lexical features whether the verb of the sentence belongs to a list of ambiguous causative verbs, and whether the nouns belongs to one of the nine semantic hierarchies in WordNet. Again, the main source of errors is the ambiguity of the projection of causality in natural language. Since the method was defined for a specific kind of causal relation between events, the authors had to first create a corpus following that requirement. The authors developed a supervised classification system using syntactic and semantic features from WordNet, and they used SVM as a classification algorithm. The drawback of the method is its reduced flexibility, because it is only defined for a specific causal construction and its confidence in syntactic features. Moreover, the authors conclude that semantic features are more adequate for causality classification than syntactic features, which goes in the direction of our claim. The authors defined a measure of the causality relation between two events according to the co-occurrence frequency between the predicates and their arguments. In these works, the authors attempted to go in the direction of using distributional semantics in order to encode the causal meaning, but they still rely on the use of lexical and syntactic features. Thus, the main drawbacks of those papers are the complex featuring engineering process to detect the causal relation, and the restricted coverage of the resulting knowledge bases. The novelty lies in the use of features of temporal relations. In contrast, we argue that the representation of causality should not be limited by syntactic patterns or the coverage of semantic resources, due to the lack of a particular linguistic construction for causality. Therefore, we propose the neural encoding of the context of the relation in order to disambiguate its causal meaning. An event may be a verb, whose arguments may be explicitly present or not (subject, verb, object), or a noun phrase. Some researchers impose two additional restrictions: the temporal restriction, which means that the causing event should take place before the caused event, and the counterfactual one, which says if the causing event did not occur, the caused event would not have occurred either. However, the human mental model of causality does not usually relate an argument with the breaking of a window. Causation is concerned with the human understanding of the world and not with the world itself. Concerning the temporal restriction, it should be interpreted as a physical ordering constraint and not as a positional one. That means, the causing event does not always appear before the caused event in the sentence. We claim that the task of causality classification has not to be restricted to a syntactic construction and it has to be set up as a two steps task: causal meaning classification and causal arguments identification. The task of causal meaning classification is a binary classification task that is defined as the disambiguation of the causal meaning of the relation of two events. The input of the task is two events and the output is the meaning of the relation (Causal or Non Causal). The following sentence is an example of the input of our system. Some of those causal expressions are almost unambiguous, but the causal meaning of others totally depends on the context. Therefore, we need a corpus in which the events, the lexical marker and the meaning of the relation are annotated. To the best of our knowledge, there are three available corpora. The corpus only provides the position of the related events in each sentence and whether the relation is causal. Unfortunately, the corpus is not freely available, and is also not large enough to train neural methods. Annotation was restricted to explicit causal relations. They followed the same annotation schema of TimeML, so they defined the label CSIGNAL for the causal lexical markers, and CLINK to mark the causal relation between two events. The authors manually annotated the causal relations between the identified events in TimeML. The fact that the causal lexical markers are annotated agrees with our data requirements, but as the previous corpus, Causal-TimeBank is too small to train neural methods. The corpus was built on the idea that causation can be expressed by different kinds of linguistic constructions. This is validated by the fact that in PDTB there are explicit causal lexical markers, and other sort of expressions that have a discourse meaning, which are called AltLex. The relations signalled by an AltLex expression are implicit relations, in which the annotator did not find an appropriate connective to insert between the events, because the meaning of the relation is entailed by other expressions, namely AltLex. Furthermore, some causal lexical markers can be perceived as variations of the basic ones, but those variations are not usually in the list of discourse connectives. Thus, the authors of the AltLex corpus decided to develop a method to identify a larger amount of AltLex expressions with causal meaning. The corpus construction leveraged Simple Wikipedia, by aligning sentences from Wikipedia that consist of unknown lexical causal markers with sentences from Simple Wikipedia that contain corresponding known lexical causal markers. Once a first set of causal and non-causal sentences were identified, a bootsrapping method was applied to enhance the corpus. The class distribution of the lexical markers should be studied, because one of the features of the corpus is the annotation of the lexical markers susceptible of expressing causation. Convolutional networks excel at pattern learning in input data, however causation does not have a particular syntactic structure as it was mentioned. Moreover, CNN requires the definition of the kernel size, which means that we should know beforehand the length of the context that projects the causal meaning of the sentence. Nevertheless, the list of expressions that may represent a causal meaning is not limited and some sentences implicitly express causality. Therefore, we propose the use of LSTM RNN instead of a CNN, because we argue that LSTM has a higher capacity of encoding meaning for the task of causality classification. We propose a neural network architecture with two inputs, which is mainly based on the encoding of the two inputs with an LSTM and the use of several dense layers with a tanh activation function. Following the assumption that some sort of relation should exist between the two events of a causal relation, the first evaluated model consists of two connected LSTMs, or in other words, the LSTM network of the second event of the causal relation is initialized with the last state of the first LSTM. We also evaluated the same architecture but without the connection between the two LSTMs. The text processing starts with the tokenization of the two inputs, and the representation of them as a matrix of word embedding vectors. The lengths of the first (n) and the second (m) input are not the same, so three zero-padding strategies were evaluated to measure which of them is the most convenient to encode the causal meaning. The context of the causal relation is represented by the concatenation of the two vectors. The last layer is composed of the softmax operation. Moreover, as it has been mentioned several times, the task of causality classification lacks of representative or large enough corpus that covers a wide range of causal expressions, as well as without the restriction of being composed of specific causal constructions. Therefore, the methods were only evaluated with the AltLex corpus. We consider two baselines in order to compare our proposal. The fact of relying on lexical resources restricts the recall of the system to the linguistic coverage of the lexical resources. In contrast, we propose a neural network architecture fed only by a set of word embedding vectors, which has a higher ability of generalization as it is shown later in the paper. a large number of sentences without a causal meaning. This is not a desirable behaviour. On the other hand, our system has a high Precision with lower Recall, indicating it mainly classifies correctly sentences with unambiguous connectives. This behaviour means that our assumption about the relation between the meanings of the two input events does not hold, so it is better to encode each argument independently and then to measure the relation between the arguments by using dense layers. Concerning the optimizers, the results show a similar behaviour in both architectures. A high value of Precision and a high value of Recall mean that there is a good balance between the accuracy in the disambiguation of the causal meaning of the relation and the coverage of different ways of expressing causality. To conclude, the system that reached the highest performance is the one that used the mean strategy for zero-padding and the optimizer Adam. This behaviour is also developed by the neural network configurations that use maximum strategy for zero-padding and Adadelta as optimizer, but in those cases the Recall and the Pre-cision are greater. As previously mentioned, this means that the systems have a large number of false positive instances, so they might not be correctly learning the characteristics of the Causal class. In contrast, our network architecture followed the same trend in both set of experiments and with the both version of the corpus. Causal Non Causal Causal Non Causal Although Roosevelt had promised to keep the United States out of the war, he nevertheless took concrete steps to prepare for war. Causal Non Causal Causal Non Causal One of these fragments gives rise to fibrils of amyloid beta, which then form clumps that deposit outside neurons in dense formations known as senile plaques. As mentioned in the previous sections, there are some linguistic constructions that can represent a causal meaning in some contexts, but not in other and vice-versa. A better balance between Precision and Recall in the Causal class means that the system learns the causal meaning better. Roughly speaking, the quality of the causal disambiguation is better, also for those lexical markers that are mostly Causal in the training data. It shows the class of the instance in the test corpus and the value of the most frequent class in the training corpus. The behaviour of the verb break should be stressed since it is considered as a causative verb. The behaviour of the proposed system with the connective which then is also remarkable. That connective is totally ambiguous, because it only has one instance labelled as Causal and other as Non Causal in the training data. We have to take into account that we work with word embeddings, so if the individual words which and then are over-represented in one class, that fact can determine the behaviour of the classification system. The individual term which does not appear in any instance of the training corpus, while the word then is mostly in sentences without a causal meaning. Therefore, the results reached and those examples allow us to confirm our claim that the encoding of the context is required for the disambiguation of the causal meaning as we have shown in this paper. We defined the task of causation classification as a task composed of another two subtasks: causal meaning classification and causal argument classification. The paper was focused on the first subtask, and we claim that the encoding of the two events of the relation is required for a suitable disambiguation of causality. We proposed an encoding system based on a neural network with two inputs, one for the first event and the other for the lexical marker and the second event. Our proposed system outperforms the state-of-the-art. One of the problems of the task is the lack of resources, so, as future work, we plan the creation of a new corpus for the two subtasks of causality classification, namely causality disambiguation and causality argument classification."
729,1,4.0,4.0,4.0,True,acl_2017,train,"How fake news goes viral via social media? How does its propagation pattern differ from real stories? In this paper, we attempt to address the problem of identifying rumors, i.e., fake information, out of microblog posts based on their propagation structure. We firstly model microblog posts diffusion with propagation trees, which provide valuable clues on how an original message is transmitted and developed over time. We then propose a kernel-based method called Propagation Tree Kernel, which captures high-order patterns differentiating different types of rumors by evaluating the similarities between their propagation tree structures. Experimental results on two real-world datasets demonstrate that the proposed kernel-based approach can detect rumors more quickly and accurately than state-ofthe-art rumor detection models.","html was shared or promoted by some influential online communities and users (including Trump himself), resulting in its wide spread. This unverified information may eventually turn out to be true, or partly or entirely false. Therefore, it is crucial to track and debunk such rumors in timely manner. However, such endeavor is manual, thus prone to poor coverage and low speed. But such an approach was over simplified as they ignored the dynamics of rumor propagation. So, can the propagation structure make any difference for differentiating rumors from nonrumors? We refer to this as a constrained mode propagation, relative to the open mode propagation of normal messages that everyone is open to share. Such different modes of propagation may imply some distinct propagation structures between rumors and nonrumors and even among different types of rumors. Due to the complex nature of information diffusion, explicitly defining discriminant features based on propagation structure is difficult and biased. Many of such implicit distinctions throughout propagation are hard to hand craft specifically using flat summary of statistics as previous work did. In addition, unlike representation learning for plain text, learning for representation of structures such as networks is not well studied in general. Then, we propose a kernel-based data-driven method called Propagation Tree Kernel (PTK) to generate relevant features (i.e., subtrees) automatically for estimating the similarity between two propagation trees. The basic idea is to find and capture the salient substructures in the propagation trees indicative of rumors. We also extend PTK into a context-enriched PTK (cPTK) to enhance the model by considering different propagation paths from source tweet to the roots of subtrees, which capture the context of transmission. Extensive experiments on two real-world Twitter datasets show that the proposed methods outperform state-of-the-art rumor detection models with large margin. Moreover, most existing approaches regard rumor detection as a binary classification problem, which predicts a candidate hypothesis as rumor or not. All such approaches are over simplistic because they ignore the dynamic propagation patterns given the rich structures of social media data. Some studies focus on finding temporal patterns for understanding rumor diffusion. Our work will consider temporal, structural and linguistic signals in a unified framework based on propagation tree kernel. Most previous work formulated the task as classification at event level where an event is comprised of a number of source tweets, each being associated with a group of retweets and replies. Here we focus on classifying a given source tweet regarding a claim which is a finer-grained task. These kernels are not suitable for modeling the social media propagation structures because the nodes are not given as discrete values like part-of-speech tags, but are represented as high dimensional real-valued vectors. Our proposed method is a substantial extension of tree kernel for modeling such structures. Once a user has posted a tweet, all his followers will receive the tweet. If there exists a directed edge from vi to vj, it means vj is a direct response to vi. In this section, we describe our rumor detection model based on propagation trees using kernel method called Propagation Tree Kernel (PTK). Tree kernel was designed to compute the syntactic and semantic similarity between two natural language sentences by implicitly counting the number of common subtrees between their corresponding parse trees. Given a syntactic parse tree, each node with its children is associated with a grammar production rule. A subtree is defined as any subgraph which have more than one nodes, with the restriction that entire (not partial) rule productions must be included. To classify propagation trees, we can calculate the similarity between the trees, which is supposed to reflect the distinction of different types of rumors and non-rumors based on structural, linguistic and temporal properties. For example, a questioning message posted very early may signal a false rumor while the same posted far later from initial post may indicate the rumor is still unverified, despite that the two messages are semantically similar. Here E is used to capture the characteristics of users participating in spreading rumors as discriminant signals, throughout the entire stage of propagation. For n-grams here, we adopt both uni-grams and bi-grams. commonly occurring in rumors but not in non-rumors. Intuitively, propagation paths provide further clue for determining the truthfulness of information since they embed the route and context of how the propagation happens. cPTK evaluates the occurrence of both context-free (without considering ancestors on propagation paths) and contextsensitive cases. We treat each tree as an instance, and its similarity values with all training instances as feature space. To our knowledge, there is no public dataset available for classifying propagation trees, where we need source tweets, more accurately, tree roots together with propagation structure, to be appropriately annotated. The original datasets were used for binary classification of rumor and non-rumor with respect to a given event that contain its relevant tweets. We then collected all the propagation threads (i.e., retweets and replies) for these source tweets. Finally, we annotated the source tweets by referring to the events they are from. BOW: A naive baseline we worked by representing the text in each tree using bag-of-words and building the rumor classifier with linear SVM. Our models: PTK and cPTK are our full PTK and cPTK models, respectively; PTK-and cPTKare the setting only using content while ignoring user properties. This also justifies the good performance of BOW even though it only uses uni-grams for representation. That is why the results of DTR are not satisfactory. SVM-TS and RFC are comparable because both of them utilize an extensive set of features especially focusing on temporal traits. But none of the models can directly incorporate structured propagation patterns for deep similarity comparison between propagation trees. SVM-RBF, although using a non-linear kernel, is based on traditional hand-crafted features instead of the structural kernel like ours. So, they performed obviously worse than our approach. Representation learning methods like GRU cannot easily utilize complex structural information since learning features from networked data is not studied well. Our models capture complex propagation patterns from structured data rich of linguistic, user and temporal signals. The superiority of our models is clear: PTK-which only uses text is already better than GRU, demonstrating the importance of propagation structures. PTK that combines text and user yields better results on both datasets, implying that both properties are complementary and PTK integrating flat and structured information is more effective. It is also observed that cPTK outperforms PTK except for non-rumor class. This suggests the context-sensitive modeling based on PTK is effective for different types of rumors, but for nonrumors, it seems that considering context of propagation path is not always helpful. This is not an issue in cPTK-since user information is not considered. Over all classes, cPTK achieves the highest accuracies on both datasets. This is because the exiting features were defined for binary (rumor vs. non-rumor) classification in previous works. So, they do not perform well for finer-grained classes. Our approach can differentiate various classes much better by deep, detailed comparison of different patterns based on propagation structure. Detecting rumors at an early stage of propagation is very important so that preventive measures could be taken. In early detection task, all the posts after a detection deadline are invisible during test. The earlier the deadline, the less propagation information can be available. In the first few hours, our approach demonstrates superior early detection performance than other models. Our analysis shows that rumors typically demonstrate more complex propagation substructures especially at early stage. Many textual signals (underlined) can also be observed in that early period. Our method can learn such structures and patterns naturally, but it is difficult to know and hand-craft them in feature engineering. We propose a novel approach for detecting rumors in microblog posts based on kernel learning method using propagation trees. A propagation tree encodes the spread of a hypothesis (i.e., a source tweet) with complex structured patterns and flat information regarding content, user and time associated with the tree nodes. Enlightened by tree kernel techniques, our kernel method learns discriminant clues for identifying rumors of finer-grained levels by directly measuring the similarity among propagation trees via kernel functions. Experiments show that our approach outperforms state-of-the-art baselines with large margin for both general and early rumor detection tasks."
338,2,4.0,4.56,4.56,True,acl_2017,train,"Solving cold-start problem in review spam detection is an urgent and significant task. It can help the on-line review websites to relieve the damage of spammers in time, but has never been investigated by previous work. This paper proposes a novel neural network model to detect review spam for cold-start problem, by learning to represent the new reviewers’ review with jointly embedded textual and behavioral information. Experimental results prove the proposed model achieves an effective performance and possesses preferable domain-adaptability. It is also applicable to a large scale dataset in an unsupervised way.","It has led to a significant influence on financial gains and fame for businesses. This, unfortunately, gives strong incentives for imposters (called spammers) to game the system. They post fake reviews or opinions (called review spam) to promote or to discredit some targeted products and services. Most efforts are devoted to explore effective linguistic and behavioral features by subsequent work to distinguish such spam from the real reviews. However, to notice such patterns or form behavioral features, developers should take long time to observe the data, because the features are based on statistics. It usually takes several months to count the difference of timestamps between the last and first reviews for reviewers. When the features show themselves finally, some major damages might have already been done. Thus, it is important to design algorithms that can detect review spam as soon as possible, ideally, right after they are posted by the new reviewers. It is a coldstart problem which is the focus of this paper. In this paper, we assume that we must identify fake reviews immediately when a new reviewer posts just one review. Unfortunately, it is very difficult because the available information for detecting fake reviews is very poor. In the scenario of cold-start, a new reviewer only has a behavior: post a review. As a result, we can not get effective behavioral features from the data. We could employ behavioral information of existing similar reviewers to a new reviewer to approximate his behavioral features. For example, the students of college are likely to choose the youth hostel during summer vacation, and tend to comment the room price in their reviews. But the financial analysts on business trip may tend to choose the business hotel, the environment and service are what they care about in their reviews. To augment the behavioral information of the new reviewers in the cold-start problem, we first try to find the textual information which is similar with that of the new reviewer, from the existing reviews. Then we employ the behavioral information which is correlated with the found textual information to approximate the behavioral information of the new reviewer. So, we propose a neural network model to jointly encode the textual and behavioral information into the review embeddings for detecting the review spam in cold-start problem. When a new reviewer posts a review, the proposed model can represent the review with the similar textual information and the correlated behavioral information encoded in the word embeddings. Finally, the embeddings of the new review are fed into a classifier to identify whether it is spam or not. We qualitatively and quantitatively prove that the traditional linguistic and behavioral features are not effective enough in detecting review spam for the coldstart task. It is an unsupervised distributional representation model which can learn from large scale unlabeled review data. Subsequent work devoted most efforts to explore effective features and spammerlike clues. As a new reviewer posted just one review and we have to identify it immediately, the major challenge of the cold-start task is that, the available informations about the new reviewer are very poor. The new reviewer only provide us with one review record. The linguistic features need not to take much time to form. So, we conduct experiments with the word bigrams feature. We investigated the previous work and found that there are three behavioral features can be applied to the cold-start task. It indicates that the traditional features are not effective enough with poor behavioral information. It is obvious that there is more false-positive review spam caused by the behavioral features as compared to only using bigrams. To go a step further, we carried experiments with the three behavioral features which are formed on abundant behavioral information. When the new reviewers continue to post more reviews in after weeks, their behavioral information gets to be more. Then the review system could obtain more sufficient data to extract behavior features as compared to the poor information in the cold-start period. The added behavioral information in the hotel domain is richer than that in the restaurant domain. It indicates that: The difficulty of detecting review spam in the cold-start task is that the available behavioral information of new reviewers is very poor. The new reviewer just posted one review and we have to filter it out immediately, there is not any historical reviews provided to us. So, to augment the behavioral information of new reviewers, we try to find the textual information which is similar with that of the new reviewer, from existing reviews. Then we take the behavioral information which is correlated with the found textual information as the most possible behavioral information of the new reviewer. When a new reviewer posts a review, the neural network can represent the review with the similar textual information and the correlated behavioral information encoded in the word embeddings. Finally, embeddings of the new review are fed into a classifier to identify whether it is spam or not. As it shows, the review graph contains the global behavioral information (footprints) of the existing reviewers. So the behavioral footprints of the spammers are decided by the demands of the businesses. But the real reviewers only post reviews to the product or services they have actually experienced. Their behavioral footprints are influenced by their own characteristics. Previous work extracts behavioral features for reviewers from these behavioral information. But it is impractical to the new reviewers in the cold-start task. When we represent the product, reviewer and review in low dimension vector space, the reviewer embeddings can be taken as a translation vector, which has translated the product embeddings to the review embeddings. To encode the textual information into our model, we adopt a convolutional neural network (CNN) to learn to represent the existing reviews. By statistics, we find that a review usually refers to several aspects of the products or services. Compared with the recurrent neural network (RNN), the CNN can do a better job of modeling the different aspects of a review. To model the correlation of the textual and behavioral information, we employ the jointly information encoding. By jointly learning from the global review graph, the textual and behavioral information of existing spammers and real reviewers are embedded into the word embeddings. The spammers often review their target products with low rating for discredited purpose, and with high rating for promoted purpose. They are taken as the constraints of the review embeddings during the joint learning. The reviewed product here refers to a hotel or restaurant. The hyper-parameters are tuned by grid search on the development dataset. The accuracy (A) of the classification under the balance distribution reflects the ability of identifying both the review spam and the real review. The learnt review embeddings with encoded linguistic and behavioral information are more effective in detecting review spam for the cold-start task. We suppose that different rating embeddings are encoded with different semantic meanings. They reflect the semantic divergences be-tween the average rating of the product and the review rating. The experiment results proves that our model is effective. It can learn to represent the reviews with global linguistic and behavioral information from large scale unlabeled existing reviews. parison. Behavioral Information To further evaluate the effectiveness of encoding the global behavioral information in our model, we build an independent supervised convolutional neural network which has the same structure and parameter settings with the CNN part of our model. The results show that our model can represent the new reviews posted by the new reviewers with the correlated behavioral information encoded in the word embeddings. The transE part of our model has effectively recorded the behavioral informa-tion of the review graph. Thus, our model is more effective by jointly embedding the textual and behavioral informations, it helps to augment the possible behavioral information of the new reviewer. It indicates that the CNN do a better job in identifying the real review than the review spam. We suppose that the possible reason is that the CNN is good at modeling the different semantic aspects of a review. And the real reviewers usually tend to describe different aspects of a hotel or restaurant according to their real personal experiences, but the spammers can only forge fake reviews with their own infinite imagination. This paper analyzes the importance and difficulty of the cold-start challenge in review spam combat. We propose a neural network model that jointly embeds the existing textual and behavioral information for detecting review spam in the coldstart task. It can learn to represent the new review of the new reviewer with the similar textual information and the correlated behavioral information in an unsupervised way. Then, a classifier is applied to detect the review spam. Experimental results prove the proposed model achieves an effective performance and possesses preferable domain-adaptability. It is also applicable to a large scale dataset in an unsupervised way. To our best knowledge, this is the first work to handle the coldstart problem in review spam detection. We are going to explore more effective models in future."
553,2,4.0,4.0,3.57,False,acl_2017,train,"We propose a general framework for performing cross-context lexical analysis; that is, analyzing similarities and differences in term meaning and representation with respect to different, potentially overlapping partitions of a text collection. We apply our framework to three different tasks: semantic change detection (discovering words whose meanings changed over time), comparative lexical analysis over context (finding context-sensitive and context-insensitive terms), and word representation comparison (investigating randomness inherent in word embeddings).","Natural language is almost always used in a particular context (e.g., a particular time, location, or purpose), and thus the interpretation of a sentence, phrase, or word inherently depends on this context. We use CCLA to generally refer to any analysis of term meaning or term representation in different contexts, especially for understanding the differences and similarities in multiple contexts. Due to the generality of the notion of context, CCLA can be useful in many ways. Any meaningful partitioning of text data may also be regarded as implicitly defining a context value for each partition; sentiment analysis may allow us to define a sentiment context so positive and negative sentences would be regarded as belonging to different categories. We can characterize any term in a specific context by its similarity to other terms in corresponding contexts. The similarity can be computed in many ways, including (e.g.) with word embeddings. These profiles for the same term computed from different contexts can be compared to analyze the variations of term meaning across contexts. However, this is limiting because only word co-occurrence data can be used to estimate the model. Thus, including distributional similarity metrics (or any other representation) is not built-in, and it is not obvious how to include it in a probabilistic model in an easily-interchangeable way. These focuses often intermix and overlap. This paper is organized in the following manner. All datasets used in our experiments are also freely and publicly available. We now formally define the framework for crosscontext lexical analysis. Critical to CCLA is the idea of a context view. This allows us to compare the usage of the token amazing respective to each context. Due to this flexible nature of context views, it is not a requirement that all contexts partition D; contexts may even overlap. We could add a background context CALL with a vocabulary consisting of all terms used across both datasets and with word vectors learned on the union of both datasets.     For example, if our task is to identify words used similarly across contexts, our scoring function can be specified to give high scores to terms whose usage is similar across the contexts. We discuss this particular application scenario in more depth in the next section. The evolution of word usage is a well-studied area in linguistics. All three methods are based on word embedding similarity, and learn separate embeddings for distinct time periods. With these techniques, we can discover how words such as awful change meaning over time. Detecting and analyzing these semantic shifts allows us to learn about the culture and evolution of language. We next formalize the problem in the CCLA framework and compare our findings to previous results. In this task, we will use disjoint temporal segments as our context views in a term-focused task. Since the word vectors are normalized to unit length, the nearest neighbors are calculated using a dot product. To find the most stable words (i.e., those whose meaning changed the least), we would instead use head. As with the previous work, we found SVD and SGNS to outperform PPMI. Interestingly, SVD appears to be slightly ahead of SGNS, in contrast to the previous results. headed, gay) and some words were detected by multiple methods with CCLA (figured, gay, handling, compound). We see that plane shifted from meaning a type of inclined or flat surface to a shortened form of airplane. A context-aware lexical analysis allows us to discover both context-sensitive and contextinsensitive terms. Each method operates on a type of word representation (PPMI, SVD, or SGNS). For example, excellent and great could represent a positive sentiment context and bad and horrible could represent negative sentiment contexts. Context-insensitive terms are those that do not change across contexts, such as stop words. Intelligently assigning scores to these word types will allow us to rank words per context, and even allow us to discover ambiguous words (those whose meaning changes between contexts). In the next sections, we will show how to address these goals with CCLA. Here, we will use contexts from the same time, but with different metadata attributes. Concretely, imagine D is a sentiment analysis dataset. Second, we want to find words that are representative of their context. Each corpus treated independently as a separate CCLA problem. loggia i always enjoyed watching in just seeing him yell ridiculous attempt at a hispanic accent. sorry loggia.) krige great cast with alice krige and brian krause usually excellent alice krige is wasted in this one alice krige plays the borg queen again fantastically alice krige seems to shoulder the film, played by beautiful and talented alice krige. his companion clifford grayson. In some cases, the same person is discussed in different ways; in others, people share the same name, leading to ambiguity. reviews and the Yelp academic dataset. In IMDB, the most ambiguous terms are all names of actors and actresses. In Yelp, the ambiguous terms are more varied; staff behavior could be good or bad and silverware could be clean or dirty. Ambiguous words in these two rows refer to distinguishing terms between all positive documents based on the corpus or all negative documents based on the corpus.  Cross-corpus lists show words that are used similarly in both collections. Corpus-specific lists show words that are used differently given a particular collection. the issue of disjoint vocabulary. To combat this, we filter the lists from each cross-corpus analysis, only keeping adjectives. Since each word is scored with respect to its context, it is a natural extension to use these scored terms in feature selection or even to estimate word polarity scores. We would expect shared positive and negative terms between IMDB and Yelp to aid in other sentiment analysis tasks, where the corpus-specific terms are less helpful. The fact that this works even when there is no labeled data in the target domain results in a completely unsupervised way to received specialized knowledge. It is educational so study how annotations drawn from the same data are similar or different. But is there a way to explicitly compare the structure learned by these models? If we have a quantification of this structure, does it give any information about task performance? Other configurations mentioned (but not tested) are number of iterations, vector di-mensionality, and effect of randomness. We define word embedding stability as a measure of how consistent nearest-neighbor lists are across different runs of the same algorithm. Consistency is an important attribute when replicating results or comparing two methods against one another. Different random seeds may play some role in the quality of the word vectors, and methods that use random sampling (like SGNS) may be affected. Nearestneighbor lists are critical when solving word analogy problems or measuring the similarity between words, so this is the aspect of the word vectors that we will consider while measuring stability. Now, we will vary the word annotations instead in annotation-focused experiments. We wish to measure how similar the embeddings are for different runs of the same algorithm. In the CCLA framework, one way to address this situation requires a similarity metric to measure the nearest-neighbors of the two runs. Before, we used cosine similarity with the term annotation dot product scores as term weights. If we want to stress the orders of the lists themselves, we should ignore the weights and use a ranking correlation metric. The flexibility of CCLA allows us to choose the best measure to suit our task. A rank difference near the top of the lists should be more detrimental than a rank difference farther down the list. In other words, heavy bias should be placed on getting similar top terms to match, rather than terms farther down the list. reason, we choose normalized discounted cumulative gain (NDCG) as our measure. sorting the top n elements by their relevance and taking their DCG.   Note that we can use this framework to compare embeddings not only from different seeds, but from different algorithms or even dimensions. This measure could be used to see how similarly two or more algorithms perform on the same data. SGNS is initially stable, but starts to drop as iterations increase, perhaps indicative of overfitting or model divergence. We used standard benchmarks for word analogy solving and word similarity scoring. SGNS outperformed GloVe in all tasks, even at low stability. Thus, comparing stability across methods may not be a viable metric at suggested performance. This is an especially interesting result for SGNS, since the high stability point is actually at a much lower number of iterations. This suggests that we might use stability as an early-stopping criterion when learning the word representations, potentially saving much compute time while increasing performance. Our work spans several areas of research: Detecting semantic change. Unlike the previous two works, this does not produce a mapping function. We propose an approach that does not require embedding matrix alignment and thus does not require an optimization algorithm; we utilize withinperiod word similarities to create word representations that are comparable across time. Contextual text mining. Topic models have been extended to support analysis of topic variations over different contexts in many ways. A common idea in all these and other methods is to model the association of context and topics as word distributions, facilitating cross-context topic analysis, but cannot easily support cross-context lexical analysis, which is our main goal. An important difference between our work and these contextual topic mod-els is that our approach does not make parametric assumptions in modeling text (which are generally needed in topic models) and is very flexible, allowing it to easily work with any context and contextspecific word annotations. Word embedding evaluation. Our method is able to compare the embedding spaces themselves, which may be a useful alternative to premade similarity datasets or the less direct application tasks. We propose a general way to perform crosscontext lexical analysis to accommodate any notion of context, any similarity function, and any type of word annotation. This enables many new applications all under the same framework (e.g. development of a common toolkit to support all applications), including analysis of semantic change, comparative analysis of meaning over context, and word embedding stability evaluation. CCLA opens up interesting new directions for further study, especially in additional applications. One use is to investigate framing bias on political viewpoints. Another is a more fine-grained comparative analysis over specific products as opposed to movies or businesses. Term scoring can be further taken advantage of in sentiment valence prediction. Using CCLA as a tool in a larger system is desirable, such as learning to automatically partition a corpus to maximize word differences, or using it for event detection when tones shift from a monitored stream. We want to investigate embedding comparisons further using larger training data and automatically determine an optimal dimensionality or window size given new scoring functions."
251,1,3.0,4.0,4.0,True,acl_2017,train,"In recent years word-embedding models have gained great popularity due to their remarkable performance on several tasks, including word analogy questions and caption generation. An unexpected “sideeffect” of such models is that their vectors often exhibit compositionality, i.e., adding two word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words, e.g., “man” + “royal” = “king”. In this work we provide a mathematical formalism for compositionality, and a theoretical justification for the presence of additive compositionality in word vectors learned using the Skip-Gram model. In particular, we show that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. As a corollary, we explain the success of vector calculus for solving word analogies. When these assumptions do not hold, we show that compositionality is no longer additive, and provide the correct composition operator. Finally, we establish a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby. SDR models provide information-theoretically optimal symbol-embeddings for problems where the training data takes the form of co-occurrence statistics. We prove that the parameters of Skip-Gram models can be readily modified to obtain the parameters of SDR models by simply adding information on symbol frequencies. This shows that the Skip-Gram model is essentially learning optimal word embeddings in the sense of Globerson and Tishby. Further, it implies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit SDR models.","In particular, we show that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. As a corollary, we explain the success of vector calculus for solving word analogies. When these assumptions do not hold, we show that compositionality is no longer additive, and provide the correct composition operator. Finally, we establish a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby. SDR models provide information-theoretically optimal symbol-embeddings for problems where the training data takes the form of co-occurrence statistics. We prove that the parameters of Skip-Gram models can be readily modified to obtain the parameters of SDR models by simply adding informa-tion on symbol frequencies. This shows that the Skip-Gram model is essentially learning optimal word embeddings in the sense of Globerson and Tishby. Further, it implies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit SDR models. The idea of representing words as vectors has a long history in computational linguistics and machine learning. The general idea is to find a map from words to vectors such that wordsimilarity and vector-similarity are in correspondence. Whilst vector-similarity can be readily quantified in terms of distances and angles, quantifying word-similarity is a more ambiguous task. In the simplest case, one seeks vectors whose similarity approximates the co-occurrence frequencies.   This work guarantees that word vectors can be recovered by factorizing the so-called PMI matrix, and that algebraic operations on these word vectors can be used to solve analogies, under certain conditions on the process that generated the training corpus. The discourse vector is assumed to evolve according to a random walk on the unit sphere that has a uniform stationary distribution. By way of contrast, our results assume nothing about the properties of the word vectors a priori. We find the exact non-linear composition operator when no assumptions are made on the context word. While our primary motivation has been to provide a better theoretical understanding of the popular SG model, our connection with the SDR method opens up the possibility of practical applicability of our approach more generally. However, the fact that when we fit an SG model we are fitting an SDR model (up to frequency information), and the fact that SDR models are informationtheoretically optimal in a certain sense, argues that regardless of whether the SG assumptions hold, SG always gives us optimal features in the following sense: the learned context embeddings and target embeddings preserve the maximal amount of mutual information between any pair of random variables X and Y consistent with the observed co-occurence matrix, where Y is the target word and X is the predictor word (in a min-max sense, since there are many ways of coupling X and Y, each of which may have different amounts of mutual information). Importantly, this statement requires no assumptions on the distribution P (X,Y). In this section, we first give a mathematical formulation of the intuitive notion of compositionality of words. We then prove that the composition operator for the Skip-Gram model in full generality is a non-linear function of the vectors of the words being composed. Under a single simplifying assumption, the operator linearizes and reduces to the addition of the word vectors. Finally, we explain how linear compositionality allows for solving word analogies with vector algebra.     Although this is an intuitively satisfying definition, we never expect it to hold exactly; instead, we replace exact equality with the minimization of KL-divergence. The second is that KL-divergence minimization is a hard problem, as it involves optimization over many high dimensional probability distributions.     Our next result proves that although the composition operator is nontrivial in the general case, to recover vector addition as the composition operator, it suffices to assume that the word frequency is uniform.    We see that solving analogies when the composition operator is nonlinear requires the solution of two highly nonlinear systems of equations. In sharp contrast, when the composition operator is linear, the solution of analogies delightfully reduces to elementary vector algebra. Note that because this expression for u? is in terms of k, w, andm, there is actually no need to assume that R is a set of actual words in V.                     cm.     cm. In both cases, we thus need to project the vector(s) onto words in our vocabulary in some manner. Minimizing the angle has been empirically successful at capturing composition in multiple loglinear word models. The peaks of this categorical distribution are precisely the words with which c co-occurs most often. There is empirical evidence that this model generates features that are useful for NLP tasks, but there is no a priori guarantee that the training corpus was generated in this manner. In this section, we provide theoretical support for the usefulness of the features learned even when the SkipGram model is misspecified. As it turns out, these embeddings, like Skip-Gram, are obtained by learning the parameters of an exponentially parameterized distribution. As stated above, the SDR factorization solves the problem of finding information-theoretically optimal features, given co-occurrence statistics for a pair of discrete random variables X and Y. Globerson and Tishby show that such optimal features can be obtained from a low-rank factorization of the matrix G of co-occurence measurements: Gij counts the number of times state i of X has been observed to co-occur with state j of Y. The expression eWH T denotes entrywise exponentiation of WHT. Now we revisit the Skip-Gram training objective, and show that it differs from the SDR objective only slightly. Whereas the SDR objective measures the distance between the pmfs given by (normalized versions of) G and eWH T, the SkipGram objective measures the distance between the pmfs given by (normalized versions of) the rows of G and eWH T. That is, SDR emphasizes fitting the entire pmfs, while Skip-Gram emphasizes fitting conditional distributions. Before presenting our main result, we state and prove the following lemma, which is of independent interest and is used in the proof of our main theorem. Recall that Skip-Gram represents each word c as a multinomial distribution over all other words w, and it learns the parameters for these distributions by a maximum likelihood estimation. Let G be the word co-occurrence matrix constructed from the corpus on which a SkipGram model is trained, in which case Gcw is the number of times word w occurs as a neighboring word of c in the corpus. We now prove our main theorem of this section, which states that SDR parameters can be obtained by augmenting the Skip-Gram embeddings to account for word frequencies. To establish this result, we use a chain rule for the KL-divergence. Here, the notation x y denotes entry-wise multiplication of vectors."
178,3,3.64,3.73,2.0,False,acl_2017,train,"Recent work on embedding ontology concepts has relied on either expensive manual annotation or automated concept tagging methods that ignore the textual contexts around concepts. We propose a novel method for jointly learning concept, phrase, and word embeddings from an unlabeled text corpus, by using the representative phrases for ontology concepts as distant supervision. We learn embeddings for medical concepts in the Unified Medical Language System and generaldomain concepts in YAGO, using a variety of corpora. Our embeddings show performance competitive with existing methods on concept similarity and relatedness tasks, while requiring no human corpus annotation and demonstrating more than 3x coverage in the vocabulary size.","Despite the lack of lexical overlap between these phrases, we would like a semantic model that can represent the underlying concept, regardless of the specific textual form used. Our novel approach to this task combines structured knowledge with proven techniques for learning word embeddings to train context-based representations for contexts, phrases, and words. The model requires no human annotations: we use known phrases as distant supervision in distributional similarity training over an unannotated corpus. We experimentally validate our approach by learning embeddings for biomedical concepts and real-world entities. An evaluation on concept similarity and relatedness tasks shows that our embeddings are competitive with prior models that required human annotations for concepts. We also present a novel dataset of similarity and relatedness of real-world entities, identified both by Wikipedia page and text phrase. Furthermore, analysis suggests the source ontology structure is reflected in the organization of our embeddings: concepts and their representative phrases are embedded close to one another, and embedded concepts of the same semantic type cluster together with some regularity. Additionally, a number of models have been proposed for the related task of separately embedding different word senses. In all of these cases, however, words are used as atomic units, and can be composed in order to model concepts. Several alternative methods for embedding concepts atomically have been proposed in the biomedical domain. Their best results also rely on human annotation via Wikipedia links. We propose a method to jointly embed concepts, phrases, and words into a real-valued space, using structured knowledge from an ontology and an unannotated training corpus. Let C denote the set of canonical concepts in an ontology. Additionally, let T denote the sequence of tokens in a training corpus, and W be the word vocabulary used in it. After extracting the mapping between concepts and phrases from the ontology, each phrase is assigned a unique identifier. Then, all occurrences of the mapped phrases in the training corpus are replaced with the unique identifiers, converting them to unigrams. For example, in the sentence Patient was diagnosed with chronic obstructive lung disease in December. This replacement is done greedily over the number of tokens matched, producing two parallel versions of the training corpus: the original, untagged text (TU), and the text tagged for mapped phrases (TT). One token in TT may correspond to multiple tokens in TU. To train the embeddings, we iterate over the tagged and untagged versions of the training corpus in parallel, using a sliding context window of k untagged words. Word embeddings are trained using all words around them, and phrase embeddings are trained using the word contexts around the complete phrase. Concept embeddings are updated using the contexts of phrases that can represent them; for a mention of phrase p, these updates are normalized by the number of concepts p can represent. In all cases, the embeddings are trained to maximize the log-likelihood of their observed contexts, and to minimize the log-likelihood of randomly-selected negative samples. the observed phrase just completed, and o is the observed context word, we do as follows. Let Cp be the set of concepts that p can represent, and N be the set of negative samples for this observation. To construct the updated loss function, let Wp be the list of words in phrase p, andWp be the average of their embeddings. For a given concept c, let Pc denote the set of phrases that can represent it, with Pc being the average of their embeddings. CLINICAL A set of clinical notes from the Ohio State Wexner Medical Center, regarding patients diagnosed with a variety of chronic conditions. CUIs are further mapped to one or more textual forms, along with semantic type information. As with the UMLS, YAGO maps between canonical entity identifiers and one or more textual forms. We therefore present two novel datasets composed of human judgments of similarity and relatedness between pairs of people, places, and organizations. For further details about our data collection and analysis of response, please see the supplemental material. We then rank each list of entity pairs in order of decreasing cosine similarity, and compare our ranking against the ranked list of human similarity or relatedness judgments. On our real-world entity datasets, the phrase-based concept approximation outperforms the direct concept embeddings. Since our model jointly embeds concepts, phrases, and words into the same space, we assess how well the ontological links between concepts and phrases and between phrases and words are preserved. We first approximate each concept and phrase by averaging the embeddings of their representative phrases and words, and calculate co-mance in extrinsic downstream applications. Our analysis is restricted to evaluations relying on the affine organization of the vector space; thus, we note that these optimized hyperparameters will likely change for downstream tasks. sine similarity with the learned concept or phrase embedding. To assess if this pattern holds for individual cases, we examine the phrases and words in the neighborhood of a parent concept or phrase. In particular, given a parent concept or phrase p, we calculate the mean average precision (MAP) of Cp, the child phrases or words connected to it, within the full phrase or word vocabulary V. This suggests that most concepts are not especially near their individual representative phrases in the vector space, in comparison to the full phrase vocabulary. Taken together, these results indicate that our model seems to be distributing component phrases or words fairly equally around their parents, though at varying distance. Best-performing settings for each dataset are marked in bold. type cluster in a fixed neighborhood of k nearest neighbors. To obtain a more complete picture of the space, we adapt their methodology to find all concepts of a given semantic type within the entire space; as this is a ranking problem, we report mean average precision. Despite the noisiness of our distantly-supervised training method, our results indicate that we capture certain semantic characteristics of ontology concepts nearly as well as prior methods that require significantly more annotation. Furthermore, we can scale to a much larger vocabulary without additional expense. However, there are a number of points that bear further examination. Highest MAP per semantic type is marked in bold. However, the extreme similarity observed in our compositionality analysis, and the high performance of concept approximations, suggests that some non-lexical information about concepts is not being captured in our method. While we address some of their concerns (e.g., our concept embeddings rule out polysemy), several of the issues they describe remain. Finally, we note a slight mismatch between the ontologies we used. The UMLS encodes information about abstract biomedical concepts, while YAGO is focused on concrete, real-world entities. We propose a novel model to jointly embed canonical concepts, the phrases that represent them, and words into a shared vector space. Our method only uses distant supervision from phrases linked to concepts in an ontology, but we experimentally demonstrate that our embeddings preserve several semantic properties comparably to recent methods that require human-annotated data. In particular, our concept embeddings maintain similarity and relatedness, as evaluated by cosine similarity, and preserve the links between concepts of the same type and between concepts and their representative phrases. With regards to the other hyperparameters we evaluated, no clear trends emerged, as each hyperparameter behaved differently in each corpus. However, given the small size of our Clinical corpus, we experimented with smaller thresholds; we found overall decreases in performance and only small gains in the number of concepts modeled. We included these validation questions at random indices in our surveys. All participants were paid, regardless of if we used their data or not. In assessing inter-annotator agreement (IAA), we considered each HIT individually, as we had neither the same participants nor the same number of accepted responses for all HITs."
343,1,4.0,4.0,3.0,True,acl_2017,train,"Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important sub module using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.","Neural network models have been exploited due to their strength in non-sparse representation learning and non-linear power in feature combination, which have led to advances in many NLP tasks. So far, neural word segmentors have given comparable accuracies to the best statictical models. With respect to non-sparse representation, character embeddings have been exploited as a foundation of neural word segmentors. Pretraining can be regarded as one way of leveraging external resources to improve accuracies, which is practically highly useful and has become a standard practice in neural NLP. To our knowledge, such rich external information has not been systematically investigated for neural segmentation.   cm.            We fill this gap by investigating rich external pretraining for neural segmentation. Different from previous work, we make our model conceptually simple and modular, so that the most important sub module, namely a five-character window context, can be pretrained using external data. After pretraining, the character window network is used to initialize the corresponding module in our segmentor. Our work belongs to recent neural word segmentation. To our knowledge, there has no work in the literature systematically investigating rich external resources for neural word segmentation training. Their baseline is similar to ours in the sense that both character and word contexts are considered. On the other hand, their model is statistical while ours is neural. Consequently, they integrate external knowledge as features, while we integrate it by shared network parameters. Our results show a similar degree of error reduction compared to theirs by using external data. However, words play a relatively less important role in our model, and we find that word LSTM, which has been used by all previous neural segmentation work, is unnecessary for our model. Our segmentor works incrementally from left to right. In the figure, V denotes the score of a state, given by a neural network model. The score of the initial state (i.e. It consists of three main layers. On the bottom is a representation layer, which derives dense representations XW, XP and XC for W,P and C, respectively. Characters. a window approach and an LSTM approach. Intuitively, a five-character window provides a local context from which the meaning of the middle character can be better disambiguated. LSTM, on the other hand, captures larger contexts, which can contain more useful clues for dismbiguation but also irrelevant information. Partial Word. We take a very simple approach to representing the partial word P, by using the embedding vectors of its first and last characters, as well as the embedding of its length. Length embeddings are randomly initialized and then tuned in model training. XP has relatively less influence on the empirical segmentation accuracies. Similar to the character case, we investigate two different approaches to encoding incoming characters, namely a window approach and an LSTM approach.                  MLP. silver hete. The three basic elements in our neural segmentor, namely characters, character bigrams and words, can all be pretrained over large unsegmented data. We consider four types of commonly explored external data to this end, all of which have been studied for statistical word segmentation, but not for neural network segmentors. Raw Text. For neural segmentation, these distributional statistics can be implicitly learned by pretraining character embeddings. We leverage this source of information by extracting character five-grams excluding punctuation from raw sentences, using them as inputs to classify whether there is punctuation before middle character. After such training, the embedding Vci and MLP values can be used to initialize the corresponding parameters for DC in the main segmentor, before its training. Automatically Segmented Text. We adopt a simple strategy, taking automatically segmented text as silver data to pretrain the five-character window network. Training can be done in the same way as training with punctuation. Heterogenous Training Data. Multiple segmentation corpora exist for Chinese, with different segmentation granularities. We verify the utility of POS information for our segmentor by pretraining a classifier that predicts the POS on each character, according to the character window representation DC. While each type of external training data can offer one source of segmentation information, different external data can be complimentary to each other. We aim to inject all sources of information into the character window representation DC by using it as a shared representation for different classification tasks. We randomly sample from different training sources according to their sizes, performing mixed training. For decoding, standard beam search is used, where the B best partial output hypotheses at each step are maintained in an agenda. Initially, the agenda contains only the start state. At each step, all hypotheses in the agenda are expanded, by applying all possible actions and B highest scored resulting hypotheses are used as the agenda for the next step. For training, the same decoding process is applied to each training example (xi, yi). Data. We compare pretraining using ZPar results only and using results that both segmentors agree on. Evaluation. Hyper-parameter Values. We adopt commonly used values for most hyperparameters, but tuned the sizes of hidden layers on the development set. We perform development experiments to verify the usefulness of various context representations, network configurations and different pretraining methods, respectively. Character Context. This is likely because they contain more distinct tokens and hence offer a larger parameter space. Word Context. The fact that word contexts can contribute relatively less than characters in a word is also not surprising in the sense that word-based neural segmentors do not outperform the best character-based models by large margins. Given that character context is what we pretrain, our model relies more heavily on them. Interestingly, using a word LSTM does not bring further improvements, even when it is combined with a window context. This suggests that global word contexts may not offer crucial additional information compared with local word contexts. Intuitively, words are significantly less polysemous compared with characters, and hence can serve as effective contexts even if used locally, to supplement a more crucial character context. We verify the effectiveness of structured learning and inference by measuring the influence of beam size on the baseline segmentor. As the size of the beam increases, more global structural ambiguities can be resolved since learning is designed to guide search. the beam size increases, the gain by doubling the beam size decreases. Our findings show that external data can be as useful for neural segmentation as for statistical segmentation. We achieve the best reported F-score on this dataset. Most neural models reported results only on the PKU and MSR datasets of the bakeoff test sets, which are in simplified Chinese. The AS and CityU corpora are in traditional Chinese, sourced from Taiwan and Hong Kong corpora, respectively. We map them into simplified Chinese before segmentation. The Weibo corpus is in a yet different genre, being social media text. It verifies that knowledge learned from a certain set of resources can be used to enhance cross-domain robustness in training a neural segmentor for different datasets, which is of practical importance. We investigated rich external resources for enhancing neural word segmentation, by building a globally optimised beam-search model that leverages both character and word contexts. Taking each type of external resource as an auxiliary classification task, we use neural multi-task learning to pre-train a set of shared parameters for character contexts."
752,2,3.0,3.5,4.0,True,acl_2017,train,"While sequence-to-sequence (seq2seq) models have been broadly used, their application to Abstract Meaning Representation (AMR) parsing and AMR realization has been limited, at least in part because data sparsity was thought to pose a significant challenge. In contrast, we show that with careful preprocessing and a novel training procedure that allows us to incorporate millions of unlabeled sentences, we can significantly reduce the impact of sparsity. For parsing, we obtain competitive results of 61.9 SMATCH, the current best reported without significant use of external annotated semantic resources. For realization, we outperform state of the art by over 5 points, achieving 32.3 BLEU. We also present extensive ablative and qualitative analysis in addition to showing strong evidence that seq2seq models are robust to artifacts introduced by converting AMR graphs to sequences.","In contrast, we show that with careful preprocessing and a novel training procedure that allows us to incorporate millions of unlabeled sentences, we can significantly reduce the impact of sparsity. Our approach is two-fold. The paired training allows both the parser and realizer to learn high quality representations of input and output language, respectively, from millions of weakly labeled examples, that are then further improved by training on human annotated AMR data. We also provide extensive ablative and qualitative analysis, quantifying the contributions that come from preprocessing and the paired training procedure. Both systems rely on features based on a set of alignments produced using bi-lexical cues and hand-written rules. In contrast, our models train directly on parallel corpora, and make only minimal use of alignments to anonymize named entities. Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities. We avoid reducing our vocabulary by introducing a large set of unlabeled sentences from an external corpus. Our models do not need to rely on a particular linearization of the input, attaining comparable performance even with a per example random traversal of the graph. Finally, all three systems intersect with a large LM trained on Gigaword. They improve neural MT performance for low resource language pairs, by creating synthetic output for a large monolingual corpus of the target language, using a back-translation MT system and mix it with the human translations. We instead pre-train on the external corpus first, and then finetune on the original dataset. The AMR is a rooted directed acylical graph. The set of node and edge names in an AMR graph is drawn from a set of concepts C, and every token in a sentence is drawn from a vocabulary W. The model uses a stacked bidirectional-LSTM encoder to encode an input sequence and a stacked LSTM to decode from the hidden states produced by the encoder. The decoder predicts an attention vector over the encoder hidden states using previous decoder states. Self-train AMR parser. Pre-train AMR parser. Fine tune AMR parser. Pre-train AMR realizer. Fine tune AMR realizer. The weighted hidden states, the decoded token, and an attention signal from the previous time step (input feeding) are then fed together as input to the next decoder state. The decoder can optionally choose to output an unknown word symbol, in which case the predicted attention is used to copy a token directly from the input sequence into the output sequence. We define a linearization order for an AMR graph as any sequence of its nodes and edges (potentially with repeats). Furthermore, for parsing, a valid AMR graph must be recoverable from the linearization. Obtaining a corpus of jointly annotated pairs of sentences and AMR graphs is expensive and current datasets only extend to thousands of examples. Neural sequence-to-sequence models suffer from sparsity with so few training pairs. To reduce the effect of sparsity, we use an external unannotated corpus of sentences Se, and a procedure which pairs the training of the parser and realizer. Then it uses self-training to improve the initial parser. After each iteration, we increase the size of the sample from Se by an order of magnitude. After we have the best parser from self-training, we use it to label AMRs for Se and pre-train the realizer. In case of re-entrant nodes we replace the variable mention with its co-referring concept. In order to reduce sparsity and be able to account for new unseen entities, we perform extensive anonymization. We exclude date entities (see the next section). We record this mapping for use during testing of realization models. If the entity was never observed, we copy its name directly from the AMR graph. Named Entity Clusters When performing AMR realization, each of the AMR fine-grained entity types is manually mapped to one of the four coarse entity types. This reduces the sparsity associated with many rarely occurring entity types. NER for Parsing When parsing, we must normalize test sentences to match our anonymized training data. Sentence-graph pairs after (a) basic preprocessing, (b) named entity anonymization, (c) named entity clustering, and (d) insertion of scope markers. If this fails, we anonymize the sentence using the coarse categories predicted by the NER system, which are also categories in AMR. After parsing, we deterministically generate AMR for anonymizations using the corresponding text span. Linearization Order Our linearization order is analogous to the order of nodes visited by depth first search, including backward traversing steps. Rendering Function Our rendering function marks scope, and generates tokens based on two cases. We select the best performing model on the development set among all of these fine-tuning attempts. Through every round of self-training, our parser improves. Our full models outperform JAMR, a graph-based model but still lags behind other parser-dependent systems (CAMR), and resource heavy approaches (SBMT). We outperform all previous state-of-the-art systems by the first round of self-training and further improve with the second round. Overall, our model incorporates less data than previous approaches as all reported methods train language models on the whole Gigaword corpus. We leave scaling our models to all of Gigaword for future work. Preprocessing Ablation Study We consider the contributation of each main component of our preprocessing stages while keeping our linearization order identical. Results indicate each of these components is required, and that scope markers and anonymization are the biggest contributors. In this section we evaluate three strategies for converting AMR graphs into sequences in the context of AMR realization and show that our models are largely agnostic to linearization orders. Random We construct a random global ordering of all edge types appearing in AMR graphs. We traverse children, based on the position in the global ordering of the edge leading to a child. Stochastic In this linearization we randomize our traversal of children, per example. Random linearization order performs only slightly worse than traversing the graph according to Human linearization order. Human-authored AMR leaks information The small difference between stochastic and random linearizations argues that our models are largely agnostic to variation in linearization order. On the other hand, the model that follows the human order performs significantly better which leads us to suspect it carries extra information not apparent in the graphical structure of the AMR. To further investigate, we compared the relative ordering of edge pairs under the same parent to relative position of children nodes derived from those edges, in a sentence, as reported by JAMR alignments. The relative ordering of some pairs of AMR edges was particularly indicative of realization order. To compare to previous work we still report using human orderings. Arguably, our models are agnostic to this choice. The second example is more challenging, with a deep right-branching structure, and a coordination of the verbs stabilize and push in the subordinate clause headed by state. The model omits some information from the graph, namely the concepts terrorist and virus. In the third example there are greater parts of the graph that are missing, such as the whole subgraph headed by expert. Also the model makes wrong attachment decisions in the last two subgraphs (it is the evidence that is unimpeachable and irrefutable, and not the equipment), mostly due to insufficient annotation (thing) thus making their realization harder. We found that the realizer mostly suffers from coverage issues, an inability to mention all tokens in the input, followed by fluency mistakes, as illustrated above. Attachment errors are less frequent, which supports our claim that the model is robust to graph linearization, and can successfully encode long range dependency information between concepts. REF: the arms control treaty limits the number of conventional weapons that can be deployed west of the Ural Mountains. REF: a technical committee of Indian missile experts stated that the equipment was unimpeachable and irrefutable evidence of a plan to transfer not just missiles but missile-making capability. COMMENT: coverage, disfluency, attachment SYS: the report stated that the Britain government must help stabilize the weak states and push international regulations to stop the use of freely available information to create a form of new biological warfare such as the modified version of the influenza. We mark, with colors, common error types: disfluency, coverage (missing information from the input graph), and attachment (implying a semantic relation from the AMR between incorrect entities)."
494,2,4.5,4.0,4.0,True,acl_2017,train,"Morphologically rich languages accentuate two properties of distributional vector space models: 1) the difficulty of inducing accurate representations for lowfrequency word forms; and 2) insensitivity to distinct lexical relations that have similar distributional signatures. These effects are detrimental for language understanding systems, which may infer that inexpensive is a rephrasing for expensive or may not associate acquire with acquires. In this work, we propose a novel morph-fitting procedure which moves past the use of curated semantic lexicons for improving distributional vector spaces. Instead, our method injects morphological constraints generated using simple language-specific rules, pulling inflectional forms of the same word close together and pushing derivational antonyms far apart. In intrinsic evaluation over four languages, we show that our approach: 1) improves low-frequency word estimates; and 2) boosts the semantic quality of the entire word vector collection. Finally, we show that morph-fitted vectors yield large gains in the downstream task of dialogue state tracking, highlighting the importance of morphology for tackling long-tail phenomena in language understanding tasks.","  This is not always considered when techniques are evaluated on languages such as English or Chinese, which do not have rich morphology. Estimating Rare Words: A single lemma can have many different surface realisations. Naively treating each realisation as a separate word leads to sparsity problems and a failure to exploit their shared semantics. On the other hand, lemmatising the entire corpus can obfuscate the differences that exist between different word forms even though they share some aspects of meaning. Embedded Semantics: Morphology can encode semantic relations such as antonymy (e.g. literate and illiterate, expensive and inexpensive) or synonymy (north, northern, northerly). In this work, we tackle the two challenges jointly by introducing a resource-light vector space finetuning procedure termed morph-fitting. The proposed method does not require curated knowledge bases or gold lexicons. rich languages. The constraints emphasise similarity on one side (e.g., by extracting morphological synonyms), and antonymy on the other (by extracting morphological antonyms), see Fig. The key idea of the fine-tuning process is to pull synonymous examples described by the constraints closer together in a transformed vector space, while at the same time pushing antonymous examples away from each other. As a post-processor, morph-fitting allows the integration of morphological rules with any distributional vector space in any language: it treats an input distributional word vector space as a black box and fine-tunes it so that the transformed space reflects the knowledge coded in the input morphological constraints (e.g., Italian words rispettoso and irrispetosa should be far apart in the transformed vector space, see Fig. Tab. The improvements are reported for all four languages, and with a variety of input distributional spaces, verifying the robustness of the approach. We then show that incorporating morph-fitted vectors into a state-of-the-art neural-network DST model results in improved tracking performance, especially for morphologically rich languages. We then query these (large) vocabularies using a set of simple language-specific if-then-else rules to extract sets of linguistic constraints, see Tab. It provides a generic framework for incorporating similarity (e.g. successful and accomplished) and antonymy constraints (e.g. nimble and clumsy) into pre-trained word vectors. Given the initial vector space and collections of ATTRACT and REPEL constraints A and R, the model gradually modifies the space to bring the designated word vectors closer together or further apart. This means that this term forces synonymous words from the in-batch ATTRACT constraints to be closer to one another than to any other word in the current mini-batch. The second term pushes antonyms away from each other. What is more, the rules for DE, IT, and RU were created by non-native, non-fluent speakers with a limited knowledge of the three languages, exemplifying the simplicity and portability of the approach. The final term of the cost function serves to retain the abundance of semantic information encoded in the starting distributional space. Semantic Specialisation with Constraints The fine-tuning ATTRACT-REPEL procedure is entirely driven by the input ATTRACT and REPEL sets of constraints. In this work, we investigate another option: extracting constraints without curated knowledge bases in a spectrum of languages by exploiting inherent language-specific properties related to linguistic morphology. This relaxation ensures a wider portability of ATTRACTREPEL to languages and domains without readily available or adequate resources. This rule yields pairs such as (look, looks), (look, looking), (look, looked). This creates pairs such as (create, creates), (create, creating) and (create, created). The other three languages, with more complicated morphology, yield a larger number of rules. An additional rule replaces the suffix-ful with-less, extracting antonyms such as (careful, careless). For instance, this generates an IT pair (rispettoso, irrispettoso) (see Fig. For DE, we use another rule targeting suffix replacement: -voll is replaced by-los. We further expand the set of REPEL constraints by transitively combining antonymy pairs from the previous step with inflectional ATTRACT pairs. This step yields additional constraints such as (rispettosa, irrispettosi) (see Fig. The final A andR constraint counts are given in Tab. The full sets of rules are available as supplemental material. Other Starting Distributional Vectors We also analyse the impact of morph-fitting on other collections of well-known EN word vectors. These vectors have varying vocabulary coverage and are trained with different architectures. Morph-fixed Vectors A baseline which utilises an equal amount of knowledge as morph-fitting, termed morph-fixing, fixes the vector of each word to the distributional vector of its most frequent inflectional synonym, tying the vectors of lowfrequency words to their more frequent inflections.   The morph-fixed vectors (MFIX) serve as our primary baseline, as they outperformed another straightforward baseline based on stemming across all of our intrinsic and extrinsic experiments. However, this model was computationally intractable with SGNSLARGE vectors. Moreover, it was consistently outperformed by ATTRACT-REPEL on vector spaces with smaller vocabularies. Since the original sets contain only word lemmas, they are unable to evaluate whether a representation model improves vectors for all synonymous word inflections. Therefore, we enrich the sets of pairs using the same set of ATTRACT rules from Sect.     We make this dataset available in hope it can aid further research on improving morphological relations in vector spaces. Morph-fitting EN Word Vectors As the first experiment, we morph-fit a wide spectrum of EN distributional vectors induced by various architectures (see Sect. The results on SimLex and SimVerb are summarised in Tab. The results with EN SGNS-LARGE vectors are shown in Fig. Morphfitted vectors bring consistent improvement across all experiments, regardless of the quality of the initial distributional space. This finding confirms that the method is robust: its effectiveness does not depend on the architecture used to construct the initial space. Experiments on Other Languages We next extend our experiments to other languages, testing both morph-fitting variants. cup and coffee) have a low rating. The numbers in parentheses refer to the vector dimensionality. Vectors Distrib. marised in Tab. These scores confirm the effectiveness and robustness of morph-fitting across languages, suggesting that the idea of fitting to morphological constraints is indeed language-agnostic, given the set of languagespecific rule-based constraints. Fig. Morph-SimLex performance across all languages shows even stronger relative gains over distributional and morph-fixed vectors. The original SimLex dataset only contains word lemmas. Consequently, it fails to penalise word vector collections with bad estimates of less-frequent word forms. The comparison between MFIT-A and MFIT-AR indicates that both sets of constraints are important for the fine-tuning process: while MFIT-A already yields consistent gains over the initial spaces, a further refinement can be achieved by also incorporating the antonymous REPEL constraints. Goal-oriented dialogue systems provide conversational interfaces for tasks such as booking flights or finding restaurants. In slot-based systems, application domains are specified using ontologies that define the search constraints which users can express. An ontology consists of a number of slots and their assorted slot values. It serves to capture the intents expressed by the user at each dialogue turn and update the belief state. User: How about something cheaper? Great hot pot. The left y axis measures the intrinsic word similarity performance, while the right y axis provides the scale for the DST performance. The NBT learns to compose these vectors into intermediate utterance and context representations. These are then used to decide which of the ontology-defined intents (goals) have been expressed by the user. The NBT model keeps word vectors fixed during training, so that unseen, yet related words can be mapped to the right intent at test time (e.g. northern to north). Users typed instead of speaking, removing the need to deal with noisy speech recognition. Conversely, the WOZ setup allowed them to use sophisticated language. In this work, we use translations of this dataset to Italian and German, provided by the authors of the original dataset. Results and Discussion The diamond-dashed lines (against the right axes) in Fig. The morph-fixed vectors do not enhance DST performance, probably because fixing word vectors to their highest frequency inflectional form eliminates useful semantic content encoded in the original vectors. On the other hand, morph-fitting makes use of this information, supplementing it with semantic relations between different morphological forms. English performance shows little variation across the four word vector collections investigated here. This corroborates our intuition that, as a morphologically simpler language, English stands to gain less from fine-tuning the morphological variation for downstream applications. This result again points at the discrepancy between intrinsic and extrinsic evaluation: the considerable gains in SimLex performance do not necessarily induce similar gains in downstream performance. Semantic Specialisation A standard approach to incorporating external information into vector spaces is to pull the representations of similar words closer together. Morph-fitting falls into the latter category. However, instead of resorting to curated knowledge bases, and experimenting solely with English, we show that the morphological richness of any language can be exploited as a source of inexpensive supervision for fine-tuning vector spaces, at the same time specialising them to better reflect true semantic similarity. Word Vectors and Morphology The use of morphological resources to improve the representations of morphemes and words is an active area of research. Contrary to our work, these models typically coalesce all lexical relations. In contrast to prior work, our model decouples the use of morphological information, now provided in the form of inflectional and derivational rules transformed into linguistic constraints, from the actual training. This pipelined approach results in a simpler, more portable model. Their work uses gold morphological lexicons; we show that competitive performance can be achieved using a nonexhaustive set of simple rules. Our framework facilitates the inclusion of antonyms at no extra cost and naturally extends to constraints from other sources (e.g., WordNet) in future work. Another practical difference is that we focus on similarity and evaluate morph-fitting in a well-defined downstream task where the artefacts of the distributional hypothesis are known to prompt statistical system failures. We have presented a novel morph-fitting method which injects morphological knowledge in the form of linguistic constraints into word vector spaces. The method makes use of implicit semantic signals encoded in inflectional and derivational rules which describe the morphological processes in a language. The results in intrinsic word similarity tasks show that morph-fitting improves vector spaces induced by distributional models across four languages. Finally, we have shown that the use of morph-fitted vectors boosts the performance of downstream language understanding models which rely on word representations as features, especially for morphologically rich languages such as German."
375,1,4.0,4.0,4.0,True,acl_2017,train,"Network embedding (NE) is playing a critical role in network analysis, due to its ability to represent vertices with efficient low-dimensional embedding vectors. However, existing NE models aim to learn a fixed context-free embedding for each vertex, and neglect the diverse roles when interacting with other vertices. In this paper, we assume that one vertex usually shows different aspects when interacting with different neighbor vertices, and should own different embeddings respectively. Therefore, we present ContextAware Network Embedding (CANE), a novel NE model to address this issue. CANE learns context-aware embeddings for vertices with mutual attention mechanism and is expected to model the semantic relationships between vertices more precisely. In experiments, we compare our model with existing NE models on three real-world datasets. Experimental results shows that CANE achieves significant improvement than state-of-the-art methods on link prediction, and comparable performance on vertex classification.","NE provides an efficient and effective way to represent and manage large-scale networks, alleviating the computation and sparsity issues of conventional symbol-based representations. In real-world social networks, it is intuitive that one vertex may demonstrate various aspects when interacting with different neighbor vertices. For example, a researcher usually collaborates with different partners on diverse research topics (as illustrated in Fig. For example, the left user and right user in Fig. To address these issues, we aim to propose a Context-Aware Network Embedding (CANE) framework for modeling relationships between vertices precisely. More specifically, we present CANE on information networks, where each vertex also contains rich external information such as text, labels or other meta-data, and the significance of context is more critical for NE in this scenario. Without loss of generality, we implement CANE on text-based information networks in this paper, which can be easily extended to other types of information networks. In conventional NE models, each vertex is represented as a static embedding vector, denoted as context-free embedding. On the contrary, CANE assigns dynamic embeddings to a vertex according to different neighbors it interacts with, named as context-aware embedding. Take a vertex u and its neighbor vertex v for example. The contextfree embedding of u remains unchanged when interacting with different neighbors. On the contrary, the context-aware embedding of u is dynamic when confronting different neighbors. When u interacting with v, their context embeddings with respect to each other are derived from their text information, Su and Sv respectively. In order to realize context-aware textbased embeddings, we introduce the selective attention scheme and build mutual attention between u and v into these neural models. The mutual attention is expected to guide neural models to emphasize those words that are focused by its neighbor vertices and eventually obtain contextaware embeddings. We conduct experiments on three real-world datasets of different areas. Experimental results on link prediction reveal the effectiveness of our framework as compared to other state-of-the-art methods. The results suggest that, context-aware embeddings are critical for network analysis, especially for those tasks concerning about complicated interactions between vertices such as link prediction. We also explore the performance of our framework via vertex classification and case studies, which again confirms the flexibility and superiority of our models. network representation learning has been proposed as a critical technique for network analysis tasks. Nevertheless, most of these NE models only encode the structural information into vertex embeddings, without considering heterogenous information accompanied with vertices in real-world social networks. To address this issue, researchers make great efforts to incorporate heterogenous information into conventional NE models. In contrast, we assume that a vertex has different embeddings according to which vertex it interacts with, and propose CANE to learn context-aware vertex embeddings. We first give basic notations and definitions in this work.   text and labels. Context-free Embeddings: Conventional NRL models learn context-free embedding for each vertex. Context-aware Embeddings: Different from existing NRL models that learn context-free embeddings, CANE learns various embeddings for a vertex according to its different contexts. Specifically, for an edge eu,v, CANE learns context-aware embeddings v(u) and u(v). When vt is context-aware, the overall vertex embeddings v will be context-aware as well. In the following part, we give detailed introduction to the two objectives respectively. Without loss of generality, we assume the network is directed, as an undirected edge can be considered as two directed edges with opposite directions and equal weights. Therefore, we propose the text-based objective to take advantage of these text information, as well as learn text-based embeddings for vertices. The text-based objective Lt(e) can be defined with various measurements. Similarly, we employ softmax function for calculating the probabilities, as in Eq. The structure-based embeddings are regarded as parameters, the same as in conventional NE models. But for text-based embeddings, we intend to obtain them from associated text information of vertices. Besides, the text-based embeddings can be obtained either in context-free ways or in context-aware ones. In the following sections, we will give detailed introduction respectively. looking-up, convolution and pooling. Looking-up.     wn). Convolution. Max-pooling.       As vt is irrelevant to the other vertices it interacts with, we name it as contextfree text embedding. As stated before, we assume that a specific vertex plays different roles when interacting with others vertices. In other words, each vertex should have its own points of focus about a specific vertex, which leads to its context-aware text embeddings. To achieve this, we employ mutual attention to obtain context-aware text embedding. It enables the pooling layer in CNN to be aware of the vertex pair in an edge, in a way that text information from a vertex can directly affect the text embedding of the other vertex, and vice versa. In Fig. After that, we conduct pooling operations along rows and columns of F to generate the importance vectors, named as row-pooling and column pooling respectively. According to our experiments, mean-pooling performs better than max-pooling.     Fm,i).     Next, we employ softmax function to transform importance vectors gp and gq to attention vectors ap and aq. According to Eq. It is intuitive that optimizing the conditional probability using softmax function is computationally expensive. In order to investigate the effectiveness of CANE on modeling relationships between vertices, we conduct experiments of link prediction on several real-world datasets. Besides, we also employ vertex classification to verify whether contextaware embeddings of a vertex can compose a highquality context-free embedding in return. Users follow each other and answer questions in this site. Structure and Text: Naive Combination: We simply concatenate the best-performed structure-based embeddings with CNN based embeddings to represent the vertices. To demonstrate the effectiveness of considering attention mechanism and two types of objectives in Eqs. Thus, we omit the results under this training ratio. It indicates the effectiveness of CANE when applied to link prediction task, and verifies that CANE has the capability of modeling relationships between vertices precisely. Specifically, CENE performs poorly under small training ratios, because it reserves much more parameters (e.g., convolution kernels and word embeddings) than TADW, which need more data for training. Different from CENE, TADW performs much better under small training ratios, because DeepWalk based methods can explore the sparse network structure well through random walks even with limited edges. On the contrary, CANE has a stable performance on various situations. It demonstrates the flexibility and robustness of CANE. It verifies our assumption that a specific vertex should play different roles when interacting with other vertices, and thus benefits the relevant link prediction task. To summarize, all the above observations demonstrate that CANE is able to learn highquality context-aware embeddings, which are conducive to estimating the relationship between vertices precisely. Moreover, the experimental results on link prediction task state the effectiveness and robustness of CANE. In CANE, we obtain various embeddings of a vertex according to the vertex it connects to. However, network analysis tasks, such as vertex classification and clustering, require a global embedding, rather than several context-aware embed-dings for each vertex. As shown in Fig. It denotes that CANE is flexible to various network analysis tasks. To demonstrate the significance of mutual attention on selecting meaningful features from text information, we visualize the heat maps of two vertex pairs in Fig. Note that, every word in this figure accompanies with various background colors. The stronger the background color is, the larger the weight of this word is. The weight of each word is calculated according to the attention weights as follows. For each vertex pair, we can get the attention weight of each convolution window according to Eq. To obtain the weights of words, we assign the attention weight to each word in this window, and add the attention weights of a word together as its final weight. The discovered significant correlations between vertex pairs reflects the effectiveness of mutual attention mechanism, as well as the capability of CANE for modeling relations precisely. In this paper, we propose the concept of ContextAware Network Embedding (CANE) for the first time, which aims to learn various context-aware embeddings for a vertex according to the neighbors it interacts with. Specifically, we implement CANE on text-based information networks with proposed mutual attention mechanism, and conduct experiments on several real-world information networks. Experimental results on link prediction demonstrate that CANE is effective for modeling the relationship between vertices. Besides, the learnt context-aware embeddings can compose high-quality context-free embeddings. In future, we will strive to implement CANE on wider variety of information networks with multi-modal data, such as labels, images and so on. Furthermore, there usually exist explicit relations in social networks (e.g., families, friends and colleagues relations between social network users), which are expected to be critical to NE. Thus, we want to explore how to incorporate and predict these explicit relations between vertices in NE."
16,1,4.0,4.0,4.0,True,acl_2017,train,"This paper tackles the task of event detection (ED), which involves identifying and categorizing events. We argue that arguments provide significant clues to this task, but they are either completely ignored or exploited in an indirect manner in existing detection approaches. In this work, we propose to exploit argument information explicitly for ED via supervised attention mechanisms. In specific, we systematically investigate the proposed model under the supervision of different attention strategies. Experimental results show that our approach advances state-ofthe-arts and achieves the best F1 score on ACE 2005 dataset.","We argue that arguments provide significant clues to this task, but they are either completely ignored or exploited in an indirect manner in existing detection approaches. In this work, we propose to exploit argument information explicitly for ED via supervised attention mechanisms. In specific, we systematically investigate the proposed model under the supervision of different attention strategies. In the ACE (Automatic Context Extraction) event extraction program, an event is represented as a structure comprising an event trigger and a set of arguments. This work tackles event detection (ED) task, which is a crucial part of event extraction (EE) and focuses on identifying event triggers and categorizing them. Besides, the task of EE also includes event argument extraction (AE), which involves event argument identification and role classification. However, this paper does not focus on AE and only tackles the former task. According to the above definitions, event arguments seem to be not essentially necessary to ED. However, we argue that they are capable of providing significant clues for identifying and categorizing events. They are especially useful for ambiguous trigger words. The pipeline method in each group was the state-of-the-art system when the corresponding joint method was proposed. We believe that this phenomenon may be caused by the following two reasons. On the one hand, since joint methods simultaneously solve ED and AE, methods following this paradigm usually combine the loss functions of these two tasks and are jointly trained under the supervision of annotated triggers and arguments. Thus, the unbalanced data may cause joint models to favor AE task. On the other hand, in implementation, joint models usually pre-predict several potential triggers and arguments first and then make global inference to select correct items. When pre-predicting potential triggers, almost all existing approaches do not leverage any argument information. In this way, ED does hardly benefit from the annotated arguments. By contrast, the component for pre-prediction of arguments always exploits the extracted trigger information. Thus, we argue that annotated arguments are actually used for AE, not for ED in existing joint methods, which is also the reason we call it an indirect way to use arguments for ED. Contrast to joint methods, this paper proposes to exploit argument information explicitly for ED. We have analyzed that arguments are capable of providing significant clues to ED, which gives us an enlightenment that ar-guments should be focused on when performing this task. Therefore, we propose a neural network based approach to detect events in texts. And in the proposed approach, we adopt a supervised attention mechanism to achieve this goal, where argument words are expected to acquire more attention than other words. The attention value of each word in a given sentence is calculated by an operation between the current word and the target trigger candidate. Specifically, in training procedure, we first construct gold attentions for each trigger candidate based on annotated arguments. Then, treating gold attentions as the supervision to train the attention mechanism, we learn attention and event detector jointly both in supervised manner. In testing procedure, we use the ED model with learned attention mechanisms to detect events. The experimental results demonstrate that the proposed approach is effective for ED task, and it outperforms state-of-the-art approaches with remarkable gains. Furthermore, we systematically investigate different attention strategies for the proposed model. The ED task is a subtask of ACE event evaluations where an event is defined as a specific occurrence involving one or more participants. We firstly introduce some ACE terminologies to facilitate the understanding of this task: Entity: an object or a set of objects in one of the semantic categories of interests. Entity mention: a reference to an entity (typically, a noun phrase). Event trigger: the main word that most clearly expresses an event occurrence. Event arguments: the mentions that are involved in an event (participants). Event mention: a phrase or sentence within which an event is described, including the trigger and arguments. The goal of ED is to identify event triggers and categorize their event types. Similar to existing work, we model ED as a multi-class classification task. In our approach, every word along with its context, which includes the contextual words and entities, constitute an event trigger candidate. In order to prepare for Context Representation Learning (CRL), we limit the context to a fixed length by trimming longer sen-tences and padding shorter sentences with a special token when necessary. Note that, both w, Cw and Ce mentioned above are originally in symbolic representation. Before entering CRL component, we transform them into real-valued vector by looking up word embedding table and entity type embedding table. Then we calculate attention vectors for both contextual words and entities by performing operations between the current word w and its contexts. Finally, the contextual words representation cw and contextual entities representation ce are formed by the weighted sum of the corresponding embeddings of each word and entity in Cw and Ce, respectively. This paper uses the learned word embeddings as the source of basic features. We randomly initialize embedding vector for each entity type (including the NA type) and update it in training procedure. In this subsection, we illustrate our proposed approach to learn representations of both contextual words and entities, which serve as inputs to the following event detector component. Recall that, we use the matrix Cw and Ce to denote contextual words and contextual entities, respectively. Then, two attention vectors, which reflect different aspects of the context, are calculated in the next step. Therefore, the entity type of a candidate trigger is meaningless for ED. Instead, we use we, which is calculated by transforming w from the word space into the entity type space, as the attention source. In this subsection, we introduce supervised attention to explicitly use annotated argument information to improve ED. Our basic idea is simple: argument words should acquire more attention than other words. To achieve this goal, we first construct vectors using annotated arguments as the gold attentions. Then, we employ them as supervision to train the attention mechanism. Constructing Gold Attention Vectors Our goal is to encourage argument words to obtain more attention than other words. To achieve this goal, we propose two strategies to construct gold attention vectors: words. That is, all argument words in the given context obtain the same attention, whereas other words get no attention. For candidates without any annotated arguments in context, we force all entities to average the whole attention. The word fired is the trigger candidate, and underline words are arguments of fired annotated in the corpus. and the words around them. The assumption is that, not only arguments are important to ED, the words around them are also helpful. And the nearer a word is to arguments, the more attention it should obtain. To achieve this goal, we design a series of systems for comparison. Since all arguments are entities, this system is designed to investigate the effects of entities. Note that, in order to avoid the interference of attention mechanisms, the last two systems are designed to use argument information (via gold attentions) in both training and testing procedure. From the table, we observe that systems with argument information (the last two systems) significantly outperform system-s without argument information (the first two systems), which demonstrates that argument information is very useful for this task. Firstly, we introduce systems implemented in this work. These two systems both employ supervised attention mechanisms. In addition, we select the following state-ofthe-art methods for comparison. It is the best structure-based system. It is the best-reported featurebased system. The first group illustrates the performances of state-of-the-art approaches. The second group illustrates the performances of the proposed approach. FN are analogous to events in ACE. It is the best-reported representation-based joint approach proposed on this task. It is the best reported representation-based approach on this task. ANN performs unexpectedly poorly, which indicates that unsupervised-attention mechanisms do not work well for ED. It is not difficult to understand. Furthermore, another noticeable advantage of our approach is that it achieves much higher precision than state-of-the-arts. To further demonstrate the effectiveness of the proposed approach, we also use the events from FN to augment the performance of our approach. However, their data can not be used in the proposed approach without further processing, because it lacks of both argument and entity information. All events they published are also frames in FN. Thus, we treat frame elements annotated in FN corpus as event arguments. This result, to some extent, demonstrates the correctness of our assumption that the data sparseness problem is the reason that causes unsupervised attention mechanisms to be ineffective to ED. This is to be expected. On the one hand, more positive training samples consequently make higher recall. On the other hand, the extra event samples are automatically extracted from FN, thus false-positive samples are inevitable to be involved, which may result in hurting the precision. Event detection is an increasingly hot and challenging research topic in NLP. Generally, existing approaches could roughly be divided into two groups. The majority of existing work followed this paradigm, which includes feature-based methods and representationbased methods. Joint approach is proposed to capture internal and external dependencies of events, including trigger-trigger, argument-argument and trigger-argument dependencies. Theoretically, both ED and AE are expected to benefit from joint methods because triggers and arguments are jointly considered. However, in practice, existing joint methods usually only make remarkable improvements to AE, but insignificant to ED. Different from them, this work investigates the exploitation of argument information to improve the performance of ED. In this work, we propose a novel approach to model argument information explicitly for ED via supervised attention mechanisms. Besides, we also investigate two strategies to construct gold attentions using the annotated arguments. Moreover, we also use events from FN to augment the performance of the proposed approach. Experimental results show that our approach outperforms state-of-the-art methods, which demonstrates that the proposed approach is effective for event detection."
288,3,3.33,3.33,3.0,False,acl_2017,train,"A writer’s style depends not just on personal traits but also on her intent and mental state. In this paper, we show how variants of the same writing task can lead to measurable differences in writing style. We present a case study based on the story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned similar writing tasks with different constraints: (1) writing an entire story, (2) adding a story ending for a given story context, and (3) adding an incoherent ending to a story. We show that a simple linear classifier informed with stylistic features is able to successfully distinguish among the three cases, without even looking at the story context. In addition, our style-based classifier establishes a new state-of-the-art result on the story cloze challenge, substantially higher than previous results based on deep learning models. Our results demonstrate that different task framings can dramatically affect the way people write.","We show that similar writing tasks with different constraints on the author can lead to measurable differences in her writing style. In this task, authors were asked to write five-sentence self-contained stories, henceforth original stories. Framed as a story cloze task, the goal of this dataset is to serve as a commonsense challenge for NLP and AI research. While the story cloze task was originally designed to be a story understanding challenge, its annotation process introduced three variants of the same writing task: writing an original, right, or wrong ending to a short story. Our results allow us to make a few key observations. First, people adopt a different writing style when asked to write coherent vs. incoherent story endings. Second, people change their writing style when writing the entire story on their own compared to writing only the final sentence for a given story context written by someone else. In order to further validate our method, we also directly tackle the story cloze task. We also show that the style differences captured by our model can be combined with neural language models to make a better use of the story context. The contributions of our study are threefold. First, findings from our study can potentially shed light on how different kinds of cognitive load influence the style of written language. Second, our results indicate that when designing new NLP tasks, special attention needs to be payed to the instructions given to authors. Third, we establish a new state-of-the-art result on the commonsense story cloze challenge. While this task was developed to facilitate representation and learning of commonsense story understanding, its design included a few key choices which make it ideal for our study. We describe the task below. ROC stories. To collect a broad spectrum of commonsense knowledge, there was no imposed subject for the stories, which resulted in a wide range of different topics. Story cloze task. A subset of the stories was selected, and only the first four sentences of each story were presented to AMT workers. Workers were asked to write a pair of new story endings for each story context: one right and one wrong. Both endings are required to complete the story using one of the characters in the story context. The resulting stories, both right and wrong, were then individually rated for coherence and meaningfulness by additional AMT workers. Only stories rated as simultaneously coherent with a right ending and neutral with a wrong ending were selected for the task. It is worth noting that workers rated the stories as a whole, not only the endings. The task is simple: given a pair of stories that differ only in their endings, the system decides which ending is right and which is wrong. The official training data contains only the original stories (without alternative endings), while development and test data consist of the revised stories with alternative endings (for a different set of original stories that are not included in the training set). This suggests that this task is challenging and that high performance is hard to achieve. First, each pair of endings was written by the same author, which ensured that style differences between authors could not be used to solve the task. implemented nine baselines for the task, using surface level features as well as narrative-informed ones, and showed that each of them reached roughly chance-level. These results suggest that real understanding of text is required in order to solve the task. Different writing tasks in the story cloze task. Several key design decisions make the task an interesting testbed for the purpose of this study. On top of that, the original endings, which serve as positive training samples, were generated differently from the right samples, which serve as the positive samples in the development and test sets. While the former are part of a single coherent story written by the same author, the latter were generated by letting an author read four sentences, and then asking her to generate a fifth right ending. Finally, although the right and wrong sentences were generated by the same author, the tasks for generating them were quite different: in one case, the author was asked to write a right ending, which would create a coherent five-sentence story along with the other four sentences. In the other case, the author was asked to write a wrong ending, which would result in an incoherent five-sentence story. We begin by computing several characteristics of the three types of endings: original endings (from the ROC story corpus training set), right endings and wrong endings (both from the story cloze task development set). Our analysis reveals several style differences between different groups. Although writing wrong sentences is not the same as lying, it is not entirely surprising to observe similar trends in both tasks. The figure shows that both original and right endings use pronouns more frequently than wrong endings. Next we show that these style differences are not anecdotal, but can be used to distinguish among the different types of story endings. The goal of this paper is to determine the extent to which different writing constraints lead the authors to adopt different writing styles. We describe our model below. We train a logistic regression classifier to categorize an ending, either as right vs. wrong or as original vs. new (right). Each feature vector is computed using the words in one ending, without considering earlier parts of the story. We use the following style features. The number of words in the sentence. Specifically, we replace content words (nouns, verbs, adjectives, and adverbs), which are often low frequency, with their part-of-speech tags. We design two experiments to answer our research questions. The first is an attempt to distinguish between right and wrong endings, the second between original endings and new (right) endings. We describe both experiments below. The goal of this experiment is to measure the extent to which style features capture differences between the right and wrong endings. It is worth noting that our classification task is slightly different from the story cloze task. Instead of classifying pairs of endings, one which is right and another which is wrong, our classifier decides about each ending individually, whether it is right (positive instance) or wrong (negative instance). By ignoring the coupling between right and wrong pairs, we are able to decrease the impact of author-specific style differences, and focus on the difference between the styles accompanied with right and wrong writings. Here the goal is to measure whether writing the ending as part of a story imposes different style compared to writing a new (right) ending to an existing story. We use the endings of the ROC stories as our original samples and right endings from the story cloze task as new samples. We report the average classification result. Experimental setup. The bottom row shows an additional experiment which classifies original vs. wrong (new) endings. Noting again that our model ignores the story context (the preceding four sentences), our model is unable to capture any notion of coherence. Story cloze task. In order to further estimate the quality of our classification results, we tackle the story cloze task using our classifier. Otherwise, the label whose posterior probability is lower is reversed. These numbers further support the claim that the styles of right and wrong endings are indeed very different. Combination with a neural language model. We investigate whether our model can benefit from state-of-the-art text comprehension models, Model Acc. The upper block shows published results, the middle block are our results. for which this task was designed. Unlike the model in this paper, which only considers the story endings, this language model follows the protocol suggested by the story cloze task designers, and harnesses their ROC Stories training set, which consists of single-ending stories, as well as the story context for each pair of endings. We show that adding our features to this powerful language model gives improvements over our classifier as well as the language model. sentences of the story (the numerator), controlling for the inherent surprisingness of the words in that ending (the denominator). On its own, our neural language model performs moderately well on the story cloze test. These results indicate that contextignorant style features can be used to obtain high accuracy on the task, adding value even when context and a large training dataset are used. A natural question that follows this study is which style features are most helpful in detecting the underlying task an author was asked to perform. these could shed light on the stylistic differences imposed by each of the writing tasks. The table shows a few interesting trends. First, authors tend to structure their sentences differently when writing coherent vs. incoherent endings. For instance, incoherent endings are more likely to start with a proper noun and end with a common noun, while coherent endings have a greater tendency to end with a past tense verb. Second, right endings will make wider use of coordination structures, as well as adjectives. Finally, we notice a few syntactic differences: right endings will more often use infinite verb structure, while wrong endings prefer gerunds (VBG). An interesting observation is that exclamation marks are a strong indication for an original ending. New Freq. that authors are more likely to show or evoke enthusiasm when writing their own text compared to ending an existing task. Finally, when comparing the two groups of salient features from both experiments, we find an interesting trend. This indicates that, for instance, incoherent endings have a stronger tendency to begin with a proper noun compared to coherent endings, which in turn are more likely to do so than original endings. The effect of writing tasks on mental states. In this paper we have shown that giving a writer different writing tasks affects her writing style in easily detected ways. Our results indicate that when authors are asked to write the last sentence of a five-sentence story, they will use different style to write a right ending compared to a wrong ending. Our findings hint that the nature of the writing task imposes a different mental state on the author, which is expressed in ways that can be observed using extremely simple automatic tools. Previous work has shown that a writing task can affect mental state. Another line of work has shown that writing style is affected by mental state. This large body of work indicates a tight connection between writing tasks, mental states, and variation in writing style. This connection hints that the link discovered in this paper, between different writing tasks and resulting variation in writing style, involves differences in mental state. Further investigation is required in order to further validate this hypothesis. Design of NLP tasks. Our study also provides important insights for the future design of NLP tasks. The story cloze task was very carefully designed. The authors also made sure each pair of endings was written by the same author, partly in order to avoid author-specific style effects. Nonetheless, despite these efforts, several significant style differences can be found between the training and the test set, as well as between the positive and negative labels. Our findings suggest that careful attention must be paid to instructions given to authors, especially in unnatural tasks such as writing a wrong ending. Writing style. Writing style has been an active topic of research for decades. The line of work that most resembles our work is the detection of deceptive text. Machine reading. In this paper, we have pointed to another methodological challenge in designing machine reading tasks: different writing tasks used to generated the data affect writing style, confounding classification problems. Different writing tasks assigned to an author result in different writing styles for that author. In both cases, a simple linear model reveals measurable differences in writing styles, which in turn allows our final model to achieve state-of-the-art results on the story cloze task. They also provide valuable lessons for designing new NLP datasets."
699,3,4.27,4.0,4.0,True,acl_2017,train,"Keyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divide the to-besummarized content into multiple text chunks, then rank and select the most meaningful ones. These approaches can neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks. We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also generates absent keyphrases based on the semantic meaning of the text.","A typical use of keyphrase or keyword is in scientific publications, to provide the core information of a paper. High-quality keyphrases can facilitate the understanding, organizing and accessing of document content. Due to the public accessibility, many scientific publication datasets are often used as the test beds for keyphrase extraction algorithms. Therefore, this study also focuses on extracting keyphrases from scientific publications. The first step is to acquire a list of keyphrase candidates. There are two major drawbacks for the above keyphrase extraction approaches. Firstly, they can only extract the keyphrases that appear in the source text, whereas they fail at predicting the meaningful keyphrases with a slightly different sequential order or using synonym words. In this paper, we denote the phrases that do not match any contiguous subsequence of source text as absent keyphrases, and the ones that fully match a part of the text as present keyphrases. The absent keyphrases cannot be extracted through previous approaches, which further urges the development of a more powerful keyphrase prediction model. Secondly, when ranking phrase candidates, previous approaches often adopted the machine learning features such as TF-IDF and PageRank. However, these features only target to detect the importance of each word in the document based on the statistics of word occurrence and co-occurrence, whereas they can hardly reveal the semantics behind the document content. Given a document, human annotators will firstly read the text to get a basic understanding of the content, then they try to digest its essential content and summarize into keyphrases. Their generation of keyphrases relies on the understanding of the content, which not necessarily to be the words that occurred in the source text. In addition to the semantic understanding, human annotators might also go back and picks up the most important parts based on syntactic features. As a result, a better keyphrase prediction model should understand the semantic meaning of the content, as well as capture the contextual features. Thus, our model can generate keyphrases based on the understanding of the text, no matter whether the keyphrases are present in the text or not; meanwhile, it does not lose important in-text information. Keyphrase provides a succinct and accurate way of describing a subject or a subtopic in a document. A number of extraction algorithms have been proposed, and typically the process of extracting can be broken down into two steps. The first step is to generate a list of phrase candidates with heuristic methods. As these candidates are prepared for further filtering, a considerable amount of candidates are produced in this step to increase the possibility that most of the correct keyphrases are kept. The second step is to score each candidate phrase regarding its likelihood of being a keyphrase in the given document. The top-ranked candidates are returned as keyphrases. Both supervised and unsupervised machine learning methods are widely employed here. Aside from the commonly adopted two steps process, another two previous studies realized the keyphrase extraction in entirely different ways. They used a word alignment model, which learns the translation from the documents to the keyphrases. This approach alleviates the problem of vocabulary gap between source and target to a certain degree. However, this translation model can hardly deal with semantic meaning. The RNN Encoder-Decoder model (also referred as Sequence-to-Sequence Learning) is an end-toend approach. Different strategies have been explored to improve the performance of Encoder-Decoder model. There exists a discrepancy between the optimizing objective during training and the metrics during evaluation. This section will introduce in detail our proposed deep keyphrase generation. Firstly, the task of keyphrase generation is defined, followed by the overview of how we apply the RNN EncoderDecoder model.   p(i,Mi)).     y (i,j) L p(i,j) Lx(i) and Lp(i,j)denotes the length of word sequence of x(i) and p(i,j) respectively. Now for each data sample, there are one source text sequence and multiple target phrase sequences. To apply the RNN Encoder-Decoder model, the data need to be converted into textkeyphrase pairs which contain only one source sequence and one target sequence.   x(i),p(i,Mi)). Then the Encoder-Decoder model is ready to be applied to learn the mapping from source sequence to target sequence. For the purpose of simplicity, (x,y) is used to denote each data pair in the rest of this section, where x is the word sequence of a source text and y is the word sequence of its keyphrase. Both the encoder and decoder are implemented with recurrent neural networks (RNN).   The encoder and decoder networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence. After training, we use the beam search to generate phrases and a max heap is maintained to get the predictions with highest probabilities. A bidirectional Gated Recurrent Unit (GRU) is applied as our encoder to replace the simple recurrent neural network. Another forward GRU is utilized as the decoder. In addition, an attention mechanism is adopted to improve the performance.   To ensure the quality of learned representation and reduce the size of vocabulary, typically the RNN model only considers a certain number of frequent words (e.g. Therefore the RNN is not able to predict any keyphrase which contains out-of-vocabulary words. The copy mechanism is one feasible solution that enables RNN to predict unknown words based on contextual features. By incorporating the copy mechanism, the probability of predicting each new word yt would consist of two parts. This section starts with discussing how we design our evaluation experiments, followed by the description of training and testing datasets. Then, we introduce evaluation metrics and baselines. There are several publicly-available datasets for evaluating keyphrase generation. However, this amount of data is unable to train a robust recurrent neural network model. In fact, there are millions of scientific papers available online, each of which contains the keyphrases assigned by authors. Therefore, we collected a large amount of high-quality scientific metadata in Computer Science domain from various online digital libraries, including ACM Digital Library, ScienceDirect, Wiley, Web of Science, and so on. For evaluating the proposed model more comprehensively, four widely-adopted scientific publication datasets are used. We take the title and abstract as the source text. Each dataset is described in details in the below text. Since the NUS dataset did not specifically mention the ways of splitting training and testing data, the results of the supervised baseline models are obtained through a five-fold cross validation. Two encoder-decoder models are trained, one with only attention mechanism (RNN) and one with both attention and copy mechanism enabled (CopyRNN). To resolve this problem, we apply a simple heuristic by preserving only the first singleword phrase (with the highest generating probability) and removing the rest. Follow the standard definition, precision is defined as the number of correctly-predicted keyphrases over the number of all predicted keyphrases, recall is computed by the number of correctly-predicted keyphrases over the total data records. Note that, when determining the match of two keyphrases, we use Porter Stemmer for pre-processing. We conduct an empirical study on three different tasks to evaluate our model. This is the same as the keyphrase extraction task in prior studies, in which we would like to analyze how well our proposed model perform on the commonly-defined task. To make a fair comparison, we only consider the present keyphrases for evaluation in this task. The best scores are highlighted in bold and the underlines indicate the second best performances. The results show that the four unsupervised models (Tf-idf, TextTank, SingleRank and ExpandRank) perform robust across different datasets. The performances of the two supervised models (i.e., Maui and KEA) are unstable on some datasets, but Maui achieves the best performances on three datasets among all the baseline models. As for our proposed keyphrase prediction approaches, the RNN model with the attention mechanism does not perform as well as we expected. It might be because the RNN model only concerns on finding the hidden semantics behind the text, which may tend to generate keyphrases or words that are too general and may not necessarily referring to the source text. This indicates that a pure generative model may not fit the extraction task, and we need to further link back to the language usage in the source text. This result demonstrates the importance of source text for extraction task. We see that both models can generate phrases that related to the topic of information retrieval and video. However most of RNN predictions are high-level terminologies, which are too general to be selected as keyphrases. Therefore, we only provide the RNN and copyRNN performances in the discussion of the results of this task. We use the absent keyphrases in the testing datasets for evaluation. This indicates that, to some extent, both models can capture the hidden semantics behind the textual content and make reasonable predictions. In addition, with the advantage of features from the source text, the CopyRNN model also outperforms the RNN model in this condition, though not improve as much as the present keyphrase extraction task. And the CopyRNN successfully predicts another two keyphrases by capturing the detailed information from the text (highlighted text segments). The RNN and CopyRNN are supervised models, and they are trained on data in specific domain and writing style. However, with sufficient training on a large-scale dataset, we expect the models to be able to learn universal language features that are effective in other corpus as well. Thus in this task, we will test our model on another type of text, to see whether the model would work when being transferred to a different environment. As transfered to corpus in a complete strange type and domain, the model encounters more unknown words and has to rely more on the syntactic features in the text. Our experimental results demonstrate that the CopyRNN model not only performs well on predicting present keyphrases but also has the ability of generating topical relevant keyphrases that are absent in the text. In a broader sense, this model attempts to map a long text (i.e., paper abstract) with representative short text chunks (i.e., keyphrases), which can potentially be applied to improve information retrieval performance by generating high-quality index terms, as well as assisting user browsing by summarizing long documents into short readable phrases. So far we have examined our model on scientific publications and news articles, demonstrating that our model has the ability to capture universal language patterns and extract key information from unfamiliar texts. We believe that the models have a greater potential to be generalized to other domains and types, like books, online reviews etc., if it is trained on larger data corpus. Also, we directly apply our model, which is trained on publication dataset, into generating keyphrases for news articles without any adaptive training. We believe that with proper training on news data, the model would make further improvement. Additionally, this work mainly studies the problem of discovering core content from textual materials. Here, the encoder-decoder framework is applied to model language; however, such framework can also be extended to locate the core information on other data resources such as to summarize content from images and videos. In this paper, we propose an RNN-based generative model for predicting keyphrase in scientific text. To the best of our knowledge, this is the first application of the encoder-decoder model to keyphrase prediction task. Our model summarizes phrases based the deep semantic meaning of the text and it is able to handle rarely-occurred phrases by incorporating a copy mechanism. Comprehensive empirical studies demonstrate the effectiveness of our proposed model for generating both present and absent keyphrases for different types of text. Our future work may include the following two directions. In the future, we are interested in comparing the model with human annotators and evaluating the quality of predicted phrases by human judges. It would also be interesting to explore the multiple-output optimization on our model."
676,3,4.25,4.0,4.0,True,acl_2017,train,"In this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a binary code for each word, and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. Experiments show the proposed model achieves translation accuracies that approach the softmax, while reducing memory usage on the order of 1/10 to 1/1000, and also improving decoding speed on CPUs by x5 to x20.","In these models, the output layer is required to generate a specific word from an internal vector, and a large vocabulary size tends to require a large amount of computation to predict each of the candidate word probabilities. However, none of these previous methods simultaneously satisfies the following desiderata, all of which, we argue, are desirable for practical use in NMT systems: Memory efficiency: The method should not require large memory to store the parameters and calculated vectors to maintain scalability in resource-constrained environments. Time efficiency: The method should be able to train the parameters efficiently, and possible to perform decoding efficiently with choosing the candidate words from the full probability distribution. In particular, the method should be performed fast on general CPUs to suppress physical costs of computational resources for actual production systems. Compatibility with parallel computation: It should be easy for the method to be minibatched and optimized to run efficiently on GPUs, which are essential for training large NMT models. In this paper, we propose a method that satisfies all of these conditions: requires significantly less memory, fast, and is easy to implement minibatched on GPUs. While this idea is simple and intuitive, we found that it alone was not enough to achieve competitive accuracy with real NMT models. Thus we make two improvements: First, we propose a hybrid model, where the high frequency words are predicted by a standard softmax, and low frequency words are predicted by the proposed binary codes separately. In experiments on two translation tasks, we find that the proposed hybrid method with error correction is able to achieve results that are competitive with standard softmax-based models while reducing the output layer to a fraction of its original size. V represents the vocabulary size of the target language. Several previous works have proposed methods to reduce computation in the output layer. However, this method still requires O(HV) space for the parameters, and requires calculation much more complicated than the standard softmax, particularly at test time. However, this method restricts the usage of hidden layer, and the size of the matrix is still in proportion to V. However, these methods are basically not able to be applied at test time, still require heavy computation like the standard softmax. bit arrays that correspond to each word. For convenience, we introduce some constraints on b. First, a wordw is mapped to only one bit array b(w). However, this calculation may generate invalid bit arrays which do not correspond to actual words according to the mapping between words and bit arrays. The constraints described here are very general requirements for bit arrays, which still allows us to choose between a wide variety of mapping functions. However, designing the most appropriate mapping method for NMT models is not a trivial problem. Actually, when we used a binarytree based mapping function for b, our model can be interpreted as the hierarchical softmax with two strong constraints for guaranteeing independence between all bits: all nodes in the same level of the hierarchy share their parameters, and all levels of the hierarchy are predicted independently of each other. By these constraints, all bits in b can be calculated in parallel. This is particularly important because it makes the model conducive to being calculated on GPGPUs. However, the binary code prediction model also introduces problems of robustness due to these strong constraints. As the experimental results show, the simplest prediction model which directly maps words into bit arrays seriously decreases translation quality. As a result, the proposed model mostly learns characteristics for frequent words and cannot obtain enough opportunities to learn for rare words. When the softmax layer predicts OTHER, then the binary code layer is used to predict the representation of rare words. However,N can be chosen asN V because the softmax prediction is only required for a few frequent words. max, because the actual size of the output layer is still small after applying the hybrid model. As a result, these models may generate incorrect words due to even a single bit error. This problem is the result of dense mapping between words and bit arrays, and can be avoided by creating redundancy in the bit array. In this case, the actual words are obtained by estimating the nearest centroid bit array according to the Hamming distance between each centroid and the predicted bit array. d is known as the free distance determined by the design of error-correcting codes. And this study also tries to solve a generation task unlike previous studies. As shown in the experiments, we found that this approach is highly effective in these tasks. It should be noted that the method for performing error correction directly affects the quality of the whole NMT model. In this case, all bits can be estimated using exactly the same parameters, and we can not expect that we will benefit significantly from applying this redundant representation. In addition, it is desirable that the decoding method of the applied error-correcting code can directly utilize the probabilities of each bit, because q generated by the network will be a continuous probabilities between zero and one. Convolutional codes perform a set of bit-wise convolutions between original bits and weight bits (which are hyper-parameters). They are well-suited to our setting here because they distribute the information of original bits in different places in the resulting bits, work robustly for random bit errors, and can be decoded using bit probabilities directly. On the other hand, there are various algorithms to decode convolutional codes with the same format which are based on different criteria. Each test is also performed on CPUs to compare its processing time. First, we can see that each proposed method largely suppresses the actual size of the output layer from ten to one thousand times compared with the standard softmax. These are not targets to be reduced in this study because these values rarely are accessed at test time because we only need to access them for input words, and do not need them to always be in the physical memory. It might be possible to apply a similar binary representation as that of output layers to the input layers as well, then express the word embedding by multiplying this binary vector by a word embedding matrix. This is one potential avenue of future work. Taking a look at the BLEU for the simple Binary method, we can see that it is far lower than other models for all tasks. In contrast, Hybrid-N and Binary-EC models clearly improve BLEU from Binary, and they approach that of Softmax. This demonstrates that these two methods effectively improve the robustness of binary code prediction models. These results show that introducing redundancy to target bit arrays is more effective than incremental prediction. In addition, the Hybrid-NEC model achieves the highest BLEU in all proposed methods, and in particular, comparative or higher BLEU than Softmax in BTEC. ior clearly demonstrates that these two methods are orthogonal, and combining them together can be effective. We hypothesize that the lower quality of Softmax in BTEC is caused by an over-fitting due to the large number of parameters required in the softmax prediction. Proposed methods also improve actual computation time in both training and test. In addition, we can also see that applying error-correcting code is also efficient at the point of the decoding speed. In this study, we proposed neural machine translation models which indirectly predict output words via binary codes, and two model improvements: a hybrid prediction model using both softmax and binary codes, and introducing error-correcting codes to introduce robustness of binary code prediction. Experiments show that the proposed model can achieve comparative translation qualities to standard softmax prediction, while significantly suppressing the amount of parameters in the output layer, and improving calculation speeds while training and especially testing. One interesting avenue of future work is to automatically learn encodings and error correcting codes that are well-suited for the type of binary code prediction we are performing here."
226,1,4.0,3.0,4.0,True,acl_2017,train,"The paper presents a procedure of building an evaluation dataset for validation of compositional distributional semantics models estimated for languages other than English. The procedure generally builds on steps designed to assemble the SICK corpus which contains pairs of English sentences annotated for semantic relatedness and entailment, as we aim at building a comparable dataset. However, the implementation of particular building steps significantly differs from the original SICK design assumptions, which is caused by both lack of necessary extraneous resources for an investigated language and the demand for language-specific transformation rules. The designed procedure is verified on Polish, a fusional language with a relatively free word order, and contributes to building a Polish evaluation dataset. The resource consists of 10K sentence pairs which are human-annotated for semantic relatedness and entailment. The dataset may be used for evaluation of compositional distributional semantics models of Polish.","The procedure generally builds on steps designed to assemble the SICK corpus which contains pairs of English sentences annotated for semantic relatedness and entailment, as we aim at building a comparable dataset. However, the implementation of particular building steps significantly differs from the original SICK design assumptions, which is caused by both lack of necessary extraneous resources for an investigated language and the demand for language-specific transformation rules. The designed procedure is verified on Polish, a fusional language with a relatively free word order, and contributes to building a Polish evaluation dataset. The dataset may be used for evaluation of compositional distributional semantics models of Polish. Can you imagine a straightforward answer to the question How to automatically analyse semantics of a natural language and to represent the meaning of phrases (or even sentences) in this language in an accessible way? bath, you can use the context of other words that surround it. bath and tree. Based on empirical observations that distributional vectors encode certain aspects of the word meaning, it is expected that similar aspects of the meaning of phrases and sentences can also be represented with vectors obtained via composition of distributional word vectors. The idea of semantic composition is not new. Fundamental principles of compositional distributional semantics, henceforth referred to as CDS, are mainly propagated with papers written on the topic. The goal of the task was to evaluate CDS models of English in terms of semantic relatedness and entailment on proper sentences from the SICK corpus. Each sentence pair is human-annotated for relatedness in meaning and entailment. This score indicates the extent to which the meanings of two sentences are related. The entailment relation between two sentences, in turn, is labelled with entailment, contradiction, or neutral. According to the SICK guidelines, the label assigned by the majority of human annotators is selected as the valid entailment label. Studying approaches to various natural language processing (henceforth NLP) problems, we have observed that the availability of language resources (e.g. training or testing data) stimulates development of NLP tools and estimation of NLP models. Hence, we aim at building datasets for evaluation of CDS models in languages other than English, which are often under-resourced. We therefore assume that availability of test data will encourage development of CDS models in these languages. We start with a high-quality dataset for Polish, which is a completely different language than English in at least two dimensions. First, it is a rather under-resourced language in contrast to the resource-rich English. Second, it is a fusional language with a relatively free word order in contrast to the isolated English with a relatively fixed word order. Since we are not aware of accessibility of analogous resources for Polish, we have to select images first and then describe the selected images. At first we wanted to take only these images the descriptions of which were selected for the SICK corpus. However, a cursory check shows that these images are quite homogeneous, with a predominant number of dogs depictions. The chosen images are given to two authors who independently of each other formulate their descriptions based on a short instruction. The authors are instructed to write one single sentence (with a sentence predicate) describing the action on a displayed image. They should not describe an imaginable context or an interpretation of what may lie behind the scene on the picture. If some details on the picture are not obvious, they should not be described too. Furthermore, the authors should avoid multiword expressions, such as idioms, metaphors, and named entities, as those are not compositional linguistic phenomena. Finally, descriptions should contain Polish diacritics and proper punctuation. The expansion step, in turn, is implemented and the sentences provided by the authors are lexically and syntactically transformed in order to obtain derivative sentences with similar, contrastive, or neutral meanings. Rowerzysta odpoczywa i obserwuje morze. Eng. Eng. Eng. Eng. Eng. Eng. Eng. Eng. Eng. Eng. Eng. Eng. Eng. Kobieta przytula trzymanego na smyczy psa. Eng. Eng. Eng. Eng. Eng. Eng. Eng. The first five transformations are designed to produce sentences with a similar meaning, the sixth transformation outputs sentences with a contradictory meaning, and the seventh transformation should generate sentences with a neutral (or unrelated) meaning. Some of the transformations are very productive (e.g. mixing dependents). Other, in turn, are sparsely represented in the output (e.g. dropping conjunction). The final step of building the SICK corpus consisted in arranging normalised and expanded sentences into pairs. As our data diverges from SICK data, the process of arranging Polish sentences into pairs also differs from pairing in the SICK corpus. For each sentence pair (a,b) created according to this procedure, its reverse (b,a) is also included in our corpus. Eng. Eng. Eng. Eng. Eng. Eng. Eng. Eng. Eng. Eng. Since semantic similarity is only a special case of semantic relatedness, semantic relatedness is thus a more general term than the other one. Polish entailment labels correspond directly to the SICK labels (i.e. entailment, contradiction, neutral). The entailment label assigned by the majority of human judges is selected as the gold label. Since the transformation process is fully automatic and to a certain extent based on imperfect dependency parsing, we cannot ignore errors in the transformed sentences. Five of them have PhD in linguistics, five are PhD students, one is a graduate, and one is an undergraduate. The leader judges should correct incomprehensible and ungrammatical sentences with a minimal number of necessary changes. Unusual sentences which are generally accepted by Polish speakers should not be modified. Moreover, the modified sentence may not be identical with the other sentence in the pair. During the annotation process it came out that sentences accepted by some human annotators are unacceptable for other annotators. These errors are fixed directly in the corpus, as they should not impact annotations of sentence pairs. Annotations of pairs with modified sentences are resent to the annotators so that they can verify and update them. The X label corresponds to pairs for which a majority label could not be established. However, this coefficient is designed to measure agreement between two annotators only. First, there are more then two raters. Second, entailment labels are categorial. Relative to semantic relatedness, the distinction in meaning of two sentences made by human judges is often very subtle. We also test interval measurement, in which the distance between the attributes does have meaning and an average of an interval variable is computed. Hence, we conclude that our dataset is a reliable resource for purpose of evaluating compositional distributional semantics model of Polish. The goal of this paper is to present the procedure of building the Polish evaluation dataset for validation of compositional distributional semantics models. As we aim at building an evaluation dataset which is comparable to the SICK corpus, the general assumptions of our procedure correspond to the design principles of the SICK corpus. However, the procedure of building the SICK corpus cannot be adapted without modifications. Second, as the process of transforming sentences seems to be language-specific, the linguistic transformation rules appropriate for Polish have to be defined from scratch. Third, the process of arranging Polish sentences into pairs is defined anew taking into account the data characteristic and bidirectional entailment annotations. The discrepancies relative to the SICK procedure also concern the annotation process itself. Since an entailment relation between two sentences must not be symmetric, each sentence pair is annotated for entailment in both directions. Furthermore, we introduce an element of human verification of correctness of automatically transformed sentences and some additional post-corrections. The presented procedure results in building the Polish test corpus of relatively high quality. The evaluation dataset will be made publicly available upon publication of this paper."
524,3,4.33,3.67,2.67,False,acl_2017,train,"This paper explores several techniques for enhancing coverage when parsing with HPSG grammars, determines appropriate evaluation methods, and uses them to compare performance. Depending on the dataset, baseline coverage gaps can be reduced by between 75% and 100%, while simultaneously improving EDM F1 scores.","While this is not the place to expound at length on the motivation for a precision grammar, it is perhaps instructive to mention two important benefits: first, by excluding analyses of strings that the grammarian judges to be outside of the language in question, the problem of spurious ambiguity is typically greatly reduced for strings that are grammatical. Second, precision is a prerequisite to bidirectionality, enabling both parsing and generation. A system that cannot differentiate between strings that are in a language and those that are not cannot be used in this way. The relatively low priority assigned to coverage in many HPSG-based computational linguistics projects is a natural corollary of the premium placed on precision, together with the Zipfian distribution of language. However, given that precision grammars are typically hand-crafted resources embodying substantial amounts of human effort, restricted coverage can at times be quite frustrating. Rely upon evidence from a second analysis component, resulting in a non-homogeneous data flow Both options are viable in certain circumstances; however, neither option is very appealing. Having to depend on two different types of signal makes the design of downstream components more complicated, and depending on the rate of coverage gaps, ignoring unparseable inputs entirely could be disastrous. A better solution is needed. This paper examines the issue of incomplete coverage in HPSG grammars, and explores several mitigation techniques, collectively dubbed robust parsing. A secondary contribution of this paper involves methodology for the evaluation of such robust parsing techniques, which is somewhat nonobvious due to the fact that gold standard evaluation data for the English Resource Grammar (henceforth ERG) is only readily available for incoverage inputs. The desire for increased coverage is at least as old as the field of computational linguistics. In the context of parsing with PCFGs, a time-honored solution is to assume that any two nonterminals can combine to form any other nonterminal (e.g. This technique is referred to as smoothing. The result of smoothing is that any input string can be assigned a sentential analysis. A major drawback is the resulting explosion of ambiguity and processing cost: any binary tree with labels drawn from the nonterminals is a legal analysis for every input of the corresponding number of words. However, if the extra probability mass added when smoothing the grammar is distributed judiciously, and if suitable efficiency methods (such as pruning) are employed, it is often still possible to arrive at useful analyses in reasonable amounts of time. The idea behind smoothing can, in principle, be applied to parsing with HPSG as well. Instead of having an enumerated set of nonterminal symbols, these grammars use feature structures to represent categories. The translation is roughly that any pair of adjacent sub-analyses with feature structures X and Y can license a larger analysis with an arbitrary feature structure Z. There are two major flies in this ointment. of possible feature structures may be enumerable, it is likely to be very large, and in other grammars it may not even be finite, since recursive structures are possible. Unfortunately, Fouvry does not give an empirical evaluation of his work. A number of considerably less general but arguably more practical approaches have also been deployed. A part-of-speech tagger with its own robustness mechanisms (including access to a larger lexicon, a smoothed HMM, and suffix analysis) provides guesses about the coarse-grained part of speech of each word. With suitable choice of parameters, such a PCFG can be quite robust. Unfortunately, the PCFG-produced analyses are frequently not fully coherent with respect to the constraints stipulated in the original HPSG grammar, preventing their use for extracting semantic information. The next section will give more details about the components used, techniques evaluated, and experiments performed in this comparison of robust parsing methods. It seems clear that the underlying goal of robustness techniques like the ones explored herein is to provide a larger quantity of useful analyses to a downstream process than could previously be produced. an extrinsic evaluation. To do a good job of extrinsic evaluation, however, two things are required: first, a large enough annotated dataset to acquire meaningful statistics for a particular task, and second, a sufficiently varied collection of tasks to exercise all interesting aspects of the analyses. Unfortunately such work is beyond the scope of this paper. A lesser attempt at extrinsic evaluation seems tempting, but prone to errors. Therefore, in this work we content ourselves with intrinsic evaluations of robust parsing techniques. Intrinsic evaluation is not without its own difficulties. There are two ways to solve this problem. The second approach is to perform robust parsing with respect to a slightly out-of-date version of the ERG, and evaluate the results only against inputs that were out of scope in that version but are in scope in a more recent grammar version. This approach is much cheaper in terms of manual labor. In prep). Information-structure predicates (bearing the suffix d rel) are ignored, since these were relocated in trunk. Only the lemma field within predicate names is compared, as many subsense fields were changed. Some slippage is allowed in aligning character positions recorded within each MRS. These techniques are described below. In this sense, it is possible to consider the ERG itself as a baseline contender for robust parsing: by assigning more generous resource limits, some previously unparseable inputs receive analyses. The quality of these analyses (as measured for instance by EDM precision) can in general be expected to be in line with the quality of other native ERG analyses, but their quantity (as measured by EDM recall) will be limited. The goal of the method is to allow the construction of a spanning analysis from analyses of shorter spans within an input that normally would not be able to combine with each other. parsing, since all of the results of the unary rule for a given span are combinatorially equivalent (i.e. The wealth of robust analyses spawned by the bridging mechanism gives rise to an important question: which one is the most useful? This problem of ambiguity is of course isomorphic to the problem of ambiguity experienced during ordinary parsing, and is amenable to the same solution, namely a statistical ranking model trained on a manually annotated treebank. As a result, the statistical model we use to select a parse contains no preferences regarding the bridging rules. With future research, it may be possible to significantly improve the performance of this disambiguation task. Formally, bridging ensures that any input is analyzable. However, even with aggressive packing techniques, the sheer ambiguity presented by the bridging rules makes the unpacking problem quite challenging. One clear disadvantage of the bridging approach is that once a bridged analysis has been invoked for a portion of an input sentence, any larger portion of the sentence containing that bridged element must also be bridged (to keep the approach at all tractable), and hence fails to produce informative semantics for the larger portions. For example, the standard ERG will not succeed in analyzing the sentence They might try to buy a cheap car, I suppose, or an old truck because of the phrase I suppose appearing in the middle of a coordination of two NPs. But a rule that consumed I suppose along with the preceding NP a cheap car to produce an NP covering both phrases would then enable a usable analysis of the full sentence. The resulting grammar should be considerably more robust than the regular ERG, and possibly more successful in preserving meaning content than with bridging rules, but it cannot aspire to full coverage, since it crucially depends on finding a host nominal or verbal phrase next to the site of any parsing obstacle. We could expand the inventory of host phrase types to close this remaining coverage gap, but experiments to date indicate that increased ambiguity can quickly make parsing with a larger set of Pacman rules intractable. In prep). The idea of combining these techniques belongs to the authors of those papers, and the details of how the systems work are given there as well. A summary will suffice to give readers the basic idea. First, a collection of derivation trees consistent with the underlying precision grammar is procured for use as training data. These trees can come either from gold standard treebanks or from parsing. The internal nodes of the trees are labeled with the rule name licensing the corresponding constituent (for example, the head specifier rule). Leaves correspond to lexemes, and are labeled with the name of the lexical type to which the lexeme belongs. In addition, some modifications are made to the shape of the tree: most lexical rules are collapsed onto the yield of the tree, and punctuation is split into binary structures. Finally, the trees can op-tionally be decorated with grandparent information to a configurable depth. A PCFG is induced by maximum likelihood estimation from the decorated trees, with no smoothing, lexicalization, hierarchical splitting, or other enhancements (although such techniques would likely be fruitful avenues for future experiments). The resulting PCFG is used in conjunction with a straightforward CKY parser to analyze arbitrary inputs. Depending on the level of grandparenting used and the size of the training set, the speed and coverage of this system can be high or low. For the purposes of this paper, two configurations are used: csaw-tb, which is an ungrandparented PCFG trained exclusively on hand-annotated gold-standard trees, and csaw-ww, which is a doubly-grandparented PCFG trained exclusively on automatically-disambiguated Wikipedia sentences. In some cases, the sequence of rules and lexemes stipulated by these pseudo-derivations are in fact compatible with the unification constraints that define those rules and lexemes. In prep) had the key insight that the unification constraints that these pseudoderivations violate are usually in parts of the feature structure geometry that are isolated from the portion used for semantic composition. This feels a bit like throwing the baby out with the bath water: the precision grammar may have a great deal of useful expertise to contribute to the analysis of large portions of the sentence, even if it is overly prescriptive or incomplete in its treatment of some particular detail. The hybrid-tb and hybrid-ww techniques attempt to exploit the best of both worlds. This is achieved by first allowing the corresponding csaw-based system to propose a single complete analysis, and then allowing the rules of the precision grammar to find additional ways to use the pieces of that robust analysis. Specifically, the constituents postulated by csaw are converted into chart edges and allowed to participate in chart parsing along with the rest of the edges licensed by the grammar. This allows a robust analysis of some particularly troublesome construction to combine with all other possible analyses of the rest of the sentence (including portions nested within the robust analysis). The best analysis is subsequently chosen by the statistical parse ranking model used when performing ordinary parsing with the precision grammar. The robust semantic readout process can also fail in the empirically very rare case that robust unification produces a cyclic structure in an infelicitous location, such as the list of semantic predications. The latter occurred just once for us. The reason that coverage for the bridging and pacnv systems is relatively low is that these techniques introduce very challenging ambiguity management problems for the parser, and as a result the parser frequently exhausts the resource allocation (which was the same as used for the baseline method). The PCFG-based methods all showed higher coverage than the ERG, across all datasets. The hybrid-ww method achieved the highest coverage on all datasets except semcor, where it was edged out by csaw-ww and hybrid-tb. While the speeds may seem slow and the scores may seem low, it is important to bear in mind that these datasets are selected specifically to be hard to parse. As the parsing was performed on a cluster where CPU speed and load varied somewhat from experiment to experiment, the speed measurements are only comparable on a broad scale. Nonetheless, we clearly see that csaw-tb is by far the fastest system: its grammar is simpler, and the expense of unification-based parsing is entirely avoided. The slowest systems are csaw-ww and hybrid-ww; the former because the corresponding PCFG is very large, and the latter because it includes the former as a component, and has to perform unification-based parsing as well. These patterns held across all four datasets. On these datasets, hybrid-ww produced the highest scores, while bridging, pacnv, and csaw-tb earned the lowest marks, edged out even by baseline. this dataset than it did on the others. Second, it has by far the lowest proportion of sentences that the ERG can analyze unassisted. From that perspective, it is the best indicator of performance on inputs where robust techniques are needed the most. However, it is also the smallest of the four datasets, so results may not be as reliable. One clear result is that the PCFG-based systems are capable of greatly enhancing the coverage of the ERG, while producing analyses whose quality is better than the other robust systems evaluated. While the speed of the best-performing csaw-ww and hybrid-ww methods is somewhat uninspiring, we console ourselves by noting that the method is at least tractable, and the performance is probably acceptable for some applications where coverage and accuracy are at a premium while time is not. In the present work, the parse ranking model is not adapted to the novel situations in which it is used, and hence likely does not perform as well as it could. New techniques are needed to train the ranking model to make informed decisions about these situations. Although coverage would be unaffected, accuracy could potentially be improved. Also, evaluating the difference in performance on data strictly outside of the coverage of the underlying precision grammar compared to data in scope would be profitable, as would exploring techniques for managing the decision about when to deploy robustness. While it remains to be seen whether the promising coverage gains and accuracy figures reported here can be borne out in extrinsic evaluation, we believe there is reason for optimism, as downstream applications may be able to mitigate coverage issues using these techniques."
318,3,3.75,3.75,3.75,True,acl_2017,train,"Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed by several sememes. Since sememes are not explicit for each word, people manually annotate word sememes and form linguistic common-sense knowledge bases. In this paper, we present that, word sememe information can improve word representation learning (WRL), which maps words into a low-dimensional semantic space and serves as a fundamental step for many NLP tasks. The key idea is to utilize word sememes to accurately capture exact meanings of a word within specific contexts. More specifically, we follow the framework of Skip-gram and present three sememe-encoded models to learn representations of sememes, senses and words, where we apply the attention scheme to detect word senses in various contexts. We conduct experiments on two tasks including word similarity and word analogy, and our models significantly outperform baselines. The results indicate that WRL can benefit from sememes via the attention scheme, and also confirm our models being capable of properly modeling sememe information.","word sense). However, sememes are not explicit for each word. Hence, people manually annotate word sememes and build linguistic common-sense knowledge bases. In this paper, we aim to incorporate word sememes into word representation learning (WRL) and learn improved word embeddings in a lowdimensional semantic space. From these previous studies we conclude that, word sense disambiguation are critical for WRL, and we believe that the sememe annotation of word senses in HowNet can provide essential semantic regularization for the both tasks. To explore its feasibility, we propose a novel Sememe-Encoded Word Representation Learning (SE-WRL) model, which detects word senses and learns representations simultaneously. In this framework, an attention-based method is proposed to automatically select appropriate word senses according to contexts. To take full advantages of sememes, we propose three different learning and attention strategies for SE-WRL. In experiments, we evaluate our framework on two tasks including word similarity and word analogy, and further conduct case studies on sememe, sense and word representations. The evaluation results show that our models outperform other baselines significantly, especially on word analogy. This indicates that our models can automatically detect appropriate word senses according to contexts, and both word sense disambiguation and representation learning can benefit from the sememe annotation in HowNet. Recent years have witnessed the great thrive in word representation learning. It is simple and straightforward to represent words using one-hot representations, but it usually struggles with the data sparsity issue and the neglect of semantic relations between words. Word distributed representations are capable of encoding semantic meanings in vector space, serving as the fundamental and essential inputs of many NLP tasks. There are large amounts of efforts devoted to learning better word representations. As the exponential growth of text corpora, model efficiency becomes an important issue. These models assume that the meanings of words can be well reflected by their contexts, and learn word representations by maximizing the predictive probabilities between words and their contexts. However, these models merely arrange only one vector for each word, regardless of the fact that many words have multiple senses. This paper, for the first time, jointly learns representations of sememes, senses and words. The sememe annotation in HowNet provides useful semantic regularization for WRL. Moreover, the unified representations incorporated with sememes also provide us more explicit explanations of both word and sense embeddings. Word sense disambiguation (WSD) aims to computationally identify word senses or meanings in a certain context. There are mainly two approaches for WSD, namely the supervised methods and the knowledge-based methods. In this paper, we follow the knowledge-based approach and automatically detect word senses according to the contexts with the favor of sememe information in HowNet. To the best of our knowledge, this is the first attempt to apply attentionbased models to encode sememe information for word representation learning. In this section we present our framework SememeEncoded WRL (SE-WRL) that considers sememe information for word sense disambiguation and representation learning. Specifically, we learn our models on a large-scale text corpus with the semantic regularization of the sememe annotation in HowNet, and obtain sememe, sense and word embeddings for evaluation tasks. In the following sections, we first introduce HowNet and the structures of sememes, senses and words. Then we discuss the conventional WRL model Skip-gram that we utilize for the the sememe-encoded framework. Finally, we propose three sememe-encoded models in details. In this section, we first introduce the arrangement of sememes, senses and words in HowNet. HowNet annotates precise senses to each word, and for each sense HowNet annotates the significance of parts and attributes represented by sememes. Fig. The third and following layers are those sememes explaining each sense. For instance, the first sense Apple brand indicates a computer brand, and thus has sememes computer, bring and SpeBrand. From Fig. In this paper, for simplicity we only consider all annotated sememes of each sense as a sememe set without considering their internal structure. HowNet assumes the limited annotated sememes can well represent senses and words in real-world scenario, and thus sememes are expected to be useful for both WSD and WRL. We introduce the notions utilized in the following sections as follows. We define the overall sememe, sense and word sets used in training as X, S and W respectively. For each target word w in a sequential plain text, C(w) represents its context word set. The standard skip-gram model assumes that word embeddings should relate to their context words. In this section, we introduce the SE-WRL models with three different strategies to utilize sememe information, including Simple Sememe Aggregation Model (SSA), Sememe Attention over Context Model (SAC) and Sememe Attention over Target Model (SAT). The Simple Sememe Aggregation Model (SSA) is a straightforward idea based on Skip-gram model. For each word, SSA considers all sememes in all senses of the word together, and represents the target word using the average of all its sememe embeddings. As compared to the conventional Skip-gram model, since sememes are shared by multiple words, this model can utilize sememe information to encode latent semantic correlations between words. In this case, similar words that share the same sememes may finally obtain similar representations. The SSA Model replaces the target word embedding with the aggregated sememe embeddings to encode sememe information into word representation learning. However, each word in SSA model still has only one single representation in different contexts, which cannot deal with polysemy of most words. It is intuitive that we should construct distinct embeddings for a target word according to specific contexts, with the favor of word sense annotation in HowNet. To address this issue, we come up with the Sememe Attention over Context Model (SAC). SAC utilizes the attention scheme to automatically select appropriate senses for context words according to the target word. That is, SAC conducts word sense disambiguation for context words to learn better representations of target words. The structure of the SAC model is shown in Fig. More specifically, we utilize the original word embedding for target word w, but use sememe embeddings to represent context word wc instead of original context word embeddings. Suppose a word typically demonstrates some specific senses in one sentence. Here we employ the target word embedding as an attention to select the most appropriate senses to make up context word embeddings. With the favor of attention scheme, we can represent each context word as a certain distribution over its sense. This can be regarded as soft WSD. As shown in experiments, it will help learn better word representations. The Sememe Attention over Context Model can flexibly select appropriate senses and sememes for context words according to the target word. The process can be also applied to select appropriate senses for the target word, by taking context words as attention. Hence, we propose the Sememe Attention over Target Model (SAT) as shown in Fig. Different from SAC model, SAT learns the original word embeddings for context words, but sememe embeddings for target words. Hence SAT is expected to conduct more reliable WSD and result in more accurate word representations, which will be explored in experiments. In this section, we evaluate the effectiveness of our SE-WRL models on two tasks including word similarity and word analogy, which are two classical evaluation tasks mainly focusing on evaluating the quality of learned word representations. We also explore the potential of our models in word sense disambiguation with case study, showing the power of our attention-based models. This indicates the significance of WSD. The two datasets both contain frequently-used Chinese word pairs with similarity scores annotated manually. We evaluate three SE-WRL models including SSA, SAC and SAT on all tasks. As for baselines, we consider three conventional WRL models including Skip-gram, CBOW and GloVe. It represents the current word embeddings with only the most probable sense according to the contexts, instead of viewing a word as a certain distribution over all its senses similar to that of SAT. For a fair comparison, we train these models with the same experimental settings and with their best parameters. WRL models typically compute word similarities according to their distances in the semantic space. In experiments, we choose the cosine similarity between two word embeddings to rank word pairs. For evaluation, we compute the Spearman correlation between the ranks of models and the ranks of human judgements. This indicates that, by utilizing sememe annotation properly, our model can better capture the semantic relations of words, and learn more accurate word embeddings. In general, SSA model performs slightly better than baselines, which tentatively proves that sememe information is helpful. The reason is that words which share common sememe embeddings will benefit from each other. Especially, those words with lower frequency, which cannot be learned sufficiently using conventional WRL models, in contrast can obtain better word embeddings from SSA simply because their sememe embeddings can be trained sufficiently through other words. This indicates that SAT can obtain more precise sense distribution of a word. SAC using only one target word as attention for WSD, SAT adopts richer contextual information as attention for WSD. The result makes sense because for many words, their various senses are not always completely different from each other, but share some common elements. In some contexts, a single sense may not convey the exact meaning of this word. The percentage of positive samples is regarded as the accuracy score for this WRL model. We use the mean rank of all gold standard words as the evaluation metric. This indicates that SAT will enhance the modeling of implicit relations between word embeddings in the semantic space. The reason is that sememes annotated to word senses have encoded these word relations. With these sememe embeddings, these low-frequent words can be learned more efficiently by SAT. Whereas for the mean rank, CBOW gets the worst results, which indicates the performance of CBOW is unstable. On the contrary, although the accuracy of SAT is a bit lower than that of CBOW, SAT seldom gives outrageous prediction. These similar sememes make the attention process less discriminative with each other. With the help of sememes, these low-frequency words arise no issue for SAT. The above experiments verify the effectiveness of our models for WRL. Here we show some examples of sememes, senses and words for case study. In this table, the first rows of three examples are word-sense-sememe structures of each word. For instance, in the third example, the word has two senses, contingent and troops; contingent has one sememe community, while troops has one sememe army. The three examples all indicate that our models can estimate appropriate distributions of senses for a word given a context. We demonstrate the effect of context words for attention in Table. From these examples, we can conclude that our Sememe Attention can accurately capture the word meanings in complicated contexts. In this paper, we propose a novel method to model sememe information for learning better word representations. Specifically, we utilize sememe information to represent various senses of each word, and propose Sememe Attention to automatically select appropriate senses in contexts. We evaluate our models on word similarity and word analogy, and results show the advantages of our Sememe-Encoded WRL models. We also analyze several cases in WSD and WRL, which confirms our models are capable of selecting appropriate word senses with the favor of sememe attention. We will explore to utilize these annotations for better WRL. We will explore the effectiveness of sememe information for WRL in other languages."
477,2,4.14,3.57,3.57,True,acl_2017,train,"Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled. Our results largely confirm previous findings that character representations are effective across many languages, though we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most other settings. However, we also find room for improvement: character models do not match the predictive accuracy of a model with access to explicit morphological analyses.","However, directly mapping a finite set of word types to a continuous representation has well-known limitations. First, it makes a closed vocabulary assumption, enabling only generic out-of-vocabulary handling. Second, it cannot exploit systematic functional relationships in learning. For example, cat and cats stand in the same relationship as dog and dogs. While this re-lationship might be discovered for these specific frequent words, it does not help us learn that the same relationship also holds for the much rarer words tarsier and tarsiers. These functional relationships reflect the fact that words are composed from smaller units of meaning, or morphemes. For instance, cats consists of two morphemes, cat and-s, with the latter shared by the words dogs and tarsiers. Modeling this effect is crucial for languages with rich morphology, where vocabulary sizes are larger, many more words are rare, and many more such functional relationships exist. However, a downside of these models is that they introduce a dependence on morphological segmentation or analysis. But morphemes typically have similar orthographic representations across words. These models can also represent rare and unknown words, and they produce compact parameterizations. They have the added appeal that they do not depend on morphological analysis, and they raise a provocative question: does NLP benefit from explicit modeling of morphology, or can this be replaced entirely by modeling of characters? How do representations based on morphemes compare with those based on characters? What is the best way to compose subword representations? Do character representations adequately substitute for morphological analysis? How do different representations interact with languages of different morphological typologies? Most models implicitly assume concatenative morphology, but typology of many widely-spoken languages is primarily non-concatenative, and it is unclear such models will behave on these languages. To answer these questions, we performed a systematic comparison across different models for the simple and ubiquitous task of language modeling. To understand the extent to which character-level models capture true morphological regularities, we present oracle experiments using human morphological annotations instead of automatic morphological segments. For most languages, character-level representations outperform the standard word representations. Most interestingly, a previously unstudied combination of character trigrams composed with bi-LSTMs performs best on the majority of languages. Bi-LSTMs and CNNs are more effective composition functions than addition. Character representations learn functional relationships between orthographically similar words, but are not as accurate as models with explicit knowledge of morphology. Character-level models are effective across a range of morphological typologies, but orthography influences their effectiveness. A morpheme is the smallest unit of meaning in a word. Some morphemes express core meaning (roots), while others express one or more depen-dent features of the core meaning, such as person, gender, or aspect. A morphological analysis identifies the lemma and features of a word. Morphological typology classifies languages based on the processes by which morphemes are composed to form words. While most languages will exhibit a variety of such processes, for any given language, some processes are much more frequent than others, and we will broadly identify our experimental languages with these processes. When morphemes are combined sequentially, the morphology is concatenative. However, morphemes can also be composed by nonconcatenative processes. We consider four broad categories of both concatenative and nonconcatenative processes in our experiments. Fusional languages realize multiple features in a single concatenated morpheme. Morphemes are concatenated to form a word and the morpheme boundaries are clear. BPE works by iteratively replacing frequent pairs of bytes with a single unused byte. We use three variants of f in Eq.     The CNN model applies filters of varying width, representing features of character n-grams. We then calculate the maxover-time of each feature map.   Highway layers allow some dimensions of wt to be carried or transformed. Since it can learn character n-grams directly, we only use the CNN with character input. We use language models (LM) because they are simple and fundamental to many NLP applications.       Using Eq. Note that this design means that we can predict only words from a fixed output vocabulary, so our models differ only in their representation of context words. This makes design it possible to compare language models using perplexity on a fixed output space, though open vocabulary word prediction is an interesting direction for future work. Preprocessing involves lowercasing (except for character models) and removing hyperlinks. Even following detailed discussion with Ling (p.c. We suspect that different preprocessing and the stochastic learning explains differences in perplexities. Japanese mixes Kanji, Katakana, Hiragana, and Latin characters (for foreign words). Hence, a Japanese character can correspond to a character, syllable, or word. The preprocessed dataset is already word-segmented. To evaluate the models, we compute perplexity on the test data. In six of ten languages, character-trigram representations composed with bi-LSTMs achieve the lowest perplexities. We can see that the performance of character, character trigrams, and BPE are very competitive. Composition by bi-LSTMs or CNN is more effective than addition, except for Turkish. We also observe that BPE always outperforms Morfessor, even for the agglutinative languages. We now turn to a more detailed analysis by morphological typology. Fusional languages. Agglutinative languages. We observe different results for each language. For Finnish, character trigrams composed with bi-LSTMs achieves the best perplexity. Surprisingly, for Turkish character trigrams composed via addition is best and addition also performs quite well for other representations, potentially useful since the addition function is simpler and faster than bi-LSTMs. suspect that this is due to the fact that Turkish morphemes are reasonably short, hence wellapproximated by character trigrams. For Japanese, we observe that the improvements from character representation more modest than in other language. Root and Pattern. For these languages, character trigrams composed with bi-LSTMs also achieve the best perplexity. We had wondered with CNNs would be more effective for root-andpatter morphology, but since these data are unvocalized, it is more likely that non-concatenative effects are minimized, though we do still find morphological variants with consonantal inflections that behave more like concatenation. For example, maktab (root:ktb) is written as mktb. We suspect this makes character trigrams quite effective since they match the tri-consonantal root patterns among words which share the same root. Reduplication. For Indonesian, BPE morphs composed with bi-LSTMs model obtain the best perplexity. For Malay, the character CNN outperforms other models. However, these improvements are small compared to other languages. This likely reflects that Indonesian and Malay are only moderately inflected, where inflection involves both concatenative and non-concatenative processes. level models were adequate to model the effects of morphology, then they would have similar predictive accuracy. To answer this question, we used the human-annotated morphological analyses provided in the UD datasets for Czech and Russian, the only languages in our set for which these analyses were available. In these experiments we treat the lemma and each morphological feature as a subword unit. These results demonstrate that neither character representations nor unsupervised segmentation is a perfect replacement for manual morphological analysis, at least in terms of predictive accuracy. Especially in light of character-level results, they also imply that current methods of unsupervised morphological analysis are inadequate substitutes for morphological analysis. The oracle experiments show promising results if we have annotated data. But these annotations are expensive, so investigated the effects of automatic morphological analysis. But these processes do not apply to all words, so it may be that the effects of specific morphological processes are washed out. In other words, we analyze the perplexities when the inflected words of interest are in the most recent history, exploiting the recency bias of our LSTM-LM. This is the perplexity most likely to be strongly affected by different representations, since we do not vary representations of the predicted word itself. We look at several cases: nouns and verbs in Czech and Russian, where word classes can be identified from annotations, and reduplication in Indonesian, which we can identify mostly automatically. For each analysis, we also distinguish between frequent cases, where the inflected word occurs more than ten times in the training data, and rare cases, where it occurs fewer than ten times. We compare only bi-LSTM models. For Czech and Russian, we again use the UD annotation to identify words of interest. We also observe that the subword models are more effective for rare verbs. We use the presence of word tokens containing hyphens to estimate the percentage of those exhibiting reduplication. contrast with the overall results, BPE bi-LSTMs model produce the worse perplexities, while character bi-LSTMs produce the best, suggesting that these models are more effective for reduplication. Character and morph representations tend to find words that are orthographically similar, suggesting that they better at modeling dependent than root morphemes. The same pattern holds for rare and OOV words. We suspect that the subword models outperform words on language modeling because they exploit affixes to signal word class. We also noticed similar patterns in Japanese. We analyze reduplication by querying reduplicated words to find their nearest neighbors using the BPE bi-LSTM model. If the model were sensitive to reduplication, we would expect to see the morphological variants word in the top-n nearest neighbors. With the partially reduplicated query berlembahlembah, we do not find the lemma lembah. Our results confirm previous findings that character-level models are effective for many languages, but these models do not match the predictive accuracy of model with explicit knowledge of morphology, and our qualitative analysis suggests that they learn orthographic similarity of affixes, and lose the meaning of root morphemes. Although morphological analyses are available in limited quantities, our results suggest that there might be utility in semi-supervised learning from partially annotated data. Across languages with different typologies, our experiments show that the subword unit models are most effective on agglutinative languages. However, these results do not generalize to all languages, since factors such as morphology and orthography affect the utility of these representations. We plan to explore these effects in future work."
134,2,4.0,3.5,4.0,True,acl_2017,train,"We investigate neural techniques for endto-end computational argumentation mining. We frame the problem as a tokenbased dependency parsing as well as a token-based sequence tagging model, including a multi-task learning setup. Contrary to models that operate on the argument component level, we find that framing the problem as dependency parsing leads to subpar performance results. In contrast, less complex (local) tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch long-range dependencies inherent to the argumentation mining problem. Moreover, we find that jointly learning ‘natural’ subtasks, in a multi-task learning setup, improves performance.","Thus, AM would have to detect claims and premises (reasons) in texts such as the following, where premise P supports claim C: Since it killed many marine livesP,::::::: tourism::: has:::::::::: threatened:::::: natureC. Argument structures in real texts are typically much more complex, cf. While different research has addressed different subsets of the AM problem (see below), the ultimate goal is to solve all of them, starting from unannotated plain text. Both solve the end-to-end task by first training independent models for each subtask and then defining an integer linear programming (ILP) model that encodes global constraints such as that each premise has a parent, etc. Besides their pipeline architecture the approaches also have in common that they heavily rely on hand-crafted features. Accordingly, datasets typically differ with respect to their annotation of (often rather complex) argument structure. Thus, feature sets would have to be manually adapted to and designed for each new sample of data, a challenging task. The same critique applies to the designing of ILP constraints. Moreover, from a machine learning perspective, pipeline approaches are problematic because they solve subtasks independently and thus lead to error propagation rather than exploiting interrelationships between variables. In contrast to this, we investigate neural techniques for end-to-end learning in computational AM, which do not require the hand-crafting of features or constraints. We investigate several approaches. First, we frame the end-to-end AM problem as a dependency parsing problem. Second, we frame the problem as a sequence tagging problem. The challenge in the end-to-end setting is to also include relations into the tagging scheme, which we realize by coding the distances between linked components into the tag label. As such, this model makes fewer assumptions than our dependency parsing and tagging approaches. The contributions of this paper are as follows. Relatively few works address the full AM problem of component and relation identification. First, major claims relate to no other components. Second, claims always relate to all other major claims. The essay is comprised of nonargumentative units (square blue) and argumentative units of different types: Premises (P), claims (C) and major claims (MC). Top: Relationsships between argumentative units. Black arrows are support (for), red dashed arrows are attack (against). to exactly one claim or premise. Since there may be several major claims, each claim connects to multiple targets, which violates the tree structure. There is another peculiarity of this data. The argumentation structure is completely contained within a paragraph, except, possibly, for the relation from claims to major claims. Thus, prediction on the paragraph level is easier than prediction on the essay level, because there are fewer components in a paragraph and hence fewer possibilities of source and target components in argument relations. Paragraphs are not only easier for relation classification, but also for component classification: a paragraph can never contain premises only, for example, since premises link to other components. This section describes our neural network framings for end-to-end AM. Sequence Tagging is the problem of assigning each element in a stream of input tokens a label. In addition to considering a left-to-right flow of information, bidirectional LSTMs also capture information to the right of the current input token. The most recent generation of neural tagging models add label dependencies to BiLSTMs (BL), so that successive output decisions are not made independently. The character-level CNN may address problems of out-of-vocabulary words, that is, words not seen during training. AM as Sequence Tagging: We frame AM as the following sequence tagging problem.     We encode the same d value for each token in a given component. Transitionbased parsers encode the parsing problem as a sequence of configurations which may be modified by application of actions such as shift, reduce, etc. tions. The system terminates after a finite number of actions, and the parse tree is read off the terminal configuration. Graph-based parsers solve a structured prediction problem in which the goal is learning a scoring function over dependency trees such that correct trees are scored above all others. Most neural parsers have not entirely abandoned feature engineering. Actions are chosen based on the stack LSTM representations of the stacks, and no more feature engineering is necessary. Moreover, their parser has thus access to any part of the input, its history and stack contents. AM as Dependency Parsing: To frame a problem as a dependency parsing problem, each instance of the problem must be encoded as a directed tree, where tokens have heads, which in turn are labeled. Links and selected labels. It has been argued that such learning scenarios are closer to human learning because humans often transfer knowledge between several domains. This forces the network to learn generalized representations. In particular, the hidden states encoded in a specific layer are fed into a multiclass classifier fk. AM as MTL: We use the same framework STagT for modeling AM as MTL. Their entity dection system is a BLC tagger and their relation detection system is a neural net that predicts a relation for each pair of detected entities. This relation module is a TreeLSTM model that makes use of dependency tree information. To adapt LSTM-ER for the argumentative structure encoded in the PE dataset, we model three types of entities (premise, claim, major claim) and four types of relations (for, against, support, attack). Afterwards, it defines an ILP model with various constraints to enforce valid argumentation structure. As features it uses structural, lexical, syntactic and context features, cf. Summarizing, we distinguish our framings in terms of modularity and in terms of their constraints. Modularity: Our dependency parsing framing and LSTM-ER are more modular than STagT because they de-couple relation information from entity information. However, part of this modularity can be regained by using STagT in an MTL setting. Moreover, since entity and relation information are considerably different, such a de-coupling may be advantageous. Thus, it is not guaranteed to produce trees, as we observe in AM datasets. The same constraint is enforced by the dependency parsing framing. All of the tagging modelings, including LSTM-ER, are local models whereas our parsing framing is a global model: it globally enforces a tree structure on the token-level. We relegate issues of pre-trained word embeddings, hyperparameter optimization and further practical issues to the supplementary material. Links to software used as well as some additional error analysis can also be found there. We train and test all parsers on the paragraph level, because training them on essay level was typically too memory-exhaustive. Mate is slightly better and in particular recognizes several major claims correctly. Kiperwasser performs decently on the approximate match level, but not on exact level. The best parser by far is the LSTM-Parser. How does performance change when we switch to the essay level? Thus, the performance drop between paragraph and essay level is in any case immense. The minimal feature set employed by Kiperwasser is apparently not sufficient for accurate AM but still a lot more powerful than the hand-crafted feature approaches. We believe that this is due to the very long sequences encountered on essay level. The ILP model operates on both levels. Again, we observe that paragraph level is considerably easier than essay level. Our explanation is that taggers are simpler local models, and thus need less training data and are less prone to overfitting. The underlying tagging framework is weaker than that of BLCC: there is no CNN which can take subword information into account and there are no dependencies between output labels: each tagging prediction is made independently of the other predictions. We refer to this system as STagBL. An analogous trend holds for argument relations. In fact, in one of the best parametrizations the C task and the full task feed from the same layer in the deep BiLSTM. Moreover, we find that the C task is consistently more helpful as an auxiliary task than the R task. Layers from which tasks feed are indicated by respective numbers. In contrast, LSTM-ER trained and tested on paragraph level substantially outperforms all other systems discussed, both for component as well as for relation identification. Our findings indicate that a similar result can be achieved for STagT via MTL when components and relations are included as auxiliary tasks, cf. Lastly, the better performance of LSTM-ER over STagBLCC for relations on paragraph level appears to be a consequence of its better performance on components.   m possible components, as LSTM-ER allows. In contrast, LSTM-ER only mildly prefers short-range dependencies over long-range dependencies, cf. The (e) ILP model has access to both paragraph and essay level information and thus has always more information than all the neural systems compared to. Thus, it also knows in which paragraph in an essay it is. This is useful particularly for major claims, which always occur in first or last paragraphs in our data. when both are evaluated on paragraph level. We present the first study on neural end-to-end AM. We experimented with different framings, such as encoding AM as a dependency parsing problem, as a sequence tagging problem with particular label set, as a multi-task sequence tagging problem, and as a problem with both sequential and tree structure information. These are also our policy recommendations. This is an even more general modeling than LSTM-ER."
564,1,5.0,3.0,4.0,True,acl_2017,train,"We present Grid Beam Search (GBS), an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model that generates a sequence ŷ = {y0 . . . yT }, by maximizing p(y|x) = ∏ t p(yt|x; {y0 . . . yt−1}). Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general way to incorporate additional knowledge into a model’s output without requiring any modification of the model parameters or training data. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios.","    Lex-ical constraints take the form of phrases or words that must be present in the output sequence. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios. The output of many natural language processing models is a sequence of text. In many real-world scenarios, additional information that could inform the search for the optimal output sequence may be available at inference time. Our goal in this work is to find a way to force the output of a model to contain such lexical constraints, while still taking advantage of the distribution learned from training data. For Machine Translation (MT) usecases in particular, final translations are often produced by combining automatically translated output with user inputs. These interactive scenarios can be unified by considering user inputs to be lexical constraints which guide the search for the optimal output sequence. Individual constraints may be single tokens or multi-word phrases, and any number of constraints may be specified simultaneously.     This makes our work applicable in a wide range of text generation scenarios, including image description, dialog generation, automatic summarization, and question answering. The output token at each timestep appears at the top of the figure, with lexical constraints enclosed in boxes. Generation is shown in blue, Starting new constraints in green, and Continuing constraints in red. The function used to create the hypothesis at each timestep is written at the bottom. Hypotheses with circles inside them are closed, all other hypotheses are open. Best viewed in colour).       Thus, a search or decoding algorithm is often used as a compromise between these two extremes. Boxes represent beams which hold k-best lists of hypotheses. A) Chart Parsing using SCFG rules to cover spans in the input. B) Source coverage as used in PB-SMT. C) Sequence timesteps (as used in Neural Sequence Models), GBS is an extension of (C). In (A) and (B), hypotheses are finished once they reach the final beam. In (C), a hypothesis is only complete if it has generated an end-of-sequence (EOS) symbol. The key idea is to discard bad options early, while avoiding discarding candidates that may be locally risky, but could eventually result in the best overall output. Beam search is simple to implement, and is flexible in the sense that the semantics of the graph of beams can be adapted to take advantage of additional structure that may be available for specific tasks. A) and (B) depend upon explicit structural information between the input and output, (C) only assumes that the output is a sequence where later symbols depend upon earlier ones. However, with neural sequence models, we cannot organize beams by their explicit coverage of the input. Our goal is to organize decoding in such a way that we can constrain the search space to outputs which contain one or more pre-specified sub-sequences. Note that each step of c covers a single constraint token. tokenj in constrainti.   Note that the scoring function of the model does not need to be aware of the existence of constraints, but it may be, for example via a feature which indicates if a hypothesis is part of a constraint or not. Once a hypothesis on the top level generates the EOS token, it can be added to the set of finished hypotheses. The highest scoring hypothesis in the set of finished hypotheses is the best sequence which covers all constraints. By distinguishing between open and closed hypotheses, we can allow for arbitrary multi-token phrases in the search. Thus, the set of constraints for a particular output may include both individual tokens and phrases. Note also that discontinuous lexical constraints, such as phrasal verbs in English or German, are easy to incorporate into GBS, by adding filters to the search, which require that one or more conditions must be met before a constraint can be used. Both the computation of the score for a hypothesis, and the granularity of the tokens (character, subword, word, etc...) are left to the underlying model. Because our decoder can handle arbitrary constraints, there is a risk that constraints will contain tokens that were never observed in the training data, and thus are unknown by the model. Especially in domain adaptation scenarios, some userspecified constraints are very likely to contain unseen tokens. Because the number of beams is multiplied by the number of constraints, the runtime complexity of a naive implementation of GBS is O(ktc). Standard time-based beam search is O(kt); therefore, some consideration must be given to the efficiency of this algorithm. Also, we find that the most time is spent computing the states for the hypothesis candidates, so by keeping the beam size small, we can make GBS significantly faster. See the Appendix for more details on our training data and hyperparameter configuration for each language pair. Because our experiments use NMT models, we can now be more explicit about the implementations of the generate, start, and continue functions for this GBS instantiation. Eq. Starting with the original translation hypothesis, a (simulated) user first picks a part of the hypothesis which is incorrect, and then provides the correct translation for that portion of the output. The userprovided correction is then used as a constraint for the next decoding cycle. The Pick-Revise process can be repeated as many times as necessary, with a new constraint being added at each cycle. To simulate user interaction, at each iteration we chose a phrase of up to three tokens from the reference translation which does not appear in the current MT hypotheses. In the strict setting, the complete phrase must be missing from the hypothesis. In the relaxed setting, only the first word must be missing. When a three-token phrase cannot be found, we backoff to two-token phrases, then to single tokens as constraints. If a hypothesis already matches the reference, no constraints are added. An attractive alternative is to simply provide term mappings as constraints, allowing any existing system to adapt to the terminology used in a new test domain. The corpus is focused upon software localization, a domain which is likely to be very different from the WMT data used to train our general domain models. When source phrases that match the terminology are observed in the test data, the corresponding target phrase is added to the constraints for that segment. This simple method of domain adaptation leads to a significant improvement in the BLEU score of the model without any human intervention. Thus, manually created domain terminologies are likely to lead to even greater performance gains. Improvement in BLEU score over the previous cycle is shown in parentheses. the training data for this system being very different from the IT domain (see Appendix). Using a terminology with GBS is likely to benefit any model where the terminology used in the test domain is significantly different from the original training data. This is a desirable effect for user interaction, since it implies that users can bootstrap quality by adding the most critical constraints (i.e. those that are most essential to the output), first. Most related work to date has presented modifications of SMT systems for specific usecases which constrain MT output via auxilliary inputs. IMT can be viewed as subcase of constrained decoding, where there is only one constraint which is guaranteed to be placed at the beginning of the output sequence. Recently, some attention has also been given to SMT decoding with multiple lexical constraints. Translators specify constraints by editing a part of the MT output that is incorrect, and then asking the system for a new hypothesis, which must contain the user-provided correction. This process is repeated, maintaining constraints from previous iterations and adding new ones as needed. Importantly, their approach relies upon the phrase segmentation provided by the SMT system. Original Hypothesis Es war auch ein Anti-Rauch-Aktiv-ist und nahmen an mehreren Kampagnen teil. Reference Constraints Ebenso setzte er sich gegen das Rauchen ein und nahm an mehreren Kampagnen teil. In contrast, our approach decodes at the token level, and is not dependent upon any explicit structure in the underlying model. The MT system decodes the source phrases which are not aligned to the user-selected phrases until the source sentence is fully covered. Such constraintaware models are complementary to our work, and could be used with GBS decoding without any change to the underlying models. To the best of our knowledge, ours is the first work which considers general lexically constrained decoding for any model which outputs sequences, without relying upon alignments between input and output, and without using a search organized by coverage of the input. Lexically constrained decoding is a flexible way to incorporate arbitrary subsequences into the output of any model that generates output sequences token-by-token. A wide spectrum of popular text generation models have this characteristic, and GBS should be straightforward to use with any model that already uses beam search. In translation interfaces where translators can provide corrections to an existing hypothesis, these user inputs can be used as constraints, generating a new output each time a user fixes an error. By simulating this scenario, we have shown that such a workflow can provide a large improvement in translation quality at each iteration. By using a domain-specific terminology to generate target-side constraints, we have shown that a general domain model can be adapted to a new domain without any retraining. Surprisingly, this simple method can lead to significant performance gains, even when the terminology is created automatically. In future work, we hope to evaluate GBS with models outside of MT, such as automatic summarization, image captioning or dialog generation. We also hope to introduce new constraintaware models, for example via secondary attention mechanisms over lexical constraints. The best single model from validation is used in all of the experiments for a language pair. We sort minibatches by source sentence length, and reshuffle training data after each epoch."
122,2,3.44,3.44,4.0,True,acl_2017,train,"One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user’s goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users’ language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.","Task-based dialogue systems help users achieve goals such as finding restaurants or booking flights. This is a probability distribution over dialogue states used by the downstream dialogue manager component to decide which action the system should perform next. In this framework, the dialogue system is supported by a domain ontology which describes the range of user intents the system can process. The ontology defines a collection of slots and the values each slot can take. The system must track the search constraints expressed by users (goals or informable slots) and questions the users ask about search results (requests), taking into account each user utterance (input via a speech recogniser) and the dialogue context (e.g., what the system just said). Some approaches assume that a separate Spoken Language Understanding (SLU) module will solve this problem for them. However, such models require vast amounts of annotated training data. on manually constructed semantic dictionaries to identify alternative mentions of ontology items that vary lexically or morphologically. This approach, which we term delexicalisation, is clearly not scalable to larger, more complex dialogue domains. In this paper, we present two new models, collectively called the Neural Belief Tracker (NBT) family. The proposed models couple SLU and DST, efficiently learning to handle variation without requiring any hand-crafted resources. To do that, NBT models move away from exact matching and instead reason entirely over pre-trained word vectors. The vectors making up the user utterance and preceding system output are first composed into intermediate representations. These are then used to decide which of the ontology-defined intents have been expressed by the user up to that point in the conversation. To the best of our knowledge, NBT models are the first to successfully use pre-trained word vector spaces to improve the language understanding capability of belief tracking models. In evaluation on two datasets, we show that: a) NBT models match the performance of delexicalisation-based models which make use of hand-crafted semantic lexicons; and b) the NBT models significantly outperform those models when such resources are not available. Consequently, we believe this work proposes a framework better-suited to scaling belief tracking models for deployment in real-world dialogue systems operating over sophisticated application domains where the creation of such domain-specific lexicons would be infeasible. In this setting, the task is defined by an ontology that enumerates the goals a user can specify and the attributes of entities that the user can request information about. However, more robust and accurate statistical SLU systems are available. Many discriminative approaches to spoken dialogue SLU train independent binary models that decide whether each slot-value pair was expressed in the user utterance. This does not mean that those models are immune to the drawbacks identified here for the two model categories; in fact, they share the drawbacks of both. The implementation of the three representation learning subcomponents can be modified, as long as these produce adequate vector representations which the downstream model components can use to decide whether the current candidate slot-value pair was expressed in the user utterance (taking into account the preceding system act). One drawback shared by these methods is their resource requirements, either because they need to learn independent parameters for each slot and value or because they need fine-grained manual annotation at the word level. This hinders scaling to larger, more realistic application domains. Joint models typically rely on a strategy known as delexicalisation whereby slots and values mentioned in the text are replaced with generic labels. To perform belief tracking, the shared model iterates over all slot-value pairs, extracting delexicalised feature vectors and making a separate binary decision regarding each pair. For toy domains, one can manually construct semantic dictionaries which list the potential rephrasings for all slot values. Again though, this will not scale to the rich variety of user language or to general domains. The primary motivation for the work presented in this paper is to overcome the limitations that affect previous belief tracking models. Its input consists of the system dialogue acts preceding the user input, the user utterance itself, and a single candidate slot-value pair that it needs to make a decision about. To perform belief tracking, the NBT model iterates over all candidate slot-value pairs (defined by the ontology), and decides which ones have just been expressed by the user. For any given user utterance, system act(s) and candidate slot-value pair, the representation learning submodules produce vector representations which act as input for the downstream components of the model. All representation learning subcomponents make use of pre-trained collections of word vectors. The NBT training procedure keeps these vectors fixed: that way, at test time, unseen words semantically related to familiar slot values (i.e. This means that the NBT model parameters can be shared across all values of the given slot, or even across all slots.   uku.   uku. We propose two model variants which differ in the method used to produce vector representations of u: NBT-DNN and NBT-CNN. Both act over the constituent ngrams of the utterance.  We maintain a separate set of parameters for each slot (indicated by superscript s). Ideally, the model should learn to recognise which parts of the utterance are more relevant for the subsequent classification task. For instance, it could learn to ignore verbs or stop words and pay more attention to adjectives and nouns which are more likely to express slot values. These models typically apply a number of convolutional filters to n-grams in the input sentence, followed by non-linear activation functions and max-pooling.   Word vectors of all unigrams, bigrams and trigrams are summed to obtain cumulative n-gram representations. The convolutions are followed by the ReLu activation function and max-pooling to produce summary n-gram representations. This component decides whether the user explicitly expressed an intent matching the current candidate pair (i.e. without taking the dialogue context into account). This is where the use of high-quality pre-trained word vectors comes into play: a delexicalisation-based model could deal with the former example but would be helpless in the latter case, unless a human expert had provided a semantic dictionary listing all potential rephrasings for each value in the domain ontology. The dot product, which may seem like the more intuitive similarity metric, would reduce the rich set of features in d to a single scalar. However, this set-up led to very weak performance since our relatively small datasets did not suffice for the network to learn to model the interaction between the two feature vectors. the flow of dialogue leading up to the latest user utterance. System Request: The system asks the user about the value of a specific slot Tq. System Confirm: The system asks the user to confirm whether a specific slot-value pair (Ts, Tv) is part of their desired constraints. If we make the Markovian decision to only consider the last set of system acts, we can incorporate context modelling into the NBT model. Let tq and (ts, tv) be the word vector representations of the arguments for the system request and confirm acts (zero vectors if none). The computed similarity terms act as gating mechanisms which only pass the utterance representation through if the system asked about the current candidate slot or slot-value pair. This type of interaction is particularly useful for the confirm system act: if the system asks the user to confirm, the user is likely not to mention any slot values, but to just respond affirmatively or negatively. This means that the model must consider the three-way interaction between the utterance, candidate slot-value pair and the slot value pair offered by the system. If (and only if) the latter two are the same should the model consider the affirmative or negative polarity of the user utterance when making the subsequent binary decision. Finally, these two context modelling summary representations are passed to the decision making module, which combines them with the semantic decoder output d to make the final binary decision. In spoken dialogue systems, belief tracking models operate over the output of automatic speech recognition (ASR). Despite improvements to speech recognition, the need to make the most out of imperfect ASR will persist as dialogue systems are used in increasingly noisy environments. In this work, we define a simple rule-based belief state update mechanism which can be applied to ASR N-best lists. For requests, all slots in V treq are deemed to have been requested. As requestable slots serve to model single-turn user queries, they require no belief tracking across turns. Both consist of user conversations with taskoriented dialogue systems designed to help users find suitable restaurants around Cambridge, UK. The two corpora share the same domain ontology, which contains three informable (i.e. goal-tracking) slots: FOOD TYPE, AREA and PRICE. The users can specify values for these slots in order to find restaurants which best meet their criteria. Once the system suggests a restaurant, the users can ask about the values of up to eight requestable slots (PHONE NUMBER, ADDRESS, etc.). We train NBT models on transcriptions and report belief tracking performance on test set ASR predictions. We evaluate two NBT model variants: NBT-DNN and NBT-CNN. The two corpora are used to create training data for two separate experiments. We iterate over all utterances, generating one example for each of the slot-value pairs in the ontology. An example consists of a transcription, its context (i.e. list of preceding system acts) and a candidate slot-value pair. This model is an n-gram based neural network model with recurrent connections between turns (but not inside utterances) which replaces occurrences of slot names and values with generic delexicalised features. This model uses an RNN for belief state updates and a CNN for turn-level feature extraction. Both baseline models map exact matches of ontology-defined intents (and their lexiconspecified rephrasings) to one-hot delexicalised ngram features. This means that pre-trained vectors cannot be incorporated directly into these models. The NBT models outperformed the baseline models in terms of both joint goal and request accuracies. dict. Moreover, there was no statistically significant variation between the NBT and the lexicon-supplemented models, showing that the NBT can handle semantic relations which otherwise had to be explicitly encoded in semantic dictionaries. While the NBT performs well across the board, we can compare its performance on the two datasets to understand its strengths. This indicates that future work should focus on better ASR compensation if the model is to be deployed in environments with challenging acoustics. The NBT models use the semantic relations embedded in the pre-trained word vectors to handle semantic variation and produce high-quality intermediate representations. For this dataset, GloVe vectors do not improve over the randomly initialised ones. We believe this happens because distributional models keep related, yet antonymous words close together (e.g. north and south, expensive and inexpensive), offsetting the useful semantic content embedded in this vector spaces. In this paper, we have proposed a novel neural belief tracking (NBT) framework designed to overcome current obstacles to deploying dialogue systems in real-world dialogue domains. The NBT models offer the known advantages of coupling Spoken Language Understanding and Dialogue State Tracking, without relying on hand-crafted semantic lexicons to achieve state-of-the-art performance. Our evaluation demonstrated these benefits: the NBT models match the performance of models which make use of such lexicons and vastly outperform them when these are not available. Finally, we have shown that the performance of NBT models improves with the semantic quality of the underlying word vectors. To the best of our knowledge, we are the first to move past intrinsic evaluation and show that semantic specialisation boosts performance in downstream tasks. In future work, we intend to explore applications of the NBT for multi-domain dialogue systems, as well as in languages other than English that require handling of complex morphological variation."
56,3,4.0,3.6,3.6,True,acl_2017,train,"The existing word representation methods mostly limit their information source to word co-occurrence statistics. In this paper, we introduce ngrams into four representation methods: SGNS, GloVe, PPMI matrix and its SVD factorization. Comprehensive experiments are conducted on word analogy and similarity tasks. The results show that improved word representations are learned from ngram cooccurrence statistics. We also demonstrate that the trained ngram representations are useful in many aspects such as finding antonyms and collocations. Besides, a novel approach of building co-occurrence matrix is proposed to alleviate the hardware burden brought by ngrams.","One of the most fundamental work in this field is word embedding, where low-dimensional word representations are learned from unlabeled corpus. The trained word embeddings reflect semantic and syntactic information of words. They are not only useful in tasks of lexical semantics (e.g. It achieves state-of-the-art results on a range of linguistic tasks with only a fraction of time compared with previous techniques. They use traditional bag-ofcontexts (concretely, PPMI matrix) to represent words, and achieve comparable results with the above neural embedding models. The relationships among different representation methods are intricate. One should not feel surprised with the conclusion, because these methods all exploit word co-occurrence statistics as the information source and no one goes beyond that. The idea of using ngrams is well supported by language modeling, one of the oldest problems studied in statistical NLP. Actually, the idea of word embedding models roots in language models. They are closely related but with different purposes. Word embedding models aim at learning word representations instead of word prediction. Since ngram is a vital part in language modeling, we are inspired to integrate ngram statistical information into recent word representation methods for better performance. The idea of using ngram is intuitive. However, there is still rare work using ngrams in recent representation methods. To evaluate the ngram-based models, comprehensive experiments are conducted on word analogy and similarity tasks. The results demonstrate that the improved word representations are learned from ngram co-occurrence statistics. Besides that, we qualitatively evaluate the trained ngram representations. The high-quality ngram representations are useful in many ways. For example, ngrams in negative form (e.g. Finally, a novel method is proposed to build ngram co-occurrence matrix. We unify different representation methods in a pipeline. SGNS, GloVe, PPMI and its SVD factorization are used as baselines. The information used by them does not go beyond word co-occurrence statistics. However, their approaches to using the information are totally different. Compared to previous neural embedding models, SGNS speeds up the training process, reducing the training time from days or weeks to hours. Also, the trained embeddings possess valuable properties. They are able to reflect relations between the two words accurately, which is evaluated by a fancy task called word analogy. Its objective is to reconstruct non-zero values in the matrix. The direct use of matrix is reported to bring more improved results and higher speed. GloVe and other embedding models are essentially based on word cooccurrence statistics of the corpus. The experiments also confirm this idea. Properties like word analogy are not restricted to neural models. In the deep learning literature, ngram has shown to be useful in generating text representations. CNNs are essentially using ngram information to represent texts. It should be noted that phrases are different from ngrams. Phrases have clear semantics and the number of phrases is much less than the number of ngrams. In this section, we introduce ngrams into SGNS, GloVe, PPMI and SVD. First we establish some notations. W and C denote the word and context vocabulary. In this section, we introduce ngrams into context vocabulary. We treat each ngram as a normal word and give it a unique embedding. During the training, the center word should not only predict its surrounding words, but also predict its surrounding ngrams. The only difference is the definition of the C(w). Two points need to be noticed from the above definition. The first is how to determine the distance between word and ngram. Another point is whether the overlap of word and ngram is allowed or not. In the overlap situation, ngrams are used as context even they contain the center word. In the non-overlap case, these ngrams are excluded. The properties of word embeddings are different when overlap is allowed or not, which is discussed in experiments section. We further extend the model to introduce ngrams into center word vocabulary. surrounding ngrams. They are just the representation method discussed in the following section, where ngrams are introduced into positive PMI (PPMI) matrix. Introducing ngrams into GloVe, PPMI and SVD is straightforward: the only change is to replace word co-occurrence matrices with ngram ones. Afterwards, we build co-occurrence matrix upon these pairs. The rest steps are identical with the original baseline models. However, building co-occurrence matrix is not an easy task as it apparently looks like. The introduction of ngrams brings huge burdens on the hardware. The two strategies optimize the process in different aspects. Computational cost is reduced significantly when they are used together. Intermediate results are written to temporary files when memory is full. In the second stage, we merge these sorted temporary files to generate co-occurrence matrix. The scalar only reflects the strength of the relation, not the type. In our view, relations are too complex to be reflected by a single score. Besides that, we have to compare pairs with different types in similarity task. In analogy task, relations between the two words are reflected by a vector, which is usually obtained by the difference between word embeddings. Different from scalar, the vector provides more accurate descriptions of relations. For example, capital-country relation is encoded in vec(Athens)-vec(Greece), vec(Tokyo)vec(Japan) and so on. To correctly answer the questions, the models should embed the two relations, vec(a)vec(b) and vec(c)-vec(d), into similar positions in the space. The latter one is more suitable for sparse representation. We implement SGNS, GloVe, PPMI and SVD in a pipeline, allowing the reuse of code and intermediate results. Win Type Sim. Rel. trix upon the pairs. GloVe and PPMI learn word representations on the basis of co-occurrence matrix. SVD factorizes the PPMI matrix to obtain low-dimensional representation. Non-overlap setting is used in default since it produces much less pairs. We only consider uni-grams (words) and bigrams in our experiments. We suspect that the higherorder grams are so sparse that may deteriorate the performance. SGNS is a highly popular word embedding model. Even compared with its challengers such as GloVe, SGNS is reported to have more robust performance with faster training speed. We can observe that the introduction of bi-grams gives the huge promotions at different hyper-parameter settings. Bi-grams are very effective for capturing semantic information. For syntactic questions, the improvements are still significant. In traditional language models, ngram is the vital part for predicting next words. Our results confirm the effectiveness of ngrams again on recent word embedding models and more advanced analogy task. The effect of overlap is large on analogy datasets. Semantic questions prefer the overlap setting. Rel. Win Type Google MSR Sim. Rel. Win Type Google MSR Sim. Rel. most cases. However, the improvements brought by bi-grams are not as big as the case in analogy task. Using ngram-ngram co-occurrence statistics bring little improvements but immense costs. PPMI prefers Multiplicative (Mul) evalution. To this end, we focus on analyzing the results on Mul columns. When bi-grams are used, significant improvements are witnessed on analogy task. They are pretty high numbers considering only default setting is used. Though bi-grams are very effective on analogy task, the improvements on similarity task are not witnessed. In GloVe, consistent but minor improvements are achieved on analogy task. Little improvement is witnessed on similarity task. In SVD, bi-grams sometimes lead to worse results in both analogy and similarity tasks. We thought ngram would be very effective in GloVe and SVD just like the cases in SGNS, since these models use exactly the same source of information. Our preliminary conjecture is that the default hyper-parameter setting should be blamed. We strictly follow the hyper-parameters used in baseline models, making no adjustments to cater to the introduction of ngrams. Besides that, some common techniques such as dynamic window, decreasing weighting function, dirty sub-sampling are discarded. The relationships between ngrams and various hyperparameters require further exploration. Though trivial, it may lead to much better results and give researchers better understanding of different representation methods. That will be the focus of our future work. The vec(is written) should be closed to vec(write) and vec(book). We divide the target ngrams into six groups according to their patterns. We can observe that the returned words and ngrams are very intuitive. As might be expected, synonyms of the target ngrams are returned in top positions (e.g. The trained n-gram embeddings also preserve some common sense. In terms of syntactic patterns, we can observe that in most cases, the returned ngrams are in the similar form of target ngrams. In general, the trained embeddings basically reflect the semantic meanings and syntactic patterns of ngrams. With high-quality ngram embeddings, we have the opportunity to do more interesting things in our future work. For example, we will construct a antonym dataset to evaluate ngram embeddings systematically. Besides that, we will find more scenarios for using ngram embeddings. In our view, ngram embeddings may be useful in many NLP tasks. Intuitively, initializing these models with pre-trained ngram embeddings may further improve the accuracies. We introduce ngrams into four representation methods. In addition, we find that the trained ngram embeddings are able to reflect their semantic meanings and syntactic patterns. To alleviate the costs brought by ngrams, we propose a novel way of building co-occurrence matrix, enabling the ngram-based models to run on cheap hardware."
335,2,4.0,4.0,4.0,True,acl_2017,train,"In this paper, we present the gated selfmatching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3% on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9%. At the time1 of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.","Rapid progress has been made since the release of the SQuAD dataset. The key contributions of this work are three-fold. By introducing a gating mechanism, our gated attention-based recurrent network assigns different levels of importance to passage parts depending on their relevance to the question, masking out irrelevant passage parts and emphasizing the important ones. Second, we introduce a self-matching mechanism, which can effectively aggregate evidence from the whole passage to infer the answer. Through a gated matching layer, the resulting question-aware passage representation effectively encodes question information for each passage word. However, recurrent networks can only memorize limited passage context in practice despite its theoretical capability. One answer candidate is often unaware of the clues in other parts of the passage. To address this problem, we propose a self-matching layer to dynamically refine passage representation with information from the whole passage. Based on question-aware passage representation, we employ gated attention-based recurrent networks on passage against passage itself, aggregating evidence relevant to the current passage word from every word in the passage. A gated attention-based recurrent network layer and self-matching layer dynamically enrich each passage representation with information aggregated from both question and passage, enabling subsequent network to better predict answers. Lastly, the proposed method yields state-of-theart results against strong baselines. At the time of submission of this paper, our model holds the first place on the SQuAD leader board. This setup challenges us to understand and reason about both the question and passage in order to infer the answer. Passage: Tesla later approached Morgan to ask for more funds to build a more powerful transmitter. Tesla wrote another plea to Morgan, but it was also fruitless. Morgan still owed Tesla money on the original agreement, and Tesla had been facing foreclosure even before construction of the tower began. Question: On what did Tesla blame for the loss of the initial money? We then match the question and passage with gated attention-based recurrent networks, obtaining question-aware representation for the passage. On top of that, we apply self-matching attention to aggregate evidence from the whole passage and refine the passage representation, which is then fed into the output layer to predict the boundary of the answer span. The character-level embeddings are generated by taking the final hidden states of a bi-directional recurrent neural network (RNN) applied to embeddings of characters in the token. Such character-level embeddings have been shown to be helpful to deal with out-ofvocab (OOV) tokens.     We propose a gated attention-based recurrent network to incorporate question information into passage representation. It is a variant of attentionbased recurrent networks, with an additional gate to determine the importance of information in the passage regarding a question. The gate effectively model the phenomenon that only parts of the passage are relevant to the question in reading comprehension and question answering. We call this gated attention-based recurrent networks. It can be applied to variants of RNN, such as GRU and LSTM. We also conduct experiments to show the effectiveness of the additional gate on both GRU and LSTM. One problem with such representation is that it has very limited knowledge of context. Passage context is necessary to infer the answer. To address this problem, we propose directly matching the question-aware passage representation against itself. Self-matching extracts evidence from the whole passage according to the current passage word and question information. In addition, we use an attention-pooling over the question representation to generate the initial hidden vector for the pointer network.   We utilize the question vector rQ as the initial state of the answer recurrent network. We specially focus on the SQuAD dataset to train and evaluate our model, which has garnered a huge attention over the past few months. The answer to every question is a segment of the corresponding passage. EM measures the percentage of the prediction that matches one of the ground truth answers exactly. Since the test set is hidden, we are required to submit the model to Stanford NLP group to obtain the test scores. As we can see, our method clearly outperforms the baseline and several strong state-ofthe-art systems for both single model and ensembles. We do ablation tests on the dev set to analyze the contribution of components of gated self-matching networks. Each row is the attention weights of the whole passage for the current passage word. The darker the color is the higher the weight is. Some key evidence relevant to the question-passage tuple is more encoded into answer candidates. networks. Character-level embeddings are not utilized. To show the ability of the model for encoding evidence from the passage, we draw the alignment of the passage against itself in self-matching. We can see that key evidence aggregated from the whole passage is more encoded into the answer candidates. For other words, the attention weights are more evenly distributed between evidence and some irrelevant parts. Selfmatching do adaptively aggregate evidence for words in passage. The point on the x-axis of figure (c) and (d) represent the datas whose passages length or questions length are between the value of current point and last point. questions. With the increase of answer length, the performance of our model obviously drops. Moreover, we discover that the performance remains stable with the increase in length, the obvious drop in longer passage is mainly because the proportion is too small. Our model is largely agnostic to long passages and focuses on important part of the passage. Reading Comprehension and Question Answering Dataset Benchmark datasets play an important role in recent progress in reading comprehension and question answering research. Existing datasets can be classified into two categories according to whether they are manually labeled. However, they are in cloze style, in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD provides a large and high-quality dataset. The answers in SQuAD often include non-entities and can be much longer phrase, which is more challenging than cloze-style datasets. The questions in the dataset are real anonymized queries issued through Bing or Cortana and the passages are related web pages. For each question in the dataset, several related passages are provided. However, the answers are human generated, which is different from SQuAD where answers must be a span of the passage. Neural network-based models demonstrate the effectiveness on the SQuAD dataset. Different from the above models, we introduce self-matching attention in our model. It dynamically refines the passage representation by looking over the whole passage and aggregating evidence relevant to the current passage word and question, allowing our model make full use of passage information. Weightedly attending to word context has been proposed in several works. Since passage information relevant to question is more helpful to infer the answer in reading comprehension, we apply self-matching based on question-aware representation and gated attention-based recurrent networks. It helps our model mainly focus on question-relevant evidence in the passage and dynamically look over the whole passage to aggregate evidence. Another key component of our model is the attention-based recurrent network, which has demonstrated success in a wide range of tasks. The gated attention-based recurrent network is a variant of attention-based recurrent network with an additional gate to model the fact that passage parts are of different importance to the particular question for reading comprehension and question answering. In this paper, we present gated self-matching networks for reading comprehension and question answering. We introduce the gated attentionbased recurrent networks and self-matching attention mechanism to obtain representation for the question and passage, and then use the pointernetworks to locate answer boundaries. Our model achieves state-of-the-art results on the SQuAD dataset, outperforming several strong competing systems."
270,2,4.5,3.5,3.5,True,acl_2017,train,"Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6% on the Stanford natural language inference dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result—it further improves the performance even when added to the already very strong model.","p: Several airlines polled saw costs grow more than expected, even after adjusting for inflation. h: Some of the companies in the poll reported cost increases. The most recent years have seen advances in modeling natural language inference. This makes it feasible to train more complex inference models. Our model may serve as a new baseline or starting point for deploying more complicated models for NLI. Exploring syntax for NLI is very attractive to us. In this paper, we are interested in exploring this within the neural network frameworks, with the presence of relatively large training data. They also experimented with simple classification models as well as simple neural networks that encode the premise and hypothesis independently. For NLI, the idea allows neural models to pay attention to specific areas of the sentences. The model decomposes the NLI problem into subproblems that can be solved separately. It is, however, not very clear if the potential of the sequential inference networks has been well exploited for NLI. In this paper, we first revisit this problem and show that enhancing sequential inference models based on chain networks can actually outperform all previous results. Our model may serve as a new baseline or starting point for deploying more complicated models for NLI. We further show that explicitly considering recursive architectures to encode syntactic parsing information for NLI could further improve the performance. We present here our natural language inference networks which are composed of the following major components: input encoding, local inference modeling, and inference composition. Vertically, the figure depicts the three major components, and horizontally, the left side of the figure represents our sequential NLI model named ESIM, and the right side represents networks that incorporate syntactic parsing information in tree LSTMs.     The goal is to predict a label y that indicates the logic relationship between a and b. We employ bidirectional LSTM (BiLSTM) as one of our basic building blocks for NLI. Here BiLSTM learns to represent a word (e.g., ai) and its context. construct the final prediction, where BiLSTM encodes local inference information and its interaction.     A bidirectional LSTM runs a forward and backward LSTM on a sequence starting from the left and the right end, respectively. The hidden states generated by these two LSTMs at each time step are concatenated to represent that time step and its context. Note that we used LSTM memory blocks in our models. As discussed above, it is intriguing to explore the effectiveness of syntax for natural language inference; for example, whether it is useful even when incorporated into the best-performing models. Specifically, the input of a node is used to configure four gates: the input gate it, output gate ot, and the two forget gates fLt and f R t. Carefully modeling local subsentential inference between a premise and hypothesis is critical to help determine the overall inference between these two statements. To closely examine local inference, we explore both the sequential and syntactic tree models that have been discussed above. The former helps collect local inference for words and their context, and the tree LSTM helps collect local information between (linguistic) phrases and clauses. Locality of inference Modeling local inference needs to employ some forms of hard or soft alignment to associate the relevant subcomponents between a premise and a hypothesis. In neural network models, this is often achieved with soft attention. While their basic framework is very effective, achieving one of the previous best results, using a pre-trained word embedding by itself does not automatically consider the context around a word in NLI. In this paper, we argue for leveraging attention over the bidirectional sequential encoding of the input, as discussed above. Again, as discussed above, we will use bidirectional LSTM and tree-LSTM to encode the premise and hypothesis, respectively. function on our hidden states before computing eij and it did not further help our models. Local inference collected over sequences Local inference is determined by the attention weight eij computed above, which is used to obtain the local relevance between a premise and hypothesis.     Local inference collected over parse trees We use tree models to help collect local inference information over linguistic phrases and clauses in this layer. The tree structures of the premise and hypothesis are produced by a constituency parser. This connects all words, constituent phrases, and clauses between the premise and hypothesis. Enhancement of local inference information In our models, we further enhance the local inference information collected. We expect that such operations could help sharpen local inference information between elements in the tuples and capture inference relationships such as contradiction. The enhancement is performed for both the sequential and the tree models. Along this direction, we have also further modeled the interaction by feeding the tuples into feed-forward networks and added the top layer hidden states to the above concatenation. We found that it does not further help the inference accuracy on the heldout dataset. To determine the overall inference relationship between a premise and hypothesis, we explore a composition layer to compose the enhanced local inference information ma and mb. We perform the composition sequentially or in its parse context using BiLSTM and tree-LSTM, respectively. The composition layer In our sequential inference model, we keep using BiLSTM to compose local inference information sequentially. This function is also applied to BiLSTM in our sequential inference composition. Pooling Our inference model converts the resulting vectors obtained above to a fixed-length vector with pooling and feeds it to the final classifier to determine the overall inference relationship. We then put v into a final multilayer perceptron (MLP) classifier. The MLP has a hidden layer with tanh activation and softmax output layer in our experiments. The entire model (all three components described above) is trained end-to-end. For training, we use multi-class cross-entropy loss. We will show that ESIM outperforms all previous results. We will show that parsing information complements very well with ESIM and further improves the performance, and we call the final model Hybrid Inference Model (HIM). As in the related work, we remove this category. We use classification accuracy as the evaluation metric, as in related work. Training We use the development set to select models for testing. Below, we list our training details. Out-of-vocabulary (OOV) words are initialized randomly with Gaussian samples. All vectors including word embedding are updated during training. In general, adding intra-sentence attention yields further improvement, which is not very surprising as it could help align the relevant text spans between premise and hypothesis. This shows that syntactic tree-LSTMs complement well with ESIM. Ablation analysis We further analyze the major components that are of importance to help us achieve good performance. From the best model, we first replace the syntactic tree-LSTM with the full tree-LSTM without encoding syntactic parse information. More specifically, two adjacent words in a sentence are merged to form a parent node, and this process continues and results in a full binary tree, where padding nodes are inserted when there are no enough leaves to form a full tree. Subfigures (a) and (b) are the constituency parse trees of the premise and hypothesis, respectively. Subfigures (e) and (f) are attention visualization of the tree model and ESIM, respectively. The darker the color, the greater the value. The premise is on the x-axis and the hypothesis is on y-axis. Such information helps the system decide that this pair is a contradiction. Accordingly, in the sequential BiLSTM, the words sitting and down do not play an important role for making the final decision. Subfigure (f) shows that sitting is equally aligned with reading and standing and the alignment for word down is not that useful. We propose neural network models for natural language inference, which achieve the best results reported on the SNLI benchmark. The results are first achieved through our enhanced sequential inference model, which outperformed the previous models, including those employing more complicated network architectures, suggesting that the potential of sequential inference models have not been fully exploited yet. Our model may serve as a new baseline or starting point for deploying more complicated architectures for NLI. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Future work interesting to us includes exploring the usefulness of knowledge resources to help alleviate data sparseness issues. We are also interested in studying more the fragments of sentences or parses highlighted by the attention mechanism in order to provide human-readable explanations of the decisions."
636,2,3.11,3.0,3.56,False,acl_2017,train,"Bi-directional LSTMs have emerged as a standard method for obtaining per-token vector representations serving as input to various token labeling tasks (whether followed by Viterbi prediction or independent classification). This paper proposes an alternative to Bi-LSTMs for this purpose: iterated dilated convolutional neural networks (ID-CNNs), which have better capacity than traditional CNNs for large context and structured prediction. We describe a distinct combination of network structure, parameter sharing and training procedures that is not only more accurate than Bi-LSTM-CRFs, but also 8x faster at test time on long sequences. Moreover, ID-CNNs with independent classification enable a dramatic 14x testtime speedup, while still attaining accuracy comparable to the Bi-LSTM-CRF. We further demonstrate the ability of IDCNNs to combine evidence over long sequences by demonstrating their improved accuracy on whole-document (rather than per-sentence) inference. Unlike LSTMs whose sequential processing on sentences of length N requires O(N) time even in the face of parallelism, IDCNNs permit fixed-depth convolutions to run in parallel across entire documents. Today when many companies run basic NLP on the entire web and large-volume traffic, faster methods are paramount to saving time and energy costs.","Speed is not sufficient of course: they must also be expressive enough to tolerate the tremendous lexical variation in input data. While these models are expressive and accurate, they fail to fully exploit the parallelism opportunities of a GPU, and thus their speed is limited. Specifically, they employ either recurrent neural networks (RNNs) for feature extraction, or Viterbi inference in a structured output model, both of which require sequential computation across the length of the input. Instead, parallelized runtime independent of the length of the sequence saves time and energy costs, maximizing GPU resource usage and minimizing the amount of time it takes to train and evaluate models. Rather than composing representations incrementally over each token in a sequence, they apply filters in parallel across the entire sequence at once. Their computational cost grows with the number of layers, but not the input size, up to the memory and threading limitations of the hardware. Despite the clear computational advantages of CNNs, RNNs have become the standard method for composing deep representations of text. The number of layers required to incorporate the entire input context grows linearly with the length of the sequence. To avoid this scaling, one could pool representations across the sequence, but this is not appropriate for sequence labeling, since it reduces the output resolution of the representation. For dilated convolutions, the receptive field can grow exponentially with the depth, with no loss in resolution at each layer and with a modest number of parameters to estimate. Like typical CNN layers, dilated convolutions operate on a sliding window of context over the sequence, but unlike conventional convolutions, the context need not be consecutive; the dilated window skips over every dilation width d inputs. Our overall iterated dilated CNN architecture (ID-CNN) repeatedly applies the same block of dilated convolutions to token-wise representations. This parameter sharing prevents overfitting and also provides opportunities to inject supervision on intermediate activations of the network. Neurons contributing to a single highlighted neuron in the last layer are also highlighted. with features extracted by a Bi-LSTM (Bi-LSTMCRF). As a feature extractor for a CRF, our model out-performs the Bi-LSTM-CRF.     Let D be the domain size of each y i. This paper considers two factorizations of the conditional distribution. Given these features, O(D) prediction is simple and parallelizable across the length of the sequence. However, feature extraction may not necessarily be parallelizable. For example, RNN-based features require iterative passes along the length of x. To avoid overfitting, p does not depend on the timestep t or the input x in our experiments. The suitability of such compilation depends on the properties and quantity of the data. However, it has worse computational complexity than independent prediction. CNNs in NLP are typically one-dimensional, applied to a sequence of vectors representing tokens rather than to a two-dimensional grid of vectors representing pixels. In this setting, a convolutional neural network layer is equivalent to applying an affine transformation, W c to a sliding window of width r tokens on either side of each token in the sequence. Here, and throughout the paper, we do not explicitly write the bias terms in affine transformations. Dilated convolutions perform the same operation, except rather than transforming adjacent inputs, the convolution is defined over a wider receptive field by skipping over inputs at a time, where is the dilation width. While the flexibility of this model is powerful, its adaptive behavior is not well-suited to GPU acceleration. We can leverage the ability of dilated convolutions to incorporate global context without losing important local information by stacking dilated convolutions of increasing width. By doubling the dilation width at each layer, the size of the receptive field grows exponentially while the number of parameters grows only linearly with the number of layers, so a pixel representation quickly incorporates rich global evidence from the entire image. Stacked dilated CNNs can easily incorporate global information from a whole sentence or document. Unfortunately, simply increasing the depth of stacked dilated CNNs causes considerable overfitting in our experiments. In response, we present Iterated Dilated CNNs (ID-CNNs), which instead iterate a small series of dilated convolutions. We also obtain significant accuracy gains with a training objective that strives for accurate labeling after each iterate, allowing follow-on iterates to observe and resolve dependency violations. ID-CNNs contain repeated blocks of several convolutional layers. To incorporate even broader context without over-fitting, we avoid making B deeper, and instead iteratively apply B L b times, which introduces no extra parameters. We next present an alternative training method that helps bridge the gap between these two techniques. Sec. In response, we compile some of this reasoning in output space into ID-CNN feature extraction. Instead of explicit reasoning over output labels during inference, we train the network such that each block is predictive of output labels. Subsequent blocks learn to correct dependency violations of their predecessors, refining the final sequence prediction. To do so, we first define predictions of the model after each of the L b applications of the block. All of our best reported results include such regularization. We encourage its further application. These outperform similar systems that use the same features, but independent local predictions. More recently, a wide variety of neural network architectures for NER have been proposed. Their Bi-LSTM-CRF obtained the state-of-the-art on four languages without word shape or lexicon features. Though our models would also likely benefit from additional features such as character representations and and lexicons, we focus on simpler models which use wordembeddings alone, leaving more elaborate input representations to future work. In these NER approaches, CNNs were used for low-level mapping feature extraction that feeds into alternative architectures. We are the first to use dilated convolutions for sequence labeling. The broad receptive field of the ID-CNN helps aggregate document-level context. We describe experiments on two benchmark English named entity recognition datasets. Hyperparameters that resulted in the best performance on the validation set were selected via grid search. A more detailed description of the data, evaluation, optimization and data pre-processing can be found in the Appendix. We compare our ID-CNN against strong LSTM and CNN baselines: a Bi-LSTM with local decoding, and one with CRF decoding (Bi-LSTMCRF). using fewer parameters). Since our task is sequence labeling, we desire a model that maintains the token-level resolution of the input, making dilated convolutions an elegant solution. All CNN models out-perform the Bi-LSTM when paired with greedy decoding, suggesting that CNNs are better feature extractors than Bi-LSTMs for independent logistic regression. When paired with Viterbi decoding, our ID-CNN out-performs the Bi-LSTM as a feature extractor, showing that the D-CNN is also a better feature extractor for Viterbi inference. Our ID-CNN is not only a better feature extractor than the Bi-LSTM but it is also faster. We recommend this regularizer as a simple and helpful tool for practitioners training neural networks for NLP. We also note that our combination of training objective (Eqn. We observe similar patterns on OntoNotes as we do on CoNLL. The gap between our greedy model and those using Viterbi decoding is wider than on CoNLL. These long entities benefit more from explicit structured constraints enforced in Viterbi decoding. Still, our ID-CNN outperforms all other greedy methods, achieving our goal of learning a better feature extractor for structured prediction. Incorporating greater context significantly boosts the score of our greedy model on OntoNotes, whereas the Bi-LSTM-CRF performs more poorly. In this scenario, adding the wider context may just add noise to the high-scoring Bi-LSTM-CRF model, whereas the less accurate dilated model can still benefit from the refined predictions of the iterated dilated convolutions. We present iterated dilated convolutional neural networks, fast feature extractors that efficiently aggregate broad context without losing resolution. These provide impressive speed improvements for sequence labeling, particularly when processing entire documents at a time. In the future we hope to extend this work to NLP tasks with richer structured output, such as parsing."
266,2,4.57,2.57,2.57,False,acl_2017,train,"Current state-of-the-art sentiment analysis techniques rely heavily on pre-trained word embeddings. However, the data used to train these embeddings normally comes from large, generic datasets, such as Wikipedia or GoogleNews, which may not include enough task-specific information to create reliable representations. This paper proposes a method to determine the subjectivity of a corpus using available tools and shows that word embeddings trained on task-specific corpora tend to outperform those trained on generic data. We then examine ways to combine information from generic and task-specific datasets and finally demonstrate that our method can work well for under-resourced languages.","The content of the corpora used to train these word embeddings, however, has not been examined in depth. This amount of data, however, is not available in all languages, nor is it necessarily an optimal use of available re-sources. The subjectivity or polarity content of a corpus could provide a good clue to the quality of word embeddings created for the task of sentiment analysis. Therefore, we attempt to use subjectivity as a metric for the appropriateness of a corpus for our task. Given the importance of initialization and the wide-spread use of sentiment analysis techniques using word embeddings, the results are of importance to the community. Subjectivity can be a clear indicator of whether a sentence or phrase contains an opinion. Reducing the amount of irrelevant or potentially misleading data presumably leads to an improvement in polarity classification. This approach could potentially improve vector representations of words by providing only relevant examples to the word embedding algorithm. Lower-dimensional dense vectors, often known as word embeddings, are able to avoid this problem to a degree. They do this by attempting to force similar words to occupy a similar area in vector space, given an objective function. Most state-of-the-art sentiment classifiers are based on deep neural networks. With these architectures it is rather difficult, if not impossible, to find the global optimum and therefore, we must usually be content to find a local optimum. This fact also means that they are sensitive to the initialization of their word embeddings. Task-specific word embeddings are a further development of word embeddings. The purpose is to provide a representation that helps a classifier, often by removing or adding task-specific information. The advantage of this technique is that any kind of word embedding can be used, as the vectors are modified post training. For the inclusion of sentiment information in word vectors, this is the most common approach. However, their sentiment vectors performed worse than generic word vectors without the sentiment information. Both of these techniques attempt to use large amounts of annotated data to extract sentiment information, but do not consider different sources of data or ways to combine them. However, it would be beneficial to examine more in depth the type of data that we use to create word vectors. They explore the effects of using in-domain and generic datasets to create embeddings for e-commerce NER. Their work shows that training on smaller amounts of task-specific data is better than training on large amounts of generic data. Unlike our work, they do not look into ways of combining information from different sources. Our first goal was to quantify the amount of subjective information contained in a corpus. We collected the following corpora, which we believe represent both generic datasets often used in the literature (Wikipedia, Europarl, Multi UN) as well as task-specific corpora (Amazon Movie Dataset). We use the English side of the English-Spanish corpus. In order to determine the amount of subjectivity contained in each corpus, we ran OpinionFinder on each of them. It has a high-precision and highrecall subjectivity classifier. The high-precision classifier uses a rule-based method, looking for well-established lexical clues. The high-recall classifier is a naive bayes classifier trained on the MPQA corpus. After running OpinionFinder on each corpus, we retrieved the results for highprecision subjectivity and high-recall subjectivity. As we expected, the Wikipedia corpus has the lowest scores for subjectivity. This is likely because of its encyclopedic format. The corpora with the highest subjectivity scores are the Amazon (high precision) and Europarl (high recall) corpora. Specifically, we take the subcorpus from the hotel domain. This neural bag-of-words model does not take the order of the words into consideration, but has proven to give results similar to more sophisticated approaches. Our datasets do not allow us to train on labeled phrases. Also the small size of the training data, especially in the OpeNER dataset, means that updating the word vectors while training the classifier actually hurts classification when using our task-specific vectors. Therefore, we do not update while training. For each sentence or phrase in the datasets, we create a training example. A training example is the average of the word embeddings for each word in the sentence or phrase. However, the vectors trained on Wikipedia perform quite well and even outperform their Amazon trained counterparts on the OpeNER dataset. This is likely due to the fact that the domain for the OpeNER dataset is different (Hotel reviews) compared to the RT dataset (Movie Reviews). Given that vectors trained on task-specific data regularly outperform those trained on much larger generic datasets, we should ask ourselves if it is possible to combine both of these datasets to get even better results. Training two separate representations and combining them through vector concatenation. Appending involves simply adding the taskspecific Amazon Movie corpus to the Wikpedia corpus and training a single set of vectors on this data. This should improve the representations of those words that are important for the task, as they will receive more positive examples while training the vectors. Splicing is similar to appending, but instead of adding the Amazon sentences, we splice them into the Wikipedia corpus. We alternate sentences from the two corpora until we have exhausted all of the Amazon sentences, after which we add the remaining Wikipedia sentences. As above, we train a single set of vectors. For concatenation, we train one set of vectors on Wikipedia and a second on the Amazon Movie Corpus separately. For a fair comparison, we concatenate vectors that are half as large as those of the other techniques. This technique may provide better representations for words that often appear in one corpus but not in the other. Note that the embeddings in these three techniques were trained on exactly the same data, the only difference being the method of combining this data. Our baseline vectors are trained only on the Wikipedia corpus. We can see that including task-specific data with any of the techniques tends to improve over the baseline. This in itself is not surprising. Given that we can combine generic and taskspecific data to improve our feature space, we would like to know how much task-specific data is necessary to see these improvements. Therefore, we perform experiments on the RT dataset with increasing amounts of task-specific data. This could potentially benefit low-resource languages which often have access only to Wikipedia pages and little task-specific data. Finally, we ask if there is a way to approximate this task-specific data given only large, generic corpora. This again is a situation that is very common among under-resourced languages. For the OpeNER dataset, accuracy is the macro average. Bold indicates the highest performance and underline indicates those that beat the generic baseline. fore, we extract the sentences labeled as subjective from the Wikipedia, Europarl and Multi UN corpora, referred to from here on as subj-Wikipedia, subj-Europarl, subj-MultiUN. Including more subjective information seems to ameliorate this situation. This suggests that it is possible to predict the amount of improvement that you can gain by extracting subjective data from a generic dataset. Given that the classification error is still relatively large, our hypothesis was that many words which are important for classification are not found in the corpora used to train the word embeddings. In order to test this possibility, we found the lexical overlap in vocabulary between the corpora used to train the word embeddings and the RT and OpeNER corpora. Let Vtr be the set of words in a training corpus and Vtest be the set of words in each test corpus test. We also test for lexical diversity of the filtered subjectivity training corpora as a measure of vocabulary. We use the Average Type Token Ratio (ATTR) as a measure. Therefore, the Wikipedia word embeddings have a larger vocabulary yet they do not always have a representation that is helpful for the task. The more subjective datasets may lack some vocabulary, but it seems that the representations that they have are able to overcome the sparsity. Bold indicates the highest performance and underline indicates those that beat the Wikipedia baseline. After seeing the number of words which were missing, we decided to study these qualitatively. We collected a list of the words that appear in the RT and OpeNER datasets which were not found in the embeddings. A revision of each part of speech category revealed that the nouns not found in the embeddings models were often misspellings of more common nouns (fantasi, alientation) or creative use of language (vidgame, dateflick, splatterfests, artsploitation, witlessness, drippiness, naturedness, diciness, gooeyness, baaaaaaaad), both of which are important for categorization. Even more important are the adjectives (unrecommendable, unsuspenseful, uncinematic, unslick, unfakeable, snazzy) and adverbs (heartwarmingly, dullingly, repellently, forgettably, uncharismatically, appallingly) which are not found in these models. These are words that often carry the most sentiment information in a sentence. The number of words lost due to filtering for subjectivity also seems to correlate with the performance of each model, as the subj-Europarl model lost the least number of wordforms and performed the best among the filtered corpora. We test this on the Catalan Aspect-level Sentiment Dataset. Three annotators annotated opinion holders, opinion targets, and four levels of sentiment (Strong Positive, Positive, Negative and Strong Negative). Bold indicates the highest performance and underline indicates those that beat the generic baseline. most likely due to the size and breadth of the English Wikipedia corpus. Therefore, for languages that do not have large, broad Wikipedia, we recommend training separate vectors and concatenating them afterwards. In this paper, we have explored several methods for improving sentiment analysis using word embeddings. First, we demonstrated that the subjectivity of a corpus is a viable method to determine the usefulness of a corpus for the task of sentiment analysis. We then show that concatenation of vectors trained on generic and taskspecific datasets outperforms a single representation trained on both datasets. We also showed that it is possible to approximate task-specific data by extracting subjective portions from generic corpora. Finally, we showed that it is possible to use this technique to improve sentiment classification for under-resourced languages, even if there is little task-specific data available. Instead of relying on subjectivity as metric, one could naturally use polarity to determine the usefulness of a passage for training embeddings for a sentiment related task. By removing neutral passages and keeping only those with positive or negative sentiment, it may be possible to achieve similar results to ours. We plan to explore this possi-bility in the future. We also believe that these techniques could improve results on tasks other than sentiment analysis, but we leave this for future work."
180,1,5.0,5.0,3.0,False,acl_2017,train,"One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. In this work, we study the task of identifying products being bought and sold in online cybercrime forums, which exhibits particularly challenging cross-domain effects. We formulate a task that represents a hybrid of slot-filling information extraction and named entity recognition and annotate datasets consisting of data from four different forums. Each of these forums constitutes its own “fine-grained domain” in that the forums cover different market sectors with different properties, even though all forums are in the broad domain of cybercrime. We characterize these domain differences in the context of a learning-based system: supervised models see decreased accuracy when applied to new forums, and standard techniques for semisupervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques. We release a dataset of 93,924 posts from across 4 forums, with annotations for 1,938 posts.","Not only that, we show that data from one cybercrime forum is even out of domain with respect to another cybercrime forum, making this data especially challenging. In this work, we present the task of identifying products being bought and sold in the marketplace sections of these online cybercrime forums. We define a token-level annotation task where, for each post, we annotate references to the product or products being bought or sold in that post. Having the ability to automatically tag posts in this way lets us characterize the composition of a forum in terms of what products it deals with, identify trends over time, associate users with particular activity profiles, and connect to price information to better understand the marketplace. Some of these analyses only require post-level information (what is the product being bought or sold in this post?) whereas other analyses might require token-level references; we annotate at the token level to make our annotation as general as possible. Our dataset has already proven enabling for case studies on these particular forums (anon. work in press). Our task has similarities to both slot-filling information extraction (with provenance information) as well as standard named-entity recognition (NER). Compared to NER, our task features a higher dependence on context: we only care about the specific product being bought or sold in a post, not other products that might be mentioned. Having annotated a dataset, we examine basic supervised learning approaches to the product extraction problem. The left columns (posts and words per post) are calculated over all data, while the right columns are based on annotated data only. Overall, our results characterize the challenges of our fine-grained domain adaptation problem in online marketplace data. We believe that this new dataset provides a useful testbed for additional inquiry and investigation into modeling of finegrained domain differences. Annotated product tokens are underlined. The first example is quite short and straightforward. The second exhibits our annotations of both the core product (mod DCIBot) and the method for obtaining that product (sombody). the text exhibits common features of web text: abbreviations, ungrammaticality, spelling errors, and visual formatting, particularly in thread titles. Additionally, note how some words are present that might be products in other contexts, but are not here (e.g., obfuscator). Each round was followed by discussion and resolution of every post with disagreements. We benefited from members of our team who brought extensive domain expertise to the task. As well as refining the annotation guidelines, the development process trained annotators who were not security experts. The development and test sets for Darkode and Hack Forums were annotated by addition team members (five for Darkode, one for Hack Forums), and then every disagreement was discussed and resolved to produce a final annotation. The annotation was performed entirely by the authors, who are researchers in either NLP or computer security. Note that many sentences in the data are already delimited by line breaks, making the sentence-splitting task slightly easier. We performed annotation on the tokenized data so that annotations would be consistent with surrounding punctuation and hyphenated words. Our full annotation guide will be available in supplementary material. Our basic annotation principle is to annotate tokens when they are either the product that will be delivered or are an integral part of the method leading to the delivery of that product. However, the Backconnect bot example is a more straightforward transaction (the human agent is less integral), so only the product is annotated. Verbs may also be annotated, as in the case of hack an account: here hack is the method and the deliverable is the account, so both are annotated. When the product is a multiword expression (e.g., Backconnect bot), it is almost exclusively a noun phrase, in which case we annotate the head word of the noun phrase (bot). Annotating single tokens instead of spans meant that we avoided having to agree on an exact parse of each post, since even the boundaries of base noun phrases can be quite difficult to agree on in ungrammatical text. The values indicate reasonable agreement. Because we annotate entities in a context-sensitive way (i.e., only annotating those in product context), our task resembles a post-level information extraction task. However, we chose to anchor the task fully at the token level to simplify the annotation task: at the post level, we would have to decide whether two distinct product mentions were actually distinct products or not, which requires heavier domain knowledge. In light of the various views on this task and its different requirements for different potential applications, we describe and motivate a few distinct evaluation metrics below. The choice of metric will impact system design, as we discuss in the following sections. This most closely mimics our annotation process. Type-level product extraction (per post) For many applications, the primary goal of the extraction task is more in line with KBP-style slot filling, where we care about the set of products extracted from a particular post. Without a domain-specific lexicon containing full synsets of products we care about (e.g., something that could recognize the synonymity of hack and access), it is difficult to evaluate this in a fully satisfying way. However, we can approximate this evaluation by comparing the set of (lowercased, stemmed) product types in a post with the set of product types predicted by the system. This metric favors systems that consistently make correct postlevel predictions even if they do not retrieve every token-level occurrence of the product. In the rest of the cases, the variations were due to slightly different ways of describing the same product. In light of this, we also might consider asking the system to extract some product reference from the post, rather than all of them. Specifically, we compute accuracy on a post-level by checking whether a single product type extracted by the system is contained in the annotated set of product types. Because most posts feature one product, this metric is sufficient to evaluate whether we understood what the core product of the post was. Another axis of variation in metrics comes from whether we consider token-level or phrase-level outputs. As noted in the previous section, we did not annotate noun phrases, but we may actually be interested in identifying them. We can convert our token-level annotations to phrase-level annotations by projecting our annotations to the noun phrase level based on the output of an automatic parser. For products realized as verbs (e.g., hack), we leave the annotation as the single token. We consider several simple baselines for product extraction as well as two learning-based methods. Baselines A simple frequency baseline is to take the most frequent noun or verb in a post and classify all occurrences of that word type as products. A more sophisticated lexical baseline is based on a product dictionary extracted from our training data: we tag the most frequent noun or verb in a post that also appears in this dictionary. This method fails primarily in that it prefers to extract common words like account and website even when they do not occur as products. The same base feature set is used for every token. Our token-classifying SVM extracts base features on the token under consideration as well as its syntactic parent. Before inclusion in the final classifier, these features are conjoined with an indicator of their source (i.e., the current token or the parent token). Our NP-classifying SVM extracts base features on first, last, head, and syntactic parent tokens of the noun phrase, again with each feature conjoined with its token source. This task may be easier than the general tagging problem: for example, if we can effectively identify the product in the title of a post, then we do not need to identify additional references to that product in the body of the post. Therefore, we also consider a post-level model, which directly tries to select one token (or NP) out of a post as the most likely product. Structuring the prediction problem in this way naturally lets the model be more conservative in its extractions and simplifies the task, since highly ambiguous product mentions can be ignored if a clear product mention is present. Put another way, it supplies a form of prior knowledge that can be useful for the task, namely that each post has exactly one product. phrases, in the NP case) in the post. Our learning-based systems substantially outperform the baselines on the metrics they are optimized for. This lends credence to our hypothesis that picking one product suffices to characterize a large fraction of posts. Gazetteers are unavailable for Blackhat and Nulled since we have no training data for those forums. forgiving (token distinctions within noun phrases are erased). While there is room for improvement, this variant of the system is accurate enough to enable analysis of the entire Darkode forum with automatic annotation. Throughout the rest of this work, we focus on NP-level evaluation and post-level NP accuracy. However, we wish to apply our system to extract product occurrences from a wide variety of forums, so we are interested in how well the system will generalize to a new forum. In the next few sections, we explore several possible methods for improving results in the crossforum settings and attempting to build a more domain-general system. These features should generalize well across domains that the clusters are formed on: if product nouns occur in similar contexts across domains and therefore wind up in the same cluster, then a model trained on domain-limited data should be able to learn that that cluster identity is indicative of products. Results are more mixed on post-level accuracy, and overall, only one result improves significantly. Our tokenlevel annotation standard is relatively complex and time-consuming, but a researcher could quite easily provide a few exemplar products for a new forum based on just a few minutes of reading posts and analyzing the forum. To incorporate this information into our system, we add a new feature on each token indicating whether or not it occurs in the gazetteer. At training time, we scrape the gazetteer from the training set. As our system is trained on an increasing amount of target-domain data (xaxis), its performance generally improves. the target test domain as a form of partial typelevel supervision. In aggregate, gazetteers appear to provide a slight gain over the baseline system, like Brown clusters, though many of these individual improvements are not statistically significant. If we assume the domain transfer problem is more complex, we really want to leverage labeled data in the target domain rather than attempting to transfer features based only on type-level information. We consider two ways of exploiting labeled target-domain data. The first is to simply take these posts as additional training data. We also show the recall of an NP-level system on seen (Rseen) and OOV (Roov) tokens. so, the model should gain some ability to separate domain-general from domain-specific feature values. Interestingly, the system trained on Darkode is not able to make good use of labeled data from Hack Forums, and the domain-specific features actually cause a substantial drop in performance until we have included a substantial amount of data from Hack Forums. This likely indicates we are overfitting the small Hack Forums training set with the domain-specific features. In order to understand the variable performance and shortcomings of the domain adaptation approaches we explored, it is useful to examine our two initial hypotheses and characterize the datasets a bit further. To do so, we break down system performance on products seen in the training set versus novel products. Because our systems depend on lexical and character n-gram features, we expect that they will do better at predicting products we have seen before. As expected, performance is substantially higher on invocabulary products. coverage. A system trained on Darkode is therefore in some sense more domain-general than one trained on Hack Forums. Duplicating features and adding parameters to the learner also has less of a clear benefit when adapting from Darkode, when the types of knowledge that need to be added are less concrete. Note, however, that these results do not tell the full story. We present a new dataset of posts from cybercrime marketplaces annotated with product references, a task which blends IE and NER. Learning-based methods degrade in performance when applied to new forums, and while we explore methods for fine-grained domain adaption in this data, effective methods for this task are still an open question."
657,2,3.8,2.4,2.2,False,acl_2017,train,"We report efforts to interpret a Long-Short Term Memory neural network trained to recognize gender and writing instructions on a set of essays from a psychological educational intervention known as a values affirmation. Adjusting the model at test time to output sequential probabilities as each new token is encountered, rather than predicting the class holistically, we query the model with carefully constructed sentences designed to test a theoretically informed hypothesis: male versus female students write in a way that reflects a greater emphasis on independence versus interdependence, respectively. The LSTM model outperforms the baseline, and the model’s predictions to our constructed test sentences suggest modest support for these hypotheses.","These efforts are important for understanding where and why a model ex-periences shortcomings. Additionally, as we address here, finding straightforward ways to interpret these models will increase their applicability across the sciences more generally. Rather than exploring the components of the model itself, we take a hypothesis-driven approach in which we allow the model to respond (i.e., make predictions) to stimuli we devise that is of theoretical interest to the investigators. This approach is imported directly from experimental psychology in which the objective is to mechanistically understand cognition. This is a flexible approach that allows the investigator to directly devise stimuli in an intuitive manner which address questions about what the model has learned from the information it was trained on. This intervention is designed to mitigate the detrimental effects of stress on student performance. Researchers have demonstrated its effectiveness at boosting the performance of students who are under excessive amounts of stress due to having an identity that is stigmatized in academic settings (e.g. African American, or women in STEM classes). The typical format of the intervention has students select from a list of values (e.g. religion, sports, relationships) and write about why the value they selected is important to them. However, there has been little work exploring the text that students produce while they are writing these essays. We study the justifications that students provide when writing about their values. That is, because the writing prompts instruct students to select a value, and then justify that selection by writing about why it is important, we sought to understand whether these justifications differ between categories of students. In particular, we examine whether female students are more likely than male students to justify their value selection by making reference to others. Both of these hypotheses are motivated by previous psychological research. In short, we specify a set of inputs that vary along some dimension of theoretical interest, and observe the outputs (probability distributions of classes) that correspond to these inputs. To achieve this, we train the proposed neural network architecture in a supervised way with a dense softmax layer as the last layer at the end of the last time step. During test time, we use this last layer at each time step and record the outputs to understand the sequential class probabilities. We show that our methods lead to more interpretable results in the task of understanding the gender-specific characteristics of text from values affirmation essays. Linguistic Analysis of Values Affirmation To our knowledge, there are just three other papers directly addressing linguistic data in valuesaffirmation interventions. Their results found that not only were essays written by writers in the treatment condition more likely to contain such themes, but those writers in the treatment condition who wrote about these topics were also more likely to perform well academically than writers in the treatment condition who did not write about social belonging. This effect was unexpectedly reversed for students who were not under threat-White students in these data. Interpreting Neural Networks Due to their high-level performance on a number of tasks, there has been some recent work attempting to understand neural networks beyond simple performance metrics. This approach borrows directly from standard experimental psychology methods. These methods are designed to test hypotheses about why a decision maker behaves in as observed. Applying these same approaches to understanding LSTM can enable quantitative scientists to explore specific hypotheses in their data, as we demonstrate below. Our aim in this work is to develop a system that allows the applied quantitative scientist to query the network with a specific set of test input (stimuli) of his or her choosing that can highlight a question of theoretical interest. In our case, we are interested in understanding textual features of writing that make a values affirmation essay more likely to have been written by a male versus female student in the treatment or control conditions. At the beginning of their time in middle school, and at several other times throughout their middle school years, students completed a short essay in response to either a treatment or control writing prompt. The control condition was similar, but students were instead asked to pick one to three values that might be important to someone else and write about why they might be important to someone else. I like having friends and I also enjoy being funny sometimes. Control: Art might be important to someone else because they might like art, want to go to school to study art to one day become an artist. For an unrelated project, research assistants manually fixed all gross spelling and grammatical mistakes. Our classification task is to categorize an essay into one of four classes consisting of the four combinations of gender (male versus female) and experimental condition (treatment versus control). Input Layer:. All essays are first tokenized, and each word in the essay is mapped to an index in a dictionary. The essays are fed as input as a list of constantly-sized integers to the neural network-shorter essays were padded to create inputs of the same length. writing about one versus three values). If these vectors are found in the GloVe dataset, they are derived from the corresponding GloVe vector. This layer is trainable. During test time, no units are dropped. LSTM layer: The output from the dropout layer is fed to an LSTM layer to capture long term and short term dependencies. This modification helps us to understand how specified sequences of text contribute to the classification decision. As described above, our model emits class probabilities at each word. In order to establish that these probabilities are valid ways to interpret the model, we compare them with coefficients from our baseline classifier. Because the probability change for the LSTM is context dependent, each token is possibly associated with multiple changes in probability. Thus, for each token, we obtained the average probability change for each class. This provides some evidence that the probability shifts for a token measure the same underlying construct as SVM coefficients. In this example, we use an example essay written by a female student in the treatment condition: They are important because I like listening to music and playing my flute. I like having friends and I also enjoy being funny sometimes. There are a number of noteworthy points in the example essay where the probabilities shift in a way that would be expected based on the nature of the writing prompt and the interests of male and female American middle schoolers. For instance, after the first five words, the model is beginning to predict that the essay is from the treatment condition. The first five words, They are important because I are more typical for writing in response to the treatment prompt (write about why the values are important to you) as opposed to the control prompt (write about why the values are important to someone else). Another noteworthy shift here is when the model encounters the word flute, an instrument that is relatively gendered in American culture (i.e. it is more strongly associated with women than with men). At this point, the model is positive that this is an essay from the treatment condition, written by a female student. Having validated the approach to interpreting the model and shown an example case, we next turn to asking richer questions about our data. In particular, we present evidence that suggests that male writers and female writers use slightly different justifications for selecting values. After adjusting to the model to obtain class probabilities through the essay as described above, we adapted methods from experimental psychology to query what the model has learned. In psychological experiments, human participants are often given linguistic items that differ along some key dimension of interest while the experimenter records the responses to these items. We took this approach to investigating the model, with the responses consisting of the sequential probabilities. The final word (justification) is marked as j. The dimension of interest that was systematically varied was the type of justification used in a stereotypical treatment essay. athletic ability, getting good grades, etc), and J was a series of bigrams that was one of two types-either selffocused justifications, or other-focused justifications. As described in the introduction, our expectation is that female students will be more likely to use other-focused justifications in which they describe their value in relation to other people because this reflects a greater degree of interdependent self-construal (e.g. For male students, we expect them to be more likely to use self-focused justi-fications, reflecting a greater degree of independent self-construal. Such a justification would take them form of their explaining a value by making some reference directly to the self doing, thinking, or performing something (e.g. This methodological approach can tell us whether one of these classes is more or less likely to use other-focused versus self-focused justifications. The final word (justification) is marked as j. As expected, the probabilities for a given value are identical for a given class until the justification word is encountered, at which points the probabilities diverge. Because these probabilities are compositional in nature (i.e. In mixed-effects terminology, these model had fixed effects terms for class, time, and their interaction, and a three fully crossed random effects-one for sentence fragment, one for the value being justified, and one for the different justifications. Each of these crossed random effects has its own vector of intercept and time coefficients. All priors were set to a standard normal distribution. Because our hypotheses were primarily about how male and female students would justify their selection in values, we then subtracted these two difference distribution, yielding a posterior distribution that quantifies the degree to which the lstm model believes that female students are more likely to use a given type of justification. The analysis of selffocused justifications is in the opposite direction, with the effect suggesting stronger support for the hypothesis. We have described a novel way to explore neural networks. By treating the model as a input-output device and carefully crafting stimuli that systematically varies along a dimension of interest, we can map systematic differences in the output to the stimulus input. Doing so allows us to understand whether the model has learned about this dimension, and consequently, whether there is information of this type in the training data, even if the same language does not appear verbatim. Our work also extends literature in psychology, as this is the first instance in which the justifications of values-affirmation essays were explored. Future work should disentangle the degree to which these justifications influence the effectiveness of the intervention on academic outcomes."
483,2,2.38,4.0,3.38,False,acl_2017,train,"Argumentation mining seeks to uncover the argument structure present in argumentative text. In order to determine this structure, one must understand how different individual components of the overall argument are linked. General consensus in this field dictates that the argument components form a hierarchy of persuasion, which manifests itself in a tree structure. This work provides the first neural network-based approach to argumentation mining, focusing on the dual tasks of extracting links between argument components, and classifying types of argument components. We propose to use a joint model based on a Pointer Network architecture to simultaneously solve these tasks. In doing so, we construct a joint model that simultaneously attempts to learn the type of argument component, as well as continuing to predict links between argument components. The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, achieving far superior performance than a regular Pointer Network model. Our results show that optimizing for both tasks is crucial for high performance.","One fundamental assumption when working with argumentative text is the presence of Arguments Components (ACs). There are two key assumptions our work makes going forward. ACs have already been identified. Specifically, a given AC can only have a single outgoing link, but can have numerous incoming links. The left side shows raw text that has been annotated for the presence of ACs. Squiggly and straight underlining means an AC is a claim or premise, respectively. The ACs in the text have also been annotated for links to other ACs, which is show in the right figure. Squiggly verse straight underlining differentiates between claims and premises, respectively. The ACs have been annotated as to how they are linked, and the right side of the figure reflects this structure. We also specify the type of AC, with the head AC marked as a claim and the remaining ACs marked as premises. Lastly, we note that the order of arguments components can be a strong indicator of how components should related. A PN is a sequence-to-sequence model that outputs a distribution over the encoding indices at each decoding timestep. This is equivalent to only allowing backward links. Alternatively, the PN we implement differs from the original model in that we decode for the same number of timesteps as there are input components. We also propose a joint PN for both extracting links between ACs and predicting the type of AC. Lastly, in respect to the broad task of parsing, our model is flexible because it can easily handle non-projective, multiroot dependencies. In this section, we discuss previous argumentation mining work that deals with the subtask pipeline we discussed in the previous section. Unlike these methodologies, recent work offers data-driven approaches to the task of predicting links between ACs. The authors train an SVM with various semantic and structural features. The ILP framework is effective in enforcing a tree structure between ACs when predictions are made from otherwise naive base classifiers. The authors linearize input parse graphs using a depth-first search, allowing it to be consumed as a sequence, achieving state-of-the-art results on several syntactic parsing datasets. The text is annotated with brackets, in an original attempt to provide easy input into a recursive neural network. However, standard recurrent neural networks can take in complete sentence sequences, brackets included, and perform competitively with a recursive neural network. In this section we will describe how we use a PN for the problem of extracting links between ACs. We begin by giving a general description of the PN model. The original motivation for a pointer network was to allow networks to learn solutions to algorithmic problems, such as the traveling salesperson and convex hull, where the solution is a sequence over candidate points. We will discuss shortly why we need to modify the original definition of m for our application. In practice, the PN has two separate LSTMs, one for encoding and one for decoding. Therefore, by taking the softmax of ui, we are able to create a distribution over the input. Therefore, at encoding timestep i, the model is fed a representation of Ci. At encoding step i, the encoding LSTM produces hidden layer ei, which can be thought of as a hidden representation of AC Ci. In order to make the PN applicable to the problem of link extraction, we explicitly set the number of decoding timesteps to be equal to the number of input components. The result of this distribution will indicate to which AC component Ci links. Finally, we note that we modify the PN structure to have a Bidirectional LSTM as the encoder. The decoder remains a standard forward LSTM. At each timestep of the decoder, the network takes in the representation of an AC. Up to this point, we focused on the task of extracting links between ACs. For example, claims do not have an outgoing link, so knowing the type of AC can aid in the link prediction task. Predicting AC type is a straightforward classification task: given AC Ci, we need to predict whether it is a claim or premise. For encoding timestep i, the model creates hidden representation ei. This can be thought of as a representation of AC Ci. The dimensionality of Wcls, bcls is determined by the number of classes. Lastly, we use softmax to form a distribution over the possible classes. As we have previously mentioned, our work assumes that ACs have already been identified. That is, the token sequence that comprises a given AC is already known. The order of ACs corresponds directly to the order in which the ACs appear in the text. ambiguity in this ordering. There are three AC types in this corpus: major claim, claim, and premise. Also, in this corpus individual structures can be either trees or forests. We follow the creators of the corpus and only evaluate ACs within a given paragraph. Unlike the persuasive essay corpus, each text in this corpus is itself a complete example, as well as a single tree. This corpus contains only two types of ACs: claim and premise. The annotation of argument structure of the microtext corpus varies from the persuasive essay corpus; ACs can be linked to other links, as opposed to ACs. Once training is completed, we select the model with the highest validation accuracy (on the link prediction task) and evaluate it on the held-out test set. At test time, we take a greedy approach and select the index of the probability distribution (whether link or type prediction) with the highest value. However, we do not disable the model from predicting this class: the model was able to avoid predicting this class on its own. Neither of these classifiers enforce structural or global constraints. For example, the model attempts to enforce a tree structure among ACs within a given paragraph, as well as using incoming link predictions to better predict the type class claim. The EG models the potential argument structure, and offers a global optimization objective that the base classifiers attempt to optimize by adjusting their individual weights. The performance on the Microtext corpus is very encouraging for several reasons. First, the fact that the model can perform so well with only a hundred training examples is rather remarkable. For example, only premises can have outgoing links, and there can be only one claim in an AC. As for the other neural models, the BLSTM model performs competitively with the ILP Joint Model on the persuasive essay corpus, but trails the performance of the Joint Model. Conversely, the BLSTM model must encode information relating to type as well as link prediction in a single hidden representation. Another interesting outcome is the importance of the fully-connected layer before the LSTM input. The results show that this extra layer of depth is crucial for good performance on this task. Without it, the Joint Model is only able to perform competitively with the Base Classifier. The results dictate that even a simple fullyconnected layer with sigmoid activation can provide a useful dimensionality reduction for feature representation. Finally, the PN model, only optimized for link prediction, suffers a large drop in performance, conveying that the dual optimization of the Joint Model is crucial for high performance in the link prediction task. Regarding link prediction, BOW features are clearly the most important, as their absence results in the highest drop in performance. Conversely, the presence of structural features provides the smallest boost in performance, as the model is still able to record stateof-the-art results compared to the ILP Joint Model. This shows that the Joint Model is able to capture structural cues through sequence modeling and semantics. When considering type prediction, both BOW and structural features are important, and it is the embedding features that provide the least benefit. First, it is not surprising to see that the model performs best when the sequences are the shortest. As the sequence length increases, the accuracy on link prediction drops. This is possibly due to the fact that as the length increases, a given AC has more possibilities as to which other AC it can link to, making the task more difficult. Conversely, there is actually a rise in no link prediction accuracy from the second to third row. This is likely due to the fact that since the model predicts at most one outgoing link, it indirectly predicts no link for the remaining ACs in the sequence. Since the chance probability is low for having a link between a given AC in a long sequence, the no link performance is actually better in longer sequences. The Joint Model records state-of-the-art results on the persuasive essay corpus, as well as achieving state-of-the-art results for link prediction on the microtext corpusThe results show that jointly modeling the two prediction tasks is crucial for high performance. supporting or attacking; this is the fourth subtask in the pipeline."
21,2,4.57,4.0,4.0,True,acl_2017,train,"Finding the correct hypernyms for entities is essential for taxonomy learning, finegrained entity categorization, knowledge base construction, etc. Due to the flexibility of the Chinese language, it is challenging to identify hypernyms in Chinese accurately. Rather than extracting hypernyms from texts, in this paper, we present a transductive learning approach to establish mappings from entities to hypernyms in the embedding space directly. It combines linear and non-linear embedding projection models, with the capacity of encoding arbitrary language-specific rules. Experiments on real-world datasets illustrate that our approach outperforms previous methods for Chinese hypernym prediction.","For example, the word country is the hypernym of the entity Canada. While these approaches have relatively high precision over English corpora, extracting hypernyms for entities is still challenging for Chinese. For instance, there are no word spaces, explicit tenses and voices, and distinctions between singular and plural forms in Chinese. The order of words can be changed flexibly in sentences. However, we argue that these projection-based methods may have three potential limitations: (i) Only positive is-a relations are used for projection learning. The distinctions between is-a and not-is-a relations in the embedding space are not modeled. ii) These methods lack the capacity to encode linguistic rules, which are designed by linguists and usually have high precision. In this paper, we address these limitations by a two-stage transductive learning approach. In the initial stage, we train linear projection models on positive and negative training data separately and predict isa relations jointly. In the transductive learning stage, the initial prediction results, linguistic rules and the non-linear mappings from entities to hypernyms are optimized simultaneously in a unified framework. This optimization problem can be efficiently solved by blockwise gradient descent. Pattern based methods identify is-a relations from texts by handcraft or automatically generated patterns. Automatic approaches mostly use iterative learning paradigms such that the system learns new is-a relations and patterns simultaneously. However, the proposed method is not entirely language-specific and has the potential to be adapted to other languages. We also discuss some potential applications of our method. Inference based methods take advantage of distributional similarity measures (DSM) to infer relations between words. They assume that a hypernym may appear in all contexts of the hyponyms and a hyponym can only appear in part of the contexts of its hypernyms. The reason why DSM is not effective for Chinese is that the contexts of entities in Chinese are flexible and sparse. Encyclopedia based methods take encyclopedias as knowledge sources to construct taxonomies. These approaches have higher accuracy than mining hypernym relations from texts directly. However, they heavily rely on existing knowledge sources and are difficult to extend to different domains. To tackle these challenges, word embedding based methods directly model the task of hypernym prediction as learning a mapping from entity vectors to their respective hypernym vectors in the embedding space. They can reduce data noise by avoiding direct parsing of Chinese texts, but still capture the linguistic regularities of is-a relations based on word embeddings. In our paper, we extend these approaches by modeling non-linear mappings from entities to hypernyms and adding linguistic rules via a unified transductive learning framework. This section begins with a brief overview of our approach. After that, the detailed steps and the learning algorithm are introduced in detail. Denote xi as the embedding vector of word xi, pre-trained and stored in a lookup table. It aims to minimize the prediction errors based on the human labeled data, the initial model prediction and linguistic rules. It also employs nonlinear mappings to capture linguistic regularities of is-a relations other than linear projections. The initial stage models how entities are translated to their hypernyms or non-hypernyms by projection learning. However, this model does not capture the semantics of not-isa relations. This approach is equivalent to learning two separate translation models within the same semantic space. Higher prediction score indicates there is a larger probability of an is-a relation between xi and yi. However, we found that this practice is less efficient and the performance does not improve significantly. Higher confidence score means that there is a larger probability that the models can predict whether there is an is-a relation between xi and yi correctly. This score gives different data instances different weights in the transductive learning stage. Although linear projection methods are effective for Chinese hypernym prediction, it does not encode non-linear transformation and only leverages the positive data. Although linguistic rules can only cover a few circumstances, they are effective to guide the learning process. In our model, let C be the collection of linguistic rules. In our work, we extend their work for our task, modeling non-linear mappings from entities to hypernyms. For is-a relations, we find that if y is the hypernym of x, it is likely that y is the hypernym of entities that are semantically close to x. However, our method can deal with these cases by using some simple heuristics. if we know United States is a country, we can infer country is the hypernym of similar entities such as Canada, Australia, etc. For example, the score of is-a relations between United State and country will propagate to pairs such as (Canada, country) and (Australia, country) by random walks. This means we minimize the training error and encourage F to have a smooth propagation with respect to the similarities among pairs defined by Eq. For example, nouns close to country in our Skip-gram model are region, department, etc. They are not all correct hypernyms of United States, Canada, Australia, etc. A basic approach to learn the optimal values of F is via gradient descent. Based on Eq. To speed up the learning process, we introduce a blockwise gradient descent technique. From the definition of Eq. Therefore, the original optimization problem can be decomposed and solved separately according to different candidate hypernyms. LetH be the collection of candidate hypernyms in DU. This technique can be also viewed as optimizing Eq. Each pair is labeled with an is-a or not-is-a tag. For each pair in BK, we ask multiple human annotators to label the tag and discard the pair with inconsistent labels by different annotators. The word embeddings are pre-trained by ourselves on the Chinese corpus. It can be seen that the performance of our method is generally better in BK than FD. See submitted dataset. and negative data. Interested readers may refer to their papers for the experimental results. It shows that the semantics of is-a relations are better modeled by multiple projection models, with a slightly improvement in F-measure. To make fair comparison, we take the training data as the same knowledge source to train models for all methods. pus over which we train word embedding models. The main reason of the improvement may be that the projection models have a better generalization power by applying an iterative learning paradigm. As seen, the classification models using addition and subtraction have similar performance in two datasets, while the concatenation strategy outperforms previous two approaches. The most case is that the features in that work are designed specifically for the Chinese Wikipedia category system. lines. Therefore, our method is effective to predict hypernyms of Chinese entities. We state that currently we only use a few handcraft linguistic rules in our work. The proposed approach is a general framework that can encode arbitrary numbers of rules of any kind and in any language. We analyze correct and error cases in the experiments. We can see that our method is generally effective. However, some mistakes occur mostly because it is difficult to distinguish strict is-a and topic-of relations. For example, the entity Nuclear Reactor is semantically close to Nuclear Energy. Two possible solutions are: i) adding more negative training data of this kind; and ii) constructing a large-scale thematic lexicon automatically from the Web. To examine how our method can benefit hypernym prediction for the English language, we use two standard datasets in this paper. The results suggest that although our method is not necessarily the state-of-the-art for English hypernym prediction, it has several potential applications. From the experiments, we can see that the proposed approach outperforms the state-of-the-art methods for Chinese hypernym prediction. Although the English language is not our focus, our approach still has relatively high performance. If the task is to predict relations between words when it is related to a specific domain or the contexts are sparse, even for English, traditional pattern-based methods are likely to fail. Our method can predict the existence of relations without explicit textual patterns and requires a relatively small amount of pairs as training data. Under-resourced Language Learning. This is because it does not require deep NLP parsing of sentences in a text corpus. In summary, this paper introduces a transuctive learning approach for Chinese hypernym prediction. By modeling linear projection models, linguistic rules and non-linear mappings, our method is able to identify Chinese hypernyms with high accuracy. Experiments show that the performance of our method outperforms previous approaches. We also discuss the potential applications of our method besides Chinese hypernym prediction."
440,2,4.0,4.0,4.0,True,acl_2017,train,"We propose a new A* CCG parsing model in which the probability of a tree is decomposed into factors of CCG categories and its dependency structure both defined on bi-directional LSTMs. Our factored model allows the precomputation of all probabilities and runs very efficiently, while modeling sentence structures explicitly via dependencies. Our model achieves the stateof-the-art results on English and Japanese CCG parsing1.","Although much ambiguity is resolved with this supertagging, some ambiguity still remains. The key idea is to predict the head of every word independently as in Eq. Unfortunately, this formulation loses the efficiency of the original one since it needs to compute a recursive neural network every time it searches for a new node. Our model does not resort to the recursive networks while modeling tree structures via dependencies. Besides English, we provide experiments on Japanese CCG parsing. Japanese employs freer word order dominated by the case markers and a deterministic rule such as the attach low method may not work well. CCG has a nice property that since every category is highly informative about attachment decisions, assigning it to every word (supertagging) determines most of its syntactic structure.   Their model looks for the most probable y given a sentence x from the set Y (x) of possible CCG trees under the model of Eq. Since this score is factored into each supertag, they call it supertag-factored. ci,j is the sequence of categories on such Viterbi parse, and thus b is called the Viterbi inside score, while a is the approximation (upper bound) of the Viterbi outside score. The heuristics a gives an upperbound on the true Viterbi outside score (i.e., admissible). We base our model on this bi-LSTM architecture, and extend it to modeling a head word at the same time. We provide a simple solution to this problem by explicitly modeing bilexical dependencies. Though the model is first-order, bi-LSTMs enable conditioning on the entire sentence and lead to the state-of-the-art performance. Note that this mechanism is similar to modeling of the supertag distribution discussed above, in that for each word the distribution of the head choice is unigram and can be precomputed.       We can calculate the dependency terms in Eq. Let the current popped edge be Ai,k, which will be combined with Bk,j into Ci,j. We regard root(hCi,j) as an outside word since its head is undefined yet. Note that the dependency component of it is the same for every word. First we map every word xi to their hidden vector ri with bi-LSTMs. For the backward rules, the conversions are defined as the reverse of the corresponding forward rules. Boku wa eigo wo hanasi tai. d) HEADFIRST Boku wa eigo wo hanasi tai. Forutnatelly the dependency annotations of CCGbank matches LEWISRULE above in most cases and thus they can be a good approximation to it. HEADFINAL Among SOV languages, Japanese is known as a strictly head final language, meaning that the head of every word always follows it. Inspired by this tradition, we try a simple HEADFINAL rule in Japanese CCG parsing, in which we always select the right argument as a head. HEADFIRST We apply the similar idea as HEADFINAL into English. Another difficulty is that in English CCGbank a combinatory rule name is not annotated explicitly. RULE. First, since the model with LEWISRULE is trained on the CCGbank dependencies, at inference, occasionally the two components Pdep and Ptag cause some conflicts on their predictions. For example, the true Viterbi parse may have a lower score in terms of dependencies, in which case the parser slows down and may degrade the accuracy. HEADFIRST, in contract, does not suffer from such conflicts. Second, by forcing the direction of arcs, the prediction of heads becomes easier, meaning that the dependency predictions become more reliable. We extend the existing tri-training method to our models and apply it to our English parsers. Tri-training is one of the semi-supervised methods, in which the outputs of two parsers on unlabeled data are intersected to create (silver) new training data. We simply combine the two previous approaches. For LEWISRULE, we extract the original dependency annotations of CCGbank. For HEADFIRST, we extract the head first dependencies from the CCG trees. We perform experiments on English and Japanese CCGbanks. We do not use affix vectors as affixes are less informative in Japanese. We use the same parameter settings as English for bi-LSTMs, MLPs, and optimization. One issue in Japanese experiments is evaluation. The Japanese CCGbank is encoded in a different format than the English bank, and no standalone script for extracting semantic dependencies is available yet. We perform this conversion for both gold and output CCG trees and calculate the (unlabeled) attachment accuracy. Though this is imperfect, it can detect important parse errors such as attachment errors and thus can be a good proxy for the performance as a CCG parser. Effect of Dependency We first see how the dependency components added in our model affect the performance. Choice of Dependency Conversion Rule To our surprise, our simple HEADFIRST strategy always leads to better results than the linguistically motivated LEWISRULE. From another viewpoint, this fixed direction means that the constituent structure behind a (head first) dependency tree is unique. Since the constituent structures of CCGbank trees basiclly follow the normal form (NF), we hypothesize that the model learned with HEADFIRST has an ability to force the outputs in NF automatically. We compare our proposed parser against neuralccg and our reimplementation of EasySRL. Our parser lags behind in the overall parsing speed to neuralccg and EasySRL reimpl. Method Category Bunsetsu Dep. The result suggests the importance of modeling dependencies in some languages, at least Japanese. By explicitly modeling the dependency structure, we do not require any deterministic heuristics to resolve attachment ambiguities, and keep the model factored so that all the probabilities can be precomputed before running the search. Our parser efficiently finds the the optimal parses, achieving the state-of-the-art performance in both English and Japanese parsing."
769,2,3.0,3.57,3.0,True,acl_2017,train,"We study a symmetric collaborative dialogue setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.","However, human conversation often requires richer dialogue states and more nuanced, pragmatic dialogue acts. While these models capture open-ended aspects of dialogue, the lack of structured dialogue state prevents them from being directly applied to settings that require interfacing with structured knowledge. In order to bridge the gap between the two types of systems, we focus on a symmetric collabora-tive dialogue setting, which is task-oriented but encourages open-ended dialogue acts. In our setting, two agents, each with a private list of items with attributes, must communicate to identify the unique shared item. Such conversational implicature is lost when interpreting it as simply requesting information about Columbia. The difference is that we structure entities as a knowledge graph; as the dialogue proceeds, new nodes are added and new context is propagated on the graph. We compare DynoNet with baseline neural models and a strong rule-based system. The results show that DynoNet can perform the task with humans efficiently and naturally; it also captures some strategic aspects of human-human dialogues. The contributions of this work are: (i) a new symmetric collaborative dialogue setting and a large dialogue corpus that push the boundaries of existing dialogue systems; (ii) DynoNet that integrates semantically rich utterances with structured knowledge to represent open-ended dialogue states; (iii) multiple automatic metrics based on bot-bot chat and a comparison of third-party and partner evaluation. We introduce a collaborative task between two agents and describe the human-human dialogue collection process. We show that our data exhibits diverse, interesting language phenomena. Each knowledge base includes a list of items, where each item has a value for each attribute. There is a shared item that A and B both have; their goal is to converse with each other to determine the shared item and select it. Formally, an agent is a mapping from its private KB and the dialogue thus far (sequence of utterances) to the next utterance to generate. A dialogue is considered successful when both agents correctly select the shared item. This setting has parallels in human-computer collaboration where each agent has different expertise. To elicit linguistic and strategic variants, we generate a random scenario for each task by varying the number of items, the number attributes, and the distribution of values for each attribute. See Appendix B for details of scenario generation. An utterance is defined as a message sent by the user. The average utterance length is shorter due to the informality of the chat, however, a user usually sends multiple utterances in one turn. Even the more standard ones can be challenging due to coreference and conjunctions. Coreference occurs frequently when people check attributes jointly. Sometimes the mentions are dropped: it simply continues from the previous partner utterance. People occasionally use external knowledge to group items with out-of-schema attributes (e.g., gender based on names, location based on schools). One strategic aspect of this task is the order in which attributes are mentioned. As the conversation unrolls, utterances are embedded and incorporated to node embeddings of mentioned entities. Further, mentioned nodes pass on this new information to their neighbors so that related entities (e.g., those in the same row or column) also receive the information. In our example, jessica and josh both receive new context when google and columbia are mentioned. Edges between nodes represent their relations. The node embedding V is built from three parts: structural properties of an entity defined by the KB, embeddings of utterances in the dialogue history, and message passing between neighboring nodes. Node Features. Recursive Node Embeddings. We propagate information between nodes according to the structure of the knowledge graph. Therefore, we want this information to be sent to item nodes connected to columbia, and one step further to other attributes of these items because they might be implicitly referred to (google which was mentioned previously) or mentioned next (jessica and josh which will be generated).   The node embedding is updated periodically whenever Mt(v) is updated by a new utterance. In this section, we describe how an utterance is embedded and generated. Our model is based on a recurrent neural network (RNN). We use the last hidden state hn as the utterance embedding u which updates the mention vectors. Given (updated) embeddings of each node, we use another RNN to generate the next utterance, where each token is either copied from a node or generated from the vocabulary. In our setting, the role of an entity is governed by its relation to other entities and relevant utterances. Note that we do not use an embedding matrix for any entity when computing V (v). Furthermore, in the input embedding layer of the LSTM, we represent an entity by its type embedding concatenated with its node embedding. Accordingly, a regular word is represented as its word embedding concatenated with a zero vector of the same dimension as V. We maximize the likelihood of each utterance from perspectives of both agents. We randomly split the data into train, dev, and test sets and only train on successful dialogues. We compare DynoNet with its static cousion (StanoNet) and a rule-based system (Rule). Rule maintains weights for each entity and each item in the KB to decide what to talk about and which item to select. It has a pattern-matching semantic parser, a rule-based policy, and a templated generator. See Appendix G for details. We perform both automatic evaluation and human evaluation to test syntactic, semantic, and pragmatic competence of each system. Automatic Evaluation. For Rule, the decision about which action to take is written in the rules, while StanoNet and DynoNet learned to behave in a more human-like way, frequently informing and asking questions. To measure effectiveness, we compute the success rate per turn (CT) and per selection (CS). Next, we investigate the strategies leading to these results. main size. Humans and DynoNet strategically focus on a few attributes and entities, whereas Rule needs almost twice entities to achieve similar success rates. This suggests that the effectiveness of Rule mainly comes from large amounts of unselective information, which is consistent with feedback from their human partners. Partner Evaluation. Rule always informs true facts but poorly follows the partner. Third-party Evaluation. Surprisingly, there is a discrepancy between the two evaluation modes. Our model has two novel designs: entity abstraction and message passing for node embeddings. This shows that DynoNet benefits from contextually-defined, structural node embeddings rather than ones based on a classic lookup table. There has been a recent surge of interest in endto-end task-oriented dialogue systems. However, the memory is unstructured and interfacing with KBs relies on API calls, whereas our model embeds both the dialogue history and the KB structurally. In conclusion, we believe the symmetric collaborative dialogue setting and our dataset provide unique opportunities at the interface of traditional task-oriented dialogue and open-domain chat. We also offered DynoNet as a promising means for open-ended dialogue state representation. It would be interesting to integrate supervised learning with game-theoretic approaches in future."
723,3,4.42,3.75,4.0,True,acl_2017,train,"We present in this paper a novel framework for morpheme segmentation which uses the morpho-syntactic regularities preserved by word representations, in addition to orthographic features, to segment words into morphemes. This framework is the first to consider vocabulary-wide syntactico-semantic information for this task. We also analyze the deficiencies of available benchmarking datasets and introduce our own dataset that was created on the basis of compositionality. We validate our algorithm across datasets and present state-of-the-art results.","This has led to an oversegmentation of words because a change of the surface form pattern is a necessary but insufficient indication of a morphological change. A limitation of such approach is due to noise in word representations, even more so in the case of rare words. To address this issue, we introduce a new framework (MORSE), the first to bring semantics into morpheme segmentation both on a local and a vocabulary-wide level. That is, when checking for the morphological relation between two words, we not only check for the semantic relatedness of the pair at hand (local), but also check if the difference vectors of pairs showing similar orthographic change are consistent (vocabulary-wide). To evaluate it from an orthographic corpus-wide perspective, we check for the size of each cluster of an affix. To evaluate each pair in a cluster locally from a semantic standpoint, we check if a pair of words in a valid affix cluster are morphologically related by checking if its difference vector is consistent with other members in the cluster and if the words in the pair are semantically related (i.e. close in the vector space). We would expect such a pair to fail the last two local evaluation methods. Our proposed segmentation algorithm is evaluated using benchmarking datasets from the Morpho Challenge (MC) for multiple languages and a newly introduced dataset for English which compensates for lack of discriminating capabilities in the MC dataset. Experiments reveal that our proposed framework not only outperforms the widely used approach, but also performs better than published state-of-the-art results. The central contribution of this work is a novel framework that performs morpheme segmentation resulting in new state-of-the-art results. To the best of our knowledge this is the first unsupervised approach to consider the vocabulary-wide semantic knowledge of words and their affixes in addition to relying on their surface forms. Moreover we point out the deficiencies in the MC datasets with respect to the compositionality of morphemes and introduce our own dataset free of these deficiencies. Given the less complex nature of morpheme segmentation in comparison to the other tasks, most systems developed for morpheme segmentation have been unsupervised to minimally supervised with the minimal supervision mainly used for parameter tuning. His work falls under the framework of Letter Successor Variety (LSV) which builds on the hypothesis that predictability of successor letters is high within morphemes and low otherwise. In other words, they aim to minimize describing the lexicon of morphs as well as minimizing the description of an input corpus. In contrast to the previously mentioned generative approaches of MDL and AG, this method takes a discriminative approach and allows for the inclusion of a larger set of features. All the previously mentioned morpheme segmenters rely solely on orthographic features of morphemes. They use LSA to generate word representations and then evaluate if two words are morphologically related based on semantic relatedness, as well as deterministic orthographic methods. Similarly, MORSE introduces semantic information into its morpheme segmenters via distributed word representations while also relying on orthographic features. The key limitation of previous frameworks that rely solely on orthographic features is the resulting over-segmentation. Our framework combines semantic relatedness with orthographic relatedness to eliminate such error. These word representations capture the semantics of the vocabulary through statistics over the context in which they appear. As a high level description, we first learn all possible affix transformations (morphological rules) in the language from pairs of words from an orthographic standpoint. Now we formalize the objects we learn in MORSE and the scores (orthographic and semantic) used for validation. This constitutes the training stage. Finally, we formalize the inference stage, where we use these objects and scores to perform morpheme segmentation. This pair of words is related via r on an orthographic level, but this score reflects the validity of the morphological relation via r on a semantic level. This pair of words is related via r on an orthographic level, but this score reflects the semantic relatedness between the pair. In this stage we perform morpheme segmentation using the knowledge gained from the first stage. We first introduce some notation. In other words, we divide the rules to those where an affix is added (Radd) and to those where an affix is replaced (Rrep). We treat prefixes similarly. The program terminates when both optimization problems are unfeasible. We describe in this section the experiments done to assess the performance of MORSE. First, the performance is assessed intrinsically on the task of morpheme segmentation and against the most widely used morpheme segmenter: Morfessor. To evaluate the language agnosticity of the algorithm, we perform evaluation across three languages of varying morphology levels: English, Turkish, Finnish, with Finnish being the richest in morphology and English being the poorest. Second, we show the inadequacies of benchmarking gold datasets for this task and describe a new dataset that we create to address the inadequacy. For every language, two datasets are supplied: training and development. In this section we pinpoint the weaknesses of the MC dataset in assessing the relative performance of different morpheme segmenters. Non-compositional segmentation: One of the key requirements of morpheme segmentation is the compositionality of the meaning of the word from the meaning of its morphemes. This requirement is violated on multiple occasions in the MC dataset. Trivial instances: The second weakness in the MC dataset is due to abundance of trivial instances. For genetive cases, segmenting at the apostrophe leads to perfect precision and recall, and thus such instances are deemed trivial. In the case of hyphenated words, segmenting at the hyphen is a correct segmentation with a very high probability. There are instances of wrong segmentations possibly due to human error. Another issue, which is hard to avoid, is ambiguity of segmentation boundaries. In such situations, the MC dataset favors complete affixes rather than complete lemmas. This also favors MDL-based segmenters. For the reasons mentioned above, we decide to create a new dataset for English gold morpheme segmentations with compositionality guiding the annotations. The segmentation criterion was to segment the word to the largest extent possible while preserving its compositionality from the segments. One of the main claims of this paper is stating that following the MDL principle (such as Morfessor) will lead to over-segmentation. This oversegmentation happens specifically when the meaning of the word does not follow from the meaning of its morphemes. MORSE does not suffer from this issue owing to its use of a semantic model. We compare MORSE with Morfessor, and place the performance alongside the state-of-the-art published results. This supports our claim for the need of semantic cues in morpheme segmentation, and also validates the method used in this paper. With English being considered a less systematic language in terms of the orthographic structure of words, semantic cues are of larger need, and hence a system which relies on semantic cues is expected to perform better. On the other hand, Morfessor surpasses MORSE in performance on Finnish by a large margin as well, especially in terms of recall. We hypothesize that the richness of morphology in Finnish led to suboptimal performance of MORSE. In a language with productive morphology, limiting its vocabulary results in a lower chance of finding morphologically related word pairs. This negatively impacts the training stage of MORSE which relies on the availability of such pairs. We show how the detection of rules is affected by considering the number of candidate rules detected as well as the number of candidate morphologically related word pairs detected. This confirms our hypothesis. This issue can be directly attributed to the limited vocabulary size in MORSE. With the increase in processing power, and thus larger vocabulary coverage, MORSE is expected to perform better. We next compare MORSE against published state-of-the-art results. The more segments a system generates, the worse is its performance. One of the benefits of MORSE against other frameworks such as MDL is its ability to identify the lemma within the segmentation. The lemma would be the last non-segmented word in the iterative process of segmentation. Hence, an advantage of our framework is its easy adaptability into a lemmatizer and even a stemmer. Another key aspect which is not present in some of the competitive systems is the need for a small dataset for hyperparameter tuning. This is a point in favor of completely unsupervised systems such as Morfessor. On the other hand, these hyperparameters could allow for flexibility. As one would expect, increasing the hyperparameters, in general, leads to a stricter search space and thus increases precision and decreases recall. Putting these results in perspective, the user of MORSE is given the capability of controlling for precision and recall based on the needs of the downstream task. Moreover, to check for the level of dependency of MORSE on a set of gold morpheme segmentations for tuning, we check for the variation in performance with respect to size of tuning data. This reflects the minimal dependency on gold morpheme segmentations. As for the inference stage of MORSE, the greedy inference approach limits its performance. In other words, a wrong segmentation at the beginning will propagate and result in consequent wrong segmentations. This opens the stage for further research on a more optimal inference stage and a more global modeling of orthographic morphological transformations. In this paper, we have presented MORSE, a first morpheme segmenter to consider semantic structure at this scale (local and vocabulary-wide). We show its superiority over state-of-the-art algorithms using intrinsic evaluation on a variety of languages. We also pinpointed the weaknesses in current benchmarking datasets, and presented a new dataset free of these weaknesses. For future work, we plan to address the limitations of MORSE: minimal supervision, greedy inference, and concatenative orthographic model. Moreover, we plan to computationally optimize the training stage for the sake of wider adoption by the community."
627,1,4.0,3.0,4.0,True,acl_2017,train,"This paper proposes KB-InfoBot — a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the KB to retrieve entries based on their attributes. However, such symbolic operations break the differentiability of the system and prevent endto-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced “soft” posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users. We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards personalized dialogue agents.","Apple Siri, Microsoft Cortana, Google Allo). These agents can perform simple tasks, answer factual questions, and sometimes also aimlessly chit-chat with the user, but they still lag far be-hind a human assistant in terms of both the variety and complexity of tasks they can perform. In particular, they lack the ability to learn from interactions with a user in order to improve and adapt with time. Such agents must necessarily query databases in order to retrieve the requested information. We call such an operation a HardKB lookup. This makes online endto-end learning from user feedback difficult once the system is deployed. In this work, we propose a probabilistic framework for computing the posterior distribution of the user target over a knowledge base, which we term a Soft-KB lookup. The dialogue policy network, which decides the next system action, receives as input this full distribution instead of a handful of retrieved results. An entity-centric knowledge base is shown above the KB-InfoBot (missing values denoted by X). logue turns. Further, the retrieval process is differentiable, allowing us to construct an end-to-end trainable KB-InfoBot, all of whose components are updated online using RL. Reinforcement learners typically require an environment to interact with, and hence static dialogue corpora cannot be used for their training. Running experiments on human subjects, on the other hand, is unfortunately too expensive. Unlike previous KB-based QA systems, our focus is on multi-turn interactions, and as such there are no publicly available benchmarks for this problem. We evaluate several versions of KB-InfoBot with the simulator and on real users, and show that the proposed Soft-KB lookup helps the reinforcement learner discover better dialogue policies. Initial experiments on the end-to-end agent also demonstrate its strong learning capability. Instead we focus on how to query a KB interactively without composing such complicated queries in the first place. Also related is the growing body of literature focused on building end-to-end dialogue systems, which combine feature extraction and policy optimization using deep neural networks. As a result, training of various components of the dialogue system is performed separately. The intent network and belief trackers are trained using supervised labels specifically collected for them; while the policy network and generation network are trained separately on the system utterances. We retain modularity of the network by keeping the belief trackers separate, but replace the hard lookup with a differentiable one. The API calls modify a query hypothesis maintained outside the end-to-end system which is used to retrieve results from this KB. This framework does not deal with uncertainty in language understanding since the query hypothesis can only hold one slot-value at a time. Our approach, on the other hand, directly models the uncertainty to construct the posterior over the KB. The agent always asks for the value of the slot with maximum entropy over the remaining entries in the database, which is optimal in the absence of LU errors, and serves as a baseline against our approach. The interface considered in that work is a one-dimensional memory tape, while in our work it is an entity-centric KB. Such a KB can be converted to a table format whose rows correspond to the unique head entities, columns correspond to the unique relation types (slots henceforth), and some entries may be missing. Let T denote the KB table described above and Ti,j denote the jth slot-value of the ith entity. the set of all distinct values in the j-th column. Note that the user may still know the actual value of Ti,j, and we assume this lies in V j. We do not deal with new entities or relations at test time. We assume that column values are independently distributed to each other. This is a strong assumption but it allows us to model the user goal for each slot independently, as opposed to model-ing the user goal over KB entities directly. In this section we describe several agents to test these claims. Components with trainable parameters are highlighted in gray. an action at as output. The dialogue ends once the agent chooses inform. We assume the agent only responds with dialogue acts. A template-based Natural Language Generator (NLG) can be easily constructed for converting dialogue acts into natural language. The InfoBot consists of M belief trackers, one for each slot, which get the user input xt and produce two outputs, ptj and q t j, which we shall collectively call the belief state: ptj is a multinomial distribution over the slot values v, and qtj is a scalar probability of the user knowing the value of slot j. We describe two versions of the belief tracker. Each element of xt is an integer indicating the count of a particular n-gram in ut.   xt) (see appendix B for details). Collectively, outputs of the belief trackers and the soft-KB lookup can be viewed as the current dialogue state internal to the KB-InfoBot. It is pos-sible for the agent to directly use this state vector to select its next action at. Then the summary statistic for slot j is the entropy H(wtj). The KB posterior ptT is also summarized into an entropy statistic H(ptT). We present a hand-crafted baseline and a neural policy network.   iR) in the KB to the user. This is done by sampling R items from the KB-posterior ptT. This mimics a search engine type setting, where R may be the number of results on the first page. We assume that the learner has access to a reward signal rt throughout the course of the dialogue, details of which are in the next section. In this case, credit assignment is difficult for the agent, since it does not know whether the failure is due to an incorrect sequence of actions or incorrect set of results from the KB. Hence, at the beginning of training we have an Imitation Learning (IL) phase where the belief trackers and policy network are trained to mimic the hand-crafted agents. The expectations are estimated using a minibatch of dialogues of size B. Previous work in KB-based QA has focused on single-turn interactions and is not directly comparable to the present study. We compare three variants of both sets of agents, which differ only in the inputs to the dialogue policy. The Hard-KB version performs a hard-KB lookup and selects the next action based on the entropy of the slots over retrieved results. At the end of the dialogue, all versions inform the user with the top results from the KB posterior ptT, hence the difference only lies in the policy for action selection. The user, however, may still know these values. We compare each of the discussed versions along three metrics: the average rewards obtained (R), success rate (S) (where success is defined as providing the user target among top R results), and the average number of turns per dialogue (T). T: Average number of turns. S: Success rate. R: Average reward. In each case the Soft-KB versions achieve the highest average reward, which is the metric all agents optimize. In general, the trade-off between minimizing average turns and maximizing success rate can be controlled by changing the reward signal. This shows that having full information about the current state of beliefs over the KB helps the Soft-KB agent discover better policies. Further, reinforcement learning helps discover better policies than the handcrafted rule-based agents, and we see a higher reward for RL agents compared to Rule ones. This is due to the noisy natural language inputs; with perfect information the rule-based strategy is optimal. This agent does not receive any information about the uncertainty in semantic parsing, and it tends to inform as soon as the number of retrieved results becomes small, even if they are incorrect. The difficulty of a KB-split depends on number of entities it has, as well as the number of unique values for each slot (more unique values make the problem easier). Left: Success rate, with the number of test dialogues indicated on each bar, and the p-values from a twosided permutation test. To simulate the scenario where end-users may not know slot values correctly, the subjects in our evaluation were presented multiple values for the slots from which they could choose any one while interacting with the agent. We test RL-Hard and the three Soft-KB agents in this study, and in each session one of the agents was picked at random for testing. Each turn begins with a user utterance followed by the agent response. Rank denotes the rank of the target movie in the KB-posterior after each turn. Higher temperature leads to more noise in output. higher average reward as well. Between RL-Soft and Rule-Soft agents, the success rate is similar, however the RL agent achieves that rate in a lower number of turns on average. However, all agents take a higher number of turns against real users as compared to the simulator, due to the noisier inputs. Since it has more trainable components, this agent is also most prone to overfitting. A higher temperature means a more uniform output distribution, which leads to generic simulator responses irrelevant to the agent questions. This is a simple way of introducing noise in the utterances. This work is aimed at facilitating the move towards end-to-end trainable dialogue agents for information access. We show that such a framework allows the downstream reinforcement learner to discover better dialogue policies by providing it more information. Given these results, we propose the following deployment strategy that allows a dialogue system to be tailored to specific users via learning from agent-user interactions. The system could start off with an RL-Soft agent (which gives good performance out-of-the-box).   xt. We assume that the learner has access to a reward signal rt throughout the course of the dialogue, details of which are in the next section. Specifically, eq. At the beginning of each dialogue, the simulated user randomly samples a target entity from the ECKB and a random combination of informable slots for which it knows the value of the target. The remaining slot-values are unknown to the user. The user initiates the dialogue by providing a subset of its informable slots to the agent and requesting for an entity which matches them. If the agent informs results from the KB, the simulator checks whether the target is among them and provides the reward. We convert dialogue acts from the user into natural language utterances using a separately trained natural language generator (NLG). The NLG is trained in a sequence-to-sequence fashion, using conversations between humans collected by crowd-sourcing. It takes the dialogue actions (DAs) as input, and generates template-like sentences with slot placeholders via an LSTM decoder. There are several sources of error in user utterances. The NLG described above is inherently stochastic, and may sometimes generate utterances irrelevant to the agent request. By increasing the temperature of the output softmax in the NLG we can increase the noise in user utterances."
365,3,3.91,3.55,3.64,True,acl_2017,train,"Automated processing of historical texts often relies on pre-normalization to modern word forms. Training encoder-decoder architectures to solve such problems typically requires a lot of training data, which is not available for the named task. We address this problem by using several novel encoder-decoder architectures, including a multi-task learning (MTL) architecture using a grapheme-to-phoneme dictionary as auxiliary data, pushing the state-of-theart by an absolute 2% increase in performance. We analyze the induced models across 44 different texts from Early New High German. Interestingly, we observe that, as previously conjectured, multi-task learning can learn to focus attention during decoding, in ways remarkably similar to recently proposed attention mechanisms. This, we believe, is an important step toward understanding how MTL works.","Training data for supervised learning of historical text normalization is typically scarce. On the other hand, neural networks are said to work best when trained on large amounts of data. It is therefore not clear whether neural networks are a good choice for this particular task. Furthermore, we explore using an auxiliary task for which data is more readily available, namely grapheme-tophoneme mapping (word pronunciation), to regularize the induction of the normalization models. In sum, we both push the state-of-the-art in historical text normalization and present an analysis that, we believe, brings us a step further in understanding the benefits of multi-task learning. For all texts, we removed tokens that consisted solely of punctuation characters. We also lowercase all characters, since it helps keep the size of the vocabulary low, and uppercasing of words is usually not very consistent in historical texts. Tokenization was not an issue for pre-processing these texts, since modern token boundaries have already been marked by the transcribers. the website). Grapheme-to-phoneme mappings We use learning to pronounce as our auxiliary task. This task consists of learning mappings from sequences of graphemes to the corresponding sequences of phonemes. LSTMs are designed to allow recurrent networks to better learn long-term dependencies, and have proven advantageous to standard RNNs on many tasks. We found no significant advantage from stacking multiple LSTM layers for our task, so we use the simplest competitive model with only a single LSTM unit for both encoder and decoder. Embedding layers for the inputs are not explicitly shown. pairs of different lengths. Another important property is that the model does not start to generate any output until it has seen the full input sequence, which in theory allows it to learn from any part of the input, without being restricted to fixed context windows. An example illustration of the unrolled network is shown in Fig. During training, the encoder inputs are the historical wordforms, while the decoder inputs correspond to the correct modern target wordforms. In other words, we evaluate our models across all the test examples. For prediction, our base model generates output character sequences in a greedy fashion, selecting the character with the highest probability at each timestep. This works fairly well, but the greedy approach can yield suboptimal global picks, in which each individual character is sensibly derived from the input, but the overall word is non-sensical. Finally, we also experiment with using a lexical filter during the decoding step. This is again intended to reduce the occurrence of nonsensical outputs. For the lexicon, we use all word forms from CELEX (cf. Sec. This is a strong assumption, especially with long input sequences. Attention mechanisms give us more flexibility. Importantly, we let the model learn what to attend to based on the input sequence and what it has produced so far. While a precise alignment of input and output sequences is sometimes difficult, most of the time the sequences align in a sequential order, which can be exploited by an attentional component. The multitask architecture only differs from the base architecture in having two classifier functions at the outer layer, one for each of our two tasks. Our auxiliary task is to predict a sequence of phonemes as the correct pronunciation of an input sequence of graphemes. This choice is motivated by the relationship between phonology and orthography, in particular the observation that spelling variation often stems from phonological variation. We train our multi-task learning architecture by alternating between the two tasks, sampling one instance of the auxiliary task for each training sample of the main task. We use the encoderdecoder to generate a corresponding output sequence, whether a modern word form or a pronunciation. Doing so, we suffer a loss with respect to the true output sequence and update the model parameters. The update for a sample from a specific task affects the parameters of corresponding classifier function, as well as all the parameters of the shared hidden layers. We used a single manuscript (B) for manually evaluating and setting the hyperparameters. This manuscript is left out of the averages reported below. We believe that using a single manuscript for development, and using the same hyperparameters across all manuscripts, is more realistic, as we often do not have enough data in historical text normalization to reliably tune hyperparameters. All these parameters were set on the B manuscript alone. Baselines We compare our architectures to several competitive baselines. We evaluate this tagger using both standard and multi-task learning. While we also measure character-level metrics, minor differences on character level can cause large differences in downstream applications, so we believe that perfectly matching the output sequences is more useful. We first see that almost all our encoder-decoder architectures perform significantly better than the four state-of-the-art baselines. Dev. Evaluation on the base encoder-decoder model (Sec. For our multi-task architecture, we also observe gains when we add beam search and filtering, but importantly, adding attention does not help. In fact, attention hurts the performance of our multitask architecture quite significantly. Also note that the multi-task architecture without attention performs on-par with the single-task architecture with attention. This is the hypothesis that we will try to validate in Sec. dicke, herzel). We will investigate this property further in Sec. Fig. In the representations from the base model (Fig. On the other hand, the MTL model shows a better generalization of the training data (Fig. We can also visualize the internal word representations that are produced by the encoder (Fig. In the MTL model, however, these examples are indeed clustered together. However, we observe a decline in word accuracy for models that combine multi-task learning with attention. We put this hypothesis to the test by closely investigating properties of the individual models below. First, we are interested in the weight parameters of the final layer that transforms the decoder output to class probabilities. Next, we compare the effect that employing either an attention mechanism or multi-task learning has on the actual output of our system. Our last analysis regards the saliency of the input timesteps with respect to the predictions of our models. Our hypothesis is that the attentional and the multi-task learning model should be more similar in terms of saliency scores than either of them compared to the base model. the input sequence, as calculated from the base model (left), the attentional model (center), and the MTL model (right). This is conceptually very similar to our approach, except that we substitute the classical SMT algorithms for neural networks. We presented an approach to historical spelling normalization using neural networks with an encoder-decoder architecture, and showed that it consistently outperforms several existing baselines. Specifically, we demonstrated the aptitude of multi-task learning to mitigate the shortage of training data for the named task. We included a multifaceted analysis of the effects that MTL introduces to our models and the resemblance that it bears to attention mechanisms. We believe that this analysis is a valuable contribution to the understanding of MTL approaches also beyond spelling normalization, and we are confident that our observations will stimulate further research into the relationship between MTL and attention. Finally, many improvements to the presented approach are conceivable, most notably introducing some form of token context to the model. Reranking the predictions with a language model could be one possible way to improve on this. Such an approach could also deal with the issue of tokenization differences between the historical and the modern text, which is another challenge often found in datasets of historical text. Perc. Sec."
220,1,5.0,4.0,5.0,True,acl_2017,train,"Named entities are frequently used in a metonymic manner. They serve as references to related entities such as people and organisations. Accurate identification and interpretation of metonymy can be directly beneficial to various NLP applications, such as Named Entity Recognition and Geographical Parsing. Until now, metonymy resolution (MR) methods mainly relied on parsers, taggers, dictionaries, external word lists and other handcrafted lexical resources. We show how a minimalist neural approach combined with a novel predicate window method can achieve state-of-the-art results on the SemEval 2007 task on Metonymy Resolution. Additionally, we contribute with a new Wikipedia-based MR dataset called RelocaR, which is tailored towards locations as well as improving previous deficiencies in annotation guidelines.","These irregular expressions are understood with little difficulty by humans but require special attention in NLP. One of these is metonymy, a type of common figurative language, which stands for the substitution of the concept, phrase or word being meant with a semantically related one. Named Entity Recognition (NER) taggers have no provision for handling metonymy, meaning that this frequent linguistic phenomenon goes largely undetected within current NLP. Classi-fication decisions presently focus on the entity using features such as orthography to infer its word sense, largely ignoring the context, which provides the strongest clue about whether a word is used metonymically. A common classification approach is choosing the N words to the immediate left and right of the entity or the whole paragraph as input to the model. In order to accurately identify and ground location entities, for example, we must recognise that metonymic entities constitute false positives and should not be treated the same way as regular locations. PreWin outperforms other systems, which use many external features and tools. We also improve the annotation guidelines in MR and contribute with a new Wikipedia-based MR dataset called ReLocaR to address the training data shortage. The decision list classifier with backoff was evaluated using syntactic head-modifier relations, grammatical roles and a thesaurus to overcome data sparseness and generalisation problems. However, the method was still limited for classifying unseen data. Our method uses the same paradigm but adds more features, a different machine learning architecture and a better usage of the parse tree structure. Once again, we draw attention to the extra training, external tools and additional feature generation. As one of their extrinsic evaluation tasks, metonymy resolution was tested. Global context (whole paragraph) was used to interpret the target word. The related work on MR so far has made limited use of dependency trees. Typical features came in the form of a head dependency of the target entity, its dependency label and its role (subj-of-win, dobj-of-visit, etc). However, other classification tasks made good use of dependency trees. Our methods also exploit the versatility of dependency parsing to leverage information about the sentence structure. Two types of entities were evaluated, organisations and locations, randomly retrieved from the British National Corpus (BNC). For coarse evaluation, the classes are literal (geographical territories and political entities), metonymic (place-for-people, place-forproduct, place-for-event, capital-for-government or place-for-organisation) and mixed (metonymic and literal frames invoked simultaneously or unable to distinguish). The metonymic class further breaks down into two levels of subclasses allowing for medium and fine evaluation. This seems to be the approximate natural distribution of the classes for location metonymy, which we have also observed while sampling Wikipedia for our new dataset. Our contribution broadly divides into two main parts, data and methodology. Our corpus is designed to evaluate the capability of a classifier to distinguish literal, metonymic and mixed location mentions. We do not break down the metonymic class further as the distinction between the subclasses is subtle and hard to agree on. We leveraged their linguistic training and expertise to make decisions rather than imposing some specific scheme. Unresolved sentences would receive the mixed class label. The most prominent difference is a small change in the annotation scheme (after independent linguistic advice). In ReLocaR, we consider a political entity a metonymic reading. ReLocaR has three classes, literal, metonymic and mixed. In difficult cases such as these, the mixed class is assigned. We give the IAA for the test partition only. The whole dataset was annotated by the first author as the main annotator. Due to the lack of annotated training data for MR, this is a valuable resource. The data was annotated by the first author, there are no IAA figures. In other words, most of the sentence is not only superfluous for the task, it actually lowers the accuracy of the model due to irrelevant input. Each dependency tree provides the following features: dependency labels and entity head dependency. Rather than using most of the tree, we only use a single local head dependency relationship to point to the predicate. Leveraging a dependency parser helps PreWin with selecting the minimum relevant input to the model while discarding irrelevant input, which may cause the neural model to behave unpredictably. Finally, the entity itself is never used as input in any of our methods, we only rely on context. The output of PreWin is processed using the following machine learning model. In this case, the left hand side input to the model is set to zeroes (see the Appendix for full architecture). The full architecture is available in the Appendix. The Ensemble method enabled us to reach SOTA results. For ReLocaR data, we let three models vote, all trained on the ReLocaR training data. We evaluate all methods using (any one or a combination of) three datasets for training (ReLocaR, SemEval, CoNLL) and two for testing (ReLocaR, SemEval). We only evaluate at the coarse level, which means literal versus nonliteral (metonymic and mixed are merged into one class). Conventional classification methods (Immediate, Paragraph) can also be seen as prioritising either feature precision or feature recall. Paragraph maximises the input sequence size, which maximises recall at the expense of including features that are either irrelevant or mislead the model, lowering performance. Immediate maximises precision by using features close to the target entity at the expense of missing important features positioned outside of its small window, once again lowering performance. PreWin can be understood as an integration of both approaches. Further prototypical examples of the method can be viewed in the Appendix. Our intuition that most words in the sentence, indeed in the paragraph do not carry the semantic information required to classify the target entity is ultimately based on evidence. Aiming to approximate human decision making, the neural model uses only a small window (which may be far away from the entity), linked to the entity via a head dependency relationship for the final classification decision. However, it does not work well in some cases. PreWin discards the right-hand side input, which is required in this case for a correct classification. Parsing mistakes were less common though still present. It is important to choose the right dependency parser for the task since different parsers will often generate slightly different parse trees. The top accuracy figures for ReLocaR are almost identical to SemEval. methods for SemEval. Both were achieved using the same methods (PreWin or Ensemble), neural architecture and size of corpora. This shows a good degree of flexibility in our minimalist neural network. We think the reason for this is the difference in annotation guidelines; the government is a metonymic reading, not a literal one. This causes the model to make more mistakes. The highest accuracy and f-scores were achieved with the ensemble method for both datasets. We posit that fewer dimensions of the distributed word representations force the abstraction level higher as the meaning of words must be expressed more succinctly. We think this helps the model generalise better, particularly for smaller datasets. Note the model class bias for SemEval. The model was equally capable of distinguishing between literal and nonliteral cases. The method uses non-local (long range) dependencies to construct a short input sequence. We think the next frontier is a NER tagger, which actively handles metonymy. Metonymy is a frequent linguistic phenomenon and could be handled by NER taggers to enable many innovative downstream NLP applications. Geographical Parsing is a pertinent use case. This means Vancouver is usually a location but is used metonymically in this case. How this information is used will be up to the developer. Organisations behaving as persons, share prices or products are but a few other examples of metonymy. We managed to achieve higher performance with a small neural network, minimal training data, a basic dependency parser and the new PreWin method by being highly discriminating in choosing signal over noise. We showed how a minimalist neural approach can replace substantial external resources, handcrafted features and how the PreWin method can even ignore most of the paragraph where the entity is positioned and still achieve state of the art performance in metonymy resolution. We have also presented a case for better annotation guidelines for MR (after consulting with a number of linguists), which now means that a government is not of a literal class, rather it is a metonymic one. We fully agreed with the rest of the previous annotation guidelines. If it does perform better, this will be of considerable interest to classification research (and beyond) in NLP."
467,3,4.67,4.0,4.0,True,acl_2017,train,"Most methods to learn bilingual word embeddings rely on large parallel corpora, which is difficult to obtain for most language pairs. This has motivated an active research line to relax this requirement, with methods that use document-aligned corpora or bilingual dictionaries of a few thousand words instead. In this work, we further reduce the need of bilingual resources using a very simple self-learning approach that can be combined with any dictionary-based mapping technique. Our method exploits the structural similarity of embedding spaces, and works with as little bilingual evidence as a 25 word dictionary or even an automatically generated list of numerals, obtaining results comparable to those of systems that use richer resources.","However, dictionaries of that size are not readily available for many language pairs, specially those involving less-resourced languages. In this work, we reduce the need of large bilingual dictionaries to much smaller seed dictionaries. The method can also work with trivially generated seed dictionaries of numerals (i.e. In either case, we obtain very competitive results, comparable to other state-of-the-art methods that make use of much richer bilingual resources. In spite of its simplicity, our analysis of the implicit optimization objective reveals that the method is exploiting the structural similarity of independently trained embeddings. Previous works learn a mapping W based on the seed dictionary D, which is then used to learn the full dictionary. In our proposal we use the new dictionary to learn a new mapping, iterating until convergence. We will first focus on bilingual embedding mappings, which are the basis of our proposals, and then on other unsupervised and weakly supervised methods to learn bilingual word embeddings. Methods to induce bilingual mappings work by independently learning the embeddings in each language using monolingual corpora, and then learning a transformation from one embedding space into the other based on a bilingual dictionary. Our experiments will show that, although their method captures coarse-grained relations, it fails on finergrained tasks like bilingual lexicon induction. Although promising, the reported performance of the learned embeddings is poor in comparison to other methods. However, decipherment is only concerned with translating text from one language to another and relies on complex statistical models that are designed specifically for that purpose, while our approach is more general and learns task-independent multilingual embeddings. This way, one can say that the seed (train) dictionary is used to learn a mapping, which is then used to induce a better dictionary (at least in the sense that it is larger). The process can then be repeated iteratively to obtain a hopefully better mapping and dictionary each time until some convergence criterion is met. However, efficiency turns out to be critical for a variety of reasons. First of all, by enclosing the learning logic in a loop, the total training time is increased by the number of iterations. Even more importantly, our framework requires to explicitly build the entire dictionary at each iteration, whereas previous work tends to induce the translation of individual words ondemand later at runtime. Moreover, from the second iteration onwards, it is this induced, full dictionary that has to be used to learn the embedding mapping, and not the considerably smaller seed dictionary as it is typically done. In the following two subsections, we respectively describe the embedding mapping method and the dictionary induction method that we adopt in our work with these efficiency requirements in mind. In the following, we present their method, explicitly incorporating the dictionary in the formalization so it can be smoothly integrated in our selflearning algorithm. Since the dictionary matrix D is sparse, this can be efficiently computed in linear time with respect to the number of dictionary entries. In nearest neighbor retrieval, each source language word is assigned the closest word in the target language. We experimented with other alternatives in development, with minor differences. While we find that independently computing the similarity measure between all word pairs is prohibitively slow, the computation of the entire similarity matrix XWZT can be easily vectorized using popular linear algebra libraries, obtaining big performance gains. However, the resulting similarity matrix is often too large to fit in memory when using large vocabularies. For that reason, instead of computing the entire similarity matrix XWZT in a single step, we iteratively compute submatrices of it using vectorized matrix multiplication, find their corresponding minima each time, and then combine the results. In this section, we experimentally test the proposed method in bilingual lexicon induction and crosslingual word similarity. In addition to English-Italian, we selected two other languages from different language families with publicly available resources. This was done by shuffling once the training dictionaries and taking their first k entries, so it is guaranteed that each dictionary is a strict subset of the bigger dictionaries. In addition to that, we explored using automatically generated dictionaries as a shortcut to practical unsupervised learning. While more sophisticated approaches are possible (e.g. involving the edit distance of all words), we believe that this method is general enough that should work with practically any language pair, as arabic numerals are often used even in languages with a different writing system (e.g. Chinese and Russian). For that reason, we carried out some experiments in crosslingual word similarity as a way to test our method in a different task and allowing compare it to systems that use richer bilingual data. As for the convergence criterion required by our method, we decide to stop training when the improvement on the average similarity for the automatically generated dictionary falls below a given threshold from one iteration to the next. The curves in the next section show that this threshold was a reasonable choice. In any case, the main focus of our work is on smaller dictionaries, and it is under this setting that our method really stands out. solute accuracy numbers, which can be attributed to the linguistic proximity of the languages involved. In this regard, we believe that the good results with small dictionaries are a strong indication of the robustness of our method, showing that it is able to learn good bilingual mappings from very little bilingual evidence even for distant language pairs where the structural similarity of the embedding spaces is presumably weaker. However, it is not clear how to introduce monolingual corpora on those methods. They both had coverage problems that made the results hard to compare, and, when considering the correlations for the word pairs in their vocabulary, their performance was poor. All in all, our experiments show that it is better to use large monolingual corpora in combination with very little bilingual data rather than a bilingual corpus of a standard size alone. It might seem somehow surprising at first that, as seen in the previous section, our simple selflearning approach is able to learn high quality bilingual embeddings from small seed dictionaries instead of falling in degenerated solutions. In this section, we try to shed light on our approach, and give empirical evidence supporting our claim. Intuitively, a random solution would map source language embeddings to seemingly random locations in the target language space, and it would thus be unlikely that they have any target language word nearby, making the optimization value small. close to their translation equivalents in the target language space, and they would thus have their corresponding embeddings nearby, making the optimization value large. While it is certainly possible to build degenerated solutions that take high optimization values for small subsets of the vocabulary, we think that the structural similarity between independently trained embedding spaces in different languages is strong enough that optimizing this function yields to meaningful bilingual mappings when the size of the vocabulary is much larger than the dimensionality of the embeddings. The reasoning for how the self-learning framework is optimizing this objective is as follows. At the end of each iteration, the dictionary D is updated to assign, for the current mapping W, each source language word to its closest target language word. This way, when we update W to maximize the average similarity of these dictionary entries at the beginning of the next iteration, it is guaranteed that the value of the optimization objective will improve (or at least remain the same). In addition to that, it is also possible that, for some source words, some other target words get closer after the update. Thanks to this, our self-learning algorithm is guaranteed to converge to a local optimum of the above global objective, behaving like an alternating optimization algorithm for it. It is interesting to note that the above reasoning is valid no matter what the the initial solution is, and, in fact, the optimization objective does not depend on the seed dictionary nor any other bilingual resource. For that reason, it should be possible to use a random initialization instead of a small seed dictionary. However, we empirically observe that this works poorly in practice, as our algorithm tends to get stuck in poor local optima when the initial solution is not good enough. As it can be seen, the objective function is improved from iteration to iteration and converges to a local optimum just as expected. Finally, we empirically observe that our algorithm learns similar mappings no matter what the seed dictionary was. This clearly shows that these different systems do not only obtain a similar accuracy, but also have a very similar behavior. In other words, our algorithm tends to converge to similar solutions even for disjoint seed dictionaries, which is in line with our view that we are implicitly optimizing an objective that is independent from the seed dictionary, yet a seed dictionary is necessary to build a good enough initial solution to avoid getting stuck in poor local optima. For that reason, it is likely that better methods to tackle this opti-mization problem would allow learning bilingual word embeddings without any bilingual evidence at all and, in this regard, we believe that our work opens exciting opportunities for future research. In this work, we propose a simple self learning framework to learn bilingual word embedding mappings in combination with any embedding mapping and dictionary induction technique. In spite of its simplicity, a more detailed analysis shows that our method is implicitly optimizing a meaningful objective function that is independent from any bilingual data which, with a better optimization method, might allow to learn bilingual word embeddings in a completely unsupervised manner. In the future, we would like to delve deeper into this direction and fine-tune our method so it can reliably learn high quality bilingual word embeddings without any bilingual evidence at all."
563,3,3.8,1.8,2.4,True,acl_2017,train,"Word embeddings are used with success for a variety of tasks involving lexical semantic similarities between individual words. Using unsupervised methods and just cosine similarity, encouraging results were obtained for analogical similarities. In this paper, we explore the potential of pre-trained word embeddings to identify generic types of semantic relations in an unsupervised experiment. We propose a new relational similarity measure based on the combination of word2vec’s CBOW input and output vectors which outperforms concurrent vector representations, when used for unsupervised clustering on SemEval 2010 Relation Classification data.","Word vectors are used with success because they capture a notion of semantics directly extracted from corpora. The closer two entities are in the vector space (quantified usually, but not necessarily in terms of cosine similarity), the more similar they are semantically. This similarity can be exploited for lexical substitution, synonym detection, subcategorization learning etc. In what follows, we will concentrate on exploring whether and how pre-trained, general-purpose word embeddings encode relational similarities. Whether the vector spaces of pretrained word embeddings are appropriate for discovering or identifying relational similarities remains to be seen. analogies between pairs of words. Relationships are assumed to be present as vector offsets, so that in the embedding space, all pairs of words sharing a particular relation are related by the same constant offset. In fact, a big part of their merits is likely to come from the precise calculation of individual similarities instead of relational similarities. Hence, they can be approximated using relation-independent baselines. Different word embedding combinations in supervised learning of taxonomical relations do not seem to learn the relations themselves, but individual properties of words. They tested previously suggested vector compositions for supervised learning of inference relations: concatenation, difference, comparing only the first or only the second element of the pairs. The study concludes that the classifiers only learn individual properties (e.g. a ""category"" type word is a good hyponym candidate), but not semantic relations between words. These studies suggest that the semantic information obtained from word embeddings is correct for identifying similar or related units, but is already self-contained and difficult to enrich in order to retrieve more specific semantic contents such as relational similarities or specific relations. In this paper, we aim to challenge this conclusion within a large scale semantic relation classification experiment, and show that it is possible to achieve improvements compared to baselines and current methods. We apply known vector composition methods, and propose a new one, to unsupervised large-scale clustering of entity pairs categorized according to their semantic relation. the entity tuples, and categorizing their relation according to an existing typology. In an unsupervised framework, relation types are inferred directly from the data. Another way of constructing a distributional vector space to represent quantifiable context features for relation extraction is to combine the vectors of the two entities. As of now, these vector combinations had two types of applications in semantic relation classification. These experiments either aim to identify very specific relation types (typically taxonomical relations) with a mixture of features and a supervised classifier, or target analogy pairs: a task in which, as we have seen, relation-unaware baselines approximate relation-aware representations. Vector quality (do semantically close elements have a higher cosine similarity?) Density and structure of the vector space. How much information about the semantic relation is actually in the text and how fit is the vector combination method to encode this information? This way, we rely less on the neighborhood structure and more on actual ""linguistic regularities"". We evaluate different vector combination methods, and propose a new one, for calculating relational similarities. The evaluation concentrates on the aspects above. We test whether cosine similarity over these vector spaces is adapted for discovering groups and classifying individual instances. We report clustering results and compare the vector combinations by their performance. Some relations are more lexical by nature: relations such as dog is an animal; a teacher works at a school; a car is kept at a parking lot, can be interpreted independently of the context. Contextual relations (e.g. the accident was caused by the woman"") tend to be expressed explicitly, but rarely, in a corpus. They can be handled by pattern-based approaches rather than distributional representation combinations. We expect to be able to identify prototypical lexical relation instances with vector combination methods. In the scope of the current experiment, we do not intend to combine contextual information with lexical semantic representations, since our primary goal is to argue that vector combinations may encode lexical relational similarities in themselves. If a representation is more capable than others to group together word pairs according to relational similarities, this potential can further be exploited in unsupervised as well as in supervised experiments. On the other hand, well-known outliers (contextual relations and less typical examples) will require a complementary approach. This task is difficult and requires a change of perspective: when we look for missing elements in an analogy we know the word exists and we presume to know where it will be in the vector space, while in unsupervised clustering, our aim is to infer a global structure from the data. This data set is very challenging, not only because of the fine semantic distinctions, but also because semantic relations were annotated in context and contain many less typical relation instances. Contexts in the training data were discarded. The recall of this approach is expected to be limited: the same relation can hold between different types of entities. IN-OUT similarities: a new combination that builds on the integration of second order similarities. If we use cosine similarity, this is only slightly different from the concatenative method. In this model, the IN vectors of words get closer to the OUT vectors of other words that they co-occur with. Words with a high input-output similarity tend to appear in the context of each other. Their proposed formula combines first and second order similarity, normalized by the reflective second order similarity of the words with themselves. This is based on the observation that ""words are similar if they tend to appear in similar contexts, or if they tend to appear in the contexts of each other (and preferably both)."" Second-order similarities are calculated between opposite elements of the entity couples. Every cluster is mapped to the standard class that shares the more elements with. Average class-based precision and recall is reported, as well as the number of classes in the standard that could be assigned. These scores were published for the SemEval task participants, but ours are not comparable because we only consider one cluster for each class, and because we did the clustering on the training data. We ran complementary experiments with different numbers of clusters. The input-output combination method still has an advantage, and concatenation and multiplication also perform well. However, the advantages over the baseline are less significant than when the number of clusters was identical to the standard. have significantly more clusters. Class-based precision and recall are less relevant measures in this setting, since they take the average over the nine standard classes and not over the produced clusters. It favorizes small clusters, but singleton clusters were discarded. This measure corresponds to predic-tion accuracy in classification if we assign the majority label to clusters. Average results are reported. These scores indicate the average purity of clusters over different runs. This indicates a good potential in differentiating between relation types, especially because the standard is conceived in a way that it contains strongly related classes. The baseline performs well, but additive methods all beat it, while difference is especially weak. Multiplicative methods show a fluctuating performance, especially the multiplicative inputoutput combination. Individual similarities have a strong precision for the easily identifiable clusters, while additional relational information is mostly expected to improve recall. We presented an experiment to identify relational similarities in word embedding compositions at a large scale, using an unsupervised approach. On the one hand, our results confirm the recent finding that many of the success attributed to vector arithmetics for analogies come from similarities between individual elements. On the other hand, taking second order similarity into account, we can improve relational similarities and take a step toward a meaningful representation for entity couples in a semantic relation. The baseline performs well and is difficult to enrich with relation-aware information. The results indicate that the vector offset method for analogies, which replaces the pairwise similarity, is the least efficient in capturing generic semantic relations at a large scale. The vector difference representation does not conserve pairwise similarities and the offsets do not prove to be constant enough for unsupervised clustering. Multiplicative methods do not scale up either, although to a lesser extent: they capture some of the relational information, but this happens at the expense of losing precision from individual similarities. Pairwise similarities can be better exploited in an additive or concatenative setting. Moreover, they can be meaningfully complemented by including second order similarities without losing too much information for precise classification. The input-output combination measure coherently outperformed the other combinations in almost every setting, indicating a better potential for unsupervised experiments. Unsupervised relation classification is a very challenging task for several reasons. Some relation instances are lexical by nature and, there-fore, can be expected to show up in the same cluster based on distributional cues. On the other hand, contextual relation instances tend to have relation-specific indicators when they co-occur, but their individual vectors will not reveal this information (unless they co-occur very often). Moreover, semantic relations differ with respect to the semantic constraints they put on their arguments. For instance, the second argument of the Content-Container relation tend to belong to a specific semantic class in the standard (bag, box, trunk, case, drawer...), while both arguments of the CauseEffect relation are much freer (gas, prices, pain, acts, species and pyrolysis, collapse, contraction, society, noise). Any future development towards an automated unsupervised classification needs to take these aspects into account and work towards a hybrid solution by separating relations with semantically constrained arguments from free ones, as well as adapting the clustering method to handle outliers."
543,1,2.0,4.0,4.0,True,acl_2017,train,"Previous work has modeled the compositionality of words by creating characterlevel models of meaning, reducing problems of sparsity for rare words. However, in many writing systems compositionality has an effect even on the character-level: the meaning of a character is derived by the sum of its parts. In this paper, we model this effect by creating embeddings for characters based on their visual characteristics, creating an image for the character and running it through a convolutional neural network to produce a visual character embedding. Experiments on a text classification task demonstrate that such model allows for better processing of instances with rare characters in languages such as Chinese, Japanese, and Korean. Additionally, qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry categorical content which resulting in embeddings that are coherent in visual space.","However, in many writing systems compositionality has an effect even on the character-level: the meaning of a character is derived by the sum of its parts. In this paper, we model this effect by creating embeddings for characters based on their visual characteristics, creating an image for the character and running it through a convolutional neural network to produce a visual character embedding. Experiments on a text classification task demonstrate that such model allows for better processing of instances with rare characters in languages such as Chinese, Japanese, and Korean. Additionally, qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry categorical content which resulting in embeddings that are coherent in visual space. Recently, neural models have provided a powerful tool for learning how to compose words together into a meaning representation of whole sentences for many downstream tasks. In fact, a growing body of evidence shows that it is essential to look below the word-level and consider compositionality within words themselves. Thesemodels help to learnmore robust representations for rare words by exploiting morphological patterns, as opposed to models that operate purely on the lexical level as the atomic units. can be derived from the sum of its parts, is very much a reality. This is similar to how English characters combine into the meaning or pronunciation of an English word. Even in languages with phonemic orthographies, where each character corresponds to a pronunciation instead of a meaning, there are cases where composition occurs. Fig. In this paper, we investigate the feasibility of modeling the compositionality of characters in a way similar to how humans do: by visually observing the character and using the features of its shape to learn a representation encoding its meaning. These features then serve as inputs to a down-stream processing task and trained in an end-to-end manner, which first calculates a loss function, then back-propagates the loss back to the CNN. As demonstrated by our motivating examples in Fig. Consequently, characters with similar visual appearances will be biased to have similar embeddings, allowing our model to handle rare characters effectively, just as character-level models have been effective for rare words. We show that our proposed framework outperforms baseline model that use standard character embeddings for instances containing rare characters. A qualitative analysis of the characteristics of the learned embeddings of our model demonstrates that visually similar characters share similar embeddings. Before delving into the details of our model, we first describe a dataset we constructed to examine the ability of our model to capture the compositional characteristics of characters. All three languages have a long-tail distribution. positionality in the characters of the language. To satisfy these desiderata, we create a text classification dataset where the input is a Wikipedia article title in Chinese, Japanese, or Korean, and the output is the category to which the article belongs. Wikipedia has a hierarchical structure, where each of these main categories has a number of subcategories, and each subcategory has its own subcategories, etc. We traverse this hierarchical structure, adding each main category tag to all of its descendants in this subcategory tree structure. A string of characters (e.g. dc is the dimension of the character embedding for the Lookup model. As shown in Fig. The category distribution for each language can be seen in Tab. Chinese has two varieties of characters, traditional and simplified, and the dataset is a mix of the two. Hence, we transform this dataset into two separate sets, one completely simplified and the other completely traditional using the Chinese text converter provided with Mac OS. to combine the character representations into a sentence representation, and then add a softmax layer after that to predict the probability for each class. As shown in Fig. Our proposed model, the Visual model instead learns the representation of each character from its visual appearance via CNN. Lookupmodel Given a character vocabularyC, for the Lookup model as in the bottom part of Fig. Visual model The proposed method aims to learn a representation that includes image information, allowing for better parameter sharing among characters, particularly characters that are less common. We then pass the image through a CNN to get the embedding for the image. The parameters for the CNN are learned through backpropagation from the classification loss. In more detail, the specific structure of the CNN that we utilize consists of three convolution layers where each convolution layer is followed by the max pooling and ReLU nonlinear activation layers. The configurations of each layer are listed in Tab. The output vector for the image embeddings also has size dc which is the same as the Lookupmodel. Each of the GRU units processes the character embeddings sequentially. At the end of the sequence, the incremental GRU computation results in a hidden state e embedding the sentence. The encoded sentence embedding is passed through a linear layer whose output is the same size as the number of classes. B is the batch size and L is the number of categories. One thing to note is that the Lookup and the Visual models have their own advantages. The Lookup model learns embedding that captures the semantics of each character symbol without sharing information with each other. On the contrary, the proposed Visual model directly learns embedding from visual information, which naturally shares information between visually similar characters. This characteristic gives the Visual model the ability to generalize better to rare characters, but also has the potential disadvantage of introducing noise for characters with similar appearances but different meanings. With the complementary nature of these two models in mind, we further combine the two embeddings to achieve better performances. Early Fusion Early fusion works by concatenating the two varieties of embeddings before feeding them into the RNN. The whole model is then fine-tuned with training data. Late Fusion Instead of learning a joint representation like early fusion, late fusion averages the model predictions. Specifically, it takes the output of the softmax layers from both models and averages the probabilities to create a final distribution used to make the prediction. Fallback Fusion Our final fallback fusion method hypothesizes that our Visual model does better with instances which contain more rare characters. First, in order to quantify the overall rareness of an instance consisting of multiple characters, we calculate the average training set frequency of the characters therein. In this section, we compare our proposed Visual model with the baseline Lookup model through three different sets of experiments. First, we examine whether our model is capable of classifying text and achieving similar performance as the baseline model. when dealing with low frequency characters. In this experiment, we examine whether our Visual model achieves similar performance with the baseline Lookup model in classification accuracy. The results in Tab. On the contrary, the Visual model learns embeddings from visual information, which constraints characters that has similar appearance to have similar embeddings. This is an advantage for rare characters, but a disadvantage for high frequency characters because being similar in appearance does not always lead to similar semantics. To demonstrate that this is in fact the case, besides looking at the overall classification accuracy, we also examine the performance on classifying low frequency instances which are sorted according to the average training set frequency of the characters therein. Tab. We calculate the accumulated number of correctly predicted instances for the Visualmodel (solid lines) and the Lookup model (dashed lines). We performed this experiment under the hypothesis that because the proposed method was more robust to infrequent characters, the proposed model may perform better in low-resourced scenarios. If this is the case, the intersection point of the two models will shift right because of the increase of the number of instances with low average character frequency. As we can see in Fig. This disagrees with our hypothesis; this is likely because while the number of low-frequency characters increases, smaller amounts of data also adversely impact the ability of CNN to learn useful visual features, and thus there is not a clear gain nor loss when using the proposed method. data. This result demonstrates that the model is able to transfer between similar scripts, similarly to howmost Chinese speakers can guess the meaning of the text, even if it is written in the other script. Results of different fusion methods can be found in Tab. The results show that late fusion gives the best performance among all the fusion schemes combining the Lookup model and the proposed Visual model. Early fusion achieves small improvements for all languages except Japanese, where it displays a slight drop. Two characters are shown per radical to emphasize that characters with same radical have similar patterns. fallback fusion performs better than the Lookup model and the Visual model alone, since it directly targets the weakness of the Lookup model (e.g., rare characters) and replaces the results with the Visual model. These results show that simple integration, no matter which schemes we use, is beneficial, demonstrating that both methods are capturing complementary information. Finally, we qualitatively examine what is learned by our proposed model in two ways. Specifically, we leave only the upper half, bottom half, left half, or right half of the image, and mask the remainder with white pixels since Chinese characters are usually formed by combining two radicals vertically or horizontally.      The highlighted red indicates the radical along with the meaning of the characters. the masked image embeddings with the full image embedding. The larger the distance, the more the masked part of the character contributes to the original embedding. The contribution of each part (e.g. The visualization is shown in Fig. The meaning of each Chinese character in English is shown below the Chinese character. The opacity of the character strokes represent how much the corresponding parts contribute to the original embedding (the darker the more). In general, the darker part of the character is related to its semantics. We can also find similar results for other examples shown in Fig. As can be seen, the Visual embedding for characters with similar appearances are close to each other. In addition, similarity in the radical part indicates semantic similarity between the characters. The Lookup embedding do not show such feature, as it learns the embedding individually for each symbol and relies heavily on the training set and the task. In fact, the characters shown in Fig. There are two potential explanations for this: First, the category classification task that we utilized do not rely heavily on the finegrained semantics of each character, and thus the Lookup model was able to perform well without exactly capturing the semantics of each character precisely. Second, the Wikipedia dataset contains a large number of names and location and the characters therein might not have the same semantic meaning used in daily vocabulary. Methods that utilize neural networks to learn distributed representations of words or characters have been widely developed. They tested on realworld rendered mathematical expressions paired with LaTeX markup and show the system is effective at generating accurate markup. These models look at the sequential dependencies at the word or character-level and achieve the state-of-the-art results. These works inspire us to use CNN to extract features from image and serve as the input to the RNN. Our model is able to directly back-propagate the gradient all the way through the CNN, which generates visual embeddings, in a way such that the embedding can contain both semantic and visual information. However, most of these techniques still have no mechanism for handling low frequency characters, which are the target of this work. The motivation of this method is similar to ours, but is only applicable to Chinese, in contrast to the method in this paper, which works on any language for which we can render text. In this paper, we proposed a new framework that utilizes appearance of characters, convolutional neural networks, recurrent neural networks to learn embeddings that are compositional in the component parts of the characters. More specifically, we collected a Wikipedia dataset, which consists of short titles of three different languages and satisfies the compositionality in the characters of the language. Next, we proposed an end-to-end model that learns visual embeddings for characters using CNN and showed that the features extracted from the CNN include both visual and semantic information. Furthermore, we showed that our Visual model outperforms the Lookup baseline model in low frequency instances. Additionally, by examining the character embeddings visually, we found that our Visual model is able to learn visually related embeddings. In summary, we tackled the problem of rare characters by using embeddings learned from images. In the future, we hope to further generalize this method to other tasks such as pronunciation estimation, which can take advantage of the fact that pronunciation information is encoded in parts of the characters as demonstrated in Fig. We also hope to apply the model to other languageswith complicated compositional writing systems, potentially including historical texts such as hieroglyphics or cuneiform."
447,2,4.43,4.0,3.57,True,acl_2017,train,"We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.","We introduce a new, unnormalized attention mechanism to this end. Experimental results show that variants of our model outperform prior work on four out of five tasks considered. Further experiments show the effect of discourse parse quality on text categorization performance, suggesting that future improvements to discourse parsing will pay off for text categorization, and validate our new attention mechanism. RST posits that a document can be represented by a tree whose leaves are elementary discourse units (EDUs, typically clauses or sentences). Internal nodes in the tree correspond to spans of sentences that are connected via discourse relations such as CONTRAST and ELABORATION. The six EDUs are indexed from A to F; the discourse tree organizes them hierarchically into increasingly larger spans, with the last CONTRAST relation resulting in a span that covers the whole review. Within each relation, the RST tree indicates the nucleus pointed by an arrow from its satellite (e.g., in the ELABORATION relation, A is the nucleus and B is the satellite). In most applications, RST trees are built by automatic discourse parsing, due to the expensive cost of manual annotation. It is clear that C is the root of the tree, and in fact this clause summarizes the review and suffices to categorize it as negative. form of inductive bias for our neural model, helping it to discern the most salient parts of a text in order to assign it a label. Our model is a recursive neural network built on a discourse dependency tree. It includes a distributed representation computed for each EDU, and a composition function that combines EDUs and partial trees into larger trees. At the top of the tree, the representation of the complete document is used to make a categorization decision. Let e be the distributed representation of an EDU. Future work might consider alternatives; we chose the bidirectional LSTM due to its effectiveness in many settings. Given the discourse dependency tree for an input text, our recursive model builds a vector representation through composition at each arc in the tree. Let v i denote the vector representation of EDU i and its descendants. For an internal node i, the composition function considers a parent and all of its children, whose indices are denoted by children(i). In defining this composition function, we seek for (i.) the contribution of the parent node e i to be central; and (ii.) the contribution of each child node e j be determined by its content as well as the discourse relation it holds with the parent. Cont. Exp. Exp. Cont. b.) The corresponding recursive neural network model built on the tree. This is motivated by RST, in which the presence of a node does not signify lesser importance to its siblings. Our recursive composition function, through the attention mechanism and the relation-specific weight matrices, is designed to learn how to differently weight EDUs for the categorization task. We refer to this model as the FULL model, since it makes use of the entire discourse dependency tree. We consider two additional baselines that are even simpler. No composition function is needed. This serves as a baseline to test the benefits of discourse, controlling for other design decisions and implementation choices. The parameters of all components of our model (top-level classification, composition, and EDU representation) are learned end-to-end using standard methods. Preprocessing. We lowercased all the tokens and removed tokens that contain only punctuation symbols. We replaced numbers in the documents with a special number token. Discourse parsing. Our model requires the discourse structure for each document. DPLP provides discourse segmentation, breaking a text into EDUs, typically clauses or sentences, based on syntactic parses provided by Stanford CoreNLP. Word embeddings. For larger datasets, we randomly initialize word embeddings and train them alongside other model parameters. Learning and hyperparameters. Online learning was performed with the optimization method and initial learning rate as hyperparameters. We selected five datasets of different sizes and corresponding to varying categorization tasks. Sentiment analysis on Yelp reviews. Framing dimensions in news articles. Dataset Task Classes Total Training Development Test Vocab. To compare with prior work, we use different experimental settings. immigration issue within the articles. We focused on predicting the single primary frame of each article. To select hyperparameters, we used a small set of examples from the corpus as a development set. Congressional floor debates. Each regularization method induces a linguistic bias to improve text classification accuracy, where the best result we repeated here is from the model with sentence regularizers. Movie reviews. On this corpus, we used the standard tenfold data split for cross validation and reported the average accuracy across folds. Congressional bill corpus. The task is to predict whether a bill will survive based on its content. Results. Indeed, on the MFC and Movies tasks, the discourse-ignorant ADDITIVE outperforms the FULL model. Results from prior work are reprinted from the corresponding publications. Boldface marks performance stronger than the previous state of the art. Even though the discourse parser is trained on news text, it still offers benefit to restaurant and movie reviews and to the genre of congressional debates. Even for news text, if the training dataset is small (e.g., MFC), a lighter-weight variant of discourse (UNLABELED) is preferred. Legislative bills, which have technical legal content and highly specialized conventions (see Appendix A in the supplementary material for an example), are arguably the most distant genre from news among those we considered. On that task, we see discourse working against accuracy. Note that the corpus of bills is more than ten times larger than three cases where our UNLABELED model outperformed past methods, suggesting that the drop in performance is not due to lack of data. It is also important to notice that the ROOT model performs quite poorly in all cases. This implies that discourse structure is not simply helping by finding a single EDU upon which to make the categorization decision. Qualitative analysis. In both cases, discourse structures help the FULL model bias to the important sentences. In addition, the weights produced by the FULL model do not make much sense, which we suspect the model was confused by the structure. Effect of parsing performance. A natural question is whether further improvements to RST discourse parsing would lead to even greater gains in text categorization. While advances in discourse parsing are beyond the scope of this paper, we can gain some insight by exploring degradation to the DPLP parser. An easy way to do this is to train it on subsets of the RST discourse treebank. We did not repeat the hyperparameter search. Unsurprisingly, lower parsing performance implies lower classification accuracy. The numbers on dependency edges are attention weights produced by the FULL model. parsing, through larger annotated datasets or improved models, could lead to greater gains. Attention mechanism. Of course, further architecture improvements may yet be possible. The assumption that all parts of a text should influence categorization equally persists even as more powerful representation learners are considered. The former used a latent variable for the informativeness of each sentence, and the latter used a neural network to learn an attention function. Neither used any linguistic bias, relying only on task supervision to discover the latent variable distribution or attention function. Our work builds the neural network directly on a discourse dependency tree, favoring the most central EDUs over the others but giving the model the ability to overcome this bias. The novelty in their approach was a data-driven regularization method that encouraged the model to collectively ignore groups of features found to coocur. Discourse structure was not considered. Discourse for sentiment analysis. Recently, discourse structure has been considered for sentiment analysis, which can be cast as a text categorization problem. One of the models is also based on discourse dependency trees, but using a handcrafted weighting scheme. We conclude that automatically-derived discourse structure is often helpful to categorization of many kinds of text, and the benefit increases with the accuracy of discourse parsing. This effect reverses for legislative bills, a text genre whose discourse structure diverges from that of news. These findings motivate further improvements to discourse parsing, especially for new genres."
369,3,3.31,4.0,1.69,False,acl_2017,train,"Morphology in unbalanced languages remains a big challenge in the context of machine translation. In this paper, we propose to de-couple machine translation from morphology generation in order to better deal with the problem. We investigate the morphology simplification with a reasonable trade-off between expected gain and generation complexity. For the Chinese-Spanish task, optimum morphological simplification is in gender and number. For this purpose, we design a new classification architecture which, compared to other standard machine learning techniques, obtains the best results. This proposed neural-based architecture consists of several layers: an embedding, a convolutional followed by a recurrent neural network and, finally, ends with sigmoid and softmax layers. We obtain classification results over 98% accuracy in gender classification, over 93% in number classification, and an overall translation improvement of 0.7 METEOR.","One of the most popular paradigms is still Statistical Machine Translation (SMT), which consists in finding the most probable target sentence given the source sentence using probabilistic models based on co-ocurrences. Recently, deep learning techniques applied to natural language processing, speech recognition and image processing and even in MT have reached quite successful results. In this paper, we are focusing on a challenging translation task, which is Chinese-to-Spanish. This translation task has the characteristic that we are going from an isolated language in terms of morphology (Chinese) to a fusional language (Spanish). This means that for a simple word in Chinese (e.g. alentar, alienta, alentamos, alientan. It is still difficult for MT in general (no matter which paradigm) to extract information from the source context to give the correct translation. We propose to divide the problem of translation into translation and then a postprocessing of morphology generation. However, the main contribution of our work is that we are using deep learning techniques in morphology generation. This gives us significant improvements in translation quality. The rest of the paper is organised as follows. In this section we are reviewing the previous related works on morphology generation for MT and on Chinese-Spanish MT approaches. Morphology generation There have been many works in morphological generation and some of them are in the context of the application of MT. In this cases, MT is faced in two-steps: first step where the source is translated to a simplified target text that has less morphology variation than the original target; and then, second step, a postprocessing module (morphology generator) adds the proper inflections. The main difference is that in PoS tagging the word itself has information about morphological inflection, whereas in our task, we do not have this information. In this paper, we use deep learning techniques to morphology generation or classification. Chinese-Spanish There are few works in Chinese-Spanish MT despite being two of the most spoken languages in the world. The first task was based on a direct translation for Chinese-Spanish. The second task provided corpus in Chinese-English and English-Spanish and asked participants to provide Chinese-Spanish translation through pivot techniques. The second task obtained better results than direct translation because of the larger corpus provided. Authors describe a hybrid method for constructing this system taking advantage of available resources such as parallel corpora that are used to extract dictionaries and lexical and structural transfer rules. In this section, we review the baseline system which is a standard phrase-based MT system and explain the architecture that we are using by dividing the problem of translation into: morphologically simplified translation and morphology generation. As a consequence, phrase-based MT is a commoditized technology used at the academic and commercial level. However, there are still many challenges to solve, such as morphology generation. This may be due to the fact that phrase-based MT uses a limited source context information to translate. In order to design the morphology generation module, we have to decide the morphology simplification we are applying to the translation. The main challenge of this task is that number and gender are generated from words where this inflection information (both number and gender) has been removed beforehand. With these results at hand, we propose an architecture of the morphology generation module, which is language independent and it is easily generalizable to other simplification schemes. The morphology generation procedure is summarised as follows and further detailed in the next subsections. We have investigated different set of features including information from both source and target languages. We propose a new deep learning classification architecture composed of different layers. We generate different alternatives of the classification output and rerank them using a language model. After, we use hand-crafted rules that allow to solve some specific problems. Figure also shows in red the main subprocesses that have been developed on this work. We propose to compare several features for recovering morphological information. Given that both Chinese and simplified Spanish languages do not contain explicit morphology information, we start by simply using windows of words as source of information. information about pronouns and the number of characters in the word. The main advantage of the second one is that it is not dependant on the alignment file generated during translation. Our classifiers did not have to train all types of words. Some types of words, such as prepositions (a, ante, cabo, de...), do not have gender or number. variations in gender or number. However, note that all types of words are used in the windows. Description We propose to train two different models: one to retrieve gender and another to retrieve number. Each model decides among three different classes. This recurrent neural network is relevant because it keeps information about previous elements in a sequence and, in our classification problem, context words are very relevant. This kind of recurrent neural network is able to maintain information for several elements in the sequence and to forget it when needed. Then, each word is represented as a numeric array and each window is a matrix. Convolutional. We add a convolutional neural network. This happens for types of words (determiners, nouns, verbs, pronouns and adjectives) that can have gender and number in other cases. words. Max Pooling. This layer allows to extract most relevant features from the input data and reduces feature vectors to half. LSTM. Each feature array is treated individually, generating a fixed size representation hi of the ith word using information of all the previous words (in the sequence). Sigmoid. Softmax. Motivation Our input data is PoS tagged and morphogically simplified before the classification architecture which largely reduces the information that can be extracted from individual words in the vocabulary. In addition, we can encounter out-ofvocabulary words for which no morphological information can be extracted. The main source of information available is the context in which the word can be found in the sentence. Considering the window as a sequence enforces the behaviour a human would have while reading the sentence. The information of a word consists in itself and the words that surround it. Sometimes information preceeds the word and sometimes information is after the word. like adjetives), which are modifying or complementing another word, generally take information from preceeding words. For example, in the sequence casa blanca, the word blanca could also be blanco, blancos or blancas but because noun and adjective are required to have gender and number agreement, the femenine word casa forces the femenine for blanca. While, for example, determiners usually take information from posterior words. This fact motivates that the word to classify has to be placed at the center of the window. Finally, given that we rely only on the context information since words themselves may not have any information, makes the recurrent neural network a key element in our architecture. The output h of the layer can be considered a context vector of the whole window maintaining information of all the previously encountered words (in the same window). At this point in the pipeline, we have two models (gender and number) that allow us to generate the full Part-of-Speech (PoS) tag by combining the best results of both classifiers. However, in order to improve the overall performance, we add a rescoring step followed by some hand-crafted rules. This way we force the original sentence order. This acyclic structure finds the best path in linear time, due to the fact that it goes through all layers and it picks the node with the greatest weight. The algorithm ends when this A set contains K paths or no further paths available to explore. B contains all suboptimal paths that can be elected in future iterations. In this section, we describe the data used for experimentation together with the corresponding preprocessing. In addition, we detail chosen parameters for the MT system and the classification algorithm. One of the main contributions of this work is using the Chinese-Spanish language pair. Number of sentences (S),words (W), vocabulary (V). M stands for millions and K stands for thousands. Therefore, differently from previous works on this language pair, we can test our approach in both a small and large data sets. Development and test sets are taken from UN corpus. All chunking or name entity recognition was disabled to preserve the original number of words. In both cases increasing this size lowers the accuracy of the system. The vocabulary size is fixed as a trade-off between giving enough information to the system to perform the classification while removing enough words to train the classifier for unknown words. Adding Chinese has a negative effect in the classifier accuracy. In order to generate the final full form we use the full PoS tag, generated from the postprocessing step, and the lemma, taken from the morphology-simplified translation output. If none matched, the lemma is used as translation, which usually happens only in the case of cities or demonyms. In this section we discuss the results obtained both in classification and in the final translation task. All algorithms were tested using features and parameters described in previous sections with the exception of random forests in which we added the one hot encoding representation of the words to the features. We observe that our proposed architecture achieves by large the best results in all tasks. It is also remarkable that the accuracy is lower using the bigger corpus, this is due to the fact that the small set consisted in texts of the same domain and the vocabulary had a better representation of specific words such as country names. In bold, best results. In bold, best results. Rescoring step improves final results. Chinese-to-Spanish translation task is challenging, specially because of Spanish being morphologically rich compared to Chinese. Main contributions of this paper include correctly de-coupling the translation and morphological generation tasks and proposing a new classification architecture, based on deep learning, for number and gender. Standard phrase-based MT procedure is changed to first translating into a morphologically simplified target (in terms of number and gender); then, introducing the classification algorithm, based on a new proposed neural network-based architecture, that retrieves the simplified morphology; and composing the final full form by using the standard Freeling dictionary. As further work, we intend to further simplify morphology and extend the scope of the classification."
105,2,5.0,4.0,3.0,True,acl_2017,train,"We present a neural model for morphological inflection generation which employs a hard attention mechanism, inspired by the nearly-monotonic alignment commonly found between the characters in a word and the characters in its inflection. We evaluate the model on three previously studied morphological inflection generation datasets and show that it provides state of the art results in various setups compared to previous neural and nonneural approaches. Finally we present an analysis of the continuous representations learned by both the hard and soft attention (Bahdanau et al., 2014) models for the task, shedding some light on the features such models extract.","We evaluate the model on three previously studied morphological inflection generation datasets and show that it provides state of the art results in various setups compared to previous neural and nonneural approaches. Morphological inflection generation involves generating a target word (e.g. The task is important for many down-stream NLP tasks such as machine translation, especially for dealing with data sparsity in morphologically rich languages where a lemma can be inflected into many different word forms. compress the input sequence to a single, fixed-sized continuous representation. in languages with suffixing morphology). This modeling suits the natural monotonic alignment between the input and output, as the network learns to attend to the relevant inputs before writing the output which they are aligned to. The combination of the bi-directional encoder and the controllable hard attention mechanism enables to condition the output on the entire input sequence. Moreover, since each character representation is aware of the neighboring characters, non-monotone relations are also captured, which is important in cases where segments in the output word are a result of long range dependencies in the input word. The recurrent nature of the decoder, together with a dedicated feedback connection that passes the last prediction to the next decoder step explicitly, enables the model to also condition the current output on all the previous outputs at each prediction step. The hard attention mechanism allows the network to jointly align and transduce while using a focused representation at each step, rather then the weighted sum of representations used in the soft attention model. In contrast to previous sequence-to-sequence work, we do not require the training procedure to also learn the alignment. The network can then be trained using straightforward cross-entropy loss. Finally, we analyze and compare our model and the soft attention model, showing how they function very similarly with respect to the alignments and representations they learn, in spite of our model being much simpler. We present a hard attention model for nearlymonotonic sequence to sequence learning, as common in the morphological inflection setting. We evaluate the model on the task of morphological inflection generation, establishing a new state of the art on three previouslystudied datasets for the task. We perform an analysis and comparison of our model and the soft-attention model, shedding light on the features such models extract for the inflection generation task. Imagine a machine with read-only random access to the encoding of the input sequence, and a single pointer that determines the current read location. We can then model sequence transduction as a series of pointer movement and write operations. We will now describe the network architecture. Notation We use bold letters for vectors and matrices. The equations for the LSTM variant we use are detailed in the supplementary material of this paper. However, as we see in the experiments section, the best-derivation approximation is effective in practice. These embeddings are parameters of the model which will be learned during training. The sequence is dependent on the alignment between the input and the output: ideally, the network will attend to all the input characters aligned to an output character before writing it. For this purpose, we first run a character level alignment process on the training data. We perform extensive experiments with three previously studied morphological inflection generation datasets to evaluate our hard attention model in various settings. In all experiments we report the results of the best performing neural and nonneural baselines which were previously published on those datasets. The implementation details for our models are described in the supplementary material section of this paper. We train a separate model for each fold and report exact match accuracy, averaged over the five folds. It was built by extracting Finnish, German and Spanish inflection tables from Wiktionary, used in order to evaluate their system based on string alignments and a semiCRF sequence classifier with linguistically inspired features. Their system also performs an align-and-transduce approach, extracting rules from the aligned training set and applying them in inference time with a proprietary character sequence classifier. It is also, to our knowledge, the first model that surpassed in overall accuracy the latent variable model on this dataset. While both models perform similarly on the train-set (with the soft attention model fitting it slightly faster), the hard attention model performs significantly better on the dev-set. Since Russian, German and Spanish employ a suffixing morphol-ogy with internal stem changes, they are more suitable for monotonic alignment as the transformations they need to model are the addition of suffixes and changing characters in the stem. The rest of the languages in the dataset employ more context sensitive morphological phenomena like vowel harmony and consonant harmony, which require to model long range dependencies in the input sequence which better suits the soft attention mechanism. c.h. templatic RU DE ES GE FI TU HU NA AR MA Avg. First, we notice the alignments found by the soft attention model are also monotonic, supporting our modeling approach for the task. This is performed by two consecutive write operations after the step operation of the relevant character to be replaced. Notice that in this case, the soft attention model performs a different alignment by aligning the character i to o and the character g to the sequence eg, which is not the expected alignment in this case from a linguistic point of view. The Learned Representations How does the soft-attention model manage to learn monotonic alignments? Perhaps the the network learns to encode the sequential position as part of its encoding of an input element? More generally, what information is encoded by the soft and hard alignment encoders? Since those vectors are outputs from the bi-LSTM encoders of the models, every vector of this form carries information of the specific character with its entire context. b) Colors indicate which character is encoded. This may be explained by the soft-attention mechanism encouraging the encoder to encode positional information in the input representations, which may help it to predict better attention scores, and to avoid collisions when computing the weighted sum of representations for the context vector. In contrast, our hardattention model has other means of obtaining the position information in the decoder using the step actions, and for that reason it does not encode it as strongly in the representations of the inputs. This behavior may allow it to perform well even with fewer examples, as the location information is represented more explicitly in the model using the step actions. Both models are trained using a forwardbackward approach, marginalizing over all possible alignments. Our model differs from the above by learning the alignments independently, thus enabling a dependency between the encoder and decoder. They divide the input into blocks of a constant size and perform soft attention separately on each block. We presented a hard attention model for morphological inflection generation. The model employs an explicit alignment which is used to train a neural network to perform transduction by decoding with a hard attention mechanism. Our model performs on par or better than previous neural and non-neural approaches on various morphological inflection generation datasets, while staying competitive with dedicated models even with very few training examples. Future work may include applying the model to other nearly-monotonic alignand-transduce tasks like abstractive summarization, transliteration or machine translation."
26,2,3.5,3.5,4.0,True,acl_2017,train,"With the rapid growth of knowledge bases (KBs) on the web, how to take full advantage of them becomes increasingly important. Knowledge base-based question answering (KB-QA) is one of the most promising approaches to access the substantial knowledge. Meantime, as the neural network-based (NN-based) methods develop, NN-based KB-QA has already achieved impressive results. However, previous work did not put emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers. This simple representation strategy is unable to express the proper information of the question. Hence, we present an end-to-end neural network model to represent the questions and their corresponding scores dynamically according to the various candidate answer aspects via cross-attention mechanism. In addition, we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers. Moreover, it also alleviates the out-ofvocabulary (OOV) problem, which helps the cross-attention model to represent the question more precisely. The experimental results on WebQuestions demonstrate the effectiveness of the proposed approach.","However, to handle such query languages, users are required to not only be familiar with the particular language grammars, but also be aware of the architectures of the KBs. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers. They belong to IR-based methods. Different from previous methods, NN-based methods represent both of the questions and the answers as semantic vectors. Then the complex process of KB-QA could be converted into a similarity matching process between an input question and its candidate answers in a semantic space. The candidates with the highest similarity score will be selected as the final answers. In NN-based methods, the crucial step is to compute the similarity score between a question and a candidate answer, where the key is to learn their representations. Previous methods put more emphasis on learning representation of the answer end. However, the representation of the question end is oligotrophic. Meantime, some questions may value answer type more than other answer aspects. While in some other questions, answer relation may be the most important information we should consider, which is dynamic and flexible corresponding to different questions and answers. Obviously, this is an attention mechanism, which reveals the mutual influences between the representation of questions and the corresponding answer aspects. We believe that such kind of representation is more expressive. We argue that simply selecting three independent CNNs is mechanical and inflexible. Thus, we go one step further, and propose a crossattention based neural network to perform KBQA. answer aspects, contains two parts: the answertowards-question attention part and the questiontowards-answer attention part. The former help learn flexible and adequate question representation, and the latter help adjust the question-answer weight, getting the final score. In this way, we formulate the cross-attention mechanism to model the question answering procedure. Note that our proposed model is an entire end-to-end approach which only depends on training data. Some integrated systems which use extra patterns and resources are not directly comparable to ours. Our target is to explore a better solution following the end-to-end KB-QA technical path. Moreover, we notice that the representations of the KB resources (entities and relations) are also limited in previous work. specifically, they are often learned barely on the QA training data, which results in two limitations. Due to the limited coverage of the training data, the OOV problem is common while testing, and many answer entities in testing candidate set have never been seen before. The attention of these resources become the same because they shared the same OOV embedding, and this will do harm to the proposed attention model. To tackle these two problems, we additionally incorporates KB itself as training data for training embeddings besides original questionanswer pairs. In this way, the global structure of the whole knowledge could be captured, and the OOV problem could be alleviated naturally. In summary, the contributions are as follows. It also alleviates the OOV problem, which is very helpful to the cross-attention model. The goal of KB-QA task could be formulated as follows. Given a natural language question q, the system returns an entity set A as answers. First, we identify the topic entity of the question, and generate candidate answers from Freebase. Then, a cross-attention based neural network is employed to represent the question under the influence of the candidate answer aspects. Cross-Attention based Neural Network q: Who is the president of France? In Freebase, the facts are represented by subject-predicate-object triples (s, p, o). For clarity, we call each basic element a resource, which could be either an entity or a relation. All the entities in Freebase should be candidate answers ideally, but in practice, this is time consuming and not really necessary. These entities constitute a candidate set Cq. We present a cross-attention based neural network, which represents the question dynamically according to different answer aspects, also considering their connections. Concretely, each aspect of the answer focuses on different words of the question and thus decides how the question is represented. Then the question pays different attention to each answer aspect to decide their weights. We will illustrate how the system works as follows. First of all, we have to obtain the representation of each word in the question. These representations retain all the information of the question, and could serve the following steps. Here, d means the dimension of the embeddings and vw denotes the vocabulary size of natural language words. Note that only one aspect(in orange color) is depicted for clarity. The other three aspects follow the same way. sentences. Note that if we use unidirectional LSTM, the outcome of a specific word contains only the information of the words before it, whereas the words after it are not taken into account. The forward LSTM handles the question from left to right, and the backward LSTM processes in the reverse order. Here, vk means the vocabulary size of the KB resources. Concretely, we employ four kinds of answer aspects: answer entity ae, answer relation ar, answer type at and answer context ac. Their embeddings are denoted as ee, er, et and ec, respectively. The crossattention mechanism is composed of two parts: the answer-towards-question attention part and the question-towards-answer attention part. The extent of attention can be measured by the relatedness between each word representation hj and an answer aspect embedding ei. We propose the following formulas to calculate the weights. Let n be the length of the question. Both of them are randomly initialized and updated during training. Subsequently, according to the specific answer aspect ei, the attention weights are employed to calculate a weighted sum of the hidden representations, resulting in a semantic vector that represent the question. Since we have already calculated the scores of (q, ei), we define the final similarity score of the question q and each candidate answer a as follows. Our aim is to select correct answers from a candidate set. When we judge a candidate answer, suppose we first look at its type, and we will re-read the question to find out which part of the question should be more focused (handling attention). Then we go to next aspect and re-read the question again, until the all the aspects are utilized. We believe that this mechanism is beneficial for the system to better understand the question with the help of the answer aspects, and it may lead to a performance promotion. We first construct the training data. Since we have (q, a) pairs as supervision data, candidate set Cq can be divided into two subsets, namely, correct answer set Pq and wrong answer set Nq. For some topic entities, there may be not enough wrong answers to acquire k wrong answers. With the generated training data, we are able to make use of pairwise training. The training loss is given as follows, which is a hinge loss. The intuition of this training strategy is to guarantee the score of positive question-answer pairs be higher than negative ones with a margin. The objective function is as follows. If the score of an candidate answer is within the margin compared with Smax, we put it in the final answer set. As stated before, we try to take into account the complete knowledge information of the KB. In TransE, relations are considered as translations in the embedding space. For consistency, we denote each fact as (s, p, o). TransE utilizes pairwise training strategy as well. And the training loss is given as follows. In our QA task, we filter out the completely unrelated facts to save time. Specifically, we first collect all the topic entities of all the questions as initial set. This is a compromising solution due to the large scale of Freebase. To employ the global information in our training process, we adopt a multi-task training strategy. Specifically, we perform KB-QA training and TransE training in turn. The proposed training process ensures that the global KB information act as additional supervision, and the interconnections among the resources are fully considered. In addition, as more KB resources are involved, the OOV problem is relieved. Since all the OOV resources have exactly the same attention towards a question, it will weaken the effectiveness of the attention model. So the alleviation of OOV is able to bring additional benefits to the attention model. The questions are collected from Google Suggest API, and the answers are labeled manually by Amazon MTurk. All the answers are from Freebase. We use threequarter of the training data as training set, and the left as validate set. Note that our proposed approach is an entire end-to-end method, which totally depends on training data. Their staged system is able to address more questions with constraints and aggregations. However, their approach applies numbers of manually designed rules and features, which come from the observations on the training set questions. These particular manual efforts reduce the adaptability of their approach. For KB-QA training, we use mini-batch stochastic gradient descent to minimize the pairwise train-ing loss. Both the word embedding matrix Ew and KB embedding matrix Ev are normalized after each epoch. All these hyperparameters of the proposed network is determined according to the performance on the validate set. To demonstrate the effectiveness of the proposed approach, we compare our method with state-of-the-art end-to-end NN-based methods. Besides the answer path, the subgraph contains all the entities and relations connected to the answer entity. The final vector is also obtained by bag-of-words strategy. They jointly consider the two mapping processes. Our approach employs bidirectional LSTM, cross-attention model and global KB information. Besides, our approach employs the global KB information. So, we believe that the results faithfully shows that the proposed approach is more effective than the other competitive methods. Model Analysis In this part, we further discuss the impacts of the components of our model. LSTM employs unidirectional LSTM, and uses the last hidden state as the question representation. Bi LSTM adopts a bidirectional LSTM. A-Q-ATT denotes the answer-towards-question attention part, and C-ATT stands for our crossattention. GKI means global knowledge information. The results prove that the proposed cross-attention model is effective. This directly shows the power of the attention model and the global KB information. It is instructive to figure out the attention part of the question when dealing with different answer aspects. The heat map will help us understand which parts are most useful for selecting correct answers. As for Q-A attention part, we see that answer type and answer relation are more important than other answer aspects in this example. We think this is due to the bias of the training data, and we believe these errors could be solved by introducing more instructive training data. Other errors include topic entity generation error and the multiple answers error (giving more answers than expected). We guess these errors are caused by the simple implementations of the related steps in our method, and we will not explain them in detail due to space limitation. In recent years, deep neural networks have been applied to many NLP tasks, showing promising results. The questions and KB triples were represented by vectors in a low dimensional space. Thus the cosine similarity could be used to find the most possible answer. BOW method was employed to obtain a single vector for both the questions and the answers. Pairwise training was utilized, and the negative examples were randomly selected from the KB facts. The key idea was to involve as much information as possible in the answer end. Besides the answer triple, the subgraph contained all the entities and relations connected to the answer entity. The final vector was also obtained by bag-of-words strategy. The KB-QA task was divided into two steps. Firstly, they found the topic entity of the question. Then, the rest of the question was represented by CNNs and used to match relations. Actually, these two methods followed the SP-based manner, but they took advantage of neural networks to obtain intermediate mapping results. They considered the different aspects of answers, using three columns of CNNs to represent questions respectively. The difference is that our approach uses cross-attention mechanism for each unique answer aspect, so the question representation is not fixed to only three types. Moreover, we utilize the global KB information. The attention mechanism has been widely used in different areas. They improved the encoder-decoder Neural Machine Translation (NMT) framework by jointly learning align and translation. They argued that representing source sentence by a fixed vector is unreasonable, and proposed a soft-align method, which could be understood as attention mechanism. They utilized local attention-based model that generated each word of the summary conditioned on the input sentence. They stacked an attentive maxpooling above convolution layer to model the relationship between predicates and question patterns. Our approach differs from previous work in that we use attentions to help represent question dynamically, not generating current word from vocabulary as before. In this paper, we focus on KB-QA task. Firstly, we consider the impacts of the different answer aspects when representing the question, and propose a novel cross-attention model for KB-QA. Specifically, we employ the focus of the answer aspects to each question word and the attention weights of the question towards the answer aspects. This kind of dynamic representation is more precise and flexible. Secondly, we leverage the global KB information, which could take full advantage of the complete KB, and also alleviate the OOV problem for the attention model. The extensive experiments demonstrate that the proposed approach could achieve better performance compared with state-of-the-art end-to-end methods."
484,2,4.0,4.0,3.0,True,acl_2017,train,"End-to-end automatic speech recognition (ASR) has become a popular alternative to conventional DNN/HMM hybrid systems because it avoids the need for linguistic resources such as pronunciation dictionary, tokenization, and contextdependency trees, leading to a greatly simplified model-building process. There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming. This paper proposes joint CTCattention end-to-end ASR, which effectively utilizes both advantages in training and decoding. We have applied the proposed method to two ASR benchmarks (spontaneous Japanese and Mandarin Chinese), and showing the comparable performance to conventional hybrid ASR systems without linguistic resources.","However, current systems lean heavily on the scaffolding of complicated legacy architectures that grew up around traditional techniques. These present the following problems that we may seek to eliminate. Flat start: many module-specific processes are required to build an accurate module: for example, when we build an acoustic model from scratch, we have to first build hidden Markov model (HMM) and Gaussian mixture model (GMM) followed by deep neural networks (DNN). Linguistic knowledge: to well factorize acoustic and language models, we need to have a lexicon model, which is usually based on a hand-crafted pronunciation dictionary to map word to phoneme sequence. Conditional independence assumptions: the current ASR systems often use conditional independence assumptions (especially Markov assumptions) during the above factorization and to make use of GMM, DNN, and n-gram models. The data do not necessarily follow such assumptions leading to model mis-specification. Although this integration is often efficiently handled by finite state transducers, the construction and implementation of welloptimized transducers is very complicated. End-to-end ASR has the goal of simplifying the above module-based architecture into a singlenetwork architecture within a deep learning framework, in order to address the above issues. End-to-end ASR methods typically rely only on paired acoustic and language data. Without the additional language data, they can suffer from data sparseness or outof-vocabulary issues. To improve generalization, and handle out-of-vocabulary problems, it is typical to use the letter representation rather than the word representation for the language output sequence, which we adopt in the descriptions below. The attention-based end-to-end method solves the ASR problem as a sequence mapping from speech feature sequences to text by using encoderdecoder architecture. At each output position, the decoder network computes a matching score between its hidden state and the states of the encoder network at each input time, to form a temporal alignment distribution, which is then used to extract an average of the corresponding encoder states. This basic temporal attention mechanism is too flexible in the sense that it allows extremely non-process in the above modules including lattice generations. sequential alignments. However in speech recognition, the feature inputs and corresponding letter outputs generally proceed in the same order, with only small within-word deviations (e.g. Another problem is that the input and output sequences in ASR can have very different lengths, and these vary greatly from case to case, depending on the speaking rate and writing system, making it more difficult to track the alignment. However, an advantage is that the attention mechanism does not require any conditional independence assumptions, and could address all the problems cited above. We propose to take advantage of the constrained CTC alignment in a hybrid CTC-attention based system. This greatly reduces irregular alignments without any heuristic search techniques. During decoding, we propose to use a rescoring technique, where hypotheses of attentionbased ASR are refined by scores obtained by using encoder outputs. The formulation is intended to clarify the conditional independence assumption (Markov assumption), which is an important property to characterize these three methods. xt is a D dimensional speech feature vector (e.g., log Mel filterbanks) at frame t and wn is a word at position n in vocabulary V. Eq. The conditional independence assumption in Eq. Therefore DNNs with long context features or recurrent neural networks are often used to mitigate the issue. The conversion from W to HMM states is deterministically performed by using a pronunciation dictionary through a phoneme representation.   The CTC formulation also follows from Bayes decision theory (Eq. Although Eq. alt is an attention weight, and represents a soft alignment of hidden vector ht for each output cl based on the weighted summation of hidden vectors to form letter-wise hidden vector rl in Eq. It implicitly combines acoustic models, lexicon, and language models as encoder, attention, and decoder networks, which can be jointly trained as a single deep neural network. A major focus of this paper is to address this problem by proposing joint CTC-attention models. This section explains a joint CTC-attention network, which utilizes both benefits of CTC and attention during training and decoding steps in ASR. coder within the multi-task learning (MTL) framework. Unlike the sole attention model, the forward-backward algorithm of CTC can enforce monotonic alignment between speech and label sequences during training. That is, rather than solely depending on data-driven attention methods to estimate the desired alignments in long sequences, the forwardbackward algorithm in CTC helps to speed up the process of estimating the desired alignment. The inference step of our joint CTC-attention based end-to-end speech recognition is performed by output-label synchronous decoding with a beam search like attention-based ASR. Since the attention is generated by the decoder network it may prematurely predict the end-of-sequence label, even when it has not attended all the encoder frames, making the hypothesis too short. On the other hand, it may predict the next label with a high probability by attending the same portions as those attended before. In this case, the hypothesis becomes very long and includes repetitions of the same label sequence. It is also effective to control the hypothesis length by minimum and maximum lengths to some extent, where the minimum and the maximum are selected as fixed rations to the length of input speech. But, since there are exceptionally long or short transcripts compared to the input speech, it is difficult to balance saving such exceptional transcripts and preventing hypotheses with irrelevant lengths. Accordingly, it increases when paying close attention to some frames for the first time, but does not increase when paying attention again to the same frames. This property is effective to avoid looping the same label sequence within a hypothesis. Our joint CTC-attention approach combines CTC and attention-based sequence probabilities in the inference step, as well as the training step. The decoding objective is defined similarly to Eq. We implement the joint CTC-attention inference method as a two-pass strategy. The first pass obtains a set of complete hypotheses using the beam search, where only attention-based sequence probabilities are considered. We used Japanese and Mandarin Chinese ASR benchmarks to show the effectiveness of the proposed joint CTC-attention approach. The main reason for choosing these two languages is that those ideogram languages have relatively shorter lengths for letter sequences than those in alphabet languages, which reduces computational complexities greatly, and makes it easy to handle context information in a decoder network. Our preliminary investigation shows that Japanese and Mandarin Chinese end-to-end ASR can be easily scaled up, and shows state-of-the-art performance without using various tricks developed in English tasks. CSJ is a standard Japanese ASR task based on a collection of monologue speech data including academic lectures and simulated presentations. Corpus of Spontaneous Japanese speech recognition (CSJ) task. MTL significantly outperformed attention-based ASR in the all evaluation tasks, which confirms the effectiveness of joint CTC-attention model. Unlike the proposed method, these methods use linguistic resources including a morphological analyzer, pronunciation dictionary, and language model. Note that since the amount of training data and experimental configurations of the proposed and reference methods are different, it is difficult to compare the performance listed in the table directly. However, since the CERs of the proposed method are comparable to those of the best reference results, we can state that the proposed method reaches the state-of-the-art performance. HKUST Mandarin Chinese conversational telephone speech recognition (MTS) task. This is an advantage of joint decoding over conventional approaches that require many tuning parameters. This paper proposes end-to-end ASR by using joint CTC-attention, which outperformed attention-based end-to-end ASR by solving the misalignment issues. This method does not require the use of use linguistic resources including morphological analyzer, pronunciation dictionary, and language model, which are essential component of building conventional Japanese and Mandarin Chinese ASR systems. Nevertheless, the method achieved comparable performance to the state-of-the-art conventional systems for the CSJ and MTS tasks. In addition, the proposed method does not require GMM-HMM construction for initial alignments, DNN pre-training, lattice generation for sequence discriminative training, complex search in decoding (e.g., FST decoder or lexical tree search based decoder). Thus, the method greatly simplifies ASR building process with even smaller amounts of coding. Future work will apply this technique to the other languages including English, where we have to solve an issue of long sequence lengths, which requires heavy computation cost and makes it difficult to train a decoder network. The first chunk of blue characters in attention-based ASR is appeared again, and the whole second chunk part becomes insertion errors. The latter half of the sentence in attention-based ASR is broken, which causes deletion errors. The proposed joint CTC-attention avoids these issues. b and g are vector parameters."
715,2,4.57,3.0,3.0,True,acl_2017,train,"This paper proposes to tackle opendomain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This problem combines the challenges of document retrieval, to find relevant articles, and of machine comprehension of text to identify the answer spans from those articles. Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.","Using Wikipedia articles as the knowledge source causes the task of question answering (QA) to combine the challenges of both large-scale open-domain QA and of machine comprehension of text. Our work treats Wikipedia as a collection of articles and does not rely on its internal graph structure. As a result, our approach is generic and could be switched to another collection of documents. As a result, such systems heavily rely on information redundancy among the sources to answer correctly. Having a single knowledge source forces the model to be very precise while searching for an answer as the evidence might appear only once. However, those machine comprehension resources typically assume that a short piece of relevant text is already identified and given to the model, which is not realistic for building an opendomain QA system. In sharp contrast, methods that use KBs or information retrieval over documents have to employ search as an integral part of the solution. Instead we aim at the setting of simultaneously maintaining the challenge of machine comprehension, which requires the deep understanding of text, while keeping the realistic constraint of searching over a large open resource. In this paper, we show how multiple existing QA datasets can be used to evaluate this framework by requiring an open-domain system to perform well on all of them at once. Finally, our full system is evaluated using multiple benchmarks. In particular, we show that performance is improved across almost all datasets through the use of multitask learning and distant supervision compared to single task training. However, KBs have inherent limitations (incompleteness, fixed schemas) that motivated researchers to return to the original setting of answering from raw text. An objective of this paper is to test how such new methods can perform in an open-domain QA framework. QA using Wikipedia as a resource has been explored previously. Instead of using it as a resource for seeking the answers to questions, they focused on validation of the answers as returned by their QA system, and the use of Wikipedia categories for determining a set of patterns that should fit with the expected answer. In our work, we consider the comprehension of text only, and use the Wikipedia text documents as a sole resource for the reasons described in the introduction, that is to help focus on the machine comprehension task. DeepQA is a very sophisticated system that relies on both unstructured information including text documents as well as structured data such as KBs, databases and ontologies to generate candidate answers or vote over evidence. YodaQA is an open source system modeled after DeepQA, similarly combining websites, information extraction, databases and Wikipedia in particular. Several works have attempted to combine multiple QA training datasets via multitask learning to (i) achieve improvement across the datasets via task transfer; and (ii) to provide a single general system capable of asking different kinds of questions due to the inevitably different data distributions across the source datasets. Our work follows similar themes, but in the setting of having to retrieve and then read text documents, rather than using a KB. Following classical QA systems, we use an efficient (non-machine learning) document retrieval system to first narrow our search space and focus on reading only articles that are likely to be relevant. Articles and questions are compared as TF-IDF weighted bag-ofword vectors. We further improve our system by taking local word order into account with n-gram features. Those articles are then processed by Document Reader.         We use three simple binary features, indicating whether pi can be exactly matched to one question word in q, either in its original, lowercased or lemma form. We also add a few manual features which reflect some properties of token pi in its context, which include its part-of-speech (POS) and named entity recognition (NER) tags and its (normalized) term frequency (TF). Compared to the exact match features, these features add soft alignments between similar but non-identical words (e.g., car and vehicle).   Prediction At the paragraph level, the goal is to predict the span of tokens that is most likely the correct answer.   To make scores compatible across paragraphs in one or several retrieved documents, we replace the softmax with an unnormalized exponential. Our final prediction is then the argmax over all considered paragraph spans. Some of these were later absorbed into the Ottoman Empire, while others were granted various types of autonomy during the course of centuries. It is possibly the best-known of all state mottos, partly because it conveys an assertive independence historically found in American political philosophy and partly because of its contrast to the milder sentiments found in other state mottos. The atomic mass of these isotopes varied by integer amounts, called the whole number rule. WikiMovies Q: Who wrote the film Gigli? In each case we show an associated article where distant supervision (DS) correctly identified the answer within it, which is highlighted (See Sec. DS: distantly supervised training data. distant supervision. Each example is composed of a paragraph extracted from a Wikipedia article and an associated human-generated question. The answer is always a span from this paragraph and a model is given credit if its predicted answer matches it. For the task of evaluating open-domain question answering over Wikipedia, we use the SQuAD development set QA pairs only, and we ask systems to uncover the correct answer spans without having access to the associated paragraphs. in the standard SQuAD setting. SQuAD is one of the largest general purpose QA datasets currently available. SQuAD questions have been collected via a process involving showing a paragraph to each human annotator and asking them to write a question. As a result, their distribution is quite specific. We hence propose to train and evaluate our system on other datasets developed for open-domain QA that have been constructed in different ways (not necessarily in the context of answering from Wikipedia). It was created by crawling questions through the Google Suggest API, and then obtaining answers using Amazon Mechanical Turk. We converted each answer to text by using entity names so that the dataset does not reference Freebase IDs and is purely made of plain text question-answer pairs. Originally created from Freebase, the examples are built such that they can also be answered by using a subset of Wikipedia as the knowledge source (the title and the first section of articles from the movie domain). All the QA datasets presented above contain training portions, but CuratedTREC, WebQuestions and WikiMovies only contain question-answer pairs, and not an associated document or paragraph as in SQuAD, and hence cannot be used for training Document Reader directly. We use the following process for each questionanswer pair to build our training set. All paragraphs from those articles without an exact match of the known answer are directly discarded. If any named entities are detected in the question, we remove any paragraph that does not contain them at all. If there is no paragraph with nonzero overlap, the example is discarded; otherwise we add it to our DS training dataset. Note that we can also generate additional DS data for SQuAD by trying to find mentions of the answers in other paragraphs other than the original paragraph provided in the dataset (from other pages or the same page that the given paragraph was in). This section first presents evaluations of our Document Retriever and Document Reader modules separately, and then describes tests of their combination, DrWiki, for open-domain QA on the full Wikipedia. We first examine the performance of our Document Retriever module on all the QA datasets. Results on all datasets indicate that our simple approach outperforms Wikipedia Search, especially with bigram hashing. SQuAD has been a very competitive machine comprehension benchmark since its creation and we only list the best-performing systems in the table. Additionally, we think that our model is conceptually simpler than most of the existing systems. We conducted an ablation analysis on the feature vector of paragraph tokens. More interestingly, if we remove both faligned and fexact match, the performance drops dramatically, so we conclude that both features play a similar but complementary role in the feature representation related to the paraphrased nature of a question vs. the context around an answer. Despite the difficulty of the task compared to machine comprehension (where you are given the right paragraph) and unconstrained QA (using redundant resources), DrWiki still provides reasonable performance across all four datasets. We are interested in a single full system that can answer any question using Wikipedia. The single model trained only on SQuAD is outperformed on three of the datasets by the multitask system that uses distant supervision. However performance when training on SQuAD alone is not far behind indicating that task transfer is occurring. Nevertheless, the best single model that we can find is our overall goal, and that is the Multitask(DS) system. The gap is slightly bigger on WebQuestions likely because that is based on Freebase which YodaQA uses directly. Overall, our results indicate that we have identified a key challenging task for researchers to focus on. We studied the task of open-domain QA using Wikipedia as the unique knowledge source. This brings unique challenges for machine comprehension systems where integrating search, distant supervision and multitask learning provides an effective complete system, whereas machine comprehension systems alone cannot solve the overall task. Evaluating the individual components and the overall system across multiple benchmarks showed the efficacy of our approach. Future work should aim to improve over our DrWiki system. Two obvious angles of attack are: (i) incorporate the fact that Document Reader aggregates over multiple paragraphs and documents directly in the training, as it currently trains on paragraphs independently; and (ii) perform endto-end training across the Document Retriever and Document Reader pipeline, rather than independent systems."
579,1,4.0,3.0,3.0,True,acl_2017,train,"The goal of open information extraction (OIE) is to extract surface relations and their arguments from naturallanguage text in an unsupervised, domainindependent manner. In this paper, we explore how overly-specific extractions can be reduced in OIE systems without producing uninformative or inaccurate results. We propose MinIE, an OIE system that produces minimized, annotated extractions. At its heart, MinIE rewrites OIE extractions by (1) identifying and removing parts that are considered overly specific; (2) representing information about polarity, modality, attribution, and quantities with suitable annotations instead of in the actual extraction. We conducted an experimental study with several real-world datasets and found that MinIE achieves competitive or higher precision and recall than most prior systems, while at the same time producing much shorter extractions.","In this paper, we explore techniques to minimize OIE extractions and propose a new OIE system called MinIE. To retain the original meaning to the extent possible, MinIE provides suitable annotations for each extraction. MinIE follows OLLIE, but uses additional and more expressive annotations. In fact, MinIE considers the relations was born on and was not born on equivalent up to polarity. In practice, these systems generally perform better than indicated in this example. born on), argument (beau. Apart from polarity annotations, MinIE also provides annotations for modality, attribution, quantities, and for other rewrites that have been performed. Generally, minimizing OIE extractions is inherently limited in scope due to the absence of domain knowledge. Thus MinIE does not and cannot fully minimize its extractions in all cases. Instead, MinIE supports multiple minimization modes, which differ in their aggressiveness. We conducted an experimental study with several realworld datasets and found that the various modes of MinIE produced much shorter extractions than prior systems, while simultaneously achieving competitive or higher precision. MinIE sometimes fell behind prior systems in terms of the total number of extractions. We found that those prior system can output a large number of redundant extractions; if redundant extractions were discounted, MinIE was competitive. Since then, many different OIE systems have been proposed in the literature. A general challenge in OIE is to avoid both uninformative and overly specific extractions. MinIE currently does not handle nested extractions. Stanford OIE deletes all subconstituents connected by certain typed dependencies (e.g., amod). For some typed dependencies (e.g., prep or dobj), the system makes use of a frequency constraint along the lines of ReVerb. The goal of MinIE is to provide minimized, annotated OIE extractions. As ClausIE, MinIE focuses on extractions obtained from individual clauses (with the exception of attribution; see Sec. Each clause is broken up into its constituents: one subject (S), one verb (V) and optionally an indirect object (O), a direct object (O), a complement (C) and one or more adverbials (A). ClausIE then identifies the clause type, which indicates which of the constituents are obligatory and which optional from a syntactic point of view. The general architecture of MinIE is outlined in Fig. Each input sentence is run through ClausIE and a separate extractor for implicit facts (Sec. We refer to the resulting extractions as input extractions. MinIE then rewrites each input extraction to a basic extraction by detecting and eliminating information about polarity (Sec. To further minimize basic extractions, MinIE can be run in various modes (Sec. The modes differ in the amount of minimizations being applied and which additional resources are being used. The result of this phase is a minimized extraction. Finally, MinIE outputs each minimized extraction along with annotations that provide information about the minimization process. Some annotations (such as polarity) are crucial to correctly represent the extraction, others (such as original relation) give additional information about which parts have been removed during minimization. Before we discuss how we minimize extractions, we summarize how MinIE obtains its input extractions. As mentioned before, MinIE uses ClausIE as its underlying OIE system. The relations extracted by ClausIE consist of only verbs and negation particles (cf. Tab. To do so, we aim to decide which constituents of the input sentence should be pushed into the relation. This allows us, for example, to handle cases with multiple adverbials or with adverbials that do not start with a preposition. Note that the relations produced in this step may sometimes be considered overly specific; they will be minimized further in subsequent steps. If the adverbial starts with a preposition, we push the preposition into the relation. For example, we rewrite (Superman, lives, in Metropolis) to (Superman, lives in, Metropolis). This allows us to distinguish live in from relations such as live during, live until, live through, and so on. SVOO, SVOC. We generally push the indirect object (SVOO) or direct object (SVOC) into the relation. In both cases, the verb requires two additional constituents: we use the first one to enrich the relation and the second one as an argument. For example, we rewrite (Superman, declared, the city safe) to (Superman, declared the city, safe). As this example indicates, this rewrite is somewhat unsatisfying; further exploration is an interesting direction for future work. SVOA. If the adverbial consists of a single adverb, we push it to the relation and use the object as an argument. Otherwise, we proceed as in SVOC, but additionally push the starting preposition (if present) of the adverbial to the relation. For example, (Ana, turned, the light off) becomes (Ana, turned off, the light), and (The doorman, leads, visitors to their destination) becomes (The doorman, leads visitors to, their destination). Optional adverbials. If the clause contains optional adverbials, ClausIE creates one extraction per optional adverbial and one without any optional adverbial. We process the former extractions as if the adverbial were obligatory. For example, the extraction (Faust, made, a deal with the Devil) becomes (Faust, made a deal with, the Devil). Here the actual clause type is SVO, but we process it as if it were SVOA. Infinitive forms. If the argument starts with a to-infinitive verb, we move it to the relation. For example, (Superman, needs, to defeat Lex) becomes (Superman, needs to defeat, Lex). ClausIE produces non-verb-mediated extractions from appositions and possesives. We refer to these extractions as implicit extractions. In MinIE, we use implicit extractions as a signal for the minimization of non-implicit ones (see Sec. To make the minimization more effective, we include additional patterns identified in prior work. For example, if the sentence contains president Barack Obama, we obtain (Barack Obama, is, president). The basic minimization step of MinIE detects and eliminates information about polarity (Sec. Our focus is on methods that are both domain-independent and (considered) safe to use. MinIE annotates each extraction with information about its factuality. Tab. The polarity indicates whether or not a triple occured in negated form. In order to assign a polarity value to a triple, we aim to detect whether the relation indicates a negative polarity. If so, we assign negative polarity to the whole triple. We detect negations using a small lexicon of negation words (consisting of the words no, not, never, none). Whenever a word from the lexicon is detected, it is dropped from the relation and the triple is annotated with negative polarity (-) and the negation word. In Tab. We found that this simple approach successfully spots many negations present in the input relations. Note that whenever a negation is missed, MinIE still produces correct results because such negations are retained in the triple. For example, if a negations occurs in the subject or argument of the extraction, MinIE misses it. This extraction is correct, but can be further minimized to (people; were hurt in; fire) with a negative polarity annotation. MinIE does not address these problems, but restricts attention to the easy cases. The modality of a triple indicates whether the triple is considered a certainty (CT) or a possibility (PS) in the clause in which it occurs. We proceed similarly to the detection of negations and consider a triple certain unless we find evidence indicating possibility. It mainly contains adverbs such as probably, possibly, maybe, likely and infinitive verb phrases such as is going to, is planning to, or intends to. Whenever words indicating possibility are detected, we remove these words from the triple and annotate the triple as possible (PS) along with the words just removed. The attribution of a triple is the supplier of information given in the input sentence, if any. The factuality is independent from the factuality of the extracted triple; it indicates whether the supplier expresses a negation or a possibility. Tab. For the former, MinIE searches for extractions that contain entire clauses as arguments. We then compare the relation against a domain-independent dictionary of relations indicating attributions (e.g., say or believe). If we find a match, we create an attribution annotation and use the subject of the extraction as the supplier of information. Each entry in the attribution dictionary is annotated with a modality. For example, relations such as know, say, or write express certainty, whereas relations such as believe or doubt express possibility. If the relation is modified by a negation word (see Sec. After the attribution has been established, we run ClausIE on the main clause and add the attribution to each extracted triple. The remaining part of the clause is processed as before. A quantity is a phrase that expresses an amount of something. It either modifies a noun phrase (e.g. Whenever we detect a quantity, we replace it by a placeholder Q and add an annotation with the original quantity. The goal of this step is to unify extractions that only differ in quantities. We detect quantities by looking for numbers (NER types such as NUMBER or PERCENT) or words expressing quantities (such as all, some, many). We then extend these words via relevant typed dependencies, such as quantity modifiers (quantmod) and or adverbial modifiers (advmod). Once basic minimization has been performed, MinIE further minimizes extractions by dropping additional words. Since such minimization is risky, MinIE employs various minimization modes with different levels of aggressiveness. We discuss these modes in turn, from the safest mode to the most aggressive one. MinIE represents each constituent of a basic extraction by its words, its dependency structure, its part-of-speech tags, and its named entities (detected by a named-entity recognizer). Whenever a word is dropped from a constituent, we add an annotation with the original, unmodified constituent. We first drop all constituents that are covered by the implicit extractions discussed in Sec. We then drop all determiners, all possessive pronouns, all adverbs modifying the verb in the relation, and all adjectives and adverbials modifying persons. An exception to these rules is given by named entities, which we consider as stable subconstituents. Note that this procedure cannot be considered safe when used on input extractions. We consider it safe, however, when applied to basic extractions. In particular, all determiners, pronouns, and adverbs indicating negation, modality, or quantities are already processed during basic minimization and are consequently retained in annotations. Moreover, due to the way we construct input extractions, each relation has at most one adverb modifying its verb. The safe mode thus only performs simple rewrites such as the great city to great city, his presidency to presidency, had also to had, and the eloquent president Mr. Barack Obama to Barack Obama. The frequency and collocations modes make use of a dictionary D of stable constituents. For each instance of P, we drop a certain subset of its words. In particular, we do not touch subconstituents that contain prepositions because these are notoriously difficult to handle (e.g., we do not want to minimize Bill of Rights to Bill). Our main goal is to retain phrases that occur in D, even if they occur in different order or with additional modifiers. To achieve this goal, we proceed as follows for each instance I of P. We first mark all nouns (or the named entity) as stable. Afterwards, we create a set of potentially stable subconstituents (PSS). Once all PSS have been processed, we drop all words from I that are not marked stable. To generate the set of PSS, we enumerate all subconstituents of I that are syntactically valid. For example, infamous symbol or cold infamous war are syntactically valid, whereas very symbol or very cold war are not. For each such subsequence, we generate all permutations of adverbial and adjective modifiers originating from the same node in the dependency structure. This step ensures that the order of modifiers in I does influence whether or not a word is marked stable. In our experimental study, we included all multi-word expressions from WordNet and Wiktionary, which we consider domain-independent. MinIE-A proceeds the other way around: all words for which we are not sure that they need to be retained are dropped. We mainly include this mode as a baseline and to study how effective such an approach is empirically. For every word in a constituent of a basic extraction, we drop all adverbial, adjective, possessive, and temporal modifiers (recursively along with their modifiers). We also drop prepositional attachments (e.g., man with apples becomes man), quantifiers modifying nouns, auxiliary modifiers to the main verb (e.g., have escalated becomes escalated), and all compound nouns that have a different named-entity type than their head word (e.g., European Union offical becomes offical). In most cases, after applying these steps, only a single word, named entity, or a sequence of nouns remains for subject and argument constituents. The goal of our experimental study was to investigate the differences in the various modes of MinIE w.r.t. precision, recall, and extraction length as well as to compare it to popular prior methods. All source code, datasets, extractions, labels, and labelling guidelines will be made publically available. We also submitted them to SoftConf. Datasets. Datasets NYT and Wiki were used in the evaluation of ClausIE and NestIE. Methods. We used ClausIE, OLLIE, and Stanford OIE as baseline systems. For MinIE-F, we built the dictionary using the entire NYT corpus. Labeling. We labeled every extraction of NYT and Wiki with two labels: one for the triple and one for the attribution. A triple is labeled as correct if it is entailed by its corresponding clause; here factuality annotations are taken into account. The attribution is labeled incorrect if there is an attribution which is neither present in the triple nor in the attribution annotation. In Tab. Measures. For each system, we measured the total number of extractions (termed recall), the fraction of correct triples (factual precision), and the fraction of correct triples with correct attributions (attribution precision). Finally, we observed that some systems produced a large number of redundant extractions. To obtain insight into the amount of redundancy, we also report the number of nonredundant extractions. Tab. For MinIE, we show the fraction of negative polarity and possibility annotations for triples only (i.e., we exclude the attribution annotation). In terms of recall, MinIE and Stanford OIE were roughly on par; OLLIE fell behind and ClausIE went ahead. The reason why ClausIE has more extractions than MinIE is that different (partly redundant) extractions from ClausIE may lead to the same minimized extraction. This is also also the reason why recall drops in the more aggressive modes of MinIE. A notable exception is Stanford OIE, which produces many extraction variants by dropping different subsets of words. We observed that all modes of MinIE achieved significantly smaller extractions than ClausIE (its underlying OIE system), and that the average extraction length indeed dropped as we used more aggressive modes. Only MinIE-A produced shorter extractions than Stanford OIE. prec. prec. Only OLLIE and MinIE make use of annotations. The statistics also indicate that the basic minimizations of MinIE fire quite frequently, with the notable exception of negations. In our second experiment, we compared the precision of the various systems on the smaller NYT and Wiki datasets. We did not run MinIE-F on Wiki because it was trained on the full NYT corpus. Our results are summarized in Tab. For MinIE, the factual precision dropped as expected when we use more aggressive modes. Interestingly, the drop in precision between MinIE-S, MinIE-F, and MinIE-C was quite low, even though extractions get shorter. The aggressive minimization of MinIE-A led to a more severe drop in precision. ClausIE and OLLIE had similar factual precision. Note that MinIE-S had higher precision than ClausIE. Reasons include that MinIE-S produces additional high-precision implicit extractions, breaks up very long and thus error-prone extractions, and uses a different version of Stanford CoreNLP. As for attribution precision, most of the sentences in our samples did not contain attributions; these numbers should thus be taken with a grain of salt. OLLIE and MinIE achieved similar results, even though MinIE additionally annotated attributions with factuality information. This problem can be addressed by using richer collocation dictionaries. We presented MinIE, a system that minimizes and annotates OIE extractions. We believe that the use of minimized extractions with suitable annotations are a promising direction for OIE. The techniques presented here can be seen as a step towards this goal. Future directions include additional annotation types (e.g."
71,2,3.5,5.0,4.0,True,acl_2017,train,"The ambitious goal of this work is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in Wikipedia. Given a document in any of these languages, our framework is able to identify name mentions, assign a coarse-grained or fine-grained type to each mention, and link it to an English Knowledge Base (KB) if it is linkable. We achieve this goal by performing a series of new KB mining methods: generating “silver-standard” annotations by transferring annotations from English to other languages through crosslingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from crosslingual links. Both name tagging and linking results for 282 languages are promising on Wikipedia data. The results on non-Wikipedia data (news articles and discussion forum posts) are also comparable to the supervised models trained from manually annotated documents including thousands of names. All the data sets, resources and systems for 282 languages will be made publicly available as a new benchmark.","The founder of the Chinese Communist Party (CCP) and the People's Republic of China.) Name mentions in Wikipedia are often labeled as anchor links to their corresponding referent pages. The major challenges and our proposed new solutions are summarized as follows. The first step is to classify English Wikipedia entries into certain entity types and then propagate these labels to other languages. We also map each AMR type to YAGO type hierarchy. Therefore, the generated typing schema has multiple levels of granularity. Refine annotations through self-training. However, such initial annotations are too incomplete and inconsistent. Previous work used name string match to propagate labels. Customize annotations through cross-lingual topic transfer. For some populous languages, we obtain millions of labeled sentences from selftraining. For the first time, we propose to customize the annotations for specific down-stream applications. Derive morphology analysis from Wikipedia markups. Another unique challenge for many morphologically rich languages is to segment each token into its stemming form and affixes. Mine word translations from cross-lingual links. Name translation is a crucial step to generate candidate entities in cross-lingual entity linking. Only a small percentage of names can be directly translated by matching against cross-lingual Wikipedia title pairs. He graduated from Harvard University.) For example, an entity with a birth date is likely to be a person, while an entity with a population property is likely to be a geo-political entity. Using entity properties as features, we train Maximum Entropy models to assign types with three levels of granularity to all English Wikipedia pages. Next, we propagate the label of each English Wikipedia page to all entity mentions in all languages in the entire Wikipedia through monolingual redirect links and cross-lingual links. The name annotations acquired from the above procedure are far from complete to compete with manually labeled gold-standard data. For example, if a name mention appears multiple times in a Wikipedia article, only the first mention is labeled with an anchor link. We apply self-training to propagate and refine the labels. We first train an initial name tagger using seeds selected from the labeled data. Then we apply the initial tagger to the first unlabeled cluster, select the automatically labeled sentences with high confidence, add them back into the training data, and then re-train the tagger. This procedure is repeated n times until we scan through all unlabeled data. For some populous languages that have many millions of pages in Wikipedia, we obtain many sentences from self-training. Therefore we develop the following effective methods to rank and select high-quality annotated sentences. We rank names by their frequency and dynamically set the frequency threshold to select a list of common names. Using an emergent disaster setting as a use case, we prefer sentences that include entities related to disaster related topics. The Leidos corpus consists of documents related to various disaster topics. We use such affix lists to perform basic word stemming, and use them as additional features to determine name boundary and type. Note that this approach can only perform morphology analysis for words whose stem forms and affixes are directly concatenated. If the source language is not English, an important step in the linking process is to translate identified name mentions into English. We can utilize crosslingual Wikipedia title pairs for exact name string matching. Fortunately, these cross-lingual title pairs, generated through crowd-sourcing, generally follow a consistent style and format in each language. Therefore, we propose a new and simple method to mine word and phrase translation pairs as follows. For each name mention, we generate all possi-ble combinations of continuous tokens. We then extract all cross-lingual Wikipedia title pairs containing each combination. If no surface form can be matched then we determine the mention as unlinkable. Not surprisingly, name tagging performs better for languages with more mentions. The training instances for various entity types are quite imbalanced for many languages. The documents are from news sources and discussion fora. We also measured the impact of our morphology analysis methods based on the affix lists derived from Wikipedia markups on two morphologicallyrich languages: Turkish and Uzbek. As expected the learning curve on Wikipedia data is more smooth and converges more slowly than that of Non-Wikipedia data. Our framework performs both name identification and typing and takes advantage of richer structures in the KBs. We developed a new method to derive generalizable affixes for morphologically rich language based on Wikipedia markups. Without using any manual annotations, our name taggers outperform previous methods on the same data sets for many languages. But their methods required labeled data and name transliteration. They exploited Wikipedia links to train a supervised linker. We mine reliable word translations from cross-lingual Wikipedia titles, which enables us to adopt unsupervised English entity linking techniques. This framework follows a fully automatic training and testing pipeline, without the needs of any manual annotations or knowledge from native speakers. We evaluated our framework on both Wikipedia articles and external formal and informal texts and obtained promising results. To the best of our knowledge, our multilingual name tagging and linking framework is applied to the largest number of languages. In this work, we treat all languages independently when training their corresponding name taggers. In the future, we will explore the topological structure of related languages and exploit cross-lingual knowledge transfer to enhance the quality of extraction and linking. The general idea of deriving noisy annotations from KB properties can also be extended to other IE tasks such as relation extraction."
216,2,4.0,4.0,4.0,True,acl_2017,train,"This paper presents an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words. The coherence between topics is ensured through a copula, binding the topics associated to the words of a segment. In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments. We show that the proposed model naturally encompasses other state-of-the-art LDA-based models designed for similar tasks. Furthermore, our experiments, conducted on six different publicly available datasets, show the effectiveness of our model in terms of perplexity, Normalized Pointwise Mutual Information, which captures the coherence between the generated topics, and the Micro F1 measure for text classification.","in terms of perplexity) of LDAbased models. The question nevertheless remains as to which segmentation one should rely on. Furthermore, text segments can refer to topics that are barely present in other parts of the document. In this paper, we propose a novel LDA-based model that automatically segments documents into topically coherent sequences of words. In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments. A simple switching mechanism is used to select the appropriate distribution (document or segment specific) for assigning a topic to a word. The interdependence assumption allows the parameter estimation and the inference of the LDA model to be carried out efficiently, but it is not realistic in the sense that topics assigned to similar words of a text span are generally incoherent. Different studies, presented in the following sections, attempted to remedy this problem and they can be grouped in two broad families depending on whether they make use of external knowledgebased tools or not in order to exhibit text structure for word-topic assignment. The main assumption behind these models are that text-spans such as sentences, phrases or segments are related in their content. Therefore, the integration of these dependent structures can help to discover coherent latent topics for words. Though, the topic assignments follow the structure of the text; these models suffer from the bias of statistical or linguistic tools they rely on. To overpass this limitation, other systems integrated automatically the extraction of text structure, in the form of phrases, in their process. Our proposed approach also does not make use of external statistical tools to find text segments. The main difference with the previous knowledgefree topic model approaches is that the proposed approach assigns topics to words based on two, segment-specific and document-specific distributions selected from a Bernoulli law. Topics within segments are then constrained using copulas that bind their distributions. In this way, segmentation is embedded in the model and it naturally comes along with the topic assignment. We define here a segment as a topically coherent sequence of contiguous words. By topically coherent, we mean that, even though words in a segment can be associated to different topics, these topics are usually related.                      a joint probability for all the topics used in a segment. One important problem, however, with copLDA is its reliance on a predefined segmentation. Although the information brought by the segmentation based on NPs helps to improve topic assignment, it may not be flexible enough to capture all the possible segments of a text. It is easy to correct this problem by considering all possible segmentations of a document and by choosing the most appropriate one at the same time that topics are assigned to words. This entails that, in these models, one cannot differentiate the main topics of a document from potential segment specific topics that can explain some parts of it. Indeed, some text segments can refer to topics that are barely present in other parts of the document; relying on a single topic distribution may prevent one from capturing those segment specific topics. The choice between them is based on the Bernoulli variable f, as explained in the generative story given below. We will simply refer to this model as segLDAcop. Lower indices are used to denote coordinates of the above vectors. Lastly, Dir denotes the Dirichlet distribution, Cat the categorical distribution (which is a multinomial distribution with one draw) and we omit, as is usual, the generation of the length of the document.   The other steps are similar to the standard LDA steps. The parameters of the complete model can be directly estimated through Gibbs sampling. The different segmentations of a document are thus based on its sentence segmentations. In the remainder, we use L to denote the maximum length of a segment and g(M;L) to denote the number of segmentations in a sentence of length M, each segment comprising at most L words. Generating all possible segmentations of a sentence and then selecting one at random is not an efficient process as the number of segments rapidly grows with the length of the sentence. Furthermore, the following process is equivalent to choosing sentence segmentations uniformly from the set of possible segmentations. From pos. to P; (b) Add segment to current segmentation; (c) Move to position after the segment. We conducted a number of experiments aimed at studying the impact of simultaneously segmenting and assigning topics to words within segments using the proposed segLDAcop model. To deal with relatively homogeneous collections, we also removed documents that are too long. In addition, a copula is also used to bind topics within NPs, from the document specific topic distribution. We now compare the different models along three main dimensions: perplexity, use of topic representations for classification and topic coherence. The lower the perplexity is, the better the model fits the test data. Some studies compare topic models using extrinsic tasks such as document classification. For datasets where certain documents have more than one label (Pubmed, Reuters), we used the one-versus-all approach for performing multi-label classification. This shows the importance of relying on both document and segment specific topic distributions. As conjectured before, our model is able to captures fine grained topic assignments within documents. This shows the importance of being able to discover flexible segmentations for assigning topics within documents. Another common way to evaluate topic models is by examining how coherent the produced topics are. Doing this manually is a time consuming pro-cess and cannot scale. For this purpose an external, large corpus is used as a meta-document where the PMI scores of pairs of words are estimated using a sliding window. As discussed above, calculating the co-occurrence measures requires selecting the top-N words of a topic and performing the manual or automatic evaluation. Hence, N is a hyper-parameter to be chosen and its value can impact the results. In their work, they found that aggregating the topic coherence scores over several topic cardinalities leads to a substantially more stable and robust evaluation. segLDAcop model which uses copulas and segmentation together, shows the best score for the given reference meta-data (Wikipedia) in all of the datasets. This means using copula has more effect on the topic coherence than only the segment-specific topic distribution. However, a similar topic learned by LDA appears to involve less such words (year, january, february), indicating a less coherent topic. In particular, as one can note, the NP Rear Admiral Benjamin Sands is segmented in two parts, the first one, Rear Admiral, corresponding to the title, and the second one, Benjamin Sands, to the first and family names of the person. data-driven approach we have adopted here can discover such fine grained differences, something the approaches based on fixed segmentations (either based on sentences or NPs), are less likely to achieve. In this paper, we have introduced an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words. In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments. Our results confirm the importance of a flexible segmentation as well as a binding mechanism to produce topically coherent segments. Indeed, the impact of the inference approach on the different usages of latent topic models for text collections remains to be better understood. Furthermore, the following process is equivalent to choosing sentence segmentations uniformly from the set of possible segmentations. From pos. to P; (b) Add segment to current segmentation; (c) Move to position after the segment. This concludes the proof of the proposition. In addition, as the number of different sentence lengths is limited, one can store the values of g to reuse them during the segmentation phase."
67,2,3.67,2.33,2.0,False,acl_2017,train,"Semantic hierarchies construction means to build structure of concepts linked by hypernym-hyponym (“is-a”) relations. A major challenge for this task is the automatic discovery of hypernym-hyponym (“is-a”) relations. We propose a fusion learning architecture based on word embeddings for constructing semantic hierarchies, composed of discriminative generative fusion architecture and a very simple lexical structure rule for assisting, getting an F1-score of 74.20% with 91.60% precision-value, outperforming the stateof-the-art methods on a manually labeled test dataset. Subsequently, combining our method with manually-built hierarchies can further improve F1-score to 82.01%. Besides, the fusion learning architecture is language-independent.","A number of papers have proposed some approach to extract semantic hierarchies automatically. hypernym-hyponym relation discovery is the key point of semantic hierarchies construction, also the major challenge. The usage of the context is a bottleneck in improving performance of hypernym-hyponym relation discovery. Besides, distributional inclusion hypothesis, which states that hypernyms tend to occur in a superset of contexts in which their hyponyms are found. However, it is not always rational. The method works well for named entities. But for class names with wider range of meanings, this assumption may fail. But the method just focuses on linear transformation of word embeddings, using shallow level semantic of the representation. Besides, the method need clustering for hypernymhyponym relation word pairs in advanced. Such manuallybuilt hierarchies have exact structure and high accuracy, but their coverage is limited, especially for fine-grained concepts and entities. Some researchers presented automatic approaches for supplementing manually-built semantic resources. However, the coverage is still limited by the scope of Wikipedia. The major challenge for building semantic hierarchies is the discovery of hypernym-hyponym relations automatically. The usage of the context is a bottleneck in improving performance of discovery of hypernym-hyponym relations. Some researchers proposed method based on lexical pattern abstracted from context manually or automatically to mine hypernym-hyponym relations. But the method suffers from semantic drift of auto-extracted patterns. Generally speaking, these pattern-based methods often suffer from low recall-value or precision-value because of the coverage and quality of extracted patterns. Some measures rely on the assumption that hypernyms are semantically broader terms than their hyponyms. The assumption is that the hypernyms will co-occur with its hyponym entity frequently. But the assumption maybe failed when involved with concept words which have boarder semantic compared with entities. However the method just focuses on linear transformation of word embeddings, using shallow level semantic of the representation. Besides, the method need clustering for hypernym-hyponym relation word pairs in advanced and the precision-value on test data is not good enough for practical application. Above all, the method we proposed do not need clustering for hypernymhyponym relation word pairs in advanced. The whole information all we have to figure out whether there exists a hypernym-hyponym relation for the word pair is word embeddings representation. Two major kind of architectures for such problem in general, one is discriminative and another is generative. Generative Architecture: Generative architecture focus on generating the hypernym of a given hyponym directly. Direct generation is usual impractical, so generative method produces a fake target which is very similar with the true one. We consider fusing these two models to discovery hypernym-hyponym relation much more precisely. Additionally, their experiment results have shown that the Skip-gram model performs best in identifying semantic relationship among words. For this reason, we employ the Skip-gram model for estimating word embeddings in this study. An MLP consists of multiple layers connecting each layer fully connected to the next one. Except for the input nodes, each node is a neuron with a nonlinear activation function(e.g. sigmoid). For single hidden layer MLP, there are input layer x, hidden layer h and output layer y range from bottom to top. The value of neurons in each layer is a nonlinear projection of the previous layer. And fh, fy are nonlinear activation functions for nonlinear transformation. In our work, we use Multilayer Perceptron as the main component for generative architecture. The inputs of MLP is word embedding representation of hyponym and outputs a fake hypernym embedding which is very similar with the true hypernym vector. The model produces final result by calculating the distance between the fake hypernym and the candidate hypernym word in continuous space, subsequently, comparing the distance and a predefined threshold to give a judgment. By adjusting the threshold value of similarity, we expect MLP model obtain much higher precision compared with the discriminative one. Unlike feedforward neural networks (e.g. MLP), RNNs can use their internal memory to process future inputs. The features of RNN makes them applicable to tasks such as unsegmented connected handwriting or speech recognition. In this paper we use the original simple recurrent networks (SRN). And the output layer yt is update by the current time step hidden layer ht. And fh, fy are nonlinear activation functions for nonlinear transformation. The inputs of RNN is word embeddings representation of hyponym and candidate hypernym sequence. We regard the hyponym and candidate hypernym as a sequence, because that the judgment of candidate hypernym is depended on hyponym in discrimination process. Ignoring the outputs during recurrent process, we take the final output of the last input in the sequence as the result of discrimination. Generative architecture can get a very high precision by adjusting the threshold value of similarity, but will pay a high price for low recall-value. Compared with generative method, discriminative architecture can obtain a higher recall-value with low guarantee for precision-value. The feature of discriminative architecture indicates that if it determines a candidate hypernymhyponym relation word pair as negative, then the word pair will have high probability for negative. We can use discriminative architecture to help the generative one to get rid of some false positive instance. Excepting a much higher precision-value than the precious two models and almost the same recall-value as the generative one. Root word of a CN can be obtained via using syntax dependency parsing or semantic dependency parsing of CN. Due to the word formation rule of Chinese, the root word is usually the last word in CN segmentation result. To supplement the capacity of learning semantic hierarchy from lexical structure, we use the simple lexical structure rule to assist previous fusion model. In the experimental stage, we implement our fusion architecture for learning semantic hierarchies. To the end of this, we first introduce the preparation of experimental setup. Next, we report the performance of fusion architecture and its components. Subsequently, we compare the performance of our method to those of several previous methods in different aspects and give an example for construction of semantic hierarchies. Since the discovery of hypernymhyponym relation is a binary classification task, we only report the performance of the positive instances recognition in the experiments. We experimentally study the effects of several hyperparameters on this two neural networks: the number of neutrons in hidden layer, the selection of activation function. In the training stage, we train the discriminative architecture and generative architecture respectively. For training discriminative architecture based on RNN, we use the whole training data. In the test stage, we tune the performance of architecture by changing the thresholds for components of fusion architecture. There two kinds of thresholds need to be tuned: the similar distance of generative architecture and the probability for positive instances of discriminative architecture. We first experimentally study the effects of semantic distance threshold on generative architecture based on MLP neural network. The result shows that only a small part of the hypernyms can be extracted based on these patterns because only a few hypernym relations are expressed in the fixed patterns, and most of them are expressed in highly flexible manners. This method relies on an accurate syntactic parser, and the quality of the automatically extracted patterns is difficult to guarantee. However the rule only takes effect for CNs which are minority. MMLPgen. MRNNdis.  Further, we experimentally study the effects of lexical rule for MEmb. The results also indicate that the improvement is mainly caused by discriminative generative architecture. We assume that as long as there is one word in the pair not existing in CilinE, the word pair is outside of CilinE. For further exploration, we combine our method MFusion with the existing manually-built hierarchies in Wikipedia and CilinE. The same manner allied to precious method MEmb to be compared. From the results, we can see that our method can actually learn the semantic hierarchies for a given word and its hypernyms list relatively precisely. The reason maybe that their semantic similarity effects representations close to each other in the embedding space and our method can not find suitable projection for these pairs. By combining our method with manually-built hierarchies, we can improve the capacity of learning semantic hierarchies. This paper proposes a novel method for learning semantic hierarchies based on discriminative generative fusion architecture combined with a very simple lexical structure rule. The fusion architecture method can be easily expanded to be suitable for other languages. Further experiments show that our method is complementary with some manually-built hierarchies to learn semantic hierarchy construction more precisely."
169,2,4.0,2.62,2.0,True,acl_2017,train,"Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. In this paper, we overcome this problem by using a linguisticallyenhanced alignment to automatically extract the edits between parallel original and corrected sentences and then classify them using a new dataset-independent rule-based classifier. As human experts rated the predicted error types as “Good” or “Acceptable” in at least 95% of cases, we applied our approach to the system output produced in the CoNLL-2014 shared task to carry out a detailed analysis of system error type performance for the first time.","While the submitted systems were evaluated against text that had been explicitly annotated with error type information, the teams themselves were not required to annotate their output in a similar way. This mismatch ultimately meant that a detailed error type analysis of each system was impossible and that error type performance could only be measured in terms of recall. The main aim of this paper is to rectify this situation and provide a method by which unannotated error correction data can be automatically annotated with error type information. This is important because some systems may be more effective at correcting certain error types than oth-ers, yet this information is otherwise concealed in an overall score. Our approach consists of two main steps. This enables us to automatically annotate system hypothesis corrections with the same alignment and error type information as the reference and hence carry out a more detailed evaluation. The tool we use to do this will be released with this paper. The first stage of automatic annotation is edit extraction. Specifically, given an original and corrected sentence pair, we need to determine the start and end boundaries of any edits. Having extracted the edits, the next step is to assign them error types. In particular, the fact that different datasets are annotated according to different standards means that it is inappropriate to evaluate the predicted error types of an in-domain corpus against the predicted error types of an out-of-domain corpus (c.f. Instead, a dataset-agnostic error type evaluation is much more desirable. We then added another rule to differentiate Missing, Unnecessary and Replacement errors depending on whether tokens were inserted, deleted or substituted. From there, we extended our approach to classify errors that are not well-characterised by POS alone (such as Spelling or Word Order) and ensured that all types are assigned based only on automatically obtained properties of the data. This is in contrast with machine learning approaches which require different classifiers for different datasets and which ultimately may not be entirely compatible with each other. Instead, our approach is analogous to automating the annotation guidelines given to human annotators. A second significant advantage of our approach is that it is also always possible to determine precisely why an edit was assigned to a particular error category. In contrast, human and machine learning classification decisions are often less transparent and may furthermore be subject to annotator bias. Moreover, by being fully deterministic, our approach bypasses bias effects altogether and should hence be more consistent. The prerequisites for our rule-based classifier are that each token in both the original and corrected sentence is POS tagged, lemmatized, stemmed and dependency parsed. The marked-up tokens in an edit span are then input to our classifier and an error type is returned. As it is impractical to create new categories and rules to differentiate between such narrow distinctions however, our final framework aims to be a compromise between informativeness and practicality. As our new error scheme is based only on automatically obtained properties of the data, there are no gold standard labels against which to evaluate classifier performance. Raters were warned that edit boundaries had been determined automatically, and hence might be unusual, but that they should focus on the appropriateness of the error category regardless of whether they agreed with the boundary or not. E.g. Although these results are incomparable with previous approaches which were evaluated using a different metric and error scheme, we nevertheless believe that the high scores awarded by the raters validate the efficacy of our rule-based approach. One benefit of explicitly annotating the hypothesis files is that it makes evaluation much more straightforward. Specifically, if both the hypothesis and reference files are annotated in the same format, we need only compare the edits in each file to produce an F-score. In particular, for a given sentence, any edit with the same span and correction in both files is a true positive (TP), while the remaining edits in the hypothesis are false positives (FP) and the remaining edits in the reference are false negatives (FN). Our approach, on the other hand, treats edit extraction and evaluation as separate tasks. Before evaluating the newly annotated hypothesis files against the reference, we must also address another mismatch: namely that the hypothesis edits were aligned and classified automatically, while the reference edits were aligned and classified manually using a different framework. Since evaluation is now a straightforward comparison between two files however, it is especially important that both files are processed in the same way. We can solve this problem by reprocessing the reference file in the same way as the hypothesis file. This means all the reference edits are subject to the same alignment and classification criteria as the hypothesis edits. This leads us to conclude that our auto annotations are qualitatively as good as human annotations. In our first category experiment, we simply investigated the performance of each system in terms of Unnecessary, Missing or Replacement edits. This is especially surprising given that we would expect unnecessary token errors to be easier to correct than others; a system need only detect and delete without having to propose any alternative. There is also no obvious explanation as to why these teams had difficulty with this error type because each of them employed different combinations of correction strategies including machine translation (MT), language modelling, classifiers and rules. The highest F-score for each type is highlighted. MT approaches were most effective at correcting NOUN errors (AMU, CAMB, UMC), while fairly high scores for NOUN:NUM errors showed that this category could be successfully handled by MT (AMU, CAMB), classifiers (CUUI) or language model approaches (NTHU). Although spell checkers are widespread nowadays, many teams did not seem to employ them; this would have been an easy way to boost overall performance. In addition to analysing general error types, the modular design of our framework also allows us to evaluate error type performance at an even greater level of detail. Note that this is a representative example of detailed error type performance as an analysis of all error type combinations for all teams would take up too much space. This shows that even though one approach might be better than another overall, other approaches may still have complementary strengths. Another benefit of explicitly annotating all hypothesis edits is that edit spans become fixed; this means we can evaluate system performance in terms of edit size. target side. In general, teams did not do well at multi-token edits. If no system is particularly adept at correcting multi-token errors however, robust fluency correction will likely require more sophisticated methods than are currently available. the extent to which a system can identify erroneous tokens in text. This suggests that even though AMU detected roughly as many errors as CUUI, it was less successful at correcting them. IPN and PKU are also notable for detecting many more errors than they were able to correct. Although we do not do so here, our scorer is also capable of providing a detailed error type breakdown for detection. In this paper, we have described a method to automatically annotate parallel error correction data with explicit edit spans and error type information. This can be used to standardise existing error correction corpora or facilitate a detailed error type evaluation. The tool we use to do this will be released with this paper. Our approach makes use of previous work to align sentences based on linguistic intuition and then introduces a new rule-based framework to classify edits. This framework is entirely dataset independent, and relies only on automatically obtained information such as POS tags and lemmas. We found that different systems had different strengths and weaknesses which we hope researchers can exploit to further improve general performance. This table shows all of them except UNK, which indicates an uncorrected error. A dash indicates an impossible combination."
31,3,3.77,2.69,2.69,False,acl_2017,train,"Event factuality identification plays an important role in deep NLP applications. In this paper, we propose a deep learning framework for this task which first extracts essential information from raw texts as the inputs and then identifies the factuality of events via a deep neural network with a proper combination of Bidirectional Long Short-Term Memory (BiLSTM) neural network and Convolutional Neural Network (CNN). The experimental results on FactBank show that our framework significantly outperforms several state-of-the-art baselines.","From these sentences, we can learn that predicates and cues can determine the event factuality to a great degree. Such event factuality provides the way to distinguish fact events from speculative and negative ones. This section presents the concepts and basic factors of event factuality. We consider the events which can be critical for computing the factuality. In the literature, three kinds of factors play critical roles in event factuality identification: Source Introducing Predicate (SIP), source and cue. SIP. Source Introducing Predicates (SIPs) are events that can not only introduce additional sources to assess the factuality of embedded events, but also influence their factuality. Source. By default, AUTHOR is considered the only source if the sentence contains no SIPs. Further sources can be incorporated by SIPs. The factuality value is not the inherent property of the event but always relevant to the sources. McCulley commits to the event increase as a possibility, while AUTHOR remains uncommitted. Only AUTHOR can be displayed by a simple source. Cues are words that have speculative or negative meanings. So cues are key signals to identify the event factuality. Firstly, various useful factors, such as SIPs, sources, and cues are extracted from raw texts. Then, a classification algorithm is employed to identify event factuality. Whether an event is a SIP has relation to its semantic information. For instance, verbs state, say, tell are usually used to express opinions which are promoted by certain participants of events, so they can all introduce additional sources. This example demonstrates the structure of a sentence is also able to determine whether a token is a SIP. Therefore, we also consider the sentence level feature. We argue that PSen can characterize clearly whether there are events in the clause of the token. Besides, in PSen only the candidate token and the tokens that are possible to be new sources are reserved, and effects of other tokens are omitted.   We learn effective representations from Shortest Dependency Paths (SDP) by means of neural networks with attention. Different from previous studies, our model combines both BiLSTM and CNN properly which can learn meaningful features from SDPs and words, respectively. Finally, the event factuality is determined by these two outputs directly. We develop the following inputs for our neural network model: Relevant Source Path (RS Path): The SDP from the root of the dependency tree to the relevant sources of the event. Notice that this path contains all the sources in the chain form. SIP Path: The SDP from the SIP that introduces the current source to the event. Cue Path: The SDP from the cue to the event. Auxiliary Words (Aux Words) include auxiliary and marker words that share the dependency relations aux or mark with the current event. Here, a marker is the word introducing a finite clause subordinate to the event. We argue that Aux Words can describe the syntactic structures of sentences. Traditional RNNs have problems called vanishing gradients during the gradient back-propagation phase.   text. This section first introduces experimental settings and then gives the detailed results and analysis of event factuality identification. We choose two-sample two-tailed t-test for significance test. The Stochastic Gradient Descent with momentum is applied to optimize our models. If there is more than one cue in the sentence, we consider whether the event is modified by each cue separately. Some features are not available (i.e., predicate classes and general classes of events) because they rely on annotated information which are not outputs of our basic factor extraction tasks. It is worth noting that a SIP is correctly identified means both this SIP and the new source introduced by it are correctly detected. Remember that we only consider nouns, verbs, adjectives, and rule out the effects of other tokens. Therefore, we adopt the attention-based CNN model for SIP detection. Hence, it is challenging to identify them. Particularly, our BiLSTM model can obtain better results than CNN. Compared with the simple CNN which ignores the contexts, BiLSTM can learn features from both future and past contexts. Hence BiLSTM is able to ex-tract more effective representations from syntactic paths than CNN. Therefore, attention is helpful to improve the results. On the other hand, the events embedded in other sources usually have complex syntactic structures. Therefore, it is difficult to judge whether these events are Uu or Non-Uu. These results can prove the effectiveness of our model, especially cue-related features which are helpful to identify speculative and negative values. We can conclude that only RS Path is not sufficient to identify Uu and Non-Uu events. Events in non-restrictive clauses are commonly evaluated as facts. Currently, only a few studies focused on event factuality identification. They classified predicates into Committed Belief (CB), NonCB or Not Applicable under a supervised framework using lexical and syntactic features. These studies utilized annotated events and relative factors directly. Many recent researches show that attentionbased models can improve the performances of NLP tasks. In this paper we utilize attentionbased neural networks which consider both LSTM and CNN to learn effective features from syntactic paths and words, respectively. This paper presents a supervised framework to identify factuality of events from raw texts. We firstly extract events, SIPs, relevant sources and cues in texts, and then employ an attention-based neural network model combining BiLSTM and CNN for event factuality identification. Our model can identify negative and speculative factuality values more effectively with the help of corresponding cues. Experimental results show that our model can outperform the state-of-the-art ones. We will develop more useful features for basic factor extraction tasks and design better neural network models to improve the results of event factuality identification in the future work."
66,3,4.58,2.83,2.58,False,acl_2017,train,"The major system is a mnemonic system that can be used to memorize sequences of numbers. In this work, we present a method to automatically generate sentences that encode a given number. We propose several encoding models and compare the most promising ones in a password memorability study. The results of the study show that a model combining part-of-speech sentence templates with an n-gram language model produces the most memorable password representations.","The difficulty of generating a memorable sequence of words that encodes a number with the major system stems from the constraint that the sequence of words must encode exactly the given digits. While there are many sequences of words that correctly encode a given number, the vast majority of these sequences are incoherent and thus difficult to remember. So, this task requires the use of a language model that balances the encoding constraints with syntactic plausibility and some notion of memorability. We have developed a system that automatically produces a sequence of words to encode a se-quence of digits. Each such encoding is a sequence of sentences that balance memorability and length. We sample from a distribution of partof-speech (POS) templates to produce a syntactically plausible sentence, then use an n-gram language model to fill each POS slot in the selected template to produce an encoding. One could use our system to encode a smartphone passcode as a short sentence. Thus, our system can help improve the strength of security practices. To test the effectiveness of our system, we conducted a study on password memorability. Participants were asked to memorize an eight-digit number representing a numeric password and a phrase produced by our system to encode the same number. After seven days, participants remembered the encoding produced by our final model better than the number itself. The rest of this paper is organized as follows. Several tools are available online for generating naive encodings of numbers using the major system. In this section, we describe those tools and identify the shortcomings in those implementations that are addressed by our work. We also put our work into the context of previous studies on password memorability. There are a number of existing tools that use the major system to encode sequences of digits as sequences of words. However, all such tools we found have considerable limitations. Most notably, the majority of these tools simply return the entire set of words that can individually encode the given number. None of these tools rank the multiple encodings they produce, and none of them produce sentences. Thus, these existing approaches are ill-suited for the memorization of even moderately long sequences of digits. Since the most sophisticated of these approaches are equivalent to our baseline models, we do not empirically compare these tools to our models. We are only aware of two previous corpus-based methods for generating mnemonic encodings. The second corpus-based method addresses the related problem of memorizing random strings of bits. However, their methods that create the most memorable passwords do not allow the user to mentally convert their memorized sequence of words to the original string. In contrast, our use of the major system allows users to easily convert any sequence of words into the encoded number in their head. Although these methods encode a sequence of letters and a string of bits while our system encodes a sequence of digits, all aim to create memorable sentences as output. Based on the results of these two previous methods, our system favors unique words and sentences of moderate length. Because our system needs to encode any arbitrary sequence of digits, we use a language model to generate sentences instead of relying on a preexisting set of newspaper headlines. Substantially more work has been done on the memorability and security of passwords. Their positive results for mnemonicbased passwords are encouraging for our own mnemonic-based system. They found that prompts are more memorable when they are complete phrases and have fewer words. Our user study experiment evaluating the memorability of the phrases generated by our systems is informed by the existing work in this area. They examined the use of passphrases created by taking the first letter of each word in a sentence. Their user study split participants into two passphrase groups, the second of which had to include a number and a special character in the passphrase. The participants were not allowed to write the passwords down. The results showed that the second group produced much more secure passwords but at the cost of memorability. In this section, we describe the data we used to train our models and present the six mnemonicgenerating systems we considered. These models include two baseline models, three preliminary models, and the final sentence encoder model. We use two data sets in our system. We use this corpus to train our n-gram models. The corpus is also tagged with part-of-speech data, which we use to train our part-of-speech n-gram model and our sentence encoder model. This ensures that all the words produced by our language model can be pronounced from the CMU Pronouncing Dictionary. We pre-process both data sets to lowercase all words. We designed two baseline models to compare our results against. Each of these baselines satisfies the requirement that the sequences of words produced encode exactly the input digits. Both baselines are greedy: they generate encodings one word at a time. At each time step, they choose a word from the set of words that encode the maximum number of digits possible. They differ in how a word is chosen from that set: Random Encoder: At each step, a word is selected at random. Unigram Encoder: At each step, the word with the highest unigram probability is selected. We also considered three models that were more sophisticated than our baseline models. Unlike the baseline models that greedily encode as many digits per word as possible, these models consider all words that can encode at least one digit. n-gram Encoder: Words are generated one at a time. An additional hyperparameter indicates if the model should select the word with the highest n-gram probability or sample from a weighted probability distribution based on n-gram probabilities, with the former option as the default. Part-of-Speech (POS) Encoder: Words are generated one at a time, but a POS trigram model is used to restrict the set of possible words at each step so that the generated phrases are syntactically motivated. Each word is associated with the POS tag it most often has in the Brown corpus. To select each word in the encoding, the most likely POS tag is identified from the POS trigram model. From all words with that POS, we choose the word that has the greatest likelihood according to a word trigram model. Chunk Encoder: Instead of generating encodings one word at a time, we generate one sentence at a time. Additionally, each chunk encodes exactly three digits. This encoder breaks the given number into chunks of three digits and encodes each chunk as one or two words that can be parsed as the desired chunk type. For each chunk, the phrase that has the greatest likelihood according to a bigram language model is selected. After examining the output of the three preliminary models, we combined the best elements of each into a final model, which we call the Sentence Encoder. This model aims to produce a variety of sentence structures that are both adequately long and reasonably likely to occur. Then, for every part of speech in the template, the encoder selects the word that encodes at least the next digit with the most likely trigram score based on the previous words. If the encoder is unable to find a word that matches the necessary part of speech, it replaces the current sentence with a different, newly sampled sentence template. This process is repeated until all digits are encoded, possibly resulting in the end of the last sentence template being unused. We previously found that the most likely sequences of words tend to encode only one or two digits per word, resulting in sentences that are long and thus less memorable. The post-processing pass iterates over each word, calculates the probabilities for all possible words that encode the exact same digits using a bigram language model with the preceding and following words, and replaces the original word with the most likely word. With these four changes, the sentence encoder is able to encode all numbers as memorable, syntactically plausible sentences of a reasonable length. And POS Wife age at sea with law in the Chunk Half shut settle night. Sentence Officiate wasteland. Encoder Phrase Random meeting rawhide yelping hunch alum levy bog boom annum ivory gin sharing meme femme knock appeal sinning vivo readying bake twitch beaming pub hammock highlighting Unigram made right help enjoy william life back p.m. name over john sure mama foam neck able seen five right back touch p.m. baby make old n-gram Him to hire youth all. Be in show all my life. Be echoing be my own home. Of our age in which our. Him home of my own. Go up all his own. Of every day by god. Which by him by be. Him go along with. POS Matter with law by an age along mile of hope. Week by man among every age in age. Year among aim of man. Week by law use in favor with pike with age by humming by boy among week along the. Chunk Matter would leap new jail. Home life book by money home. Average enjoy her home movie. Human ego being less won five. Earth by god she poem by. Buy make lead. Sentence Matter tell been shell among life. Pickup man moving or which nature. Mama of many couples in favor. Tobacco touch pump by my cold. The random encoder generates mnemonic encodings that use obscure words without any structure. As such, the random encoder does not produce memorable encodings. The unigram encoder improves on the random encoder by favoring more common words, which tends to result in shorter encodings. However, as neither model considers part-of-speech information, the two baselines produce unrelated sequences of words, which are difficult to chunk and to remember. The n-gram encoder generates encodings that tend to be long and unmemorable. For example, the POS encoder generates longer, more syntactically plausible sentences. The biggest drawback of this model is the relative lack of verbs in its generated sentences. The chunk encoder produces encodings similar to those produced by the POS encoder but with a noun-verb-noun pattern that results in relatively short, simply structured sentences. However, the chunk encoder has a slow running time due to the relatively expensive process of parsing each possible noun phrase and verb phrase. The sentence encoder produces words such that each word encodes many digits and tends to be distinctive. This means that fewer words and thus fewer sentences are needed to encode a given sequence of digits, making the mnemonic encodings generated by the sentence encoder easy to chunk and memorable. We hypothesize that the sentence encoder generates more memorable sentences. We tested that hypothesis through a user study. We conducted a user study to test the memorability of the phrases generated by our models. We compared the ngram model (which served as our baseline) to the sentence model (which we hypothesized best balances all aspects of memorability) for four factors: Short-Term Recall: How well can participants remember the number or its encoding after five minutes? Long-Term Recall: How well can participants remember the number or its encoding after one week? Long-Term Recognition: How well can participants identify the number or its encoding from a list of options after one week? Subjective Comparison: How easy does each number or its encoding seem to users? This comparison may give an indication of how likely users are to consider trying a particular mnemonic. The study was comprised of two online surveys, which together take about fifteen minutes to complete. We recruited participants through emails to a computer science summer research program and through social media posts. Each participant was randomly assigned to one of two groups, n-gram or sentence. Participants were identified by the email address they entered in the first survey, which we then used to send them a link to the second survey and to identify their responses. The participant was asked to remember both the number and the encoding, without writing them down, and was informed that they would be asked to recall both sequences at the end of the first survey and on the second survey. After entering both sequences to confirm initial memorization, the participant was asked to read a page of baking recipes for approximately five minutes. Each correct or incorrect attempt was recorded, as was the time spent on each page of the survey. The second survey was sent to each participant seven days after their completion of the first survey. The survey again asked each participant to recall both sequences and recorded the same information. Our primary goal in performing this user study was to evaluate our hypothesis that the sentence encoder produced more memorable encodings than the n-gram encoder. Short-Term Recall: On the first survey, more participants correctly confirmed the sentence password than the n-gram password. The results of our user study indicate that the sentence encoder produces more memorable encodings than the n-gram encoder does. These results also indicate that n-gram encodings are harder to remember in the short-term than the number itself. While few participants recalled any passwords after seven days, more participants recognized the sentence password than the numeric password, indicating that the sentence password is more memorable. We conclude that the sentence encoder produces more memorable encod-ings than the n-gram encoder and effectively aids in the memorization of numbers. We have described several systems for generating encodings of numbers using the major system. While n-gram models generate sentences that accurately match their training corpora, the sentences tend to be long and unmemorable. Sentences based on POS tags tend to be more memorable but are still not syntactically reasonable. Forcing the same sentence structure on every sentence by parsing ensures a reasonable structure but at a high computational cost. Ultimately, ensuring that each sentence is of a known but randomly selected syntactic structure that favors short encodings produces a reasonable balance of syntactic correctness, length, and memorability. A user study on password memorability supports our claim that the sentence encoder produces memorable mnemonic encodings of numbers. Future work could further improve the sentence encoder. We could produce a more interesting variety of sentences by using punctuation from the training corpus besides periods, such as commas, exclamation marks, and question marks. The sentence encoder often produces a fragment as its last sentence because it runs out of digits to encode. This problem could be mitigated by making the encoder select shorter sentence templates when there are few digits remaining. Furthermore, the encoder could use more nuanced sentence templates to enforce subject-verb agreement and grammatical use of auxiliary verbs. While the sentence encoder takes a greedy approach in an effort to encode digits in as few words as possible, another potential approach would be to use a dynamic programming algorithm to efficiently search through all possible encodings. Future user studies should enforce a set distraction time through the use of a timer or other mechanism when studying shortterm recall. While our user study did not show that any particular type of password was easiest to recall after seven days, we expect that a study involving longer passwords would show the sentence password to be easiest to recall. This is because the major system is wellsuited for aiding the memorization of long numbers while the eight-digit numeric passwords used in our study are relatively short. It would also be interesting to see how memorable our encodings are in a context where users are prompted to recall the password each day over a long period of time."
128,1,4.0,4.0,2.0,False,acl_2017,train,"Natural language understanding (NLU) is a core component of a dialogue system. Recently recurrent neural networks (RNN) obtained strong results on NLU due to their superior ability of preserving sequential information over time. Traditionally, the NLU module tags semantic slots for utterances considering their flat structures, as the underlying RNN structure is a linear chain. However, natural language exhibits linguistic properties that provide rich, structured information for better understanding. This paper introduces a novel model, knowledge-guided structural attention networks (K-SAN), a generalization of RNN to additionally incorporate nonflat network topologies guided by prior knowledge. There are two characteristics: 1) important substructures can be captured from small training data, allowing the model to generalize to previously unseen test data; 2) the model automatically figures out the salient substructures that are essential to predict the semantic tags of the given sentences, so that the understanding performance can be improved. The experiments on the benchmark ATIS data show that the proposed K-SAN architecture can effectively extract salient knowledge from substructures with an attention mechanism, and outperform the state-of-the-art neural network based frameworks.","It is easy to see the relationship between the origin city and the destination city in this example, although these do not appear next to each other. However, the above studies benefit from large training data without leveraging any existing knowledge. When tagging sequences RNNs consider them as flat structures, with their underlying linear chain structures, potentially ignoring the structured information typical of natural language sequences. Hierarchical structures and semantic relationships contain linguistic characteristics of input word sequences forming sentences, and such information may help interpret their meaning. Poor generalization comes from the mismatch between knowledge bases and the input data, and then the incorrectly extracted features due to errors in previous processing propagate errors to the neural models. In order to address the issues and better learn the sequence tagging models, this paper proposes knowledge-guided structural attention networks, K-SAN, a generalization of RNNs that automatically learn the attention guided by external or prior knowledge and generate sentence-based representations specifically for modeling sequence tagging. The main difference between K-SAN and previous approaches is that knowledge plays the role of a teacher to guide networks where and how much to focus attention considering the whole linguistic structure simultaneously. In the following sections, we empirically show the benefit of K-SAN on the targeted NLU task. Different from prior work, this paper focuses on learning composable substructure embeddings that are informative for understanding. Recently linguistic structures are taken into account in the deep learning framework. Learning robust representations incorporating whole structures still remains unsolved. In this paper, we address the limitation by proposing K-SAN to learn robust representations of whole sentences, where the whole representation is composed of the salient substructures in order to avoid error propagation. The idea is to encode important knowledge and store it into memory for future usage with attention mechanisms. Attention mechanisms allow neural network models to selectively pay attention to specific parts. There are also various tasks showing the effectiveness of attention mechanisms. However, most previous work focused on the classification or prediction tasks (predicting a single word given a question), and there are few studies for NLU tasks (slot tagging). Based on the fact that the linguistic or knowledge-based substructures can be treated as prior knowledge to benefit language understanding, this work borrows the idea from memory models to improve NLU. Unlike the prior NLU work that utilized representations learned from knowledge bases to enrich features of the current sentence, this paper directly learns a sentence representation incorporating memorized substructures with an automatically decided attention mechanism in an end-toend manner. The prior knowledge obtained from external resources, such as dependency relations, knowledge bases, etc., provides richer information to help decide the semantic tags given an input utterance. This paper takes dependency relations as an example for knowledge encoding, and other structured relations can be applied in the same way. Here we do not utilize the dependency relation labels in the experiments for better generalization, because the labels may not be always available for different knowledge resources. tures may be less than the number of words in the utterance, because non-leaf nodes do not have corresponding substructure in order to reduce the duplicated information in the model. The representation of the input utterance is then compared with encoded knowledge representations to integrate the carried structure guided by knowledge via an attention mechanism. Then the knowledge-guided representation of the sentence is taken together with the word sequence for estimating the semantic tags. Four main procedures are described below. Encoded Knowledge Representation To store the knowledge-guided structure, we convert each substructure (e.g. path starting from the root to the leaf in the dependency tree), xi, into a structure vector mi with dimension d by embedding the substructure in a continuous space through the knowledge encoding network Mkg. The input utterance s is also embedded to a vector u with the same dimension through the network model Min. In the experiments, the weights of Mkg and Min are tied together based on their consistent ability of sequence encoding. Knowledge Attention Distribution In the embedding space, we compute the match between the current utterance vector u and its substructure vector mi by taking their inner product followed by a softmax. Sentence Representation In order to encode the knowledge-guided structure, a vector h is a sum over the encoded knowledge embeddings weighted by the attention distribution. Because the function from input to output is smooth, we can easily compute gradients and back propagate through it. The input, hidden and output layers consist of a set of neurons representing the input, hidden, and output at each time step t, wt, ht, and yt, respectively. RNN-GRU can incorporate the encoded knowledge in the similar way, where Mo can be added into gating mechanisms for modeling contextual knowledge similarly. Because the chain-based tagger and the knowledge-guided tagger carry different information, the joint RNN tagger is proposed to balance the information between two model architectures. In the experiments, we only use lexical features. The evaluation metrics for NLU is F-measure on the predicted slots. All reported results are from the joint RNN tagger, and the hyperparameters are tuned in the dev set for all experiments. To validate the effectiveness of the proposed model, we compare the performance with the following baselines. For baselines (models without knowledge features), CNN Encoder-Tagger achieves the best performance on all datasets. Among structural models (models with knowledge encoding), Tree-RNN Encoder-Tagger performs better for Small data but slightly worse than the DCNN Encoder-Tagger. Relations and words with darker color indicate higher attention weights generated by the proposed K-SAN with CNN. The slot tags are shown in the figure for reference. Note that the dependency relations are incorrectly parsed by the Stanford parser in this example, but our model is still able to benefit from the structural information. els, there is no significant difference. This suggests that encoding sentence information without distinguishing substructure may not capture salient semantics in order to improve understanding performance. Also, most of the proposed models outperform all baselines, where the improvement for the small dataset is more significant. This suggests that the proposed models carry better generalization and are less sensitive to unseen data. The proposed model presents the state-of-the-art performance on the large dataset, showing the effectiveness of leveraging knowledge-guided structures for learning embeddings that can be used for specific tasks and the robustness to data scarcity and mismatch. The darker color of blocks and lines indicates the higher attention for words and relations respectively. origin, destination, and time. our proposed model is able to pay correct attention to important substructures guided by the external knowledge even the training data is scarce. In order to show the capacity of generalization to different knowledge resources, we perform the KSAN model for different knowledge bases. Below we compare two types of knowledge formats: dependency tree and Abstract Meaning Representation (AMR). Unlike syntactic information from dependency trees, the AMR graph contains semantic information, which may offer more specific conceptual relations. Among four knowledge resources (different types and obtained from different parsers), all results show the similar performance for three sizes of datasets. In sum, the models applying four different resources achieve similar performance, and all significantly outperform the state-of-the-art NLU tagger, showing the effectiveness, generalization, and robustness of the proposed K-SAN model. This paper proposes a novel model, knowledgeguided structural attention networks (K-SAN), that leverages prior knowledge as guidance to incorporate non-flat topologies and learn suitable attention for different substructures that are salient for specific tasks. The structured information can be captured from small training data, so the model has better generalization and robustness. The experiments show benefits and effectiveness of the proposed model on the language understanding task, where all knowledge-guided substructures captured by different resources help tagging performance, and the state-of-the-art performance is achieved on the ATIS benchmark dataset."
578,1,4.0,5.0,5.0,True,acl_2017,train,"Parsing sentences to linguisticallyexpressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focussed almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.","Yet statistical parsing has focussed almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. An important goal of Natural Language Understanding (NLU) is to parse sentences to structured, interpretable meaning representations that can be used for query execution, inference and reasoning. restricted to projective bilexical dependencies. In this paper we focus on the robust parsing of linguistically deep semantic representations. Our main contribution is to develop a fast and robust parser for full MRS-based semantic graphs. We exploit the power of global conditioning enabled by deep learning to predict linguistically deep graphs incrementally. The model does not have access to the underlying ERG or syntactic structures from which the MRS analyses were originally derived. However, one of the main advantages of deep learning is the ability to make predictions conditioned on the unbounded contexts encoded with RNNs; this enables us to predict more complex structures than have been possible previously, without increasing algorithmic complexity. Therefore deep learning gives us the opportunity to perform robust, linguistically deep parsing. Our parser is a transition-based model for parsing semantic graphs. However, instead of generating arcs over an ordered, fixed set of nodes (the words in the sentence), we generate the nodes, including their labels and alignments to the input tokens, jointly with the transition actions. We use a variant of the arc-eager transition-system that is able to parse graphs and non-planar dependencies. The sentence is encoded with a bidirectional RNN. The transition sequence, seen as a graph linearization, can be predicted with any encoder-decoder model, but we show that using hard attention, predicting the alignment with a pointer network and conditioning explicitly on stack-based features improves performance. In order to deal with data sparsity candidate lemmas are predicted as a preprocessing step, so that the RNN decoder predicts unlexicalized predicates. We evaluate our parser on DMRS, EDS and AMR graphs. Although the model is less accurate that a high-precision grammar-based parser on a test set of sentences parsable by that grammar, our model is an order of magnitude faster due to incremental prediction and a GPU batch processing implementation of the transition system. We define a common framework for semantic graphs, in which we can place both MRSbased graph representations (DMRS and EDS) and AMR. In this framework sentence meaning is represented with rooted, labelled, connected, directed graphs. Node labels are referred to as predicates (concepts in AMR) and edge labels as arguments (AMR relations). In addition, a special class of node modifiers, constants, are used to denote the string values of named entities and numbers (including date and time expressions). Every node is aligned to a token or a continuous span of tokens in the sentence the graph corresponds to. The main units of MRS are elementary predications (EPs). An EP consists of a relation (referred to as a predicate), usually corresponding to a single lexeme, and its arguments. One of the distinguishing characteristics of MRS its support for scope underspecification; multiple scope-resolved logical representations can be derived from one MRS structure. MRS makes an explicit the distinction between lexical and non-lexical predicates (lexical predicates are prefixed by an underscore). Lexical predicates consist of a lemma followed by a coarse part-of-speech tag and an optional sense label. Predicates absent from the ERG lexicon are represented by their surface forms, POS tags and an unknown sense label. All predicates are annotated with an alignment to the character-span of the (untokenized) input sentence. We convert the character-level spans given by MRS to token-level spans. Lexical predicates usually align with the span of the token(s) they represent, while nonlexical predicates can span longer segments. In full MRS predicates are annotated with a set of morphosyntactic features consisting of attributevalue pairs, but we do not currently model these features. However, information annotated explicitly in MRS is considered as latent in AMR. This include alignments, as well as distinguishing between lexical and nonlexical concepts. Other concepts are either English words or special keywords, and can correspond to overt lexemes in some cases but not others. We parse sentences to their meaning representations by incrementally predicting semantic graphs together with their alignments.         I. We also predict the end-of-span alignments as a seperate sequence a(e). In the linearization, labels of edges whose direction are reversed in the spanning tree are marked by adding-of. Edges not included in the spanning tree, referred to as reentrancies, are represented with special with edges whose dependents are dummy nodes pointing back to the original nodes. Our potentially lossy representation represents the edge by repeating the label and alignment of the dependen node, which is recovered heuristically. The alignment does not influence the order of the nodes in this linearization. In dependency parsing the sentence tokens also act as nodes in the graph, but here we need to generate the nodes incrementally as the transitionsystem proceeds, conditioning the generation on the given sentence. The transition system consists of a stack of graph nodes being processed and a buffer, holding a single node at a time. The transition actions are shift, left-arc, right-arc and reduce. The shift transition moves the element on the buffer to the top of the stack, and generates a predicate and its alignment as the next node on the buffer. Left-arc and right-arc actions add labeled arcs between the buffer and stack top (for DMRS a transition for undirected arcs is included), but do not change the state of the stack or buffer. Finally, reduce pops the top element from the stack, and predicts its end-ofspan alignment if included in the representation. To predict non-planar arcs, we add another transition, which we call cross-arc, which first predicts the stack index of a node which is not on top of the stack, adding an arc between the head of the buffer and that node. Another special transition designates the buffer node as the root. To derive an oracle for this transition system, it is necessary to determine the order in which the nodes are generated. We consider two different orderings. The first is that of an in-order traversal of the spanning tree, where the node order is determined by the alignment. This leads to a linearization where the only non-planar arcs are reentrancies. The second is to make the ordering nondecreasing with respect to the alignments, while for nodes with the same alignment following the in-order ordering. In an arc-eager oracle arcs are added greedily, while a reduce action can either be performed as soon as the stack top node has been connected to all its dependents, or delayed until it has to reduce to allow the correct parse tree to be formed. In our model the oracle delays reduce, where possible, until the end alignment of the stack top node spans the node on the buffer. As the span end alignments often cover phrases that they head (e.g. for quantifiers) this gives a natural interpretation to predicting the span end together with the reduce action. Lexical predicates in MRS consist of a lemma followed by a sense label. Therefore if we can predict the alignment of a graph node as well as the lemma of the aligned word, the prediction required by the decoder can be simplified to only predicting the sense label. We extract a dictionary mapping words to lemmas from the ERG lexicon, which map words to their possible predicates. In combination with a lemmatizer, this can be used to predict a candidate lemma for each token. to map numbers to digit strings. For AMR parsing there is not annotated distinction between lexical and non-lexical tokens, so we automaticallyobtained alignments and the graph structure to classifiy concepts as lexical or non-lexical. The sentence e is encoded with a bidirectional RNN. For every token e we embed its word, POS tag and named entity (NE) tag as vectors xw, xt and xn, respectively. Each input position i is represented by a hidden state hi, which is the concatenation of its forward and backward LSTM state vectors. The transitions are shift (sh), reduce (re), left arc (la) and right arc (ra). The action taken at each step is given, along with the state of the stack and buffer after the action is applied, and any arcs added. Shift transitions generate the alignment and predicate of the next graph node. Items on the stack and buffer have the form (node index, alignment, predicate label), and arcs are of the form (head index, argument label, dependent index). We model the alignment of graph nodes to sentence tokens, a, as a random variable. For the arceager model, aj corresponds to the alignment of the node of the buffer after action tj is executed. The distribution of tj is over all transitions and predicate predictions (for shifts), predicted with a single softmax. Let sj be the RNN decoder hidden state at output position j.   The end-of-span alignment a(e)j for MRS-based graphs is predicted with another pointer network. The end alignment of a token is predicted only when a node is reduced from the stack, therefore this alignment is not observed at each time-step; it is also not fed back into the model. The hard attention approach, based on supervised alignments, can be contrasted to soft attention, which learns to attend over the input without supervision. This vector is used instead of haj for prediction and feeding to the next timestep. We extend the hard attention model to include features based on the transition system stack. Elements on the stack can be represented by the encoder biLSTM representations corresponding to the tokens they are aligned to. We include vectors for the top of the stack and the buffer, the latter which is predicted by the hard attention. We maintain a stack of alignment indexes for each element in the batch, which is updated inside the computation graph after each parsing action. We perform greedy decoding. For the stackbased model we ensure that if the stack is empty, the next transition predicted has to be shift. For the other models we ensure that the output is wellformed during post-processing by robustly skipping over out-of-place symbols or inserting missing ones. This data includes newswire, weblog and discussion forum text. A predicate tuples consist of the label and character span of a predicate, while an argument tuple consist of the character spans of the head and dependent of the relation, together with the argument label. This configuration, found using grid search and heuristic search within the range of models that fit into a single GPU, gave the best performance on the development set under multiple graph linearizations. We compare different linearizations and model architectures for parsing DMRS on the development data, showing that our approach is more accurate than baseline neural approaches. We compare the topdown and arc-eager linearizations, as well as the effect of delexicalizing the predicates (factorizing lemmas out of of the predicate labels and predicting them separately.) In both cases constants are predicted with a dictionary lookup based on the predicted spans; for predicates not in the lexicon an unknown-label is predicted and the words and POS tags are recovered during post-processing. The arc-eager unlexicalized linearization gives the best performance, down linearization, even though the model has to learn to model the transition stack inside the recurrent states without any supervision of the semantics of transition actions. The unlexicalized models are more accurate, mostly due to their ability to generalize to sparse or unseen predicates occurring in the lexicon. The remaining errors are due to discrepancies between the tokenization used by our system and the ERG tokenization. The results show that the arc-eager models performs better than those based on top-down representation. For the arc-eager model we use hard attention, due to the natural interpretation of the alignment prediction corresponding to the transition system. The arc-eager stack-based architecture improves further over the model that purely relies on the hard attention. We also compare the predicate prediction against a hard attention model that only predicts predicates (in monotone order) together with their start spans. Despite the promising performance of the model there is still a gap between the accuracy of our parser and ACE. One reason for this is that the test set sentences will arguably be easier for ACE to parse as their choice was restricted by the grammar that ACE uses. EDM metrics excluding endspan prediction (-Start) show that our parser has relatively more difficulty in parsing end-span predictions than the grammar-based parser. All the models are pointer-based, except where indicated otherwise. We also evaluate the speed of our model compared with ACE. These additional labels are harder to predict without access to a grammar. We now apply the same approach to AMR parsing. The arc-eager-based models again give better performance, mainly due to improved concept prediction accuracy. In contrast to DMRS parsing, the in-order linearization gives better results than the monotone one. We hypothesize that this is due to the non-compositional nature of AMR. We show that even our neural baseline outperforms existing results for neural AMR parsing. The arc-eager model does not perform better than the top-down linearization, which we hypothesize is due to noise in the automatically-obtained alignments. However these models make extensive use of external resources, including syntax trees and semantic role labelling. In this paper we advance the state of parsing by employing deep learning techniques to parse sentence to linguistically expressive semantic representations that have not previously been parsed in an end-to-end fashion. We presented a robust, wide-coverage parser for MRS that is faster than existing parsers and amenable to batch processing. We believe that there are many future avenues to explore to further increase the accuracy of such parsers, including different training objectives, more structured architectures and semisupervised learning."
201,3,4.38,4.0,3.38,False,acl_2017,train,"The number of word embedding models is growing every year. Most of them learn word embeddings based on the cooccurrence information of words and their contexts. However, it’s still an open question what is the best definition of context. We provide the first systematical investigation of different context types and context representations for learning word embeddings. Comprehensive experiments are conducted to evaluate their effectiveness under 6 tasks, which give us some insights about context selection. We hope that this paper, along with the published code, can serve as a guideline of choosing context for our community.","Words that share similar meanings tend to have short distances in the vector space. The trained word embeddings are not only useful by themselves (e.g. However, in order to distinguish linear (continuous) context and dependency-based context, we refer it as CSG. However, their ideas is actually identical. For linear context, bound word indicates word associated with positional information. For dependency-based context, bound word indicates word associated with dependency relation. one context works consistently better than the other), the characteristics of different contexts on different models are concluded according to specific tasks. We expect this paper to be a useful complement in the word embedding literature. In this section, we first introduce different contexts in detail and discuss their strength and weakness. We then show how CSG, CBOW and GloVe can be generalized to use these contexts. Linear context is defined as the positional neighbours of the target word in texts. Compared to linear context, dependency-based context is more focused and can capture more long-range contexts. in the text. The context vocabulary C is thus identical to the word vocabulary W. However, this restriction is not r quired by the model; contexts need not corr spond to words, and the number of context-types can be substantially larger than the number of word-types. W generalize SKIPGRAM by replacing the bag-of-words contexts with arbitrary contexts. In this paper we experiment with dependencybased syntactic contexts. Moreover, the contexts are unmarked, resulting in discovers being a context of both stars and scientist, which may result in stars and scientists ending up as neighbours in the embedded space. Dependency-Based Contexts An alternative to the bag-of-words approach is to derive contexts based on the syntactic relations the word participates in.     The software defaults to prune rare words based on their frequency, and has an option for sub-sampling the frequent words. These pruning and sub-sampling happen before the context extraction, leading to a dynamic window size. Top: preposition relations are collapsed into single arcs, making telescope a direct modifier of discovers. Bottom: the contexts extracted for each word in the sentence. where lbl is the type of the dependency relation between the head and the modifier (e.g. Notice that syntactic dependencies are both more inclusive and more focused than bag-ofwords. Australian is not used as the context for discovers). In addition, the contexts are typed, indicating, for example, that stars are objects of discovery and scientists are subjects. We thus expect the syntactic contexts to yield more focused embeddings, capturing more functional and less topical similarity. Note that preposition relation is collapsed in the bottom sub-figure, where telescope is con idered as a direct modifier of discovers. We name the method that bind additional information with the contextual word as bound (context) representation, as opposited to unbound (context) representation where word is used alone. For dependency-based context, the original DEPS uses bound representation by default: words are associated with their dependency relation to the target word. Similar to bound representation in linear context type, this allows word embedding models to capture more dependency information. also investigate the simpler context representation where no dependency relation is considered. This also makes a fair comparison with linear context models like CSG, CBOW and GloVe, since they do not use bound representation either. Intuitively, bound representation should work better than unbound representation, since it is more sophisticated by considering position or dependency relation. However, this is not always the case in practice. The biggest drawback of word embeddings learned with bound context type is the ignorance of syntax. The bound representation already contains a certain degree of syntactic information, thus word embedding models can not learn it from the input word-context pairs. Another drawback is that bound context representation is sparse, especially for dependency-based context. Compared to context types (linear and dependency-based), the choice of context representations (bound and unbound) have more effects to the quality of the learned word embeddings. Bound representation transfers each contextual word into a new one, and the word-context pairs are changed completely. As for context types, a lot of word-context pairs in linear context type also appear in dependency-based context type. For convenience, we first define the collection of word-context pairs as P.   australian, scientist) (scientist, australian) (scientist, discovers) (discovers, scientist) (discovers, star) (discovers, telescope).   M (australian, scientist) (scientist, australian, discovers) (discovers, scientist, star).   australian, scientist) (scientist, australian, discovers) (discovers, scientist, star, telescope).       Unbound representation is used in this example. Words in the collections are Bold. Contexts and numbers in the collections are Normal. P can also be merged based on both words and contexts to form a collection M. In another word, the original CBOW uses the sum of word vectors to predict context. This works well for linear context. But for dependency-based context with bound representation, there is only one word available for predicting its context. However, a word can be predicted by the sum of several contexts. Due to this reason, we exchange the role of word and context in GBOW. The negative sampling objective is also changed from context cN to word wN. However, in order to make it consistent with our GBOW, we also exchange the role of word and context. Note that the inputs of GSG, GBOW and Glove are the collections P, M and M respectively. Once the corpus and hyper-parameters are fixed, these collections (and thus the learned word embeddings) are determined only by the choice of context types and representations. We evaluate the effectiveness of different context types and representations on word similarity, word analogy, part-of-speech tagging, chunking, named entity recognition, and text classification tasks. In this section, we first describe the training details of word embedding models. We then report and discuss the experimental results on each task. Detailed numerical results can be found in Supplemental Material. This makes CSG model accept arbitrary contexts (e.g. dependencybased context). However, CBOW and GloVe are not considered in that toolkit. All words and contexts are converted to lower case after parsing. of this paper is not comparing the performance of different word embedding models, the results of GSG, GBOW and GloVe are reported respectively. The cosine distance is used for generating similarity scores between two word vectors. Moreover, only CSG model is compared. We revisit those claims based on more systematical experiments. However, the good performances of dependencybased context do not fully transfer to GBOW. Distinguishing bound representation from unbound representation is important. Context representations play an important role for word similarity task. tion (word or bound word) actually has much larger impact than the choice of context type (linear or dependency). The performance of linear context and dependency-based context with unbound representation is similar. The main reason for this phenomenon is that the bound representation already contains syntactic information, thus word embedding models can not learn it from the input wordcontext pairs. Although intrinsic evaluations like word similarity and word analogy tasks could provide direct insights of different context types and representations, the experimental results above cannot be directly translated to the typical uses of word embeddings. More extrinsic tasks should be considered. In this subsection, we evaluate the effectiveness of different word embedding models with different contexts on Part-of-Speech Tagging (POS), Chunking and Named Entity Recognition (NER) tasks. These tasks can be categorized as sequence labeling. It aims at automatically assigning words in texts with labels. This ensures the quality of embedding models is directly evaluated, and their strengths and weaknesses are easily observed. When the same context type is used, bound representation (dotted line) outperforms unbound representation (solid line) on all datasets. Sequence labeling tasks tend to classify words with the same syntax to the same category. The ignorance of syntax for word embeddings which are learned by bound representation becomes beneficial. Moreover, dependencybased context type works better than linear context type in most cases. These results suggest that linear context type with unbound representations (as in traditional CSG and CBOW) may not be the best choice of input word vectors for sequence labeling. Bound representations should always be used and dependency-based context type is also worth considering. Again, similar to that on word analogy task, GloVe is more sensitive to different context representations than Skip-Gram and CBOW on sequence labeling tasks. Finally, we evaluate the effectiveness of different word embedding models with different contexts on text classification task. Text classification is one of the most popular and well-studied tasks in natural language processing. They often need pre-trained word embeddings as inputs to improve their performances. Text classification has less focus on syntax and function similarity. This leads to the phenomenon that models which use bound representation perform worse than those which use unbound representation on all datasets except CR. Models that use dependency-based context type and linear context type are comparable. These observations suggest that simple linear context type with unbound representations (as in traditional CSG and CBOW) is still the best choice of pre-training word embeddings for text classification, which is already used in most researches. Previously, there are researches which directly compare different word embedding models. There are also researches which focus on evaluating different context types in learning word embeddings. The results suggest that dependencybased models are able to detect functional similarity in English. However, the advantages of dependency-based context over linear context on other languages are not as promising as that on English. The intrinsic tasks have clear preference to particular types of contexts. On the other hand, for extrinsic tasks, the optimal context types need to be carefully tuned on specific dataset. However, context representations (bound and unbound) are not evaluated in these models. Moreover, they focus only on the more popular and intuitive CSG model, but not on CBOW and GloVe. To the best of our knowledge, this paper provides the first systematical investigation of different context types and representations for learning word embeddings. We evaluate different models on intrinsic property analysis (word similarity and word analogy), sequence labeling tasks (POS, Chunking and NER) and text classification task. Overall, the tendency of different models on different tasks is similar. However, most tasks have clear preference for different context types and representations. Context representations play a more important role than context types for learning word embeddings. Bound representation already contains syntactic information, which makes it difficult to learn syntactic aware word embeddings based on the input wordcontext pairs. Linear context is enough for capturing topical similarity compared to dependency-based context. We hope researchers will take advantage of the code for further improvements and applications to other tasks. However, numerical results are more accurate and can be directly used by other researches. Upon acceptance, they will be added to the final version. Best results in group are marked Bold. Best results in group are marked Bold."
256,2,4.5,4.5,4.5,True,acl_2017,train,"While recent neural encoder-decoder models have shown great promise in modeling open-domain conversations, they often generate dull and generic responses. Unlike past work that has focused on diversifying the output of the decoder at word-level to alleviate this problem, we present a novel framework based on conditional variational autoencoders that captures the discourse-level diversity in the encoder. Our model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders. We have further developed a novel variant that is integrated with linguistic prior knowledge for better performance. Finally, the training procedure is improved by introducing a bag-of-word loss. Our proposed models have been validated to generate significantly more diverse responses than baseline approaches and exhibit competence in discourse-level decision-making.","The basic approach treats a conversation as a transduction task, in which the dialog history is the source sequence and the next response is the target sequence. The model is then trained end-to-end on large conversation corpora using the maximum-likelihood estimation (MLE) objective without the need for manual crafting. Building upon the past work in dialog managers and encoder-decoder models, the key idea of this paper is to model dialogs as a one-to-many problem at the discourse level. Previous studies indicate that there are many factors in open-domain dialogs that decide the next response, and it is nontrivial to extract all of them. This allows us to generate diverse responses by drawing samples from the learned distribution and reconstruct their words via a decoder neural network. We propose Knowledge-Guided CVAE (kgCVAE), which enables easy integration of expert knowledge and results in performance improvement and model interpretability. We evaluate our models on human-human conversation data and yield promising results in: (a) generating appropriate and discourse-level diverse responses, and (b) showing that the proposed training method is more effective than the previous techniques. Our work is related to both recent advancement in encoder-decoder dialog models and generative models based on CVAE. Since the emergence of the neural dialog model, the problem of output diversity has received much attention in the research community. Ideal output responses should be both coherent and diverse. However, most models end up with generic and dull responses. To tackle this problem, one line of research has focused on augmenting the input of encoder-decoder models with richer context information, in order to generate more spe-cific responses. On the other hand, many attempts have also been made to improve the architecture of encoderdecoder models. This approach penalized unconditionally high frequency responses, and favored responses that have high conditional probability given the input. They introduced a searchbased loss that directly optimizes the networks for beam search decoding. The resulting model achieves better performance on word ordering, parsing and machine translation. Thus, they initialized a encoderdecoder model with MLE objective and leveraged reinforcement learning to fine tune the model by optimizing three heuristic rewards functions: informativity, coherence, and ease of answering. The basic idea of VAE is to encode the input x into a probability distribution z instead of a point encoding in the autoencoder. Then VAE applies a decoder network to reconstruct the original input using samples from z. They showed that their model is able to generate diverse sentences with even a greedy LSTM decoder. They also reported the difficulty of training because the LSTM decoder tends to ignore the latent variable. We refer to this issue as the vanishing latent variable problem. To improve upon the past models, we firstly introduce a novel mechanism to leverage linguistic knowledge in training end-to-end neural dialog models, and we also propose a novel training technique that mitigates the vanishing latent variable problem. the topic). CVAE is trained to maximize the conditional log likelihood of x given c, which involves an intractable marginalization over the latent variable z. x is simply uk. The dashed blue connections only appear in kgCVAE. The response decoder then predicts the words in x sequentially. In practice, training CVAE is a challenging optimization problem and often requires large amount of data. On the other hand, past research in spoken dialog systems and discourse analysis has suggested that many linguistic cues capture crucial features in representing natural conversation. Therefore, we conjecture that it will be beneficial for the model to learn meaningful latent z if it is provided with explicitly extracted discourse features during the training. In order to incorporate the linguistic features into the basic CVAE model, we first denote the set of linguistic features as y. We denote the modified model as knowledge-guided CVAE (kgCVAE) and developers can add desired discourse features that they wish the latent variable z to capture. We found that CVAE suffers from the same issue when the decoder is an RNN. As a result, we propose a simple yet novel technique to tackle the vanishing latent variable problem: bag-of-word loss. Due to the conditional independence assumption, the latent variable is forced to capture global information about the target response. In the beginning of the call, a computer operator gave the callers recorded prompts that define the desired topic of discussion. Then the rest of SW data are labelled with dialog acts using the trained SVM dialog act recognizer. We selected the best models based on the variational lower bound on the validate data. We compared three neural dialog models: a strong baseline model, CVAE, and kgCVAE. The encoded context c is directly fed into the decoder networks as the initial state. Following our one-tomany hypothesis, we propose the following metrics. The d(rj, hi) is the cosine distance of the two embedding vectors. This impacts reliability of our measures. Both CVAE and kgCVAE outperform the baseline in terms of recall in all the metrics. This confirms our hypothesis that generating responses with discourse-level diversity can lead to a more comprehensive coverage of the potential responses than promoting only word-level diversity. As for precision, we observed that the baseline has higher or similar scores than CVAE in all metrics, which is expected since the baseline tends to generate the mostly likely and safe responses repeatedly in the N hypotheses. A low number of distinct dialog acts represents the situation where the dialog context has a strong constraint on the range of the next response (low entropy), while a high number indicates the opposite (high-entropy). Also it shows that CVAE suffers from lower precision, especially in low entropy contexts. Finally, kgCVAE gets higher precision than both the baseline and CVAE in the full spectrum of context entropy. The kgCVAE model generated highly diverse answers that cover multiple plausible dialog acts. Further, we notice that the generated text exhibits similar dialog acts compared to the ones predicted separately by the model, implying the consistency of natural language generation based on y. The kgCVAE model is also able to generate various ways of back-channeling. in low-entropy dialog contexts modeling lexical diversity while in high-entropy ones modeling discourse-level diversity. We found that the learned latent space is highly correlated with the dialog act and length of responses, which confirms our assumption. The size of circle represents the response length. The network architecture is same except we use GRU instead of LSTM. Intuitively, a well trained model should lead to a low reconstruction loss and small but non-trivial KL cost. KgCVAE also provides the predicted dialog act for each response. At last, the models with BOW loss achieved significantly lower perplexity and larger KL cost. On the contrary, the model with only KLA learns to encode substantial information in latent z when the KL cost weight is small. The model with BOW loss, however, consistently converges to a non-trivial KL cost even without KLA, which confirms the importance of BOW loss for training latent variable models with the RNN decoder. In conclusion, we identified the one-to-many nature of open-domain conversation and proposed two novel models that show superior performance in generating diverse and appropriate responses at the discourse level. While the current paper addresses diversifying responses in respect to dialogue acts, this work is part of a larger research direction that targets leveraging both past linguistic findings and the learning power of deep neural networks to learn better representation of the latent factors in dialog. In turn, the output of this novel neural dialog model will be easier to explain and control by humans. In addition to dialog acts, we plan to apply our kgCVAE model to capture other different linguistic phenomena including sentiment, named entities,etc. Last but not least, the recognition network in our model will serve as the foundation for designing a datadriven dialog manager, which automatically discovers useful high-level intents. All of the above suggest a promising research direction. The filtered lists then served as the ground truth to train our reference response classifier. Finally, we automatically annotated the rest of test set with this trained classifier and the resulting data were used for model evaluation."
606,2,4.5,4.0,4.5,True,acl_2017,train,"Harnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete operations against a large knowledge-base. In this work, we introduce a Neural Symbolic Machine, which contains (a) a neural “programmer”, i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a symbolic “computer”, i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space. We apply REINFORCE to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of REINFORCE we augment it with an iterative maximum-likelihood process. NSM outperforms state-of-the-art on the WEBQUESTIONSSP dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge.","This is because the model must interact with a computer through non-differentiable operations at training time. However, annotating programs is known to be expensive and scales poorly. This makes it difficult to use the efficient discrete operations and memory of a traditional computer, and limited the application to synthetic or small knowledge bases. Its nondifferentiable memory enables abstract, scalable and precise operations, but makes training more challenging. Within this framework, we introduce the Neural Symbolic Machine (NSM) and apply it to semantic parsing. Our technical contribution in this work is threefold. Since learning from scratch is difficult for REINFORCE, we combine it with an iterative maximum likelihood (ML) process, where beam search is used to find pseudo-gold programs, which are then used to augment the objective of REINFORCE. Unlike prior work, it is trained end-toend, and does not require feature engineering or domain-specific knowledge. Semantic parsing typically requires using a set of operations to query the knowledge base and process the results. In contrast, operations implemented in high level programming languages are abstract, scalable, and precise, thus generalizes perfectly to inputs of arbitrary size. Based on this observation, we implement operations necessary for semantic parsing with an ordinary programming language instead of trying to learn them with a neural network. v represents a variable, p a property in Freebase. We did not use full Lisp programming language here, because constructs like control flow and loops are unnecessary for most current semantic parsing tasks, and it is simple to add more functions to the model when necessary. To create a friendly neural computer interface, the interpreter provides code assistance to the programmer by producing a list of valid tokens at each step. More importantly, a valid token should not cause a semantic (run-time) error: this is detected using the denotation saved in the variables. Sequence-to-sequence models consist of two RNNs, an encoder and a decoder. The key embeddings of the key-variable memory are the output of the sequence model at certain encoding or decoding steps. This variable is keyed by the GRU hidden state at that step. Every time a new variable is added to memory, the variable token is added to the output vocabulary of the decoder. During training, the model learns to represent variables by backpropagating gradients from a time step where a variable is selected by the decoder, through the key-variable memory, to an earlier time step when the key embedding was computed. NSM executes non-differentiable operations against a KB, and thus end-to-end backpropa-gation is not possible. Since we assume only weak supervision, we suggest an iterative ML procedure for finding pseudo-gold programs that will bootstrap REINFORCE. While REINFORCE assumes a stochastic policy, we rely on beam search for gradient estimation. Thus, in contrast with common practice of approximating the gradient by sampling from the model, we use the top-k action sequences (programs) in the beam with normalized probabilities. This allows training to focus on sequences with high probability, which are on the decision boundaries, and reduces the variance of the gradient. The difficulty of training resulted from the sparse reward signal in the large search space, which caused model probabilities for programs with non-zero reward to be very small at the beginning. If the beam size k is small, good programs fall off the beam, leading to zero gradients for all programs in the beam. To combat this, we present an alternative training strategy based on maximum-likelihood. Iterative ML If we had gold programs, we could directly optimize their likelihood. Since we do not have gold programs, we can perform an iterative procedure (similar to hard EM), where we search for good programs given fixed parameters, and then optimize the probability of the best program found so far. Training with iterative ML is fast because there is at most one program per example and the gradient is not weighted by model probability. while decoding with a large beam size is slow, we could train for multiple epochs after each decoding. Even with a large beam size, some programs are hard to find because of the large search space. The size of the search space is controlled by both the set of functions used in the program and the program length. Nevertheless, iterative ML uses only pseudogold programs and does not optimize the objective we truly care about. For example, differentiating PARENTSOF vs. SIBLINGSOF vs. CHILDRENOF can be challenging. We now present learning where we combine iterative ML with REINFORCE. Augmented REINFORCE To bootstrap REINFORCE, we can use iterative ML to find pseudogold programs, and then add these programs to the beam with a reasonably large probability. We first run iterative ML for NML iterations and record the best program found for every example xi. We now empirically show that NSM can learn a semantic parser from weak supervision over a large KB. We evaluate on WEBQUESTIONSSP, a challenging semantic parsing dataset with strong baselines. Experiments show that NSM achieves new state-of-the-art performance on WEBQUESTIONSSP with weak supervision, and significantly closing the gap between weak and full supervisions for this task. Hyper-parameter settings are in the supplemental material due to page limits. Collecting semantic parse labels is a difficult, time consuming task, but can increase the accuracy of state-of-the-art question-answering systems by full supervision. These questions were collected using Google Suggest API and the answers were originally obtained using Amazon Mechanical Turk and updated by annotators who were familiar with the design of Freebase. For query pre-processing we used an in-house named entity linking system to find the entities in a question. A Freebase id contains three parts: domain, type, and property. We evaluate performance using the offical evaluation script for WEBQUESTIONSSP. Accuracy measures the proportion of questions that are answered exactly. Our model beats STAGG with weak supervision by a significant margin on all metrics, while relying on no feature engineering or handcrafted rules. Model Prec. Rec. Four key ingredients lead to the final performance of NSM. The first one is the neural computer interface that provides code assistance by checking for syntax and semantic errors. We find that semantic checks are very effective for opendomain KBs with a large number of properties. The second ingredient is augmented REINFORCE training. In contrast, augmented REINFORCE is able to bootstrap using pseudo-gold programs found by iterative ML and achieves the best performance on both the training and validation set. The third ingredient is curriculum learning during iterative ML. We find that the best programs found with curriculum learning are substantially better than those found without curriculum learning by a large margin on every metric. Settings Prec. Rec. The last important ingredient is reducing overfitting. Given the small size of the dataset, overfitting is a major problem for training neural network models. Search failure: the correct program is not found during search for pseudo-gold programs, either because the beam size is not large enough, or because the set of functions implemented by the interpreter is insufficient. Ranking failure: Pseudo-gold programs with high reward are found, but are not ranked at the top. Because training error is low, this is largely due to over-fitting. Therefore, both models rely on REINFORCE for training. The main difference between the two is the abstraction level of the programming language. These remain to be future improvements for NSM. However, their training procedure is more involved, and we did not implement it in this work. The main difference between these approaches is how an intermediate result (the memory) is represented. Neural Programmer and Dynamic-NMN chose to represent results as vectors of weights (row selectors and attention vectors), which enables backpropagation and search through all possible programs in parallel. We propose the Manager-Programmer-Computer framework for neural program induction. It integrates neural networks with a symbolic nondifferentiable computer to support abstract, scalable and precise operations through a friendly neural computer interface. Because the interpreter is non-differentiable and to directly optimize the task reward, we apply reinforcement learning and use pseudo-gold programs found by an iterative ML training process to bootstrap training. NSM achieves new state-of-the-art results on a challenging semantic parsing dataset with weak supervision, and significantly closing the gap between weak and full supervision. It is trained endto-end, and does not require any feature engineering or domain-specific knowledge. For REINFORCE training, the best hyperparameters are chosen using the validation set. Then we train the model with learning rate decay until convergence. Since decoding needs to query the knowledge graph constantly, the speed bottleneck for training is decoding. We address this problem in our implementation by partitioning the dataset, and using multiple decoders in parallel to handle each partition. The neural network model is implemented in TensorFlow."
554,3,4.0,3.67,4.0,True,acl_2017,train,"Recurrent neural networks (RNNs) have shown promising performance for language modeling. However, traditional training of RNNs using back-propagation through time often suffers from overfitting. One reason for this is that stochastic optimization (used for large training sets) does not provide good estimates of model uncertainty. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (also appropriate for large training sets) to learn weight uncertainty in RNNs. It yields a principled Bayesian learning algorithm, adding gradient noise during training (enhancing exploration of the model-parameter space) and model averaging when testing. Extensive experiments on various RNN models and across a broad range of applications demonstrate the superiority of the proposed approach relative to stochastic optimization.","However, this approach often provides a maximum a posteriori (MAP) estimate of model parameters. Natural language often exhibits significant variability, and hence such a point estimate may make over-confident predictions on test data. To alleviate overfitting RNNs, good regularization is known as a key factor to successful applications. Despite the elegant theoretical property of asymptotic convergence to the true posterior, HMC and other conventional Markov Chain Monte Carlo methods are not scalable to large training sets. Specifically, instead of training a single network, SG-MCMC is employed to train an ensemble of networks, where each network has its parameters drawn from a shared posterior distribution. Stochastic optimization used for MAP estimation puts fixed values on all weights. Naive dropout is allowed to put weight uncertainty only on encoding and decoding weights, and fixed values on recurrent weights. The proposed SG-MCMC scheme imposes distributions on all weights. averaging when testing. This simple procedure has the following salutary properties for training neural networks: (i) When training, the injected noise encourages model-parameter trajectories to better explore the parameter space. ii) Model averaging when testing alleviates overfitting and hence improves generalization, transferring uncertainty in the learned model parameters to subsequent prediction. iv) SG-MCMC is scalable; it shares the same level of computational cost as SGD in training, by only requiring the evaluation of gradients on a small mini-batch. Several scalable Bayesian learning methods have been proposed recently for neural networks. While prior work focuses on feed-forward neural networks, there has been little if any research reported for RNNs using SG-MCMC. A comparison of naive dropout and SG-MCMC is illustrated in Fig.     Both the LSTM and GRU have been proposed to address the issue of learning long-term sequential dependencies. Long Short-Term Memory The LSTM architecture addresses the problem of learning longterm dependencies by introducing a memory cell, that is able to preserve the state over long periods of time. Reading or writing the cell is controlled through sigmoid gates: input gate it, forget gate ft, and output gate ot. Variants Similar to the LSTM unit, the GRU also has gating units that modulate the flow of information inside the hidden unit. We specify the GRU in the Supplementary Material. The LSTM can be extended to the bidirectional LSTM and multilayer LSTM. A bidirectional LSTM consists of two LSTMs that are run in parallel: one on the input sequence and the other on the reverse of the input sequence. At each time step, the hidden state of the bidirectional LSTM is the concatenation of the forward and backward hidden states. The proposed Bayesian framework can be applied to any RNN model; we focus on the following tasks to demonstrate the ideas. Language Modeling In word-level language modeling, the input to the network is a sequence of words, and the network is trained to predict the next word in the sequence with a softmax classifier.   Sentence Classification Sentence classification aims to assign a semantic category label y to a whole sentence X. To ease the computational burden, stochastic optimization is often employed to find the MAP solution. SG-MCMC and stochastic optimization are parallel lines of work, designed for different purposes; their relationship has recently been revealed in the context of deep learning.   It utilizes magnitudes of recent gradients to construct a diagonal preconditioner to approximate the Fisher information matrix, and thus adjusts to the local geometry of parameter space by equalizing the gradients so that a constant stepsize is adequate for all dimensions. When the preconditioner is fixed as the identity matrix, the method reduces to SGLD. In practice, the noise injected by SGLD may not be enough. A better way that we find to improve the performance is to jointly apply SGLD and dropout. This method can be interpreted as using SGLD to sample the posterior distribution of a mixture of RNNs, with mixture probability controlled by the dropout rate. The hyperparameter settings, the initialization of model parameters and model specifications on each dataset are provided in the Supplementary Material. We do not perform any dataset-specific tuning other than early stopping on validation sets. We first test character-level and word-level language modeling. The setup for each task is as follows. We observe that pSGLD consistently outperforms RMSprop. It is clear that our sampling-based method consistently outperforms the optimization counterpart, where the performance gain mainly comes from adding gradient noise and model averaging. When compared with dropout, SGLD performs better on the small LSTM model, but slightly worse on the medium and large LSTM model. This may imply that dropout is suitable to regularizing large networks, while SGLD exhibits better regularization ability on small networks, partially due to the fact that dropout may inject a higher level of noise during training than SGLD. In order to inject a higher level of noise into SGLD, we empirically apply SGLD and dropout jointly, and found that this provided the best performace on the medium and large LSTM model. However, we provide a fair comparison to all methods. Fig. This is unsurprising, because thinned collection provides a better way to select samples. Nevertheless, averaging of samples provides significantly lower perplexity than using single samples. Note that the overfitting problem in Fig. To better illustrate the benefit of model averaging, we visualize in Fig. Their corresponding perplexities for the test sentence are also shown on the right of each row. After averaging, we can see a much lower perplexity, as the samples can complement each other. Colors indicate normalized probability of each word. Best viewed in color. Left are the given images, right are the corresponding captions. The captions in each box are from the same model sample. Consistent with the results in the basic language modeling, pSGLD yields improved performance compared to RMSprop. By comparing pSGLD with RMSprop with dropout, we conclude that pSGLD exhibits better regularization ability than dropout on these two datasets. Apart from modeling weight uncertainty, different samples from our algorithm may capture different aspects of the input image. An example with two images is shown in Fig. In Fig. The combination of pSGLD and dropout consistently provides the lowest errors. In the following, we focus on the analysis of TREC. Each sentence of TREC is a question, and the goal is to decide which topic type the question is most related to: location, human, numeric, abbreviation, entity or description. Fig. pSGLD and dropout have similar behavior: they explore the parameter space during learning, and thus coverge slower than RSMprop on the training dataset. However, the learned uncertainty alleviates overfitting and results in lower errors on the validation and testing datasets. To further study the Bayesian nature of the proposed approach, in Fig. Whatdoes adefibrillatordo? Top two rows show selected ambiguous sentences, which correspond to the points with black circles in tSNE visualization of the testing dataset. ples to estimate the prediction mean and standard derivation on the true type and predicted type. The classifier yields higher probability on the wrong types, associated with higher standard derivations. One can leverage the uncertainty information to make decisions: either manually make a human judgement when uncertainty is high, or automatically choose the one with lower standard derivations when both types exhibits similar prediction means. A more rigorous usage of the uncertainty information is left as future work. We propose a scalable Bayesian learning framework using SG-MCMC, to model weight uncertainty in recurrent neural networks. The learning framework is tested on several tasks, including language models, image caption generation and sentence classification. Our algorithm outperforms stochastic optimization algorithms, indicating the importance of learning weight uncertainty in recurrent neural networks. Our algorithm requires little additional computational overhead in training, and multiple times of forward-passing for model averaging in testing."
104,3,3.0,4.0,3.73,True,acl_2017,train,"Integrating text and knowledge into a unified semantic space has attracted significant research interests recently. However, the ambiguity in the common space remains a challenge, namely that the same mention phrase usually refer to various entities. In this paper, to deal with the ambiguity of entity mentions, we propose a novel Multi-Prototype Mention Embedding model, which learns multiple sense embeddings for each mention by jointly modeling words from textual contexts and entities derived from a knowledge base. In addition, we further design an efficient language model based approach to disambiguate each mention to a specific sense. In experiments, both qualitative and quantitative analysis demonstrate the high quality of the word, entity and multi-prototype mention embeddings. Using entity linking as a study case, we apply our disambiguation method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve the state-of-the-art.","Existing work can be roughly divided into two categories. One is encoding words and entities into a unified vector space using Deep Neural Networks (DNN). However, there are two major problems arising from directly integrating word and entity embeddings into a unified semantic space. First, mention phrases are of great ambiguity and can refer to multiple entities in the common space. Second, an entity often has various aliases when mentioned in various contexts, which implies a much larger size of mention vocabulary compared with entities. To address these issues, we propose to learn multiple embeddings for each mention. Each embedding denotes a meaning, namely mention sense. Thus, mention senses can serve as a repository and each mention can be mapped to them by a pre-defined dictionary. In this paper, we propose a novel MultiPrototype Mention Embedding (MPME) model, which jointly learns the representations of words, entities, and mentions at sense level. The basic idea behind it is to use both textual context information and knowledge of reference entity to distinguish different mention senses. In addition, we also design an language model based approach to determine the sense for each mention in a document based on multi-prototype mention embeddings. For evaluation, we first provide qualitative analysis to verify the effectiveness of MPME to bridge text and knowledge representations at the sense level. Then, separate tasks for words and entities show improvements by incorporating our word, entity and mention representations. Finally, using entity linking as a study case, experimental results on the benchmark dataset demonstrate the effectiveness of our embedding model as well as the disambiguation method. We define the neighbors of ej on the network as N (ej). We use ml to denote an entity mention (perhaps consisting of multiple words). The anchors provide mention boundaries as well as their reference entities from Wikipedia articles. Multi-Prototype Mention Embedding. Confidential Review Copy. DO NOT DISTRIBUTE. Confidential review copy. DO NOT DISTRIBUTE. computing similarity between word and mention embeddings referring to that entity. Finally, we briefly introduce the framework for entity linking. each mention corresponds multiple sense; each sense relates to one entity title. maintain the context cluster; the cluster role. Joint representation learning of text and knowledge for knowledge graph completion. Hongzhao Huang, Larry Heck, and Heng Ji. Leveraging deep neural networks and knowledge graphs for entity disambiguation. Embedding words and senses together via joint knowledgeenhanced training. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. Distributed representations of words and phrases and their compositionality. Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi Choudhury, and Michael Gamon. Representing text for joint embedding of text and knowledge bases. ACL Association for Computational Linguistics. Confidential review copy. DO NOT DISTRIBUTE. computing similarity between word and mention embeddings referring to that entity. Finally, we briefly introduce the framework for entity linking. each mention corresponds multiple sense; each sense relates to one entity title. maintain the context clust r; the clust r role. Joint representation learning of text and knowledge for knowl d e graph completion. Hongzhao Huang, Larry Heck, and Heng Ji. Leveraging deep neural networks and knowledge graphs for entity disambiguation. Embedding words and senses together via joint knowledgeenhanced training. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. Distributed representations of words and phrases and their compositionality. Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi Ch dhury, and Micha l Gamon. Representing text for joint embedding of text and knowledge bases. ACL Associatio for Computational Lin uistics. Confidential review copy. DO NOT DISTRIBUTE. tities by modeling semantic network constructed from the given knowledge base. Joint model learns multiple mention embeddings by maximizing the probability of the mention in the context referring to target entity. Text model. Kg model. Joint model. Given KB, D and the mapped anchors A, we iteratively train the three models until convergence using a joint optimization objective, which will be introduced later. This is reasonable because we output two separately semantic vector spaces for text and knowledge respectively, while we can still obtain the relatedness between word and entity indirectly by computing similarity between word and mention embeddings referring to that entity. Finally, we briefly introduce the framework for entity linking. to one entity title. m intain the co-text cluster; the cluster role. Confidential review copy. DO NOT DISTRIBUTE. tities by modeling semantic network constructed from the given knowledge base. Joint model learns multiple mention embeddings by maximizing the probability of the mention in the context referring to target entity. Text model. Kg model. Joint model. Given KB, D and the mapped anchors A, we iteratively train the three models until convergence using a joint optimization objective, which will be introduced later. This is reasonable because we output two separately semantic vector spaces for text and knowledge respectively, while we can still obtain the relatedness between ord and entity indirectly by computing similarity b tween word and mention embeddings referring to that entity. Finally, we briefly introduce the framework for entity linking. to one entity title. maint in the context cluster; the cluster role. Confidential review copy. DO NOT DISTRIBUTE. Translating embeddings for modeling multirelational data. Xu Han, Zhiyuan Liu, and Maosong Sun. Joint representation learning of text and knowledge for knowledge graph completion. Hongzhao Huang, Larry Heck, and Heng Ji. Leveraging deep neural networks and knowledge graphs for entity disambiguation. Embedding words and senses together via joint knowledgeenhanced training. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. Distributed representations of words and phrases and their compositionality. Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi Choudhury, and Michael Gamon. Representing text for joint embedding of text and knowledge bases. ACL Association for Computational Linguistics. Zhigang Wang and Juan-Zi Li. Text-enhanced representation learning for knowledge graph. Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph and text jointly embedding. ACL. Jason Weston, Antoine Bordes, Oksana Yakhnenko, and Nicolas Usunier. Connecting language and knowledge bases with embedding models for relation extraction. ACL. Confidential review copy. DO NOT DISTRIBUTE. Translating embeddings for modeling multirelati nal data. Xu Han, Zhiyuan Liu, and Maosong Sun. Joint representation learning of text and knowledge for knowledge graph completion. Hongzhao Huang, Larry Heck, and Heng Ji. Leveraging deep neural networks and knowledge graphs for entity disambiguation. Embedding words and senses together via joint knowledgeenhanced training. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. Distributed representations of words and phrases and their compositionality. Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi Choudhury, and Michael Gamon. Representing text for joint embedding of text and knowledge bases. ACL Association for Computational Linguistics. Zhigang W ng and Juan-Zi Li. Text-enhanced representation learning for knowledge graph. Confidential revi w copy. DO NOT DISTRIBUTE. Translating embeddings for modeling multirelational data. Xu Han, Zhiyuan Liu, and Maosong Sun. Joint representation learning of text and knowledge for knowledge graph completion. Hongzhao Huang, Larry Heck, and H ng Ji. Leveraging deep neural networks and knowledge graphs for entity disambiguation. Embed-ding words and senses together via joint knowledgeenhanced training. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. Distributed representations of words and phrases and their compositionality. Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi Choudhury, and Michael Gamon. Representing text for joint embedding of text and knowledge bases. ACL Association for Computational Linguistics. Zhigang Wang and Juan-Zi Li. Text-enhanced representation learning for knowledge graph. Confidential review copy. DO NOT DISTRIBUTE. Particularly, each word has a unique vector, and each mention has multiple sense vectors including two kinds of mention senses: entity-centric sense and out-of-KB sense. When encounter an mention of entity title tl, inspired by the idea of word sense disambiguation (WSD) task, we use the context information to distinguish existing mention senses, or create a new out-of-KB sense. The cluster center is the average of all the context vectors belonging to that cluster. For the similarity metric, we use cosine in our experiments. Efficient string matching: an aid to bibliographic search. J-I Aoe. An efficient digital search algorithm by using a double-array structure. Xu Han, Zhiyuan Liu, and Maosong Su. Joint representation learning of text and k owledge for knowledge graph completion. Confidential review copy. DO NOT DISTRIBUTE. Particularly, each word has a unique vector, and each mention has multiple sense vectors including tw kinds of mention senses: entity-centric sense and out-of-KB sense. When encounter an mention of entity title tl, inspired by the idea of word sense disambiguation (WSD) task, we use the context information to disti uish existing mention senses, or create a new out-of-KB sense. The cluster center is the average of all the context vectors belonging to that cluster. For the similarity metric, we use cosine in our experiments. firstly, we give the phrase embedding by its nearest words and entities. next, we give quantitative analysis on several tasks. Effi-cient string matching: an aid to bibliographic search. J-I Aoe. An efficient digital search algorithm by using a double-array structure. Xu Han, Zhiyuan Liu, and Maosong Sun. Joint representation learning of text and knowledge for knowledge graph completion. Confidential review copy. DO NOT DISTRIBUTE. Particularly, each word has a unique vector, and each mention has multiple se se vectors including two ki ds of mention senses: entity-centric sense a d o t-of-KB sense. When encounter an ti of entity title tl, inspired by the idea of word sense disambiguation (WSD) task, we use the cont xt information to disti guish existing mention senses, or create a n w ut-of-KB sense. The cluster center is the average of all the context vec-rs belo ing to th t clus er. For the sim larity me ric, we use cosin in our experi ents. firstly, we give the phrase embedding by its nearest words and entities. next, we give quanti ative analysis on several tasks. Effi-cie t string matching: an aid to bibliographic search. J-I Aoe. An efficient digital search algorit m by usi g doub e-array structure. Xu Han, Zhiyuan Liu, and Maosong Sun. Joint representation learning of text and knowledge for knowledge graph completion. Confidential review copy. DO NOT DISTRIBUTE. Particularly, ach word has a unique vector, and each m nti n has multiple sense v ctors including two kinds of mention senses: entity-centric sense and out-of-KB sense. When encounter an mention of entity title tl, inspired by the idea of word sense disambiguation (WSD) task, we use the context information to distinguish existing mention senses, or create a new out-of-KB sense. The cl ster c nter is the average of all the context vectors belonging to that cluster. For the similarity metric, we use cosine in our experiments. firstly, we give the phrase embedding by its nearest words and entities. next, we give qua titative analysis on several tasks. Effi-cient string matching: an aid to bibliographic search. J-I Aoe. An efficient digital search algorithm by using a double-arr y structure. Xu H n, Zhiyuan Liu, and Maosong Sun. Joint representation learning of text and knowledge for knowledge graph completion. Confidential review copy. DO NOT DISTRIBUTE. Particularly, each word has a unique vector, and each mention has multiple sense vectors including two kinds of mention senses: entity-centric sense and out-of-KB sense. When encounter an mentio f entity title tl, inspired by the idea of word sense disambiguation (WSD) task, we use th context inf rmation to distinguish existing mention senses, or create a new out-of-KB sense. The cluster center is the average of all the con ext vectors belonging to that cluster. For the similarity metric, we use cosine in our experiments. firstly, we give the phrase embedding by its nearest ords and entities. next, we give quantitative analysis on several tasks. Effi-ci t stri g matching: an id to bibliographic search. J-I Aoe. An efficient digital search algorithm by using double-rray structur. Xu Han, Zhiyuan Liu, and Maosong Sun. Joint representa ion learn ng of text and knowledge for knowledge graph completion. Confidential review copy. DO NOT DISTRIBUTE. Particularly, each word has a unique vector, and each mention has multiple sense v ctors including two kinds of mention senses: entity-centric sense and out-of-KB sense. When encounter an mention of enti y ti le tl, inspired by the idea of word sense di a bigua (WSD) task, we use th context informatio to distinguish existing mention se ses, or create a new out-of-KB sense. The cluster center is the average of all the context vectors belonging to that cluster. F r the similarity metric, we use cosine in our experiments. firstly, we give the phrase embedding by its nearest words and entities. next, we give quantitativ analysis on several tasks. Efficient string matching: an a d to biblio raphic search. J-I Aoe. An efficient digital s arch algorithm by using a double-array structure. Xu Han, Zhiyuan Liu, and Maoso g Sun. J int e resentati n learning of text and kn wled for knowledge graph completio. Confidential review copy. DO NOT DISTRIBUTE. Particularly, each word has a unique vector, and ach mention has multiple sense vectors including two kinds of mention senses: e tity-ce tric sense and out-of-KB sense. When encounter an m tion of entity title tl, inspired by the idea of word sense disambiguation (WSD) task, we use the context informatio to disting i h ex sting me tion enses, or create a n w out-of-KB sense. The cluster ce ter is the ave age of all the context vectors belonging to that cluster. For the similarity metric, we use cosine in our experiments. firstly, we give the phrase embedding by its nearest words and entities. next, we give quantitative analysis on several tasks. Efficient string matching: an aid to bibliographic search. J-I Aoe. An efficient digital search algorithm by using a double-array structure. Joint representation learning of text and knowldg for knowledge graph completion. Confidential review copy. DO NOT DISTRIBUTE. Particularly, each word has a unique vector, and each mention has multiple sense vectors including two kinds of mention ses: ent ty-centric sense and out-of-KB sense. When encounter an mention of entity title tl, inspired by the idea of word sense disambiguation (WSD) task, we use the cont xt information to distinguish existing mention senses, or create a new out-of-KB sense. The cluster center is the averag of all the context vectors belonging to that cluster. For the similarity metric, we use cosine in our experiments. firstly, we give the phrase embedding by its nearest wor s and entities. next, we give quantitative analysis on several tasks. Effi-cient string matching: an a d to bibliogr phic search. J-I Aoe. An efficient digit l search algorithm by using a double-array structure. Xu Han, Zhiyuan Liu, and Maosong Sun. Joint representation learning of text and knowledge for knowledg graph completion. Confidential review copy. DO NOT DISTRIBUTE. Particularly, each word has a unique vector, and each menti n has multiple sense vectors including tw kinds of mention senses: entity-centric sense and o t-of-KB sense. When encoun er an mention of entity title tl, inspir d by the idea of word sense disambiguation (WSD) task, we use the context information to distinguish existing mention senses, or create a new out-of-KB se se. Th clust r cent r is the average of all h context vectors belonging to that cluster. For the s milarity m tric, we use cosine in our experiments. firstly, we give the phrase embedding by its nearest words and entities. next, we give quantitative analysis on several tasks. Efficie t string matching: an aid to bibliographic search. J-I Aoe. An efficie t digital search algorithm by using a doubl-array structure. Xu H n, Zhiyuan Liu, and Maosong Sun. Joint representation learning of text and knowledg for knowledge graph completion. Confidential review copy. DO NOT DISTRIBUTE. Particularly, each word has a unique vector, and ea h mention has multiple se se vectors i cludi g two ki ds of mention senses: entity-centric sense a d o t-of-KB sense. Whe encount r an ti of entity title tl, inspired by the idea of word sense disambiguation (WSD) task, we use the cont xt information to disti guish existing mention senses, or cre te a n w out-of-KB sense. The clust r center is the average of all th context vectors belo ing to th t clus er. F the similarit me ric, we use cosin in our experi ents. firstly, we give the phrase embedding by its nearest words and entities. next, we give quantitative analysis on several tasks. Effi-cie t string matching: a aid to bibliographic search. J-I Aoe. An efficient digital search algorit m by usi g doub e-array structure. Xu H n, Zhiyuan Liu, and Maosong Sun. Joint representation l arning of text and knowledge for knowledge graph completion. Confidential review copy. DO NOT DISTRIBUTE. When encounter an ti of e tity title tl, inspired by the idea of word sense dis mb uati n (WSD) task, we use th cont xt information to disti guish existing mention senses, or create a n w out-of-KB sense. The cluster center is the average of al th context vectors belo ing to th t clus er. For the similarity m ric, we use cosin in ou experi ents. firstly, we giv the phrase embedding by its nearest words and entities. next, we give quantitative analysis on several tasks. Effi cie t string match ng: an id to bibliog phic earch. An efficient digital search algorit by usi g doub e-array structure. Xu Han, Zhiyuan Liu, and Maosong Sun. J int representation learning of text and knowledge for knowledge g aph completion. Confidential review copy. DO NOT DISTRIBUTE. Particularly, each word has a unique vector, and each mention has multiple sense vectors including two kinds of mention senses: entity-c n ric sense and out-of-KB sense. When encounter an mention of entity title tl, inspired by the idea of word sense disambiguation (WSD) task, we use the context information to distinguish existing ment on senses, or create new out-of-KB sense. The cluster center is the average of all the context vectors belo ging to that cluster. For the similarity etric, we use cosine in our experiments. firstly, we give the phrase emb dding by it nearest words and ent ties. next, e give quantitative analysis on several tasks. Efficient string matching: an aid to bibliogr phic search. J-I Aoe. An efficient digita search algorithm by using a double-array structure. Xu Han, Zhiyuan Liu, and Maoso Sun. Joint representation learni g of text d knowledge for knowledge graph completion. Confidential review copy. DO NOT DISTRIBUTE. Note that w,m, s ar atur lly embedded into the s m semantic space since they re basic units in texts, and e modeling the graph tructure in KB is actually in another semantic space. Thus, text an knowledge are com-bined via the bridge of mentions. We can eas-ily obtain the similarity between word and entity Similarity(wi, ej) by computing the similarity between word and its corresponding mention sense: Similarity(wi, f(ej)). The outputs of word embeddings wi and entity embeddings ej keep their own semantic space and are naturally bridged via the new learned entity title embeddings tl, which inspires u to globally optimize the robability of choosing mention s nses of all the phrases of mention names in the given document. Since each mention sense corresponds to an entity, the mentio se se disambiguation process can also be reg rded as inking entities to knowledge base in a unsupervised way, which will be detailed in Section??. The former is pre-defined at the very beginning. The latter is to find possible mention senses for the given mention name, which is similar to candidate mention generation in entity linking task. Conventional candidate mention generation generally maintains a list of pairs of mention name and entity that denotes a candidate reference in knowledge base for the mention name, and recognizes the mention name in text by accurate string matching. Or it firstly recognizes possible mention names in texts using NER (Named Entity Recognition) tool, and then approximately retrieves candidate entities via an information retrieval method. Confidential review copy. DO NOT DISTRIBUTE. N te that w,m, s are naturally embedde into the same semantic space since they are basic units n texts, and e modeling the gr ph structur in KB is actu lly in another semantic space. Then, we introduce a well d sign d mentio sense disambiguation method, which can also be used for entity linking in a unsupervis d way. Thus, text and knowledge are combined via the bridge of mentions. We can easily obtain the similarity between word and entity Similarity(wi, ej) by computing the similarity between word and its corresponding mention sense: Similarity(wi, f(ej)). Th outputs of word embeddi gs wi and entity mbeddings ej keep their own semantic space and are naturally bridged via th new learned entity title mbeddings tl, which inspires us to globally optimize the probability of choosing mention senses of all the phrases of mention names in the give document. Since each enti n sense corresponds to an entity, the mention sense disambiguation process can also be regarded as linking entities to knowledge base in a unsupervised way, which will be detailed in Section??. The former is pre-defined at the very beginning. The latter is to find possible mention senses for the given mention name, which is similar to candidate mention generation in entity linking task. Conventional candidate mention generation generally maintains a list of pairs of mention name and entity that denotes a candidate reference in knowledge base for the mention name, and recognizes the mention name in text by accurate string matching. Or it firstly recognizes possible mention names in texts using NER (Named Entity Recognition) tool, and then approximately retrieves candidate entities via an information retrieval method. Confidential review copy. DO NOT DISTRIBUTE. Note that w,m, s are naturally embedd d into t e sam semantic space since they are basic units in t xts, and modeling the graph structure in KB is actually in another semantic space. Then, we introduce a well designed mention sense disambiguation method, which can also be used for entity linking in a unsupervised way. And then learn entity embed ings so that similar entities on the graph have similar representations. Entity embeddings keep their own semantics in another vector space, be ause we only us them s answers to predi t in mention representation learning by extending Continuous BOW model, which w ll be further discussed in Section??. Since each mention se se corresponds to an entity, the mention sense disambiguation process can also be regarded as linking entities to knowledge base in a unsupervised way, which will be detailed in Section??. The former is pre-defined at the very beginning. The latter is to find possible mention senses f r the given mention name, which is similar to candidate mention generation in entity linking task. This is an ongoing work with the goal of learning additional out-ofKB senses by self-training. In this paper, we will focus on the effectiveness of our model and the quality of three kinds of learned embeddings. Confidential review copy. DO NOT DISTRIBUTE. Not that w,, s are naturally embedded into the sa e semantic space si ce they are basic u its i texts, and e modeling the graph structure in KB is actual y in another semantic space. Then, we introduce a well design d mention sense disambiguation method, which can also b use for entity linki g in a unsupervis d way. And then learn entity embeddings so that similar entities on the graph have similar representations. Since e ch mention sense corresponds to an entity, the mention sense disambiguation process can also be reg rde s l king entities t knowledge base in a unsupervised way, which will be detailed in Section??. The latter is to find possible mention sens s f r the given mention name, which is si ilar to candidate mention generation in entity linking task. Conventional candidate mention generation gener lly maintain a list of pairs of mention name and entity that denotes a candidate reference in knowledge base for the mentio name, and recog-izes the m tion ame i text by accurat s ring m tchi. This is an ongoing work with th goal of learning additional out-ofKB senses by self-training. In this paper, we will focus on the effectiveness of our model and the quality of three kinds of learned embeddings. Confident al Review Copy. DO NOT DISTRIBUTE. T e cluster enter is helpful for inducing mentio sense n co texts. In this section, we describe how to determine the mention sense for e ch m ntion ml in the document. Based on lang age model, identifying m ntion senses in a document c n be r garded as maximizing their joint probability. However, the global optimum is xp nsive, in which each mention g ts n optimum sense, to search over the space of all men ion se ses of all menti ns in the document.   sj,.   The underlying idea is to achieve consistent semantics in a pi ce of text assu ing that all entions inside it are talking about the same topic. In this paper, w regard he mention senses identified first as neighbors of the rest mentions. onfidential Review Copy. DO NOT DISTRIBUTE. Th clu t r cent r s elpful f i duci g menti n sense i c nt xts. In this section, e describe how to determine the mention sense for each m tion ml in the docum nt. Based o language model, identifying mention enses in document can be regarded as maximizing their joint probab lity. However, the global opimum s xpe sive, in which ach mention gets an optimu sense, to search over the space of ll ent o sen es of all mentions in the document.   slj,.   The underlying idea is t achieve c nsist nt se antics in a piece of text assumi g that all ment ons inside it are talk ng about the same topic. In this paper, we regard the mention senses id ntified first as neighbors of the rest mentions. C nfidential Revie Copy. DO NOT DISTRIBUTE. The cluster cent r is helpful for inducing mention sense in contexts. In this i n, we d cribe how to determine the m ntio sen for e ch ntion l in the document.   sj,.   The und rlyin ide is to achieve onsistent s mantics in a piece of text assu ing that all ntions inside it are talking about the sa e topic. In this paper, w regard the menti n senses identifie first as n ighbors of the rest mentions. Confidenti l Review Copy. DO NOT DISTRIBUTE. The cluster cent r is helpful for inducing mention sense in c ntexts. In this section, we escribe h w d termine the m ntion sens f e ch me tion ml in th d cument. Ba ed o languag m del, identi ying mentio e ses in a doc me t can be eg d d a maximiz-ing their joi t pr bability. However, the global optimum is exp nsiv, i which e ch mention gets an optimum sense, to search over the space of all me tion sens s f all mentio s in t e document.   slj,.   The underlying idea is to achi ve consistent semantics in a piece of text assuming that all mentions inside it are talking about the same topic. In this paper, we regard the mention senses identifi d first as nei hbors f the rest ntions. Confiden ial R view Copy. DO NOT DISTRIBUTE. I this section, we d scribe how to determine t e m ntion s nse f r ach me tion ml in the document. Based o language model, identifying mention sense in ocument can be r g rded as maximizi g their joint pr b bility.   slj,.   The und rlyi g idea is to achieve consistent semantics in a piece of text assum ng that all men ions inside it ar t lking about the sa e topic. In this pap r, we regard the ention senses identified first as neighbors of the rest mentions. Confidential Review Copy. DO NOT DISTRIBUTE. The cluster center is helpful for inducing mention sense in cont xts. In this section, e d s rib h w to d t rmine the entio sens f r ach men io ml in t d cument. Bas d on languag mod l, identifying m nti n senses in a d cument can be rega ded as aximizing t eir joint prob b l ty. Howeve, the gl b l optimum is xpen ive, i which each mention g ts an p imum sense, to search over p c f all mention senses of all me tions in th do ument.   slj,.   The underlyi idea is to achi ve consistent semantics i a piece of text assuming that all mentions inside it are talking about the same topic. In this paper, we regard the m tion enses identified first as neighbors of the rest mentions. Confid tia Review Copy. DO NOT DISTRIBUTE. The cluster center is helpful for inducing mention sense in contexts. In this secti, we describ how to d t min the mention sense for each m ntion l i the document. Based on langu ge m d l, id n ifyi g me t on senses i a d cume t can be reg rd d as maxim zing their j int pr bability. However, the global optimum is x ensive, in which each enti n g s an optimum sense, to sear h over the space of all mention senses of all mentions in th document.   slj,.   The underlying idea is to achieve consistent semantics in a pi ce of text assuming that all mentions inside it are talking ab ut the same topic. In this paper, we regard the mention senses identified first as neighbors of the rest mentions. Confid ntial Review Copy. DO NOT DISTRIBUTE. The cluster c nter is helpful for inducing mention sense in contexts. In this section, e describ how to d termin the mention sense for each mention l i t d-ument. Based on language mod l, identifying mention senses in a document can be r g rd d as maxi izing their joint probability. However, the global optimum is expensive, in hich each mentio gets an optimum sense, to search over the space of all mention senses of all me tions i the document.   slj,.   The underlying idea is to ach eve con-sistent manti s a piece of text assuming ha all mentions inside it are talking about the sam opi. In this pap r, w regard the mention senses identified first as neighbors of the rest mentions. Confidential Review Copy. DO NOT DISTRIBUTE. The cluster center is helpful for inducing mention sense in c nt xts. In this section, we describe how t determine the mention ense for each mentio ml in the cument. Based on language model, ide tifying mention senses in a d cument can be r g ded as aximizi g their joi t probability. However, the global optimum is expensive, in which each me tion gets an optimum sense, to search over the space of all mention s ses f all mentions i the ocum nt.   slj,.   The underlying idea is to achiev consist nt semantics in a pi ce of text assuming that all mentions inside it are talki g about the ame topic. In this paper, we r gard the m ntion senses identifi d first as neighbors of th rest e tions. dictionary. Word and mention embeddings are learned from the same semantic space. For example, mention Inde-pe dence Day and ord celebrations co-occur frequently when it refers to the hol day: Indep nd nce Day (US), thus they have similar representations. Besides, by introducing entity embeddings into our MPME fra ework, the knowledge informati n will als be distilled into mention ense mbeddings, so that the mention sense Memorial Day can be similar as Independence Day (US). Mention Sense Disambiguation Given a document, in order to disambiguate the sense for each me tion, we design a la guag model based approach to measure the similarity of each candidate mention sense with the local contexts as well as the neighbor mentions occurri g in a limited size of window. Therefore, we extend Skip-gram model to a network by maximizing the log probability of being a neighbor entity. These entity embeddings will be later used to learn mention representations. Thus, if two mentions refer to similar entities and share similar contexts, they tend to be close in semantic vector space. Thus, words and mention senses will share the same vector space, where similar words and mention senses are close to each other, such as celebrations and Independence Day (US) because they frequently occur in the same contexts. These context clusters will be later used to disambiguate the sense of a given mention with its contexts. Then, based on language model, we identify the correct sense by maximizing a joint probability of all mention senses contained in the document.     It measures how likely a mention sense occurring together with current context words. For example, given the mention sense Independence Day (film), word film is more likely to appear within the context than the word celebrations. Global probability assumes that there should be consistent semantics in a context window, and measures whether all neighbor mentions are related. For instance, two mentions Memorial Day and Independence Day occurring in the same document. If we already know that Memorial Day denotes a holiday, then obviously Independence Day has higher probability of being the holiday than the film. Baseline Methods As far as we know, this is the first work to deal with mention ambiguity in the integration of text and knowledge representations, so there is no exact baselines for comparison. To investigate the effect of multi prototype, we degrade our method to single prototype, which means to use one sense to represent all mentions with the same phrase, namely Single-Prototype Mention Embedding (SPME). For example, we only use one unique sense vector for Independence Day whatever it denotes the holiday or the film. Because ALIGN is incapable of dealing with multiple words, we only give the results of SPME and MPME. Instead, MPME successfully learns two clear and distinct senses. For the sense Independence Day (US), all of it nearest words and entities, such as parades, celebrations, and Memorial Day, are holiday related, while for another sense Independence Day (film), its nearest words and entities, such as robocop and The Terminator, are all science fiction films. All the results demonstrate the effectiveness of our framework in multi-prototype mention embedding learning. We compute cosine similarity between entity embeddings to measure their relatedness, and rank them in a descending order. For example, entity Gente (magazine) should be more relevance to entity France, the place where its company locates. However, ALIGN mixed various meanings of mention Gente (e.g. the song) and ranked some bands higher (e.g. entity Poolside (band)). We analyze the reasons and find that, it can avoid some noise by using word embeddings to predict entities. MPME outperforms all the other methods, which demonstrates that the unambiguous textual information is helpful to refine the entity embeddings. We solve it by finding the closest word vector w? We can see that ALIGN, SPME and MPME, achieve higher performance in semantic task, because relations among entities (e.g. country-capital relation for entity France and Paris) enhance the semantics in word embeddings through jointly training. On the other hand, their syntactic performance decrease. We analyze the results and find that, most of errors are caused by the bias of semantics. Given mentions in text, entity linking aims to link them to a predefined knowledge base. One of the main challenges in this task is the ambiguity of entity mentions. For evaluation, we rank the candidate entities for each mention and report both standard micro (aggregates over all mentions) and macro (aggregates over all documents) precision over top-ranked entities. By incorporating these features into a supervised learningto-rank algorithm, Gradient Boosting Regression Tree (GBRT), each pair is obtained a relevance score indicating whether they should be linked to each other. This is because SPME learns word embeddings and entity embeddings in separate semantic spaces, and fails to measure the similarity between context words and candidate entities. We evaluate our unsupervised disambiguation methods on the entire AIDA dataset. We analyze the results and observe a disambiguation bias to popular senses. Compared to the team, the sense of country occurs more frequently and has a dominant prior probability, which greatly affects the disambiguation. By incorporating local similarity and global probability, both the context words (e.g. defence or match) and the neighbor mentions (e.g. Asian Cup I) provide us enough clues to identifying a soccer related mention sense instead of the country. Influence of Smoothing Parameter As mentioned above, a mention sense may possess a dominant prior probability and greatly affect the disambiguation. These mention sense capture both textual context information and knowledge from reference entities, and provide an efficient approach to disambiguate mention sense in text. We conduct a series of experiments to demonstrate that multiprototype mention embedding improves the quality of both word and entity representations. Using entity linking as a study case, we apply our disambiguation method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve the state-of-the-art. In the future, we will improve the scalability of our model and learn multi-prototype embeddings for the mentions without reference entities in a knowledge base, and introduce compositional approaches to model the internal structures of multiword mentions. Graph ranking for collective named entity disambiguation. Diego Ceccarelli, Claudio Lucchese, Salvatore Orlando, Raffaele Perego, and Salvatore Trani. Learning relatedness measures for entity linking. Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. A unified model for word sense representation and disambiguation. In EMNLP. Silviu Cucerzan. Large-scale named entity disambiguation based on wikipedia data. Xu Han, Zhiyuan Liu, and Maosong Sun. Joint representation learning of text and knowledge for knowledge graph completion. Robust disambiguation of named entities in text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. Improving word representations via global context and multiple word prototypes. In Proc. ACL. Hongzhao Huang, Larry Heck, and Heng Ji. Leveraging deep neural networks and knowledge graphs for entity disambiguation. Cumulated gain-based evaluation of ir techniques. Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and Soumen Chakrabarti. Collective annotation of wikipedia entities in web text. Jiwei Li and Dan Jurafsky. Do multi-sense embeddings improve natural language understanding? In Proc. EMNLP. Embedding words and senses together via joint knowledgeenhanced training. Haixun Wang Yangqiu Song Zhongyuan Wang Kotaro Nakayama Takahiro Hara Masumi Shirakawa and Shojiro Nishio. Microsoft Research. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. Distributed representations of words and phrases and their compositionality. In NIPS. Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. Efficient nonparametric estimation of multiple embeddings per word in vector space. In Proc. EMNLP. Maria Pershina, Yifan He, and Ralph Grishman. Personalized page rank for named entity disambiguation. In HLT-NAACL. Joseph Reisinger and Raymond J Mooney. Multi-prototype vector-space models of word meaning. In Proc. NAACL. Introduction to information retrieval. In Proceedings of the international communication of association for computing machinery conference. Wei Shen, Jianyong Wang, and Jiawei Han. Entity linking with a knowledge base: Issues, techniques, and solutions. Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang, Enhong Chen, and Tie-Yan Liu. A probabilistic model for learning multi-prototype word embeddings. In COLING. Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi Choudhury, and Michael Gamon. Representing text for joint embedding of text and knowledge bases. ACL Association for Computational Linguistics. Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph and text jointly embedding. In Proc. EMNLP. Zhigang Wang and Juan-Zi Li. Text-enhanced representation learning for knowledge graph. In Proc. ACL. Jason Weston, Antoine Bordes, Oksana Yakhnenko, and Nicolas Usunier. Connecting language and knowledge bases with embedding models for relation extraction. In Proc. ACL. Jiawei Wu, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Knowledge representation via joint learning of sequential text and knowledge graphs. CoRR. Joint learning of the embedding of words and entities for named entity disambiguation. In Proc. CoNLL."
387,2,4.14,4.0,3.57,True,acl_2017,train,"Cognitive NLP systemsi.e., NLP systems that make use of behavioral data augment traditional text based features with cognitive features extracted from eye-movement patterns, EEG signals, brain-imaging etc.. Such extraction of features is typically manual. We contend that manual extraction of features is not good enough to tackle text subtleties that characteristically prevail in complex classification tasks like sentiment analysis and sarcasm detection, and that even the extraction and choice of features should be delegated to the learning system. We introduce a framework to automatically extract cognitive features from the eye-movement / gaze data of human readers reading the text and use them as features along with textual features for the tasks of sentiment polarity and sarcasm detection. Our proposed framework is based on Convolutional Neural Network (CNN). The CNN learns features from both gaze and text and uses them to classify the input text. We test our technique on published sentiment and sarcasm labeled datasets, enriched with gaze information, to show that using a combination of automatically learned text and gaze features yields better classification performance over (i) CNN based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features.","learning that the word deadly conveys a strong positive sentiment in opinions such as Shane Warne is a deadly bowler, as opposed to The high altitude Himalayan roads have deadly turns). It is, however, extremely difficult to tackle subtleties at semantic and pragmatic levels. In the absence of explicit clues in the text, it is difficult for automatic systems to arrive at a correct classification decision, as they often lack external knowledge about various aspects of the text being classified. So, in this work, instead of handcrafting the gaze based and textual features, we try to learn feature representations from both gaze and textual data using Convolutional Neural Networks (CNNs). We test our technique on two publicly available datasets enriched with eye-movement information, used for binary classification tasks of sentiment polarity and sarcasm detection. Our experiments show that the automatically extracted features help to achieve significant classification performance improvement over (a) existing systems that rely on handcrafted gaze and textual features and (b) CNN based systems that rely on text input alone. The rest of the paper is organized as follows. Terminology: A fixation is a relatively long stay of gaze on a visual object (such as words in text) where as a sacccade corresponds to quick shifting of gaze between two positions of rest. Forward and backward saccades are called progressions and regressions respectively. A scanpath is a line graph that contains fixations as nodes and saccades as edges. The association between linguistic subtleties and eye-movement patterns could be captured through sophisticated feature engineering that considers both gaze and text inputs. In our work, CNNs take the onus of feature engineering. Filters learned at various CNN layers are quite similar to handcrafted filters used for detection of edges, contours and removal of redundant backgrounds. We believe, a similar technique can also be applied to eye-movement data, where the learned filters will, hopefully, extract informative cognitive features. With more number of convolution filters of different dimensions, the network may extract multiple features related to different gaze attributes (such as fixations, progressions, regressions and skips) and will be free from any form of human bias that manually extracted features are susceptible to. The components are explained below. We separately experiment with static, non-static and multi-channel variants. For each possible input channel of the text component, a given text is transformed into a tensor of fixed length N (padded with zero-tensors wherever necessary to tackle length variations) by concatenating the word embeddings. This operation is applied to each possible window of H words to produce a feature map (c) for the window size H. The idea behind max-pooling is to capture the most important feature-one with the highest value-for each feature map. The model uses multiple filters (with varying window sizes) to obtain multiple features representing the text. In the MULTICHANNELTEXT variant, for a window of H words, convolution operation is separately applied on both the embedding channels. Local features learned from both the channels are concatenated before applying max-pooling. The gaze component deals with scanpaths of multiple participants annotating the same text. These channels are related to two fundamental gaze attributes such as fixation and saccade respectively. With two channels, we thus have three possible configurations of the gaze component such as (i) FIXATION, where the input is normalized fixation duration sequence, (ii) SACCADE, where the input is fixation position sequence, and (iii) MULTICHANNELGAZE, where both the inputs channels are considered. Like the text component, the gaze component uses multiple filters (also with varying window size) to obtain multiple features representing the gaze input. Unfortunately, this makes the training process significantly less scalable, as the number of model parameters to be learned becomes huge. As of now, training and testing are carried out by keeping the order of the input constant. We now share several details regarding our experiments below. Dataset: We experiment on sentiment and sarcasm tasks using two publicly available datasets enriched with eye-movement information. These model hyper-parameters are fixed by trial and error and are possibly good enough to provide a first level insight into our system. Tuning of hyperparameters might help in improving the performance of our framework, which is on our future research agenda. We have also tried randomly initializing the embeddings but better results are obtained with pre-trained embeddings. In this section, we discuss the results for different model variants for sentiment polarity and sarcasm detection tasks. While inspecting the sarcasm dataset, we observe a clear difference between the vocabulary of sarcasm and non-sarcasm classes in our dataset. This, perhaps, was captured well by the text component, especially the variant with only non-static embeddings. In this section, some important observations from our experiments are discussed. Small embedding dimensions are probably reducing the chances of over-fitting when the data size is small. We also observe that for different embedding dimensions, performance of CNN with both gaze and text components is consistently better than that with only text component. Fixation channel does not help much, may be because of higher variance in fixation duration. Accuracy reduces with gaze multichannel, may be because of higher variation of both fixations and saccades across sarcastic and non-sarcastic classes, as opposed to sentiment classes. Output of the hidden layer after merge layer is considered as features learned by the network. As one can see, addition of gaze information helps to generate features with more subtle differences (marked by blue rectangular boxes) for sarcastic and non-sarcastic texts. It is also interesting to note that in the marked region, features for the sarcastic texts exhibit more intensity than the nonsarcastic ones-perhaps capturing the notion that sarcasm typically conveys an intensified negative opinion. In cases like this, addition of gaze information does not help much in learning more distinctive features, as it becomes difficult for even humans to classify such texts. Sentiment and sarcasm classification are two important problems in NLP and have been the focus of research for many communities for quite some time. Popular sentiment and sarcasm detection systems are feature based and are based on unigrams, bigrams etc. I would like to live in Manchester, England. The transition between Manchester and death would be unnoticeable. We really did not like this camp. After a disappointing summer, we switched to another camp, and all of us much happier on all fronts! Howard is the King and always will be, all others are weak clones. Plots with thick red borders correspond to wrongly predicted examples. Recent systems are based on variants of deep neural network built on the top of embeddings. Eye-tracking technology is a relatively new NLP, with a very few systems directly making use of gaze data in prediction frameworks. These recent advancements motivate us to explore the cognitive NLP paradigm. On multiple published datasets for which gaze information is available, our systems could achieve significant performance improvements over (a) systems that rely on handcrafted gaze and textual features and (b) CNN based systems that rely on text input alone. An analysis of the learned features confirms that the combination of automatically learned features is indeed capable of representing deep linguistic subtleties in text that pose challenges to sentiment and sarcasm classifiers. Our future agenda includes: (a) optimizing the CNN framework hyper-parameters (e.g., filter width, dropout, embedding dimensions etc.) to obtain better results, (b) exploring the applicability of our technique for document-level sentiment analysis and (c) applying our framework on related problems, such as emotion analysis, text summarization and question-answering."
503,2,2.75,3.38,2.38,False,acl_2017,train,"Distributions over strings and trees can be represented by probabilistic regular languages, which characterize many models in natural language processing. Recently, several datasets have become available which represent natural language phenomena as graphs, so it is natural to ask whether there is an equivalent of probabilistic regular languages for graphs. To answer this question, we review three families of graph languages: Hyperedge Replacement Languages (HRL), which can be made probabilistic; Monadic Second Order Languages (MSOL), which support the crucial property of closure under intersection; and Regular Graph Languages (RGL; Courcelle 1991), a subfamily of both HRL and MSOL which inherits these properties, and has not been widely studied or applied to NLP. We prove that RGLs are closed under intersection and provide an efficient parsing algorithm, with runtime linear in the size of the input graph.","To preserve semantics, they must model semantics. To make use of this data, we require probabilistic models of graphs. We first parse a source sentence to its semantic representation, and then generate a target sentence from this representation. It is therefore natural to ask whether there is a family of graph languages with similar properties to finite automata. The restrictions ensure that RGLs are a subfamily of MSOL. However, for the weights to define a probability distribution, they must meet the stronger condition that the sum of multiplied weights over all definable objects is one. We suspect that there are DAGAL (hence MSOL) for which it is not possible. not been widely studied, and they have never been applied to NLP. We present two new results. We use the following notation.     vk). In figures, the ith external node will be labeled (i).   The rank of a hypergraph G is the size of extG. An edge is a terminal edge if its label is terminal and a nonterminal edge if its label is nonterminal. A graph is a terminal graph if all of its edges are labeled with terminal symbols. The terminal subgraph of a graph is the subgraph consisting of all terminal edges and their endpoints.     We call the family of languages that can produced by any HRG the hyperedge replacement languages (HRL). The labels p, q, r, s, t, and u label the productions so that we can refer to them in the text. To explain the restrictions, we first require some definitions.     Either it is a single terminal edge, all nodes of which are external, or each of its edges has at least one internal node. We call the family of languages generated by RGGs the regular graph languages (RGLs). A full explanation is beyond the scope of this paper but we provide a brief discussion of MSO in the supplementary materials. Here, we give a sufficient condition for a subfamily G to be closed under intersection. This mild assumption may be a reasonable for applications like AMR parsing, where grammars could be designed so that the external node is always the unique root. Later we will relax this assumption. Both algorithms operate by recognizing incrementally larger subgraphs of the input graph, using a succinct representation for subgraphs that depends on an arbitrarily chosen marker node m of the input graph.            Since our algorithm makes top-down predictions based on known external nodes, our boundary representation must cover the case where a subgraph is empty except for these nodes. PREDICT is applied when the edge after the dot is nonterminal, assigning any external nodes that have been identified. SCAN is applied when the edge after the dot is terminal.      Our algorithm requires a fixed ordering of the edges in the right-hand sides of each production.                    parsing complexity.     Let s be the order of a right-hand side of a production.   Arbitrary HRGs do not necessarily admit a normal ordering. However, RGGs do admit a normal ordering. Proof. If R(p) contains a single node then it must be an external node and it must have a terminal edge attached to it since R(p) must contain at least one terminal edge. Normal ordering tightly constrains recognition of edges. Assume a normally-ordered RGG. The maximum number of nodes in any right-hand side of a production (k) is also the maximum number of boundary nodes for any subgraph in the parser. COMPLETE combines subgraphs I and J only when the entire subgraph derived from Y has been recognized. Boundary nodes of J are also boundary nodes of I since they are nodes in the terminal subgraph of R(p) where Y connects. This would add a factor of n to the complexity as the graph could start at any node, requiring n runs of the algorithm. RGG supports probabilistic interpretation and is closed under intersection, making it similar to regular families of string and tree languages that are widely used in NLP. Perhaps we need less expressivity than HRG but more than RGG. Since RGLs are a subfamily of both HRL and MSOL, they inherit probability and intersection closure, respectively. However, there are two recent independently-developed formalisms that may be useful. TLGs are in MSOL (and are closed under intersection) but we do not yet know if RDG is a subfamily of MSOL, or whether they are closed under intersection. They are both incomparable to RGG, but they share important characteristics, including the fact that the terminal subgraph of every production is connected. This means that our top-down parsing algorithm is applicable to both. We conjecture that larger, less restrictive subfamilies of SCFLs may be found based on a weaker restriction of connected terminal subgraphs, and we plan to explore these questions in future work."
145,2,4.25,4.0,4.0,True,acl_2017,train,"Word embeddings provide point representations of words containing useful semantic information. We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty information. To learn these distributions, we propose an energy-based max-margin objective. We show that the resulting approach captures uniquely expressive semantic information, and outperforms alternatives, such as word2vec skip-grams, and Gaussian embeddings, on benchmark datasets such as word similarity and entailment.","We can imagine representing every word with a binary one-hot vector corresponding to a dictionary position. But such a representation contains no valuable semantic information: distances between word vectors represent only differences in alphabetic ordering. These learned word embeddings have become ubiquitous in predictive tasks. Specifically, they model each word by a Gaussian distribution, and learn its mean and covariance matrix from data. This approach generalizes any deterministic point embedding, which can be fully captured by the mean vector of the Gaussian distribution. Moreover, the full distribution provides much richer information than point estimates for characterizing words, representing probability mass and uncertainty across a set of semantics. Moreover, the mean of the Gaussian can be pulled in many opposing directions, leading to a biased distribution that centers its mass mostly around certain meaning while leaving the others not well represented. In this paper, we propose to represent each word with an expressive multimodal distribution, for multiple distinct meanings, entailment, heavy tailed uncertainty, and enhanced interpretability. It is our contention that such flexibility is critical for both qualitatively learning about the meanings of words, and for optimal performance on many predictive tasks. In the past decade, there has been an explosion of interest in word vector representations. Their approach is significantly more expressive than typical point embeddings, with the ability to represent concepts such as entailment, by having the distribution for one word (e.g. However, with a unimodal distribution, their approach cannot capture multiple distinct meanings, much like most deterministic approaches. Probabilistic word embeddings have only recently begun to be explored, and have so far shown great promise. In this paper, we propose, to the best of our knowledge, the first probabilistic word embedding that can capture multiple meanings. We use a Gaussian mixture model which allows for a highly expressive distributions over words. At the same time, we retain scalability and analytic tractability with an expected likelihood kernel energy function for training. The model and training procedure harmonize to learn descriptive representations of words, with superior performance on several benchmarks. In this section, we introduce our Gaussian mixture (GM) model for word representations, and present a training method to learn the parameters of the Gaussian mixture. This method uses an energy-based maximum margin objective, where we wish to maximize the similarity of distributions of nearby words in sentences. We propose an energy function that compliments the GM model by retaining analytic tractability. We also provide critical practical details for numerical stability and initialization. We represent each word w in a dictionary as a Gaussian mixture with K components. For instance, one component of a polysemous word such as rock should represent the meaning related to stone or pebbles, whereas another component should represent the meaning related to music such as jazz, pop. Each Gaussian component is represented by an ellipsoid, whose center is specified by the mean vector and contour surface specified by the covariance matrix, reflecting subtleties in meaning and uncertainty. On the left, we show examples of Gaussian mixture distributions of words where Gaussian components are randomly initialized. After training, we see on the right that one component of the word rock is closer to stone and basalt, whereas the other component is closer to jazz and pop. We also demonstrate the entailment concept where the distribution of more general word music encapsulates words such as jazz, rock, pop. Moreover, the mean vector for such words can be pulled between two clusters, centering the mass of the distribution on a region which is far from certain meanings. This procedure follows the distributional hypothesis that words occurring in natural contexts tend to be semantically related. For instance, the words jazz and music tend to occur near one another more often than jazz and cat; hence, jazz and music are more likely to be related. The learned word representation contains useful semantic information and can be used to perform a variety of NLP tasks such as analogy, word similarity analysis, sentiment classification, or as a preprocessed input for complex system such as statistical machine translation. For vector representation of words, a usual choice for similarity measure (energy function) is a dot product between two vectors. Our word representations are distributions instead of point vectors and therefore need a measure that reflects not only the point similarity, but also the uncertainty. We choose this form of energy since it can be evaluated in a closed form given our choice of probabilistic embedding in Eq. Observe that this term captures the similarity between the ith meaning of word wf and the jth meaning of word wg. If the semantic uncertainty (covariance) for both pairs are low, this term has more importance relative to other terms due to the inverse covariance scaling. During this time, all components learn the signals from the word occurrences equally. The negative KL divergence is another sensible choice of energy function, providing an asymmetric metric between word distributions. However, unlike the expected likelihood kernel, KL divergence does not have a closed form if the two distributions are Gaussian mixtures. We have introduced a model for multi-prototype embeddings, which expressively captures word meanings with whole probability distributions. Our model also reduces the unnecessarily large variance of a Gaussian embedding model, and has improved results on word entailment tasks. For reproducibility, we describe hyperparameter settings and implementation details in the supplementary material. Code will be made publicly available upon publication. We perform qualitative evaluation to show that our embeddings learn meaningful multi-prototype representations and compare to existing models using a quantitative evaluation on word similarity datasets and word entailment. Since our word embeddings contain multiple vectors and uncertainty parameters per word, we use the following measures that generalizes similarity scores. These measures pick out the component pair with maximum similarity and therefore determine the meanings that are most relevant. This metric incorporates the uncertainty from the covariance matrices in addition to the similarity between the mean vectors. For a Gaussian embedding, maximum similarity reduces to the usual cosine similarity. Cosine similarity is popular for evaluating embeddings. However, our training objective directly involves the Euclidean distance in Eq. For instance, a word such as rock that could mean either stone or rock music should have each of its meanings represented by a distinct Gaussian component. For instance, rockmostly has neighbors related to rock music and bank mostly related to the financial bank. The alternative meanings of these polysemous words are not well represented in the embeddings. Each dataset contains a list of word pairs with a human score of how related or similar the two words are. The Spearman correlation is a rank-based correlation measure that assesses how well the scores describe the true labels. The maximum cosine similarity yields the best performance on most datasets; however, the minimum Euclidean distance is a better metric for the datasets MC and RW. These results are consistent for both the single-prototype and the multi-prototype models. The measures mc, el, me denote maximum cosine similarity, expected likelihood kernel, and minimum Euclidean distance. For each dataset, we boldface the score with the best performance across all models. This could be due to the autocalibration of importance via the covariance learning which decrease the importance of very frequent words such as the, to, a, etc. We note that Chen model uses an external lexical source WordNet that gives it an extra advantage. We use many metrics to calculate the scores for the Spearman correlation. MaxSim refers to the maximum cosine similarity. AveSim is the average of cosine similarities with respect to the component probabilities. The metrics used are the maximum cosine similarity, or the maximum negative KL divergence. One motivation for our Gaussian mixture embedding is to model word uncertainty more accurately than Gaussian embeddings, which can have overly large variances for polysemous words. We see that our Gaussian mixture model does indeed reduce the variances of each component for such words. We also see, in the next section, that the Gaussian mixture model has desirable qualitative behavior for word entailment. The minimum KL divergence is similar to the maximum cosine similarity, but also incorporates the embedding uncertainty. And KL divergence is an asymmetric measure, which is more suitable for certain tasks such as word entailment. The multi-prototype model estimates the meaning uncertainty better since it is no longer constrained to be unimodal, leading to better characterizations of entailment. On the other hand, the Gaussian embedding model suffers from large variance problem for polysemous words, which results in less informative word distribution and inferior entailment scores. We introduced a model that represents words with expressive multimodal distributions formed from Gaussian mixtures. To learn the properties of each mixture, we proposed an analytic energy function for combination with a maximum margin objective. The resulting embeddings capture different semantics of polysemous words, uncertainty, and entailment, and also perform favorably on word similarity benchmarks. Similarly, probabilistic word embeddings can capture a range of subtle meanings, and advance the state of the art in predictive tasks. In the future, such representations could also open the doors to a new suite of applications in language modelling, where word distributions are used as inputs to probabilistic LSTMs, or in decision functions where uncertainty matters. Let f, g be Gaussian mixture distributions representing the words wf, wg. Frequent words such as the, a, to are not as meaningful as relatively more infrequent words such as dog, love, rock and we are often more interested in learning the semantics of the less frequently observed words. The spherical case which we use in all our experiments is similar since we simply replace a vector d with a single value. Optimization Constraint and Stability We optimize logd since each component of diagonal vector d is constrained to be positive. In order to stabilize the computation in eq. That is, each word has two sets of distributions qI and qO, each of which is a Gaussian mixture. For a given pair of word and context (w, c), we use the input distribution qI for w (input word) and the output distribution qO for context c (output word). We optimize the parameters of both qI and qO and use the trained input distributions qI as our final word representations. This is because the gradient computation of the model is relatively fast, so a complex gradient update algorithm such as Adam becomes the bottleneck in the optimization. Therefore, we choose to use Adagrad which allows us to better scale to large datasets."
779,3,3.42,3.08,3.75,True,acl_2017,train,"While end-to-end neural machine translation (NMT) has made remarkable progress recently, it still suffers from the data scarcity problem for low resource language pairs and domains. In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a third language. Based on the assumption, we are able to train a source-to-target NMT model without parallel corpora available (“student”) guided by an existing pivot-to-target NMT model (“teacher”) on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs.","Due to the large parameter space, neural models usually learn poorly from low-count events, resulting in a poor choice for low resource language pairs. As a result, a number of authors have endeavored to explore methods for translating language pairs without parallel corpora available. These methods can be roughly divided into two broad categories: multilingual and pivot-based. They fine-tune the attention part using pseudo bilingual sentences for the zeroresource language pair. They use parallel corpora of multiple languages to train one single model, which is then able to translate a language pair without parallel corpora available. Although these approaches prove to be effective, the combination of multiple languages in modeling and training leads to significantly increased complexity. In this paper, we propose a new method for zero-resource neural machine translation. Our method assumes that parallel sentences should have close probabilities of generating a sentence in a third language. X, Y, and Z denote source, target, and pivot languages, respectively. We use a dashed line to denote that there is a parallel corpus available for the connected language pair. Solid lines with arrows represent translation directions. The pivot-based approach leverages a pivot to achieve indirect source-to-target translation: it first translates x into z, which is then translated into y. This strategy not only improves efficiency but also avoids error propagation in decoding. Experiments on the Europarl and WMT datasets show that our approach achieves significant improvements in terms of both translation quality and decoding efficiency over a baseline pivot-based approach to zeroresource NMT on Spanish-French and GermanFrench translation tasks. As a data-driven approach, NMT treats parallel corpora as the major source for acquiring translation knowledge. Let x be a source-language sentence and y be a target-language sentence. The heavy dependence on the quantity of training data poses a severe challenge for NMT to translation zero-resource language pairs. Let X, Y, and Z to denote source, target, and pivot languages. We use dashed lines to denote language pairs with parallel corpora available and solid lines with arrows to denote translation directions. Therefore, it is necessary to explore methods for direct modeling of source-to-target translation without parallel corpora available. In this work, we propose to directly model the intended source-to-target neural translation based on a teacher-student framework. In the following subsections, we will introduce two approaches to zero-resource neural machine translation based on the two assumptions. Then, standard stochastic gradient descent algorithms can be used to optimize model parameters. For the Europarl corpus, we evaluate our approach on Spanish-French (EsFr) and Germany-French (De-Fr) translation tasks. For the WMT corpus, we our approach on the Spanish-French (Es-Fr) translation task. English is used a pivot language in all experiments. We evaluate our approach on the Europarl and WMT corpora. All the experiments treat English as the pivot language and French as the target language. To build nonoverlapping source-to-pivot and pivot-to-target datasets, we split pivot sentences shared by the original source-to-pivot and pivot-to-target corpora into two equal parts and merge them separately to the remaining source-to-pivot and pivotto-target corpora. All the sentences are tokenized by the tokenize.perl script. random init. We use a random initialized source-to-target model for comparison. a zero-resource setting. All the sentences are tokenized by the tokenize.perl script. We use case-sensitive BLEU to evaluate translation results. BPE is also used to reduce the vocabulary size. We leverage a tutorial NMT open-source code implemented by Theano for all the experiments. Then we use these two models to calculate JSENT and JWORD, which reveals how close the sentencelevel and word-level distributions are, respectively. Thus, we suspect that the sent-beam method will surpass the sent-greedy method. For word-level results, KL divergence of sampling approximation is the highest, indicating poorer performance than the other two methods. However, sampling methods introduce more data diversity at the target side. Thus it is harder to decide which factor dominates the training process. English is treated as the pivot language. pairs. Besides that, our word-sampling method surprisingly obtains improvement over the likelihood method, which leverages source-to-target parallel corpus and also takes much longer to train. The significant improvements can be explained by the error propagation problem of pivot-based methods that translation error of the source-pivot translation system will be transferred to the pivot-target translation. However, as the time complexity grows linearly in the number of beams k, the better performance is achieved at the expense of beam search time. than that by mode approximation, word-sampling introduces more data diversity for training, which dominates the effect of KLD difference. We observe that word-level models tend to have lower valid loss compared with sentencelevel methods. Generally, models with lower valid loss are inclined to have higher BLEU. The BLEU scores are case sensitive. Our proposed method even slightly outperforms standard NMT system trained with parallel source-to-target corpus when this corpus contains similar number of sentences as source-topivot corpus, demonstrating that the effectiveness of our methods to learn from teacher model. This phenomenon can be explained by that the total size of corpus involved in our method is twice that of standard MLE. Our proposed method can also be applied to zeroresource NMT with low source-to-pivot corpus. Specifically, the size of source-pivot corpus is magnitude smaller than that of pivot-target corpus. This setting makes sense in applications. For example, there are significantly fewer Urdu-English corpus available than English-French corpus. Surprisingly, our method outperforms all other methods. We leave further evaluation of this method for future work. However the model converges to similar point as without initialization. preprocessing as ours. Training NMT system in zero-resource scenario by leveraging some other languages has attracted intensive attention in last year. They use the multi-way NMT model trained by other language pairs to generate pseudo parallel corpus and fine-tune the attention mechanism of the multi-way NMT model to enable zero-resource translation. Several authors propose a universal encoder-decoder network in multilingual scenarios to perform zero-shot learning. Besides multilingual NMT, another import line of research is pivot-based methods. They suggest to generate pseudo corpus to train the student network. Our work is highly related to theirs. However, we focus on zeroresource learning instead of model compression. In this paper, we propose a novel framework to transfer the knowledge of the teacher model trained with rich-resource language pair into the student model with zero-resource with the help of source-to-pivot corpus. We introduce sentencelevel and word-level teaching to guide the learning process of the student model. Our experiments on Europarl Corpus and WMT corpus across languages show that our proposed word-level sampling method can significantly outperforms the state-of-the-art pivot-based methods and multilingual methods in terms of both translation quality and decoding efficiency. The experiments on Europarl Corpus show that our approach obtains an significant improvement over pivot method significantly."
684,3,3.9,3.7,3.7,True,acl_2017,train,"In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task–the CNN & Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention.","Intuitively, the multi-hop architecture allows the reader to incrementally refine token representations, and the attention mechanism re-weights different parts in the document according to their relevance to the query. The effectiveness of multi-hop reasoning and attentions have been explored orthogonally so far in the literature. In this paper, we focus on combining both in a complementary manner, by designing a novel attention mechanism which gates the evolving token representations across hops. Such a fine-grained attention enables our model to learn conditional token representations w.r.t. the given question, leading to accurate answer selections. Qualitatively, visualization of the attentions at intermediate layers of the GA reader shows that in each layer the GA reader attends to distinct salient aspects of the query which help in determining the answer. Below we provide an overview of representative neural network architectures which have been applied to this problem. These include the DeepLSTM Reader which performs a single forward pass through the concatenated (document, query) pair to obtain g(d, q); the Attentive Reader which first computes a document vector d(q) by a weighted aggregation of words according to attentions based on q, and then combines d(q) and q to obtain their joint representation g(d(q), q); and the Impatient Reader where the document representation is built incrementally. A probability distribution over the entities in d is obtained by computing dot products between q and the entity embeddings and taking a softmax. Then, an aggregation scheme named pointer-sum attention is further applied to sum the probabilities of the same entity, so that frequent entities the document will be favored compared to rare ones. Attention over the memory slots given the query is used to compute an overall memory and to renew the query representation over multiple iterations, allowing certain types of reasoning over the salient facts in the memory and the query. The latter allows dynamic reasoning steps and is trained with reinforcement learning. The focus of this paper, however, is on designing models which exploit the available data efficiently. In reading comprehension tasks, ideally, the semantic meanings carried by the contextual embeddings should be aware of the query across hops. As an example, human readers are able to keep the question in mind during multiple passes of reading, to successively mask away information irrelevant to the query. In contrast, we propose a finer-grained model which attends to components of the semantic representation being built up by the GRU. The new attention mechanism, called gated-attention, is implemented via multiplicative interactions between the query and the contextual embeddings, and is applied per hop to act as fine-grained information filters during the multi-step reasoning. The filters weigh individual components of the vector representation of each token in the document separately.     where denotes the Hadamard product or the element-wise multiplication.           Fig. The document embeddings are transformed by taking the full output of a document Bi-GRU (indicated in blue in Fig. Dashed lines represent dropout connections. For brevity, let us drop the superscript k in this subsection as we are focusing on a particular layer.   a cross-entropy loss between the predicted probabilities and the true answers. We also utilize a character composition model C(w) which generates an orthographic embedding of the token. We conducted several experiments both with and without this feature and observed some interesting trends, which are discussed below. Henceforth, we refer to this feature as the qe-comm feature or just feature. We evaluate the GA reader on five large-scale datasets recently proposed in the literature. A query over each article is formed by removing an entity from the short summary which follows the article. Further, entities within each article were anonymized to make the task purely a comprehension one. N-gram statistics, for instance, computed over the entire corpus are no longer useful in such an anonymized corpus. We only focus on subsets where the deleted token is either a common noun (CN) or named entity (NE) since simple language models already give human-level performance on the other types (cf. First, article pairs which appeared around the same time and with overlapping entities are chosen, and then one article forms the document and a cloze query is constructed from the other. Missing tokens are always person named entities. Questions which are easily answered by simple baselines are filtered out, to make the task more challenging. We report results on both settings which share the same validation and test sets. The numbers reported for GA Reader are for single best models, though we compare to both ensembles and single models from prior work. A detailed analysis of these differences is studied in the next section. We note that anonymization of the latter datasets means that there is already some feature engineering (it adds hints about whether a token is an entity), and these are much larger than the other four. In machine learning it is common to see the effect of feature engineering diminish with increasing data size. Similarly, fixing the word embeddings provides an improvement for the WDW and CBT, but not for CNN and Daily Mail. This is not surprising given that the latter datasets are larger and less prone to overfitting. They also outperform previous ensemble models, setting a new state of that art for both datasets. Lastly, on CBT-CN the GA Reader with the qe-comm feature outperforms all previously published single models except the NSE, and AS Reader trained on a larger corpus. In this section we do an ablation study to see the effect of Gated Attention. We compare the GA Reader as described here to a model which is exactly the same in all aspects, except that it passes document embeddings D(k) in each layer directly to the inputs of the next layer without using the GA module. This model ends up using only one query GRU at the output layer for selecting the answer from the document. On CNN when tested without feature engineering, we observe that GA provides a significant boost in performance compared to without GA. On WDW-Strict, which is a third of the size of CNN, without the feature we see an improvement when using GA versus without using GA, which becomes significant as the training set size increases. When tested with the feature on WDW, for a small data size without GA does better than with GA, but as the dataset size increases they become equivalent. We conclude that GA provides a boost in the absence of feature engineering, or as the training set size increases. Best performance on standard training sets is in bold, and on larger training sets in italics. This is a common trend in machine learning as model complexity is increased, however we note that a multi-hop architecture is important to achieve a high performance for this task, and provide further evidence for this in the next section. Lastly, we perform an ablation study for the three components of the GA Reader which were absent in the preprint version (GA Reader--). See Appendix B for more details. To gain an insight into the reading process employed by the model we analyzed the attention distributions at intermediate layers of the reader. The right plot shows attention over candidates in the document of cloze placeholder (XXX) in the query at the final layer. The full document, query and correct answer are shown at the bottom. The incorrect answer, in contrast, only attends to one of these aspects, and hence receives a lower score in the final layer despite the n-gram overlap it has with the cloze token in the query. Importantly, different layers tend to focus on different tokens in the query, supporting the hypothesis that the multihop architecture of GA Reader is able to combine distinct pieces of information to answer the query. We presented the Gated-Attention reader for answering cloze-style questions over documents. The GA reader features a novel multiplicative gating mechanism, combined with a multi-hop ar-chitecture. Our model design is backed up by an ablation study showing statistically significant improvements of using Gated Attention as information filters. We also showed empirically that multiplicative gating is superior to addition and concatenation operations for implementing gated-attentions, though a theoretical justification remains part of future research goals. Analysis of document and query attentions in intermediate layers of the reader further reveals that the model iteratively attends to different aspects of the query to arrive at the final answer. In this paper we have focused on text comprehension, but we believe that the Gated-Attention mechanism may benefit other tasks as well where multiple sources of information interact. Next, we observe a substantial drop when removing token-specific attentions over the query in the GA module, which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the overall query representation. dim() indicates hidden state size of GRU."
759,1,4.0,3.0,3.0,True,acl_2017,train,We present a joint modeling approach to identify salient discussion points in spoken meetings as well as to label the discourse relations between speaker turns. A variation of our model is also discussed when discourse relations are treated as latent variables. Experimental results on two popular meeting corpora show that our joint model can outperform SVM-based classifiers for both phrase-based content selection and discourse relation prediction tasks. We also evaluate our model on predicting the consistency among team members’ understanding of their group decisions. Classifiers trained with features constructed from our model achieve significant better predictive performance than the state-of-the-art.,"Automatically extracting the critical points and important outcomes from dialogues would facilitate generating summaries for complicated conversations, understanding the decision-making process of meetings, or analyzing the effectiveness of collaborations. Indeed, content of different speaker turns do not occur in isolation, and should be interpreted within the context of discourse. Meanwhile, content can also reflect the purpose of speaker turns, thus facilitate with discourse relation understanding. Um can either use a hand dynamo, or the kinetic type ones, you know that they use in watches, or else uh a solar powered one. B: Um the bat uh the battery for a a watch wouldn't require a lot of power, would be my one query. Is a kinetic one going to be able to supply enough power? D: Yeah, I don't think it would. C: Yeah. D: We should probably just use conventional batteries. B: Which I suppose as well would allow us to go off the shelf again, you'd say? D: Yeah. Uncertain Option an example. As can be seen, meeting participants evaluate different options by showing doubt (UNCERTAIN), bringing up alternative solution (OPTION), or giving feedback. Moreover, acquiring human annotation on discourse relations is a timeconsuming and expensive process, and does not scale for large datasets. We hypothesize that leveraging the interaction between content and discourse has the potential to yield better prediction performance on both phrase-based content selection and discourse relation prediction. Algorithms for joint learning and joint inference are proposed for our model. We also present a variation of our model to treat discourse relations as latent variables when true labels are not available for learning. Combined with the predicted discourse structure, a visualization tool can be exploited to display conversation flow to support intelligent meeting assistant systems. To the best of our knowledge, our work is the first to jointly model content and discourse relations in meetings. Our model trained with latent discourse also outperforms SVMs on both AMI and ICSI corpora. We further evaluate the usage of selected phrases as extractive meeting summaries. We construct features from our model predictions to capture different discourse patterns and word entrainment scores for discussion with different COU level. There is much less work that jointly predicts the importance of content along with the discourse structure in dialogus. Only local discourse structures from adjacent utterances are considered. Our model is built on tree structures, which captures more global information. Our work is also in line with keyphrase identification or phrase-based summarization for conversations. Our work also targets at detecting salient phrases from meetings, but focuses on the joint modeling of critical discussion points and discourse relations held between them. Our proposed model learns to jointly perform phrase-based content selection and discourse relation prediction by making use of the interaction between the two sources of information. Each discourse unit can be a complete speaker turn or a part of it. A set of candidate phrases are extracted from each discourse unit xi, from which salient phrases that contain gist information will be identified. If a candidate is a parent of another candidate in the constituent parse tree, we will only keep the parent. We further merge a verb and a candidate noun phrase into one candidate if the later is the direct object or subject of the verb. wc, wd, and wcd are corresponding feature parameters. Discourse Relations as Latent Variables. As we mentioned in the introduction, acquiring labeled training data for discourse relations is a timeconsuming process since it would require human annotators to inspect the full discussions. Its learning algorithm is slightly different as described in the next section. In general, the learning algorithm constructs a sequence of configurations for sample labels as a Markov chain Monte Carlo (MCMC) chain based on a task-specific loss function, where stochastic gradients are distributed across the chain. This is suitable for our learning problem because we aim to optimize the prediction performance for both phrase selection and discourse relations with various types of features. Multiple epochs are run through all samples. Local search is used for both proposal distributions. The parameters w are updated accordingly.   Dynamic programming can be employed to carry out joint inference, however, it would be time-consuming since our objective function has a large search space for both content and discourse labels. Hence we propose an alternating optimizing algorithm to search for c and d iteratively. We believe that candidate phrases based on the same concepts should have the same predicted label. Therefore, candidates of the same phrase type and sharing the same head word are grouped into one cluster. The inference process is the same for models trained with latent discourse relations. We use features that characterize content, discourse relations, and the combination of both. Content Features. We also consider whether the head word of the phrase has been mentioned in preceding turn, which implies the focus of a discussion. The size of the cluster this phrase belongs to is also included. Number of POS tags and phrase types are counted to characterize the syntactic structure. We thus identify the absolute and relative positions of the turn containing the candidate phrase in the discussion. Finally, we record whether the candidate phrase is uttered by the main speaker, who speakers the most words in the discussion. Discourse Features. We record whether two turns are uttered by the same speaker, for example, ELABORATION is commonly observed between the turns from the same participant. We also calculate the number of candidate phrases based on the observation that OPTION and SPECIALIZATION tend to contain more informative words than POSITIVE feedback. Length of the discourse unit is also relevant. Therefore, we compute the time span and number of words. To incorporate global structure features, we encode the depth of the node in the discourse tree and the number of its siblings. Joint Features. For modeling the interaction between content and discourse, the discourse relation is added to each content feature to compose a joint feature. Meeting Corpora. Both of the corpora are annotated with dialogue acts, adjacency pairs, and topic segmentation. Acquiring Gold-Standard Labels. Both corpora contain human constructed abstractive summaries and extractive summaries on meeting level. decisions, are identified and used as extractive summaries, and some of them are also linked to the corresponding abstracts. Since the corpora do not contain phrase-level importance annotation, we induce gold-standard labels for candidate phrases based on the following rule. A candidate phrase is considered as a positive sample if its head word is contained in any abstractive summary or participant summary. The nodes of the tree contain partial or complete speaker turns, and discourse relation types are labeled on the links between the nodes. This dataset is called AMI-SUB hereafter. Experimental Setup. Baselines and Comparisons. Therefore, we compare with linear SVM-based classifiers, trained with the same feature set of content features or discourse features. For discourse relation prediction, we use one-vsrest strategy to build multiple binary classifiers. We display results of our models trained with gold-standard discourse relation labels and with latent discourse relations. For the later, we also show results based on True Attachment Structure, where the gold-standard attachments are known, and without the True Attachment Structure. Best result for each column is in bold. Here we present the experimental results on phrase-based content selection and discourse relation prediction. Remember that we have gold-standard argument diagrams on the AMISUB dataset, we can thus conduct experiments by assuming the True Attachment Structure is given for latent versions. When argument diagrams are not available, we build a tree among the turns in each discussion as follows. Two turns are attached if there is any adjacency pair between them. If one turn is attached to more than one previous turns, the closest one is considered. For the rest of the turns, they are attached to the preceding turn. We also investigate whether joint learning and joint inference can produce better prediction performance. We consider joint learning with separate inference, where only content features or discourse features are used for prediction. We further study learning separate classifiers for content selection and discourse relations without joint features (Separate-Learn). This indicates that leveraging the interplay between content and discourse boost the prediction performance. We further evaluate whether the prediction of the content selection component can be used for summarizing the key points on discussion level. For each discussion, salient phrases identified by our model are concatenated in sequence for use as the summary. We consider two types of gold-standard summaries. One is utterance-level extractive summary, which consists of human labeled summaryworthy utterances. The other is abstractive summary, where we collect human abstract with at least one link from summary-worthy utterances. We also compare with summaries consisting of salient phrases predicted by an SVM classifier trained with on our content features. Utterancelevel extract-based baselines unavoidably contain disfluency and unnecessary details. Our phrasebased extractive summary is able to capture the key points from both the argumentation process and important outcomes of the conversation. Features Analysis. We first discuss salient features with top weights learned by our joint model. For content features, main speaker tends to utter more salient content. Higher TF-IDF scores also indicate important phrases. For discourse features, structure features matter the most. For instance, jointly modeling the discourse relation of the parent node along with the current node can lead to better inference. An example is that giving more details on the proposal (ELABORATION) tends to lead to POSITIVE feedback. We also find that main speaker information composite with ELABORATION and UNCERTAIN are associated with high weights. Error Analysis and Potential Directions. Taking a closer look at our prediction results, one major source of incorrect prediction for phrase selection is based on the fact that similar concepts might be expressed in different ways, and our model predicts inconsistently for different variations. However, our model does not group them into the same cluster and later makes different predictions. Furthermore, identifying discourse relations in dialogues is still a challenging task. Otherwise, it can be labeled as NEGATIVE. Therefore, models that better handle semantics and context need to be considered. If all decision points are consistent, the associated topic discussion is labeled as consistent; otherwise, the discussion is identified as inconsistent. Their annotation covers the AMI-SUB dataset. Furthermore, we consider discourse relations of length one and two from the discourse structure tree. The content words in the salient phrases predicted by our model is considered for entrainment computation. Results. Leave-one-out is used for experiments. For training, our features are constructed from gold-standard phrase and discourse labels. Predicted labels by our model is used for constructing features during testing. SVM-based classifier is used. A majority class baseline and a random baseline are constructed as well. We also consider an SVM classifier trained with ngram features (unigrams and bigrams). All SVMs trained with our features surpass the ngrams-based baseline. We presented a joint model for performing phraselevel content selection and discourse relation prediction in spoken meetings. Experimental results on AMI and ICSI meeting corpora showed that our model can outperform SVM-based classifiers trained with the same feature sets."
562,3,4.67,3.67,2.67,False,acl_2017,train,"We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn high-quality relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relations that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relations with high accuracy, and that zero-shot generalization to unseen relations is possible, at lower accuracy levels, setting the bar for future work on this task.","However, these approaches are incapable of extracting relation types that were not specified in advance and observed during training. In this paper, we propose an alternative approach for relation extraction, which can potentially extract relations of new types that were neither specified nor observed a priori. Relation Question Template educated at(x, y) Where did x graduate from? In which university did x study? occupation(x, y) What did x do for a living? What is the profession of x? Who did x marry? Who is x married to? We show that it is possible to reduce relation extraction to the problem of answering simple reading comprehension questions. We map each relation R(x, y) to at least one parametrized naturallanguage question qx whose answer is y. This reduction enables new ways of framing the learning problem. More specifically, the zero-shot scenario assumes access to labeled data for N relations. This data is used to train a reading comprehension model through our reduction. Assuming a good reading comprehension model has been learned, the correct values should be extracted. We introduce a crowdsourcing approach for gathering and verifying the questions for each relation. Because questions are paired with relations, not specific examples, this overall procedure has very modest costs. The key modeling challenge is that most existing reading comprehension problem formulations assume the answer to the question is always present in the given text. However, for relation extraction, this premise does not hold, and the model needs to reliably determine when a question is not answerable. This modeling approach is another key advantage of our reduction: as machine reading models improve with time, so should our ability to extract relations. Our analysis suggests that our model is able to generalize to these cases by learning typing information that occurs across many relations (e.g. We also find that there are many feasible cases that our model does not quite master, providing an interesting challenge for future work. We observe that given a natural-language question q that expresses R(e,?) e.g. The challenge now becomes one of querification: translating R(e,?) into q. Rather than querify R(e,?) e.g. occupation(x,?)) This process, schema querification, is by an order of magnitude more efficient than instance querification because annotating a relation type automatically annotates all of its instances. Applying schema querification to N relations from a pre-existing relation-extraction dataset converts it into a reading-comprehension dataset. We then use this dataset to train a readingcomprehension model, which given a sentence s and a question q returns a set of text spans A within s that answer q (to the best of its ability). Ultimately, all we need to do for a new relation is define our information need in the form of a question. Steve Jobs was an American businessman, inventor,and industrial designer. spouse Who is Angela Merkel married to? Each instance contains a relation R, a question q, a sentence s, and an answer set A. component in their programs; no linguistic knowledge or pre-defined schema is needed. To implement our approach, we require two components: training data and a reading-comprehension model. Slot-filling examples are similar to reading-comprehension examples, but contain a knowledge-base query R(e,?) spouse(Angela Merkel,?) We collect many slot-filling examples via distant supervision, and then convert their queries into natural language. Each instance in this dataset contains a relation R, an entity e, a document D, and an answer a. We used distant supervision to select the specific sentences in which each R(e, a) manifests. Specifically, we took the first sentence s in D to contain both e and a. We then grouped instances by R, e, and s to merge all the answers for R(e,?) The annotator must then come up with a question about x whose answer, given each sentence s, is the underlined span within that sentence. We ran one instance of this annotation phase where the workers were also given, in addition to the example set, the name of the relation (e.g. country), and another instance where it was hidden. relation. The annotator is required to ask a question about x whose answer is, for each sentence, the underlined spans. We also assert that the sentence does not contain the answer to q. In SQuAD, every question is answerable from the text, which is why these models assume that there exists a correct answer span. Therefore, we modify an existing model in a way that allows it to decide whether an answer exists. We first give a high-level description of the original model, and then describe our modification. The outputs of the BiDAF model are the confidence scores of ystart and yend, for each potential start and end. Assuming the answer exists, we can transform these confidence scores into pseudo-probability distributions pstart,pend via softmax. To allow the model to signal that there is no answer, we concatenate a trainable bias b to the end of both confidences score vectors zstart, zend. Conceptually, adding the bias enables the model to be sensitive to the absolute values of the raw confidence scores zstart, zend. We are essentially setting and learning a threshold b that decides whether the model is sufficiently confident of the best candidate answer span. Recall is the true positive count divided by the number of instances that have an answer. We show that our reading-comprehension approach works well in a typical relation-extraction setting by testing it on unseen entities and texts. For example, examples about Alan Turing appeared only in the training set, while examples regarding Steve Jobs were exclusive to the test set. This partition also ensures that the sentences at test time are different from those in train, since the sentences are gathered from the Wikipedia article of each entity. This result suggests that the model does indeed generalize to unseen entities. This result demonstrates that reducing relation extraction to reading comprehension is indeed a viable approach for our Wikipedia slot-filling task. Comparison System To compare this scenario with one where the question templates were previously observed, we replicated the existing test sets and replaced the unseen question templates with templates from the training set. Both results use the same trained model, but Seen Templates tests on examples with templates that appeared in the training set, while Unseen Templates tests on examples with unobserved templates. Finally, we test our approach in a true zero-shot setting, where the test-time question templates are not only unobserved during training, they describe new, unseen relations. For example, when the examples of educated at are allocated to the test set, none of them appear in the training set. Comparison Systems We compare our querification approach to two alternatives. First, we consider using formal knowledge-base relations instead of questions; these are essentially indicators (e.g. Unlike the previous baseline, this method allows our machine reading system to interpret its lexical components via word and character embeddings. We also consider a weakened version of our approach where, during training, only one question template per relation is observed. Question Ensemble We also evaluated how asking (at test time) about the same relation in multiple ways improves performance. This scenario may be more realistic in actual applications, where extractor authors could easily provide a few question paraphrases to define their target relation. We then chose the answer with the highest sum of confidence scores. As expected, knowledge-base relations are insufficient in a zero-shot setting, and must be interpreted as natural-language to allow for some generalization. However, training on a variety of question templates rather than on a single expression substantially increases performance. Type Which airport is most closely associated with Royal Jordanian?Royal Jordanian Airlines......from its main base at Queen Alia International Airport... There is also an advantage to having multiple questions at test time; an ensemble of multiple questions per instance allows for a slight but significant improvement in both precision and recall. It appears that while most of the negative examples are easy, a significant portion of them are not trivial. lexical variability). We name these categories verbatim, global, and specific, respectively. We selected the most important cue for solving each instance. If there were two important cues, each one was counted as half. Half of the cues are relation-specific, whereas global cues account for one third of the cases and verbatim cues for one sixth. This is an encouraging result, because we can potentially learn to accurately recognize verbatim and global cues from other relations. However, our method was only able to exploit these cues partially. For examples that rely on typing information, the trend is much clearer; our model is much better at detecting global type cues than specific ones. Zero-Shot Relation Extraction There has been relatively little work on zero-shot learning in information extraction. In their setting, all natural-language relations are observed at train time, while all knowledge-base relations are hidden until test time. In their setting, a few seed trigger words are given at test time, and by using a collection of lexical inference and similarity features (tuned by different event types during training), their system is able to detect the desired event-type. We focus instead on the complementary challenge of slot filling, where natural-language questions align perhaps more directly with the values we hope to recover. However, such systems often treat every possible string as a new relation while we hope to, through the reduction to reading comprehension, extract a canonical slot value independent of how the original text is phrased. A typical example involves a text, a question (or cloze), and assumes that the answer is mentioned in the text. In comparison, our questions are relatively direct and easy, but we must also predict when the question is not answerable, to support relation extraction. Querification Some recent question-answering datasets were collected by expressing knowledgebase assertions in natural language. In these efforts, the costs scale linearly in the number of instances, requiring significant investments for large datasets. We showed that relation extraction can be reduced to a reading comprehension problem, allowing us to generalize to unseen relations that are defined on-the-fly in natural language. However, the problem of zero-shot relation extraction is far from solved, and poses an interesting challenge to both the information extraction and machine reading communities. As research into machine reading progresses, we may find that more tasks can benefit from a similar approach. We hope the contributions and insights presented in this paper will support future work in this avenue."
108,2,3.0,3.5,2.5,False,acl_2017,train,"In this paper, we propose a new model that is capable of recognizing overlapping entities based on multigraphs, as opposed to simple graphs commonly used in graphical models for structured prediction. Through extensive experiments on standard datasets containing overlapping and non-overlapping entities, we demonstrate that our model outperforms previous models. We also present some analysis on the differences between our model and the previous models and discuss the possible implications of the differences. To the best of our knowledge, this is the first structured prediction model utilizing multigraphs to predict overlapping structures.","For example, the location entity China appears within the organization entity Bank of China. We show it is possible to assign explicit semantics to different edges connecting the same pair of nodes when representing structures, leading to the novel multigraph representations that can be used to represent overlapping structures. We present the training and inference procedures over such a novel representation. To the best of our knowledge, this is the first structured prediction model utilizing multigraphs to predict overlapping structures. On the other hand, it still maintains the same inference time complexity as the previous model. We also believe our proposed multigraph-based structured prediction framework can be used to solve other problems involving overlapping structures, and we hope this work can inspire further research along such a direction. We will make our system and code available for research purposes. Named entity recognition (NER) has been a research focus for quite some time. They represented entities as top-k predictions with positive score from a structured multilabel classification model. For each entity type, they used a standard linear-chain CRF model to predict entities, resulting in the time complexity of roughly O(nT) depending on how the pipeline is designed. Their model only handles overlapping entities of different types. They showed that their model is able to outperform a semi-CRF baseline. By design, their model cannot handle crossing entities (overlapping entities which are not nested). The model was shown to achieve competitive results compared to previous models on standard datasets. As we will be making extensive comparisons against this pervious state-of-the-art model, we will briefly discuss this approach in the next section. Bottom) The corresponding mention hypergraph (A-and E-nodes not shown).   T k T through a hyperedge, denoting the fact that the entities that start at k must be one of the T types. Each Tkt is connected to Ikt through an edge (denoting there is an entity of type t that starts at the k-th token) and to X through another edge (denoting there are no entities of type t that start at the k-th token). In this mention hypergraph, each possible entity is represented as a path from a T-node to the X-node through a sequence of I-nodes (each denoting the words which are part of the entity), and the set of all entities present in a sentence forms a sub-hypergraph of the overall hypergraph. We now describe our proposed multigraph-based model for recognizing overlapping entities. model, we assign for each token two states for each entity type, representing whether the token is part of an entity of a certain type. Unlike previous approaches, we assign explicit semantics to the edges in our multigraph-based model, which we call entity separators. An entity is continuing to the next token (C) For each token, the possible combinations of cases are as follows: ECS, EC, CS, C, ES, E, S, and X, where Xmeans none of the three cases applies. For example, the separator EC means there is one entity ending at the current token and another entity (overlapping) continuing to the next token. our model. The first I-and O-nodes in the sentence are connected to the T-node of the corresponding entity type, and the last I-and O-nodes are connected to the unique leaf node X. Finally, a designated root node R is added and is connected to all T-nodes by a single hyperedge to complete the multigraph structure. Note that the edges in our multigraph representations are directed, with nodes on the left serving as parents to the nodes on the right. Such directed edges will be helpful when performing inference, to be discussed in the next section. Note how each edge maps to a distinct entity separator visualized in the text. We note the mention hypergraph model also defines the objective in a similar manner. For both models, the inference is done based on a generalized inside-outside algorithm. Both models involve directed structures, on top of which the inference algorithm first calculates the inside score for each node from the leaf node to root, and then the outside score from the root to the leaf node, in very much the same way as how inference is done in a classic graphical model. Specifically, for our multigraph-based model, the inside scores are calculated using a bottom-up (right-to-left) dynamic programming procedure, where we calculate the inside score at each node by summing up the scores associated with each path connecting the current node to one of its child nodes. Each such path score is in turn defined as the product of the inside score stored in that child node and the score defined over the edge connecting them. The computation of the outside scores can be done in an analogous manner from left to right. It can be verified that the time complexity of such an inference procedure for our model is O(nT), which is the same as the mention hypergraph model. Based on how previous works model the role of each token in defining the predicted entity spans, we can classify the previous works into two paradigms, namely state-based paradigm and edge-based paradigm, which we detail below. We implemented the same interpretation process as that was done in the mention hypergraph model, and we resolved ambiguous structures by considering them as nested entities instead of crossing entities. and edges are used together to encode structured output information. In general, given an input sentence, a possible output can be represented using a linear-chain structure that involves states (nodes) and their connections (edges). One could observe that, in this case, given the state at each position in the structure, we already have enough information to interpret the output, without relying on the edge information. Similarly, this is also true for tree-based models. Edge-based Paradigm Modeling the roles each token can take as states is, however, not the only possible paradigm. Their approach does not model the roles that each token can take directly using the states only. In fact, in addition to states, they used edges (or hyperedges, and their combinations) to capture the complex combination of roles each token can take. This leads to an alternative paradigm called edge-based paradigm: the approach where the set of states together with the edges are used to define the predicted entities. The left graph and the right graph have exactly the same states. However, due to different edges (or hyperedges), these two graphs correspond to different entity combinations. It is easy to see that our multigraph-based model falls into this paradigm. Since there are multiple edges between the same pair of states, the combination of states alone do not define the roles of the tokens, and semantics are clearly assigned to the edges. One might notice that there exists some equivalence between the two paradigms. Alternative approaches such as using nodes to replace the edges such as ES in our representation between adjacent I nodes are also possible. However, such approaches essentially exploit ad-hoc representations derived from the multigraph representation, making the model significantly less intuitive and less concise. Since the edge-based paradigm is relatively unexplored compared to the state-based paradigm, we hope that this discussion can ignite further research in this direction. In previous section we see that our model falls into the same edge-based paradigm as the mention hypergraph model. One might notice that our model is similar to the mention hypergraph model, in the sense that the edges in our model represent combinations of multiple edges in the mention hypergraph model. sentences containing overlapping entities; o.l. fined using features over the edges, this raises the question: are the two models actually optimizing the same objective function? To answer this question, let us look into the normalization term Zw(x) calculated by each model using the inside-outside algorithm. Also note that the score of each of these distinct interpretation of the same sub-hypergraph might differ from each other. Refer to the supplementary material for more details. that contains spurious structures a deficient model. This can be verified by the fact that the inside score of each node in our multigraph is calculated as the sum of the scores associated with each path following that node. We can see overlapping entities are common in such datasets. For both models (mention hypergraph and our model) that fall under the edge-based paradigm, we define features over the edges in the models. In our model, when an edge represents a combination of roles, a feature is defined for each output feature. This allows us to use to make use of the identical set of features for both our multigraph model and the baseline mention hypergraph model, in order to make a proper comparison. In general, they include surrounding words, surrounding POS tags, bag-of-words, Brown clusters (for GENIA only), and orthographic features. See the supplementary material for more details on the features. For the ACE datasets, we make comparisons with the linear-chain CRF baseline (Lin-CRF), which does not support overlapping entities, as well as our implementation of the mention hypergraph baseline (MH). Best results are highlighted in bold. As expected, the linear-chain CRF baseline yields relatively lower results compared to the other models, since it cannot predict overlapping entities. However, such results give us some idea on how much performance increase we can gain by properly recognizing overlapping entities. As these datasets consist of both overlapping and non-overlapping entities, to further understand the effectiveness of each model in recognizing overlapping entities (and non-overlapping entities), we performed some additional experiments. When given sufficient training data on overlapping entities, which is the case for ACE, our model is able to better recognize overlapping entities. Such results also lead to the interesting empirical finding that our model appears to be able to do well on the task of recognizing non-overlapping entities. Such results also suggest that modeling the interactions between distinct entity types may not be crucial for achieving a good performance in entity recognition. By examining the outputs, we found that although the training set is not annotated with overlapping entities, our model (and mention hypergraph model) will be able to predict overlapping entities when it is confident, leading to improved performance. tion hypergraph model, we note that our model consistently yields a higher recall. We also empirically analyzed the training speed and convergence properties of the two models. We presented a novel multigraph-based model for entity recognition from text where entities may overlap with one another. We showed that empirically our model is able to yield better recognition results compared to previous models. We also performed theoretical analysis on the model and showed that our model resolves the spurious structures issue associated with a previous stateof-the-art model, while still maintaining the same inference time complexity. This section shows in more details how this is the case using some examples. Further assume that features are only defined on these labeled edges. The other combinations represent entity combinations which, while valid, are not represented in the model as such, and so never appear as numerator in the likelihood. This has the advantage of simplifying the tokenization procedure, although it makes the task slightly more difficult due to the higher number of tokens.    "
333,3,4.27,4.0,4.0,True,acl_2017,train,"We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with recurrent neural networks. The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, DUC 2004 and MSR abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-ofthe-art baseline models.","This is different from document level summarization task since it is hard to apply existing techniques in extractive methods, such as extracting sentence level features and ranking sentences. We focus on abstractive sentence summarization task in this paper. Recently, neural network models have been applied in this task. They use a Convolutional Neural Network (CNN) encoder and feed-forward neural network language model decoder for this task. All the above works fall into the encodingdecoding paradigm, which first encodes the input sentence to an abstract representation and then decodes the intended output sentence based on the encoded information. This approach achieves huge success in tasks like neural machine translation, where alignment between all parts of the input and output are required. However, in abstractive sentence summarization, there is no explicit alignment relationship between input sentence and the summary except for the extracted common words. A desired work-flow for abstractive sentence summarization is encoding, selection, and decoding. After selecting the important information from an encoded sentence, the decoder produces the output summary using the selected information. Although this is implicitly modeled in the encoding-decoding framework, we argue that abstractive sentence summarization shall benefit from explicitly modeling this selection process. In this paper we propose Selective Encoding for Abstractive Sentence Summarization (SEASS). We treat the sentence summarization as a threephase task: encoding, selection, and decoding. It consists of a sentence encoder, a selective gate network, and a summary decoder. First, the sentence encoder reads the input words through an RNN unit to construct the first level sentence representation. Then the selective gate network selects the encoded information to construct the second level sentence representation. The selective mechanism controls the information flow from encoder to decoder by applying a gate network according to the sentence information, which helps improve encoding effectiveness and release the burden of the decoder. Finally, the attention-equipped decoder generates the summary using the second level sentence representation.     We focus on abstracive sentence summarization task in this paper.     hn). Then the selective gate selects and filters the word representations according to the sentence meaning representation to produce a tailored sentence word representation for abstractive sentence summarization task. Lastly, the GRU decoder produces the output summary with attention to the tailored representation. In the following sections, we introduce the sentence encoder, the selective mechanism, and the summary decoder respectively. The role of the sentence encoder is to read the input sentence and construct the basic sentence representation.     In the sequence-to-sequence machine translation (MT) model, the encoder and decoder are responsible for encoding input sentence information and decoding the sentence representation to generate an output sentence. Some previous works apply this framework to summarization generation tasks. However, abstractive sentence summarization is different from MT in two ways. First, there is no explicit alignment relationship between the input sentence and the output summary except for the common words. Second, summarization task needs to keep the highlights and remove the unnecessary information, while MT needs to keep all information literally. Herein, we propose a selective mechanism to model the selection process for abstractive sentence summarization. The selective mechanism extends the sequence-to-sequence model by constructing a tailored representation for abstractive sentence summarization task. The sentence vector s is used to represent the meaning of the sentence.   This new sequence is then used as the input sentence representation for the decoder to generate the summary. On top of the sentence encoder and the selective gate network, we use GRU with attention as the decoder to produce the output summary.   Our goal is to maximize the output summary probability given the input sentence. In this section we introduce the dataset we use, the evaluation metric, the implementation details, the baselines we compare to, and the performance of our system. The parallel corpus is produced by pairing the first sentence and the headline in the news article with some heuristic rules. Since the training set is too small, we only use the test set as one of our test sets. We denote this dataset as MSR-ATC (Microsoft Research Abstractive Text Compression) test set in the following. ROUGE measures the quality of summary by computing overlapping lexical units, such as unigram, bigram, trigram, and longest common subsequence (LCS). It becomes the standard evaluation metric for DUC shared tasks and popular for summarization evaluation. The training is separated into two phases, the first phase is optimizing the loss function with Adam and the second is with simple SGD. Beam Search We use beam search to generate multiple summary candidates to get better results. To avoid favoring shorter outputs, we average the ranking score along the beam path by dividing it by the number of generated words. RG in the Table denotes ROUGE. Our SEASS model with beam search outperforms all baseline models by a large margin. To the best of our knowledge, this is the first work that reports ROUGE metric scores on the MSR-ATC dataset. Note that we only compare our model with ABS since the others are not publicly available. Beam search are used in both the baselines and our method. We then analyze selective encoding by visualizing the heat map. Overall, these improvements on all groups indicate that the selective encoding method benefits the abstractive sentence summarization task. Saliency Heat Map of Selective Gate Since the output of the selective gate network is a high dimension vector, it is hard to visualize all the gate values. Given sentence words x with associated output summary y, the trained model associates the pair (x, y) with a score Sy(x). The goal is to decide which gate g associated with x makes the most significant contribution to Sy(x). We can observe that the selective gate determines the importance of each word before decoder, which releases the burden of it by providing tailored sentence encoding. Abstractive sentence summarization, also known as sentence compression and similar to headline generation, is used to help compress or fuse the selected sentences in extractive document summarization systems since they may inadvertently include unnecessary information. Besides, they enrich the encoder with lexical and statistic features which play important roles in traditional feature based summarization systems, such as NER and POS tags, to improve performance. The model introduces a latent segmentation which determines correspondences between tokens of the input sequence and the output sequence. This paper proposes a selective encoding model which extends the sequence-to-sequence model for abstractive sentence summarization task. With the proposed selective mechanism, we build an end-to-end neural network summarization model which consists of three phases: encoding, selection, and decoding."
276,2,4.5,2.5,3.5,True,acl_2017,train,"We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the framework to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on 8 datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data.","Specialised approaches to sequence labeling often include extensive feature engineering, such as integrated gazetteers, capitalisation features, morphological information and POS tags. This feature discovery is usually driven by an objective function based on predicting the annotated labels for each word, without much incentive to learn more general language features from the available text. In many sequence labeling tasks, the relevant labels in the dataset are very sparse and most of the words contribute very little to the training process. The sequence labeling models are able to learn this bias in the label distribution without obtaining much additional information from words that have the majority label (O for outside of an entity; C for correct word). Therefore, we propose an additional training objective which allows the models to make more extensive use of the available data. Compared to any sequence labeling dataset, the task of language modeling has a considerably larger and more varied set of possible options to predict, making better use of each available word and encouraging the model to learn more general language features for accurate composition. In this paper, we propose a neural sequence labeling architecture that is also optimised as a language model, predicting surrounding words in the dataset in addition to assigning labels to each token. This secondary unsupervised objective encourages the framework to learn richer features for semantic composition without requiring additional training data. Our experiments show that by including the unsupervised objective into the training process, the sequence labeling model achieves consistent performance improvements on all the benchmarks. This multitask training framework gives the largest improvements on error detection datasets, where the system also achieves new state-of-the-art results. The model takes as input one sentence, separated into tokens, and assigns a label to every token using a bidirectional LSTM. Each LSTM takes as input the hidden state from the previous time step, along with the word embedding from the current step, and outputs a new hidden state. In order to predict a label for each token, we use either a softmax or CRF output architecture. The individual characters of a word are mapped to character embeddings and passed through a bidirectional LSTM. The input tokens are shown at the bottom, the expected output labels are at the top. Arrows above variables indicate the directionality of the component (forward or backward). nonlinear layer. The resulting vector representation is combined with a regular word embedding using a dynamic weighting mechanism that adaptively controls the balance between word-level and character-level features. This framework allows the model to learn character-based patterns and handle previously unseen words, while still taking full advantage of the word embeddings. While each token in the input does have a desired label, many of these contribute very little to the training process. The sequence labeling models are able to learn this bias in the label distribution without obtaining much additional information from the majority labels. Therefore, we propose an additional objective which would allow the models to make full use of the training data. In addition to learning to predict labels for each word, we propose optimising specific sections of the architecture as language models. The task of predicting the next word will require the model to learn more general patterns of semantic and syntactic composition, which can then be reused in order to predict individual labels more accurately. This objective is also generalisable to any sequence labeling task and dataset, as it requires no additional annotated training data. A straightforward modification of the sequence labeling model would add a second parallel output layer for each token, optimising it to predict the next word. However, the model has access to the full context on each side of the target token, and predicting information that is already explicit in the input would not incentivise the model to learn about composition and semantics. Therefore, we must design the loss objective so that only sections of the model that have not yet observed the next word are optimised to perform the prediction. This architecture avoids the problem of giving the correct answer as an input to the language modeling component, while the full framework is still optimised to predict labels based on the whole sentence. This separate transformation learns to extract features that are specific to language modeling, while the LSTM is optimised for both objectives. At each token position, the network is optimised to predict the previous word, the current label, and the next word in the sequence. The added language modeling objective encourages the system to learn richer feature representations that are then reused for sequence labeling. Performance on the development set was also used to select the best model, which was then evaluated on the test set. Dropout and LMcost modifications are added incrementally to the baseline. on each of these datasets. Therefore, we use a softmax output for error detection experiments and CRF on all other datasets. Since context prediction is not part of the main evaluation of sequence labeling systems, we expected the additional objective to mostly benefit early stages of training, whereas the model would later need to specialise only towards assigning labels. The texts were written by learners during language examinations in response to prompts eliciting freetext answers and assessing mastery of the upperintermediate proficiency level. They have been manually corrected by two separate annotators, and we report results on each of these annotations. The baseline results are comparable to the previous best results on each of these benchmarks. However, we did verify that dropout indeed gives an improvement in combination with the novel language modeling objective. Because the model is receiving additional information at every token, dropout is no longer obscuring the limited training data but instead helps with generalisation. The bottom row shows the performance of the language modeling objective when added on top of the baseline model, along with dropout on word embeddings. This task has very sparse labels in the datasets, with error tokens very infrequent and far apart. Without the language modeling objective, the network has very little use for all the available words that contain no errors. There are only two possible labels, correct and incorrect, which likely makes it more difficult for the model to learn feature detectors for many different error types. Language modeling uses a much larger number of possible labels, giving a more varied training signal. Finally, the task of error detection is directly related to language modeling. By learning a better model of the overall text in the training corpus, the system can more easily detect any irregularities. We also analysed the performance of the different architectures during training. The baseline model peaks quickly, followed by a gradual drop in performance, which is likely due to overfitting on the available data. Dropout provides an effective regularisation method, slowing down the initial performance but preventing the model from overfitting. In the next experiments we evaluate the language modeling objective on named entity recognition and chunking. Compared to error detection corpora, the labels are more balanced in these datasets. The addition of the language modeling objective consistently further improves performance on all benchmarks. While these results are comparable to the respective state-of-the-art results on most datasets, we did not fine-tune hyperparameters for any specific task, instead providing a controlled analysis of the language modeling objective in different settings. Without dropout, performance peaks quickly and then trails off as the system overfits on the training set. Using dropout, the best performance is sustained throughout training and even slightly improved. Finally, adding the language modeling objective on top of dropout allows the system to consistently outperform the other architectures. We also evaluated the language modeling training objective on two POS-tagging datasets. The baseline performance on these datasets is also close to the upper bound, therefore we expect the language modeling objective to not provide much additional benefit. While the performance improvements are small, they are consistent across both domains and datasets. Application of dropout again provides a more robust model, and the language modeling cost improves the performance further. Even though the labels already offer a varied training objective, learning to predict the surrounding words at the same time has provided the model with additional general-purpose features. Our work builds on previous research exploring multi-task learning in the context of different sequence labeling tasks. ing tasks using neural networks. While they use a regular recurrent architecture, we propose a language modeling objective that can be integrated with a bidirectional network, making it applicable to existing state-of-the-art sequence labeling frameworks. While predicting word frequency is useful for POS-tagging, language modeling provides a more general training signal, allowing us to apply the model to many different sequence labeling tasks. One half of a bidirectional LSTM is trained as a forward-moving language model, whereas the other half is trained as a backward-moving language model. At the same time, both of these are also combined, in order to predict the most probable label for each word. This modification can be applied to several common sequence labeling architectures and requires no additional annotated or unannotated data. The objective of learning to predict surrounding words provides an additional source of information during training. The model is incentivised to discover useful features in order to learn the language distribution and composition patterns in the training data. While language modeling is not the main goal of the system, this additional training objective leads to more accurate sequence labeling models on several different tasks. We found that the additional language modeling objective provided consistent performance improvements on every benchmark. The largest benefit from the new architecture was observed on the task of error detection in learner writing. The label distribution in the original dataset is very sparse and unbalanced, making it a difficult task for the model to learn. The language modeling objective also provided consistent improvements on other sequence labeling tasks, such as named entity recognition, chunking and POS-tagging. Future work could investigate the extension of this architecture to additional unannotated resources. Learning generalisable language features from large amounts of unlabeled in-domain text could provide sequence labeling models with additional benefit. While it is common to pre-train word embeddings on large-scale unannotated corpora, only minimal work has looked at useful methods for pre-training or co-training more advanced compositional modules."
775,2,4.0,3.0,3.0,False,acl_2017,train,"We present a method for metaphor identification based on an explicitly geometric approach to modelling salient lexical associations, and test it on a task of distinguishing metaphoric from literal uses of adjective-noun phrases. By contextually projecting candidate word pairs into interpretably geometric spaces, we show that it obtains state-of-the-art performance, while providing a method which is effectively zero-shot – able to be applied to new unseen phrases. Our dynamically context-sensitive model is inspired by theoretical insight into the situational nature of language and cognition.","The result has been a rich literature of both symbolic and statistical approaches to metaphor identification, interpretation, and even generation. Here, we also take a distributional semantic approach, but predicated on a slightly different theoretical premise: we hold that metaphors are linguistic phenomena that emerge dynamically in the course of language use, as a semantic phenomenon at the end of a fluid spectrum, related to the ad hoc, situated nature of concept formation. and superior to their semi-supervised method. Lakoff and Johnson, on the other hand, in work that has been much discussed by cognitive linguists and cognitive scientists, describe metaphor in terms of isomorphic mappings between broad conceptual domains, with the anatomy of these often culturally specific domains coercing the nuances of particular metaphors. NLP researchers have generally interpreted this theoretical position to indicate that concepts are stable categories essentially denoted by linguistic symbols, and that communication is more or less about the efficacious application of these symbols in trafficking propositions about things in the world. They test two versions of this approach. For each adjective a, they learn two separate tensors AM(a), AL(a) for literal and metaphoric senses, learned from labelled an pairings. This version gives good performance (details below), but has the disadvantage of requiring many labelled examples of phrases an. This reduces performance, but does not require labelled examples for every adjective a. Divergent views Our own approach, while it is situated within the distributional semantic paradigm, is inspired by theoretical considerations which diverge somewhat from the doctrinaire stance on conceptual metaphors and selectional violation. We seek to redress the evident schism between pragmatic and computational accounts of metaphor by taking into account the significance of context in the online construction of situated conceptualisations. The contextualisation should generate a space which is geometrically interpretable. This matrix can now serve as a base space which can be contextualised by selecting relevant lower-dimensional subspaces based on an analysis of input words. We expect the combination of these properties to be informative about the conceptual relationship denoted by the phrase in question; and therefore predict that the geometric relationships of the words within this subspace will be indicative of the likelihood of an assessment of metaphoric versus literal usage. With our base space established, the primary parameter of our experiment will therefore be the criteria for selecting a subspace based on any given word pair. The points A and B correspond to the word-vectors for the adjective and the noun in a potentially metaphoric adjective-noun phrase. The points of the triangle CDE sit on the surface of a hypersphere emanating from the origin of the subspace, with C and D corresponding to the vectors of A and B normalised to length r respectively. We predict that ratings of metaphoricity will correlate with features of the geometry of these triangulations in subspaces. In particular, we predict that, in the case of spaces generated from adjective-noun pairs that would be considered typically metaphoric, sides AB and CD will tend to increase in length, with the points A and B tending to diverge on a dimension-by-dimension basis as the contextual subspace selected by the corresponding input terms and word-vectors becomes less consistent. We likewise expect points A and B to move closer to the origin in the cases of jointly selected subspaces, or, in the cases of the singly selected subspaces, the point corresponding to the word-vector not involved in the selection will recede. terms as the conceptualisation suggested by their composition moves towards what is perceived as a more metaphoric interpretation of the language. its basis dimensions are the words with contextually salient associations). Note that while we train the regression model using labelled instances, our input features are only information about the individual words-there is no requirement for, or learning of, phrase representations (thus allowing application to unseen phrases). As our method uses no information about the whole phrase occurrence, and is thus suited for truly unseen cases, a more relevant comparison is the second version. The mechanisms for achieving these results, however, are evidently somewhat different. This finding could motivate an exploration of spaces employing broader cooccurrence windows, not to mention spaces spaces populated by word-vectors consisting of more sequentially and syntactically nuanced features. Next, clearly, and not surprisingly, the spaces that take information from both terms involved in the candidate phrase perform better than those that consider only one of the two words, and it must be noted that the best-performing joint and zipped subspaces will converge as the dimensionality parameter increases, with the subspace projected eventually including all the dimensions with non-zero values for both input words in all three cases, while the independent subspaces will eventually contain all dimensions with non-zero values for either input word. We interpret this as indicating that the dimensions of a subspace most predictive of metaphor contain some combination of information about the terms involved in the can-didate phrase that is neither maximally salient to both (or all) terms nor particularly skewed towards one term. Again, a more nuanced approach to subspace projection is probably available and warrants further research. Notably, the length of side BO, corresponding to the norm of the noun word-vector, is by far the feature most predictive of metaphor. That the norm of the adjective word-vector (side AO) is not nearly as predictive is to be expected given the nature of the dataset: as the data was constructed specifically to test adjectives that tend to occur in both metaphoric and literal adjective-noun compositions, each adjective contributes to the construction of both metaphoric and literal subspaces. As can be seen, there is an lengthening of the distance between adjective and noun moving from the metaphoric to the neutral example, and then an extension of the disance of both points from the origin moving from the neutral to the literal instance. We were admittedly initially surprised by this evident bulging at the middle of the metaphoricliteral spectrum; we had predicted a steady widening and corresponding move towards the periphery of subspaces as word pairs move towards the more metaphoric. The subspaces selected based only on an analysis of either noun or adjective vectors tend to be characterised by significant difference between norms of each projected vector, so this analysis is less meaningful there.) Significantly, the trend is evident across all three multi-word input subspaces, with especially pronounced differences in the joint type subspaces. It would seem that the move from metaphoric to literal is in fact characterised by a two part expansion, beginning with a widening of the space between the input terms as they become more neutrally (or perhaps ambiguously) interpreted and then an extension away from the origin as they become more literally interpreted. What seems to be happening here is actually an increase in the dimensions available to the model as the terms move towards the neutral, but at the same time a diminishing of the correspondence between the input words and so a widening of the space between them on a dimension-by-dimension basis. The end result is actually a kind of horseshoe phenomenon, with both literal and metaphoric compositions tending towards one another in the subspaces they select, but with literal compositions moving away from the origin as there tends to be more correspondence between the set of salient (ie, high valued) co-occurrence dimensions for each word. Meanwhile, what we now refer to as ambiguous rather than neutral compositions tend to select for subspaces where a wider range of dimensions have some information about both terms, but there tends to be less correspondence between each word across these dimensions. This projection preserves all lengths within the triangles, as well as the position of the smaller triangle tangentialed to a hypersphere eminating from the origin. So here at last we return to the issue of metaphor in context, and make two final comments by way of reinforcing observations which have been made frequently in the theoretical literature. The first is that metaphor is probably properly construed as being an extent of a spectrum, and a spectrum that is not clearly defined in absolute terms but rather is probably specific to a contextual expectation. The second point follows on from this: metaphor is contextual, and and given adjectivenoun phrase could probably, at a stretch, be forced into either a metaphoric or literal interpretation under the correct conditions. So, while our model provides the basic equipment for an unsupervised analysis on any given word pair, we feel it also lays the groundwork for generating more contextually nuanced spaces in which conceptual relationships map to the geometric situations of word-vectors. However, our results suggest that it might be enhanced by combining it with the insight that lexical semantics is always resolved contextually. Conceptual schemas can be recast as geometries within ad-hoc, contextualised subspaces, delineated by dimensions that are tractable, interpretable features of language use which relate to salient co-occurrence associations; this view, even simply implemented as here, gives accuracy comparable to state-of-the-art approaches while retaining the zero-shot ability to apply to truly unseen examples. The primary features of our method are its contextual dynamism and its geometry. A strength of this model is its simplicity, and likewise its availability as an interpretable framework that can be easily analysed using standard classification and regression techniques. In principle, any words in our vocabulary could be projected into a subspace, and we might expect the geometry of such subspaces to do more work in the long run, issuing features involving other semantic relationships and compositionality. Here we have attempted to consider how a computational approach to such a flexible model might work."
237,2,3.71,1.71,1.71,False,acl_2017,train,"This study implements a vector space model approach to measure the sentiment orientations of words. Two representative vectors for positive/negative polarity are constructed using high-dimensional vector space in both an unsupervised and a semisupervised manner. A sentiment orientation value per word is determined by taking the difference between the cosine distances against the two reference vectors. These two conditions (unsupervised and semi-supervised) are compared against an existing unsupervised method (Turney, 2002). As a result of our experiment, we demonstrate that this novel approach significantly outperforms the previous unsupervised approach and is more practical and data efficient as well.","A sentiment orientation value per word is determined by taking the difference between the cosine distances against the two reference vectors. As a result of our experiment, we demonstrate that this novel approach significantly outperforms the previous unsupervised approach and is more practical and data efficient as well. Previous research in sentiment analysis or opinion mining mostly focus on supervised methods, which requires labeled training data to identify properties of unseen input, and then classify input later. Probabilistic methods in particular often calculate which class a word or phrase most likely bears and then make predictions regarding the label of a given target text, using those estimations. While such methods have widely been adopted, there are few examples which measure the likelihood of lexical items in an unsupervised or semisupervised manner. However, there still exist situations where sentiment analysis should be performed using non-labeled datasets. In such cases, discovering information regarding the sentiment orientation of vocabulary in a non-supervised fashion becomes essential. Our approach employs VSM (Vector Space Models) as its main component. Syntagmatic relations are concerned with whether or not two entities are in a co-occurrence relation, and paradigmatic relations are concerned with whether the two items in question are interchangeable (substitution relation). Many collocation models using N-grams and Point-wise Mutual Information (PMI) analyze the former-type of word relations. Our work can best be understood as an exploration to find a sentiment dimension over a multidimensional vector space, which is constructed from the relations of words in a corpus. However, it is too complex to extract a specific type of relation between whole words on such a high-dimensional space. Thus, we start by selecting a small set of words that are believed to have an emotional value for a topic. Our method for choosing these point words can be divided into two sub-types: unsupervised or semi-supervised. Should the labels not be given, one alternative method would be to use the similarity between words. Following the principle of the Distributional Hypothesis, we assume that positive or negative words will tend to share similar contexts relative to their opposing stance. We argue that collocation-based methods are not a practical choice for obtaining the similarity due to data sparsity which is inherent to the model. In the unsupervised condition, a dimensionality reduction algorithm is implemented to search for sentiment dimension using the selected point words. Under the semi-supervised condition, we depend on external estimations of the terms in order to skip the exploration stage. Although the majority of previous studies on sentiment analysis have preferred to use supervised methods, some researchers have tried to develop unsupervised or semi-unsupervised approaches. In this study, the authors discuss the existence of linguistic constraints on the semantic orientations of words in conjunctions. If a given equipped corpus is not big enough for a PMI analysis, the problem of data sparseness will arise, and the PMI values become suspect. We used the same formula (Eq. Additionally, we consider another word-embedding model (GloVe). This is due to the fact that the distance between any two words in a paradigmatic relation is minimized when they share the most similar neighbors. This aspect can cause unexpected results when such a model is employed for clustering a set of items that share similar emotions, because two words in paradigmatic relations often instantiate a contrastive relation (e.g., antonym). From this perspective, we might find a specific relation to them in a subspace of the original vector space if the necessary vector calculation operations are known. He also suggests using dimensionality reduction algorithms (e.g., Multidimensional scaling) to explore the quality dimensions of multi-dimensional vector space and claims that applying these algorithms to the similarity-based vector space will generate an ordering relation for data points on an interested domain. The Word-Space model, as the name implies, models word meaning with a spatial representation. Thus, semantic similarity is represented as proximity in n-dimensional space. Our goal is to find or construct a sentiment dimension from the similarity-based vector space of words. The corpus also contains the expected polarity values for all individual tokens occurring in the reviews. This dataset is used for our vector space construction and the polarity values per word are employed under the semi-supervised condition. We select this corpus as our test dataset since it allows us to compare our sentiment orientation values with the annotated polarity value for each word. To validate this assumption, we POS-tagged the word tokens of the Stanford corpus and observe the variance of sentiment ratings of words depending on their POS-tag (Fig. Variances of polarity values per POS-tags Not surprisingly, the main tags of adjective and adverb occupy the top places in the ranking, indicating that terms bearing these tags are often used with stronger subjectivity. Based on the theory and the observation, we select the top K modifiers (adjectives or adverbs) from the extracted patterns. The number of the modifiers are automatically determined by choosing a minimum frequency for the extracted phrases. Since the distances between the tokens are measured by Cosinedistance, the distance data is Euclidean. A local structure-oriented dimensionality reduction algorithm, the Principal Component Analysis (PCA) is then used to find the sentiment dimension between the modifiers. In the analysis, we only use the value of two for the number of dimensions to project the entities into the reduced space. Based on the principle of the Distribution Hypothesis, we assume that semantically close modifiers are closer to each other than the opposites. When the dimensionality reduction phase is completed, it is possible to observe correlations between values on the found dimension and the goldstandard dataset (the annotated values of Stanford Sentiment Treebank). Since the signs of the coefficients are irrelevant for our purposes, only absolutes are considered. Now, we can determine the two sets of words distinguished by the origin of zero on the principal axis. This method does not use the dimensionality reduction algorithm to distinguish the point words into two sets, but instead employs the expected star ratings of the tokens from the IMDB dataset. The ratings represent how strongly a word belongs to positive or negative sentiment polarity. Thus, in this case, we start with almost certain information on the polarity of the words. Because adjectives and adverbs are generally used with their own static stance, we believe that the information can be applied to unseen texts over different domains. Since this semi-supervised setting is designed to compare with the unsupervised condition, the remainder of the experiment methodology is identical to the unsupervised methodology. Since the method is only applicable to a review that contains at least the one of the defined pat terns (Table. Fig. The point words are colored red (positive) or blue (negative) by the middle value on the annotation scale. The principal dimension in the embedding by PCA gives us an ordering relation between the terms. Although there undoubtedly exists noise in the results, we could still find correlations between the unsupervised ratings and the annotated values. We also note that varying the size of context window for the embedding models produces slightly different patterns. Our semi-supervised approach does not use the procedure incorporating the PCA, but the vector averaging and classification processes are identical to the unsupervised condition. Fig. The color is coded by the ratings of Stanford Sentiment Treebank (red for positive and blue for negative). Decreasing the number of the point words worsens the performance of all the semi-supervised models. The general pattern of Fig. On top of this, the semi-supervised methods show a better performance than the unsupervised settings. The core idea of our approach is to find a sentiment dimension from a high-dimensional vector space of words and use the extracted information to calculate the polarity of an individual word. We attempted to see whether or not the obtained sentiment orientations are useful by the sentiment classification task of movie reviews. Accuracy of Classifications. The x-axis indicates the cutoff frequency. We suspect that the cause of this low performance is from the common feature that the both models are based on word collocations. Even the word-embedding model (GloVe) seems unable to break free of this problem, as the models show lower results than the those using Skip-gram. This conjecture is supported by the observation that the GloVe model does not lose its correlation coefficients as the context size increases. We note that Skip-grams essentially exploit paradigmatic relations between words and produce denser vector spaces for the relation of words. Thus, methods based on dense vector modeling should be more robust for the data-sparsity problem in our task, as demonstrated by our experiment. It is worth noting that PMI-IR suffers from datadeficiency. One issue in our study is how to find optimal reference vectors to represent the sentiment polarity of a vector space. We tried to approximate by using a traditional operation (vector averaging). Note that the performance of the semi-supervised approach did not dramatically increase with the added number of the word tokens. This likely indicates that the type of vector calculation is not efficient for the purpose in this research. However, we believe that there is potential for huge improvement, as the reference vectors can be constructed in an optimal way to represent the sentiment domain between words. We predict that there could be two possible ways to achieve this goal. The first way is to select the point words that best capture the landscape of the sentiment entities in a corpus. And the second way is to exploit more relevant vector space models than the embedding methods used in this study. We leave such explorations for future work. In this study, we introduce a novel approach which implements distributional semantic models to measure the sentiment orientation of a word. We divide our experiment into two separate types (unsupervised or semi-supervised) and compare the results with a previous unsupervised approach (PMI-IR)."
561,2,4.0,3.0,3.5,True,acl_2017,train,"Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.","However, in many NLP tasks it is essential to represent not just the meaning of a word, but also the word in context. Although the token representation is initialized with pre-trained embeddings, the parameters of the bidirectional RNN are typically learned only on labeled data. In this paper, we explore an alternate semisupervised approach which does not require additional labeled data. We use a neural language model (LM) pre-trained on a large, unlabeled corpus to compute an encoding of the context at each position in the sequence (hereafter an LM embedding) and use it in the supervised sequence tagging model. Since the LM embeddings are used to compute the probability of future words in a neural LM, they are likely to encode both the semantic and syntactic roles of words in context. Our main contribution is to show that the context sensitive representation captured in the LM embeddings is useful in the supervised sequence tagging setting. As a secondary contribution, we show that using both forward and backward LM embeddings boosts performance over a forward only LM. We also demonstrate that domain specific pre-training is not necessary by applying a LM trained in the news domain to scientific papers.   To learn a context sensitive representation, we employ multiple layers of bidirectional RNNs. Finally, the output of the final RNN layer hk,L is used to predict a score for each possible tag using a single dense layer. Due to the dependencies between successive tags in our sequence labeling tasks (e.g. using the BIOES labeling scheme, it is not possible for I-PER to follow B-LOC), it is beneficial to model and decode each sentence jointly instead of independently predicting the label for each token. The top level embeddings from a pre-trained bidirectional LM are inserted in a stacked bidirectional RNN sequence tagging model. See text for details.         This is the forward LM embedding of the token at position k and is the output of the top LSTM layer in the language model. The need to capture future context in the LM embeddings suggests it is beneficial to also consider a backward LM in additional to the traditional forward LM. A backward LM predicts the previous token given the future context.     tN).   tN), the output embeddings of the top layer LSTM. Note that in our formulation, the forward and backward LMs are independent, without any shared parameters. Our combined system, TagLM, uses the LM embeddings as additional inputs to the sequence tagging model. In particular, we concatenate the LM embeddings hLM with the output from one of the bidirectional RNN layers in the sequence model. In our experiments, we found that introducing the LM embeddings at the output of the first layer performed the best. It includes standard train, development and test sets. We use CNN-BIG-LSTM to refer to this language model in our results. In addition to explicit dropout regularization, we also use early stopping to prevent over-fitting and use the following process to determine when to stop training. It is important to estimate the variance of model performance since the test data sets are relatively small. Adding external resources. Although we do not use external labeled data or gazetteers, we found that TagLM outperforms previous state of the art results in both tasks when external resources (labeled data or task specific gazetteers) are available. How to use LM embeddings? In this experiment, we concatenate the LM embeddings at different locations in the baseline sequence tagger. It was reproduced here for convenience. We speculate that the second RNN layer in the sequence tagging model is able to capture interactions between task specific context as expressed in the first RNN layer and general context as expressed in the LM embeddings in a way that improves overall system performance. Does it matter which language model to use? In this experiment, we compare six different configurations of the forward and backward language models (including the baseline model which does not use any language models). Including embeddings from these language models decreased performance slightly compared to the baseline system without any LM. tion functions (i.e., the RNN parameters in the language model) from much larger data compared to the composition functions in the baseline tagger, which are only learned from labeled data. Importance of task specific RNN. To understand the importance of including a task specific sequence RNN we ran an experiment that removed the task specific sequence RNN and used only the LM embeddings with a dense layer and CRF to predict output tags. This result confirms that the RNNs in the baseline tagger encode essential information which are not encoded in the LM embeddings. This is unsurprising since the RNNs in the baseline tagger are trained on labeled examples, unlike the RNN in the language model which is only trained on unlabeled examples. Dataset size. A priori, we expect the addition of LM embeddings to be most beneficial in cases where the task specific annotated datasets are small. Taken together, this suggests that for very small labeled training sets, transferring from other tasks yields a large improvement, but this improvement almost disappears when the training data is large. On the other hand, our approach is less dependent on the training set size and significantly improves performance even with larger training sets. Number of parameters. To confirm that this did not have a material impact on the results, we ran two additional experiments. In the first, we trained a system without a LM but increased the second RNN layer hidden dimension so that number of parameters was the same as in TagLM. In the second experiment, we decreased the hidden dimension of the second RNN layer in TagLM to give it the same number of parameters as the baseline no LM model. Unlabeled data. TagLM was inspired by the widespread use of pre-trained word embeddings in supervised sequence tagging models. It is easy to combine TagLM with any of the above methods by including LM embeddings as additional features in the discriminative components of the model (except for expectation maximization). Neural language models. In contrast, TagLM uses neural LMs to encode words in the input sequence. Unlike forward LMs, bidirectional LMs have received little prior attention. They tied the input token embeddings and softmax weights in the forward and backward directions, unlike our approach which uses two distinct models without any shared parameters. Interpreting RNN states. Recently, there has been some interest in interpreting the activations of RNNs. Our work complements these studies by showing that LM states are useful for downstream tasks as a way of interpreting what they learn. Other sequence tagging models. Current state of the art results in sequence tagging problems are based on bidirectional RNN models. LM embeddings could also be used as additional features in other models, although it is not clear whether the model complexity would be sufficient to effectively make use of them. In this paper, we proposed a simple and general semi-supervised method using pre-trained neural language models to augment token representations in sequence tagging models. Our method significantly outperforms current state of the art models in two popular datasets for NER and Chunking. Our analysis shows that adding a backward LM in addition to traditional forward LMs consistently improves performance. The proposed method is robust even when the LM is trained on unlabeled data from a different domain, or when the baseline model is trained on a large number of labeled examples."
86,3,4.67,4.67,4.0,True,acl_2017,train,"We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing datadriven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.","In parallel, the NLP community has developed methods for data-driven semantic parsing, which attempt to map NL to structured logical forms executable by computers. While these methods have the advantage of being learnable from data, compared to the programming languages (PLs) in use by programmers, the domainspecific languages targeted by these works have a schema and syntax that is relatively simple. This work treats code generation as a sequence-tosequence modeling problem, and introduce methods to generate words from character-level models, and copy variable names from input descriptions. However, unlike most work in semantic parsing, it does not consider the fact that code has to be well-defined programs in the target syntax. In this work, we propose a data-driven syntaxbased neural network model tailored for generation of general-purpose PLs like Python. In order to capture the strong underlying syntax of the PL, we define a model that transduces an NL statement into an Abstract Syntax Tree (AST; Fig. ASTs can be deterministically generated for all well-formed programs using standard parsers provided by the PL, and thus give us a way to obtain syntax information with minimal engineering. Once we generate an AST, we can use deterministic generation tools to convert the AST into surface code. We hypothesize that such a structured approach has two benefits. func: the function to be invoked. args: arguments list. test: condition expression. body: statements inside the If clause. target: iteration variable. iter: enumerable to iterate over. body: loop body.  name: function name. args: function arguments. To this end, we propose a syntax-driven neural code generation model. The underlying syntax is therefore encoded in the grammar model a priori as the set of possible actions. Our approach frees the model from recovering the underlying grammar from limited training data, and instead enables the system to focus on learning the compositionality among existing grammar rules. Second, we hypothesize that structural information helps to model information flow within the network, which naturally reflects the recursive structure of PLs. As an example, when expanding the node? in Fig. This enables us to locally pass information of relevant code segments via neural network connections, resulting in more confident predictions. Our model also gives competitive performance on a standard semantic parsing benchmark. Given an NL description x, our task is to generate the code snippet c in a modern PL based on the in-tent of x. We attack this problem by first generating the underlying AST. Before detailing our approach, we first present a brief introduction of the Python AST and its underlying grammar. The Python abstract grammar contains a set of production rules, and an AST is generated by applying several production rules composed of a head node and multiple child nodes. For instance, the first rule in Tab. Labels of each node are noted within brackets. In an AST, non-terminal nodes sketch the general structure of the target code, while terminal nodes can be categorized into two types: operation terminals and variable terminals. For instance, all terminal nodes in Fig. Before detailing our neural code generation method, we first introduce the grammar model at its core. Our probabilistic grammar model defines the generative story of a derivation AST.   Dashed nodes denote terminals. Nodes are labeled with time steps during which they are generated. Each node in Fig. Action nodes are connected by solid arrows which depict the chronological order of the action flow. We describe this process in detail below. APPLYRULE actions generate program structure, expanding the current node (the frontier node at time step t: nft) in a depth-first, left-to-right traversal of the tree. Given a fixed set of production rules, APPLYRULE chooses a rule r from the subset that has a head matching the type of nft, and uses r to expand nft by appending all child nodes specified by the selected production. As an example, in Fig.   APPLYRULE actions grow the derivation AST by appending nodes. When a variable terminal node (e.g., str) is added to the derivation and becomes the frontier node, the grammar model then switches to GENTOKEN actions to populate the variable terminal with tokens. Unary Closure Sometimes, generating an AST requires applying a chain of unary productions. Unary closures reduce the number of actions needed, but would potentially increase the size of the grammar. Once we reach a frontier node nft that corresponds to a variable type (e.g., str), GENTOKEN actions are used to fill this node with values. For generalpurpose PLs like Python, variables and constants have values with one or multiple tokens. Our model copes with both scenarios by firing GENTOKEN actions at one or more time steps. The grammar model then proceeds to the new frontier node. Terminal tokens can be generated from a predefined vocabulary, or be directly copied from the input NL. This is motivated by the observation that the input description often contains out-ofvocabulary (OOV) variable names or literal values that are directly used in the target code. We estimate action probabilities in Eq. See supplementary materials for detailed equations. The decoder uses a RNN to model the sequential generation process of an AST defined as Eq. Each action step in the grammar model naturally grounds to a time step in the decoder RNN. Therefore, the action sequence in Fig. Our implementation of the decoder resembles a vanilla LSTM, with additional neural connections (parent feeding, Fig. pt is a vector that encodes the information of the parent action. Intuitively, feeding the decoder the information of nft helps the model to keep track of the frontier node to expand. Action Embedding at We maintain two action embedding matrices, WR and WG. Parent Feeding pt Our decoder RNN uses additional neural connections to directly pass information from parent actions. Formally, we define the parent action step pt as the time step at which the frontier node nft is generated.  action. pt is the concatenation. The parent feeding schema enables the model to utilize the information of parent code segments to make more confident predictions. Given a dataset of pairs of NL descriptions xi and code snippets ci, we parse ci into its AST yi and decompose yi into a sequence of oracle actions under the grammar model. The model is then optimized by maximizing the log-likelihood of the oracle action sequence. See supplementary materials for the pseudo-code of the inference algorithm. cards for the card game HearthStone. Each card comes with a set of fields (e.g., name, cost, and description), which we concatenate to create the input sequence. Compared with the HS dataset where card implementations are somewhat homogenous, examples in DJANGO are more diverse, spanning a wide variety of real-world use cases like string manipulation, IO operations, and exception handling. Different from HS and DJANGO which are in a general-purpose PL, programs in IFTTT are written in a domain-specific language used by the IFTTT task automation App. The authors thus provide a high-quality filtered test set, where each example is verified by at least three annotators. We use this set for evaluation. This is because for HS and DJANGO terminal tokens are generated by GENTOKEN actions, but for IFTTT, all the code is generated directly by APPLYRULE actions. Metrics As is standard in semantic parsing, we measure accuracy, the fraction of correctly generated examples. Preprocessing All input descriptions are tokenized using NLTK. We perform simple canonicalization for DJANGO, such as replacing quoted strings in the inputs with place holders. See supplementary materials for details. Evaluation results for Python code generation tasks are listed in Tab. Numbers for our systems are averaged over three runs. System Comparison As in Tab. This boost in performance strongly indicates the importance of modeling grammar in code generation. For the baselines, we find LPN outperforms others in most cases. The sheer number of nodes in target ASTs makes the prediction process error-prone. In contrast, the APPLYRULE actions of our grammar model allows for generating multiple nodes at a single time step. Ablation Study We also ablated our bestperforming models to analyze the contribution of each component. This yields worse results on DJANGO while gives slight improvements in accuracy on HS. This is probably because that the grammar of HS has fewer node types, and thus the RNN is able to keep track of nft without depending on its embedding. The accuracy drops significantly on HS, with a marginal deterioration on DJANGO. This result is interesting because it suggests that parent feeding is more important when the ASTs are larger, which will be the case when handling more complicated code generation tasks like HS. The results with and without unary closure demonstrate that, interestingly, it is effective on HS but not on DJANGO. the size of the gold-standard ASTs in Figs. Not surprisingly, the performance drops when the size of the reference ASTs increases. Domain Specific Code Generation Although this is not the focus of our work, evaluation on IFTTT brings us closer to a standard semantic parsing setting, which helps to investigate similarities and differences between generation of more complicated general-purpose code and and more limiteddomain simpler code. Tab. This score is close to the best classical method (LR), which is based on a logistic regression model with rich hand-engineered features (e.g., brown clusters and paraphrase). Also note that the performance between NMT and other neural models is much closer compared with the results in Tab. This suggests that general-purpose code generation is more challenging than the simpler IFTTT setting, and therefore modeling structural information is more helpful. Case Studies Finally, we present output examples in Tab. On HS, we observe that most of the time our model gives correct predictions by filling learned code templates from training data with arguments (e.g., cost) copied from input. class Brawl(SpellCard): def init (self): super(). pred. ing. However, we find that the block A actually conveys part of the input intent by destroying all, not some, of the minions. Since we are unable to find code block A in the training data, it is clear that the model has learned to generalize to some extent from multiple training card examples with similar semantics or structure. The next two examples are from DJANGO. The first one shows that the model learns the usage of common API calls (e.g., os.path.join), and how to populate the arguments by copying from inputs. The second example illustrates the difficulty of generating code with complex nested structures like lambda functions, a scenario worth further investigation in future studies. More examples are attached in supplementary materials. Semantic Parsing Our work is related to the general topic of semantic parsing, where the target logical forms can be viewed as DSLs. This paper proposes a syntax-driven neural code generation approach that generates an abstract syntax tree by sequentially applying actions from a grammar model. Experiments on both code generation and semantic parsing tasks demonstrate the effectiveness of our proposed approach."
520,2,4.0,2.56,2.0,False,acl_2017,train,"We introduce a novel framework for evaluating multimodal deep learning models with respect to their language understanding and generalization abilities. In this approach, artificial data is automatically generated according to the experimenter’s specifications. The content of the data, both during training and evaluation, can be controlled in detail, which enables tasks to be created that require generalization abilities, in particular the combination of previously introduced concepts in novel ways. We demonstrate the potential of our methodology by evaluating a multimodal architecture on four different tasks, and show that our framework gives us insights into the model’s capabilities and limitations.","Such systems seem to solve the problems entirely on a sub-symbolic level, based only on raw image (and text) input, whereas previous approaches required a hand-crafted combination of various higher-level components. There is, however, concern about how deep neural networks learn to solve such tasks. Such results cast doubt on whether deep learning systems reliably acquire appropriate generalizations. However, given the recursive nature of language and the potentially enormous problem space of image captioning and similar tasks, acquiring the ability for reliable generalization will eventually be essential. A more theoretical issue is the capability of network architectures to be able, in principle, to learn certain classes of structure. However, the formal experiments that have been done along such lines are limited, particularly in the multimodal domain of vision and language. While recent work indicates that the information encoded in image embeddings is rich enough for good captioning results, it is an open question whether current multimodal architectures are able, in principle, to combine visual information effectively to handle the full range of linguistic constructions. This paper introduces a new test methodology for multimodal deep learning models. SHAPEWORLD is a framework for specifying datasets, which differ from standard evaluation datasets in two main ways. Secondly, the evaluation focus of the methodology is on linguistic understanding capabilities of the type investigated by formal semantics. Since it is possible to control data generation for both training and evaluation, we can introduce previously unseen configurations in the test data. This allows us to design tasks which require the system to recombine learned concepts to understand these novel instances, and is hence a form of zero-shot learning. We also present initial results on the performance of a generic multimodal architecture. The interest here is not obtaining high numbers, but rather analyzing the performance of a generic model with our evaluation methodology and demonstrating the potential of SHAPEWORLD in investigating multimodal deep neural networks. In fact, we have been surprised to see some poor performance even for simple tasks. We invite the community to explore the SHAPEWORLD tasks and use them to evaluate other successful image captioning or visual question answering systems. We want to emphasize, however, that the goal is not to achieve optimal performance by tuning a network architecture for one of these tasks. The central question is rather whether deep network architectures are able to successfully demonstrate the required understanding and generalization ability, and whether this will carry over to more complex tasks which can be defined using SHAPEWORLD. With the increasing popularity of deep learning approaches, artificial data of various kinds is again seen as a valuable tool in experimentation. An important advantage of simulated data is its infinite availability, particularly in light of the need of many deep learning models for huge amounts of data. Automatically generating data greatly reduces the cost, time and human effort. Moreover, it allows researchers to focus on particular problems, isolated from noisy and complex environments. When focusing on language tasks, the simulation paradigm faces the problem that interesting language generation is a difficult task in its own right and that the difficulty increases with the complexity of the underlying world. However, the descriptions are in an abstract, formulaic format, and the focus of the simulation is much more on the planning than the language component. At least for a start, this module is supposed to be scripted to automatically generate appropriate responses, given its internal knowledge of the world state. Automatically generated data is common for tasks specifically focusing on the ability to efficiently process data of a certain formal structure. Here, data is deliberately stripped of any realworld connection to create an abstract capability check. The reason for interest in abstract capability tests is that the learning process of deep neural networks is far more difficult to understand in a detailed way than shallower machine learning methods. Both generate artificial data fully automatically, based on abstract models, for a task which is targeted at a specific linguistic aspect, logical semantics and quantifiers, respectively. Our own work is based on automatically generated, fully artificial data. This data, however, is not designed to address only a single structural problem, but is able to cover a whole range of linguistic phenomena. However, while SHAPEWORLD uses a logical representation internally, the external representation seen by the system under evaluation does not involve any abstract formalization of text or images. It nevertheless presents the intended problems clearly, without any uncontrolled noise, biases, or potentially hidden correlations, which can obfuscate results when using real-world images and text. The SHAPEWORLD microworlds simply consist of colored shapes. The generated data is returned as NumPy arrays, so that it is possible to integrate it into Python-based deep learning projects based on common frameworks like TensorFlow, Theano, etc. In our experiments, we use Tensorflow and we provide example scripts as part of the package. on closed-class words. In the following we explain the details of the data generation process inside the SHAPEWORLD framework. In this paper we focus on the task of image caption agreement. The system to be evaluated is presented with an image and a natural language caption, and has to decide whether they are consistent with each other. Compared to the classic image captioning task, it emphasizes understanding rather than the synthesis part of language use. We therefore avoid the problem of evaluating the appropriateness of a caption. The setup allows us to control the content of both modalities and consequently force a system to cope with difficult types of captions while obtaining a clear indicator of successful understanding. One further motivation for the task is that human performance could be measured using the same setup. We would expect close-to-perfect human performance on the tasks described here, assuming time is not tightly constrained. However, we will not discuss this further in the current paper. At the core of each abstract microworld instance lies an abstract world model. In the SHAPEWORLD framework, the internal representation of a microworld is simply a list of entities given as records containing their primary attributes, such as position, shape, color, which are considered to be high-level semantic aspects reflected in captions. In addition, an entity has secondary attributes and methods which control, for instance, details of visual appearance, visual noise infusion, or the collision-free placement of entities. Currently, the SHAPEWORLD framework provides eight shape types (square, rectangle, triangle, pentagon, cross, circle, semicircle and ellipse) and seven colors (red, green, blue, yellow, magenta, cyan and white).                 per pixel). Importantly, all these ways of infusing noise can be controlled, which is useful particularly since noise is often seen as important for successful training of deep models. The generator module automatically generates a world model by randomly sampling all these attributes from a set of available values. Both the values and other aspects of the generation process can be specified and adjusted appropriately. The internal abstract representation is then used as a basis to extract a concrete microworld instance consisting of image and caption. Note that the exact visual appearance of an entity with certain primary attributes varies from instance to instance. Caption generation is more complex, as discussed in the next section. The caption interface specifies the methods a language generation module has to provide to integrate into the data generation process. We currently provide an implementation using a grammar-based approach. More specifically, Dependency Minimal Recursion Semantics (DMRS) is an abstract semantic graph representation designed for use with high-precision grammars, such as those distributed by the DELPH-IN consortium. Below an example of a DMRS semantic graph with its compositional components colored: There is a blue circle. Compositionality of the semantic representation is a useful property and an important reason for our choice of using DMRS. For instance, the semantics of words like square or red iteratively filter a subset of agreeing entities, transitive relations like to the left of act similarly on pairs of entity sets, and quantifiers compare the cardinality of two entity sets. It also illustrates how various details are automatically inferred by the ERG, including number-agreement between subject and verb, and between quantifier and noun, and realization of an adjective as relative clause. compatible approach, so SHAPEWORLD could be ported to other languages relatively easily. The logical formula gives the formal semantic interpretation over a world model. In essentially the same way, the agreement of a caption with a microworld is computed in the SHAPEWORLD framework. Similar to the generator module, the captioner module also randomly samples from a set of dataset-specific DMRS graph patterns, which are then applied to a world model to construct an agreeing caption object. Such a caption object can be turned into natural language. These captions are obtained by sampling a second, false world model, extracting a caption object from it, and ensuring that it does not accidentally also agree with the first, true microworld. Since SHAPEWORLD datasets are actually data generation processes, training and evaluation work differently from classic datasets. Where usually one has a fixed set of test instances, here models are trained and tested on a fixed set of more abstract configuration constraints. In particular, in our experiments, the constraints for testing (and validation) differ from the training constraints, hence requiring true generalization abilities. This means that, for instance, a certain shapecolor-combination, a specific number of objects, a spatial location or a caption type can be held-out and never generated during training. The model is presented with instances of this configuration only when evaluated and is hence required to recombine concepts it acquired from the training data. It is thus possible for a system to achieve optimal performance during training, but completely fail the evaluation. Another parameter is the ratio of correct instances, which may differ between training and evaluation. This adds yet another level of control to allow us to analyze the behavior of a model. In this paper we look at four datasets, each designed to investigate an aspect of the capability to understand language in a multimodal setup and to generalize to new instances not seen during training. OneShape The first and simplest dataset requires the evaluated system to learn to separate the concepts of shape and color, instead of treating each combination as an atomic object. MultiShape This dataset generates worlds with up to four objects, with the same type of existential statement as before, describing one of these objects. The system is evaluated on worlds containing five objects, requiring the ability to focus attention on one object without being distracted by other objects. right of, above or below. As in the first dataset, a previously unseen attribute combination is presented for evaluation. Quantifier The quantifier dataset generates microworlds of three to six objects, with seven used for evaluation. The captions focus on the quantifiers no, a, the, some, two, most, all and every. For instance, most is considered to be true if the target attribute applies to more than half of the domain set of objects. Since we first sample a microworld model and subsequently a caption, here we cannot easily control the sampling process to uniformly sample each possible caption, as is trivially the case for the other datasets. Consequently, we restrict the number of different shapes and colors possible in a world, so that situations for the quantifiers all, every and most become more likely. Moreover, we adapt the probabilities of the sampling process so that we observe a more uniform distribution over caption patterns when sampling a large number of instances. An important property of the SHAPEWORLD datasets is their compositionality. Instead of having to define a dataset from scratch every time, we can specify atomic datasets like the ones described above, and then combine them in a mixer dataset, which tests for various different aspects of multimodal language understanding simultaneously. Reusability in fact applies even further down in the component hierarchy. For instance, we use the same generic world generator module for all four datasets. This is even more important for caption generation where, for instance, a logical combinator dataset can reuse different world captioner modules to generate simple statements which then are merged by logical connectives. As discussed in the introduction, the aim of our experiments is to demonstrate that SHAPEWORLD allows detailed investigation of neural network architectures, rather than high performance as such, although naturally a certain minimum performance is required for the results to be interesting. The final state is used as the caption embedding, which is scaled and fused with the world embedding via pointwise multiplication. The entire architecture is trained end-toend on the task. Both the CNN module and the word embeddings are included in the training, as opposed to using pre-trained, general-purpose versions.  We observe a high accuracy on the ONESHAPE dataset on training instances, but an unexpectedly low and unstable performance on the held-back instances. Being given less memory to learn simple non-generalizing classification, the model indeed achieves similarly high accuracy numbers in evaluation as during training. The evaluation accuracy, in comparison, is not that much lower, indicating that the generalization to an unseen number of objects is learned, to some degree. Note that each caption here involves two object descriptions, so instead of simply learning classification, it is necessary to disentangle the words of the caption. The fact that the evaluation accuracy is higher than the training accuracy in the beginning is because the held-back combination red square is (unsurprisingly) easier to recognize than some others. Again, we also ran an experiment with reduced embedding and LSTM size. Contrary to ONESHAPE, the accuracy curves indicate that this impedes the learning of an appropriate generalization ability for the more complex SPATIAL task. The QUANTIFIER dataset confirms the lower accuracy of captions focusing on parts of a world which contains multiple shapes. We want to emphasize the fact that the training accuracy here represents an interesting measure on its own. Contrary to the common evaluation methodology, where training performance is less expressive after multiple iterations over the training data, each instance is randomly generated anew in our setting. Investigations of overfitting in this sense are not easily possible with non-artificial data. Another type of investigation of what a model has learned involves manipulation of the evaluation data. Looking at a few misclassifications, we found that sometimes the negative instances are problematic where either only the shape or only the color attribute differed from what the caption stated. We thus created a modified version of the ONESHAPE dataset which generates only negative instances of this structure. Changing the network parameters altered the performance for the ONESHAPE and SPATIAL datasets in interestingly different ways. In general, when we changed parameters in our experiments, we often observed either no learning at all, less (or no) improvement in evaluation accuracy compared to training accuracy, or similar curves for both numbers. Another aspect, which might turn out interesting in more complex experiments is the shape of the accuracy curves. We have presented a new test methodology and framework, SHAPEWORLD, for multimodal deep learning models with a focus on formal-semantic style generalization capabilities. In this framework, artificial data is automatically generated, and datasets specify parameters of this generation process to create appropriate data, which targets a specific multimodal understanding task. We evaluated a neural network architecture on four image caption agreement datasets, where the system has to decide whether a statement applies to an image. We show how the SHAPEWORLD framework can be used to investigate the learning process of deep neural networks, and that it clearly indicates whether certain abilities are acquired. The SHAPEWORLD framework is still under development. In particular, we plan to add new datasets addressing other aspects of language, as well as integrating options to enhance the language generation module, with the aim of providing more varied and natural image descriptions. However, we plan to stick to the simplicity and abstractness of our microworlds for now, since we think they offer several advantages and enough richness for various interesting investigations."
388,1,4.0,5.0,3.0,True,acl_2017,train,"Universal Dependencies (UD) provides a cross-linguistically uniform syntactic representation, with the aim of advancing multilingual applications of parsing and natural language understanding. Reddy et al. (2016) recently developed a semantic interface for (English) Stanford Dependencies, based on the lambda calculus. In this work, we introduce UDEPLAMBDA, a similar semantic interface for UD, which allows mapping natural language to logical forms in an almost language-independent framework. We evaluate our approach on semantic parsing for the task of question answering against Freebase. To facilitate multilingual evaluation, we provide German and Spanish translations of the WebQuestions and GraphQuestions datasets. Results show that UDEPLAMBDA outperforms strong baselines across languages and datasets. For English, it achieves the strongest result to date on GraphQuestions, with competitive results on WebQuestions.","natural language to logical forms, representing underlying predicate-argument structures, in an almost language-independent manner. We address the task of learning to map natural language to machine interpretable formal meaning representations, specifically retrieving answers to questions from Freebase. We demonstrate that UDEPLAMBDA can be used to derive logical forms for these languages using a minimal amount of language-specific knowledge. Aside from developing the first multilingual semantic parsing tool for Freebase, we also experimentally show that UDEPLAMBDA outperforms strong baselines across languages and datasets. For English, it achieves the strongest result to date on GraphQuestions, with competitive results on WebQuestions. Our implementation and translated datasets will be made publicly available. DEPLAMBDA converts a dependency tree to its logical form in three steps: binarization, substitution, and composition, each of which is briefly outlined below. Binarization A dependency tree is first mapped to a Lisp-style s-expression indicating the order of semantic composition. Here, the sub-expression (dobj won (det Oscar an)) indicates that the logical form of the phrase won an Oscar is derived by composing the logical form of the label dobj with the logical form of the word won and the logical form of the phrase an Oscar, derived analogously. An obliqueness hierarchy is employed to impose a strict ordering on the modifiers to each head in the dependency tree. Substitution Each symbol in the s-expressions is substituted for a lambda expression encoding its semantics. Words and dependency labels are assigned different types of expressions. Expressions for dependency labels glue the semantics of heads and modifiers to articulate predicate-argument structure. case, punct, aux, mark. This expression takes two functions f and g as input, where f represents the logical form of won and g represents the logical form of an Oscar. the entity Oscar. Expressions that invert the dependency direction are referred to as INVERT (e.g. amod in running horse); expressions that merge two subexpressions without introducing any relation predicates are referred to as MERGE (e.g. case in for Frozen). Sometimes the mapping of dependency label to lambda expression may depend on surrounding part-of-speech tags or dependency labels. For example, amod acts as INVERT when the modifier is a verb (e.g. in running horse), and as MERGE when the modifier is an adjective (e.g. This is accomplished by restricting the binarization, substitution, and composition steps described above to rely solely on information encoded in the UD representation. Importantly, UDEPLAMBDA is designed to not rely on lexical forms in a language to assign lambda expressions, but only on information contained in dependency labels and postags. However, some linguistic phenomena are language specific (e.g. pronoun-dropping) or meaning specific (e.g. every and the in English have very different semantics, despite being both determiners) and are not encoded in the UD schema. Furthermore, some cross-linguistic phenomena, such as long-distance dependencies, are not part of the core UD representation. graph. However, such enhancements are currently only available for a subset of languages in UD. First, we identify long-distance dependencies in relative clauses and control constructions. Here, marry is provided with its missing nsubj (dashed arc). Second, UD conflates all coordinating constructions to a single dependency label, conj. Since these are crucial to question-answering, we use a small lexicon to refine the tags for determiners (DET), adverbs (ADV) and pronouns (PRON) to DET:WH, ADV:WH and PRON:WH, respectively. This is the only part of UDEPLAMBDA that relies on language-specific information. We hope that, as the coverage of morphological features in UD improves, this refinement can be replaced by relying on morphological features, such as the interrogative feature (INT). These substitutions are based solely on unlexicalized context. For example, the part-of-speech tag PROPN of Anna invokes an ENTITY expression. DEPLAMBDA, in contrast, cannot handle graph-structured input, since it lacks a principled way of generating sexpressions from graphs. Even given the above s-expression, BIND in DEPLAMBDA is defined in a way such that the composition fails to unify v and x, which is crucial for the correct semantics. Moreover, the definition of BIND in DEPLAMBDA does not have a formal interpretation within the lambda calculus, unlike ours. Below, we highlight the most pertinent differences between UDEPLAMBDA and DEPLAMBDA, stemming from the different treatment of various linguistic constructions in UD versus SD. Other nmod constructions, such as possessives (nmod:poss), temporal modifiers (nmod:tmod) and adverbial modifiers (nmod:npmod), are handled similarly. Note how the common noun president, evokes both entity and event predicates above. Passives DEPLAMBDA gives special treatment to passive verbs, identified by the fine-grained partof-speech tags in the PTB tag together with dependency context. possible (for example, by treating nsubjpass as direct object). However, not having a special entry for passive verbs may have undesirable side-effects. We leave it to the target application to disambiguate the interpretation in such cases. in the news that Disney won an Oscar, the clause that Disney won an Oscar is a subordinating conjunction of news. In such cases, we instead assign acl the INVERT semantics. The predicate TARGET indicates that xa represents the variable of interest, that is the answer to the question. In order to achieve language independence, UDEPLAMBDA has to sacrifice semantic specificity, since in many cases the semantics is carried by lexical information. Consider the sentences John broke the window and The window broke. Although it is the window that broke in both cases, our inferred logical forms do not canonicalize the relation between broke and window. We anticipate that the ability to make such lexicalized semantic inferences in a task-agnostic cross-lingual framework would be highly useful and a crucial avenue for future work on universal semantics. Other constructions that require lexical information are quantifiers like every, some and most, negation markers like no and not, and intentional verbs like believe and said. UD does not have special labels to indicate these. Although not currently implemented, we discuss how to handle quantifiers in this framework in the supplementary material. To study the multilingual nature of UDEPLAMBDA, we conduct an empirical evaluation on question answering against Freebase in three different languages: English, Spanish, and German. Before discussing the details of this experiment, we briefly outline the semantic parsing framework employed. UDEPLAMBDA generates ungrounded logical forms that are independent of any knowledge base, such as Freebase. de Welche Sprache wird in Ghana gesprochen? en Who was Vincent van Gogh inspired by? de Von wem wurde Vincent van Gogh inspiriert? GraphQuestions en NASA has how many launch sites? de Wie viele Abschussbasen besitzt NASA? and entities denote predicates or Freebase relations. Finally, the TARGET node represents the set of values of x that are consistent with the Freebase graph, that is the answer to the question. GRAPHPARSER treats semantic parsing as a graph-matching problem with the goal of finding the Freebase graphs that are structurally isomorphic to an ungrounded graph and rank them according to a model. To account for structural mismatches, GRAPHPARSER uses two graph transformations: CONTRACT and EXPAND. CONTRACT collapses one of these edges to create a graph isomorphic to Freebase. EXPAND, in contrast, adds edges to connect the graph in the case of disconnected components. While WebQuestions is dominated by simple entityattribute questions, GraphQuestions contains a large number of compositional questions involving aggregation (e.g. How many children of Eddard Stark were born in Winterfell? ) and comparison (e.g.  Here we provide details on the syntactic analyzers employed, our entity resolution algorithm, and the features used by the grounding model. Both the tagger and parser require word embeddings. In addition, we add features encoding the semantic similarity of ungrounded events and Freebase relations. An ungrounded graph is generated by connecting all entities in the question with the TARGET node, representing a single event. Note that this baseline cannot handle compositional questions, or those with aggregation or comparison. DEPTREE An ungrounded graph is obtained directly from the original dependency tree. An event is created for each parent and its dependents in the tree. We observed that this leads to inferior entity linking results compared to those of Freebase. If a word is a question word, an additional TARGET predicate is attached to its entity node. Note that this baseline exists only for English. With both datasets, results are lower for German compared to Spanish. This agrees with the lower performance of the syntactic parser on the German portion of the UD treebank. Finally, while these results confirm that GraphQuestions is much harder compared to WebQuestions, we note that both datasets predominantly contain single-hop questions, as indicated by the competitive performance of SINGLEEVENT on both datasets. These are either symbolic like ours (first block) or employ neural networks (second block). A common trend in previous work on semantic interfaces is the reliance on rich typed feature structures or semantic types coupled with strong type constraints, which can be very informative but unavoidably language specific. Creating rich semantic types from dependency trees which lack a typing system would be labor intensive and brittle in the face of parsing errors. Instead, UDEPLAMBDA relies on generic unlexicalized information present in dependency treebanks and uses a simple type system (one type for dependency labels, and one for words) along with a combinatory mechanism, which avoids type collisions. In contrast, UDEPLAMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. We evaluate UDEPLAMBDA on semantic parsing for question answering against a knowledge base. Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. We introduced UDEPLAMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UDEPLAMBDA to natural language understanding tasks. In the future, we would like to explore how this framework can benefit other tasks such as summarization and machine translation."
222,1,5.0,5.0,5.0,True,acl_2017,train,"Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem. Then, based on our tagging scheme, we study different end-toend models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What’s more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.","It is an important issue in knowledge extraction and automatic construction of knowledge base. This separated framework makes the task easy to deal with, and each component can be more flexible. But it neglects the relevance between these two sub-tasks and each subtask is an independent model. Different from the pipelined methods, joint learning framework is to extract entities together with relations using a single model. It can effectively integrate the information of entities and relations, and it has been shown to achieve better results in this task. They need complicated feature engineering and heavily rely on the other NLP toolkits, which might also lead to error propagation. Although the joint models can represent both entities and relations with shared parameters in a single model, they also extract the entities and relations separately and produce redundant information. In this paper, we focus on the extraction of triplets that are composed of two entities and one relation between these two entities. Therefore, we can model the triplets directly, rather than extracting the entities and relations separately. Based on the motivations, we propose a tagging scheme accompanied with the end-to-end model to settle this problem. We design a kind of novel tags which contain the information of entities and the relationships they hold. Based on this tagging scheme, the joint extraction of entities and relations can be transformed into a tagging problem. In this way, we can also easily use neural networks to model the task without complicated feature engineering. LSTM is capable of learning long-term dependencies, which is beneficial to sequence modeling tasks. Therefore, based on our tagging scheme, we investigate different kinds of LSTM-based end-to-end models to jointly extract the entities and relations. We also modify the decoding method by adding a bias loss to make it more suitable for our special tags. The method we proposed is a supervised learning algorithm. In reality, however, the process of manually labeling a training set with a large number of entity and relation is too expensive and error-prone. The experimental results show that our tagging scheme is effective in this task. In addition, our end-to-end model can achieve the best results on the public dataset. It is the first work to solve the problem by a tagging manner. The tagging-based methods are better than most of the existing pipelined and joint learning methods. It can enhance the association between related entities. Two main frameworks have been widely used to solve the problem of extracting entity and their relationships. One is the pipelined method and the other is the joint learning method. While joint models extract entities and relations using a single model. Different from the above methods, the method proposed in this paper is based on a special tagging manner, so that we can easily use end-toend model to extract results without NER and RC. end-to-end method is to map the input sentence into meaningful vectors and then back to produce a sequence. Most methods apply bidirectional LSTM to encode the input sentences, but the decoding methods are always different. We propose a novel tagging scheme and an end-toend model with biased objective function to jointly extract entities and their relations. In this section, we firstly introduce how to change the extraction problem to a tagging problem based on our tagging method. Then we detail the model we used to extract results. Each word is assigned a label that contributes to extract the results. Thus they are tagged based on our special tags. We combine entities with the same relation type into a triplet to get the final result. Besides, if a sentence contains two or more triplets with the same relation type, we combine every two entities into a triplet based on the nearest principle. In this paper, we only consider the situation where an entity belongs to a triplet, and we leave identification of overlapping relations for future work. In this paper, we investigate an end-to-end Model to produce the tags sequence. It contains a bi-directional Long Short Term Memory (BiLSTM) layer to encode the input sentence and a LSTM decoding layer with bias loss. The bias loss can enhance the relevance of entity tags. The Bi-LSTM Encoding Layer. In sequence tagging problems, the Bi-LSTM encoding layer has been shown the effectiveness to capture the semantic information of each word. It contains word embedding layer, forward lstm layer, backward lstm layer and the concatenate layer. After word embedding layer, there are two parallel LSTM layers: forward lstm layer and backward lstm layer. The LSTM architecture consists of a set of recurrently connected subnets, known as memory blocks. are the parameters. The LSTM Decoding Layer. We also adopt a LSTM structure to produce the tag sequence. Because the T is similar to tag embedding and LSTM is capable of learning longterm dependencies, the decoding manner can model tag interactions. The Bias Objective Function. A large amount of training data can be obtained by means of distant supervision methods without manually labeling. While the test set is manually labeled to ensure its quality. A triplet is regarded as correct when its relation type and the head offsets of two corresponding entities are both correct. Hyperparameters Our model consists of a BiLSTM encoding layer and a LSTM decoding layer with bias objective function. There are three data sets in the public resource and we only use the NYT dataset. As for dataset Wiki-KBP, the number of relation type in the test set is more than that of the train set, which is also not suitable for a supervised training method. LSTM-CRF is proposed for entity recognition by using a bidirectional LSTM to encode input sentence and a conditional random fields to predict the entity tag sequence. Different from LSTM-CRF, LSTM-LSTM uses a LSTM layer to decode the tag sequence instead of CRF. They are used for the first time to jointly extract entities and relations based on our tagging scheme. It shows the effectiveness of our proposed method. Rec. Rec. Rec. Rec. are better than most of the jointly extracting methods. It also validates the validity of our tagging scheme for the task of jointly extracting entities and relations. When compared with the traditional methods, the precisions of the end-to-end models are significantly improved. But only LSTM-LSTM-Bias can be better to balance the precision and recall. The reason may be that these end-to-end models all use a Bi-LSTM encoding input sentence and different neural networks to decode the results. The methods based on neural networks can well fit the data. Therefore, they can learn the common features of the training set well and may lead to the lower expansibility. We also find that the LSTM-LSTM model is better than LSTM-CRF model based on our tagging scheme. The related tags may have a long distance from each other. Hence, LSTM decoding manner is a little better than CRF. LSTM-LSTM-Bias adds a bias weight to enhance the effect of entity tags and weaken the effect of invalid tag. Therefore, in this tagging scheme, our method can be better than the common LSTM-decoding methods. In this paper, we focus on extracting triplets composed of two entities and a relation. It treats an triplet is correct only when the relation type and the head offsets of two corresponding entities are both correct. It means that some of the predicted entities do not form a pair. Different from LSTM-CRF and LSTM-LSTM, our approach is biased towards relational labels to enhance links between entities. The single entities refer to those who cannot find their corresponding entities. It means that our method can effectively associate two entities when compared LSTM-CRF and LSTM-LSTM which pay little attention to the relational tags. The higher of the ratio the more entities are left. Because, LSTMLSTM and LSTM-LSTM-Bias use the softmax function to select the maximum probability label as the predicted tag for each word. Therefore, we tune the threshold for LSTM-LSTM and LSTM-LSTMBias to obtain their Precision-Recall curves that can reflect the predictive ability of models. Standard Si represents the gold standard of sentence i. The blue part is the correct result, and the red one is the wrong one. disadvantages of the methods. Each example contains three row, the first row is the gold standard, the second and the third rows are the extracted results of model LSTM-LSTM and LSTM-LSTMBias respectively. When compared with LSTM-LSTM, LSTM-LSTM-Bias uses a bias objective function which enhance the relevance between entities. There are no indicative words between entities Nuremberg and Germany. The problem can be solved by adding some samples of this kind of expression patterns to the training data. It shows that LSTM-LSTM-Bias is able to better on predicting entities pair, but it remains to be improved in distinguishing the relationship between the two entities. In this paper, we propose a novel tagging scheme and investigate the end-to-end models to jointly extract entities and relations. The experimental results show the effectiveness of our proposed method. But it still has shortcoming on the identification of the overlapping relations. In the future work, we will replace the softmax function in the output layer with multiple classifier, so that a word can has multiple tags. In this way, a word can appear in multiple triplet results, which can solve the problem of overlapping relations. Although, our model can enhance the effect of entity tags, the association between two corresponding entities still requires refinement in next works."
367,1,4.0,4.0,3.0,False,acl_2017,train,"The majority of NLG evaluation relies on automatic metrics, such as BLEU. In this paper, we investigate a wide range of these metrics, including state-of-the-art wordbased and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-toend NLG. A detailed error analysis shows that automatic metrics are particularly bad in distinguishing outputs of medium and good quality, which can be partially attributed to the fact that human judgements and metrics are given on different scales. We also show that metric performance is dataand system-specific. We then suggest an alternative metric, called RAINBOW, combining the individual strengths of different automatic scores. This new metric achieves up to ρ = 0.81 correlation with human judgements at the sentence-level (compared to a maximum of ρ = 0.33 for existing metrics) and achieves stable results across systems and datasets.","Automatic evaluation is popular because it is cheaper and faster to run than human evaluation, and is needed for automatic benchmarking and tuning of algorithms. The use of such metrics is, however, only sensible if they are known to be sufficiently correlated with human preferences. This paper follows on from this work and presents another evaluation study into automatic metrics with the aim to firmly establish the need for new metrics. We also suggest an alternative metric, which we call RAINBOW to reflect the diverse set of features it is based upon. It augments each LSTM cell with a gate that conditions it on the input MR, which allows it to keep track of MR contents generated so far. Surface realisation is performed using a separate, domain-independent rule-based module. We consider the following datasets collected via crowd-sourcing, which target utterance generation for spoken dialogue systems. Each instance consists of one MR and one or more natural language references as produced by humans, such as the following example, taken from the BAGEL dataset. food and kids-allowed for restaurants; hasinternet and dogs-allowed for hotels. NLG evaluation has borrowed a number of automatic metrics from related fields, such as MT, summarisation, or image captioning, which compare output texts generated by systems to groundtruth references produced by humans. We refer to this group as word-based metrics. This measure is based on distributional similarity and Latent Semantic Analysis (LSA) and is further complemented with semantic relations extracted from WordNet. To our knowledge, we are the first to consider GBMs for sentence-level NLG evaluation. Higher readability score indicates a less complex utterance that is easier to read. We also consider related measures, such as characters per utterance (len) and per word (cpw), words per sentence (wps), syllables per sentence (sps) and per word (spw), as well as polysyllabic words per utterance (pol) and per word (ppw). The higher these scores, the more complex the utterance. As a first approximation of grammaticality, we measure the parsing score (prs) as returned by the Stanford parser, as well as the number of misspellings (msp). The lower these scores are, the more grammatically correct an utterance is. The crowdworkers were selected from English-speaking countries only, based on their IP-addresses, and asked to confirm that English was their native language. In general, we find consistent differences in inter-annotator agreement per system and dataset, with lower agreements in LOLS than in RNNLG and TGEN. System performance is datasetspecific: For WBMs, the LOLS system consistently produces better results on BAGEL compared to TGEN, while for SFREST and SFHOTEL, LOLS is outperformed by RNNLG with WBMs. the same pattern as WBMs, while the average similarity score (sim) seems to be related to human quality ratings. Looking at GBMs, we observe that they seem to be related to naturalness and quality ratings. Less complex utterances, as measured by readability (RE) and word length (cpw), have higher naturalness ratings. More complex utterances, as measured in terms of their length (len), number of words (wps), syllables (sps, spw) and polysyllables (pol, ppw), have lower quality evaluation. Utterances measured as more grammatical are on average evaluated higher in terms of naturalness. While these initial results may suggest a relation between automatic metrics and human ratings, average scores can be misleading, as they only provide a system-level overview but do not measure the strength of association on sentence-level. This leads us to inspect the correlation of human and automatic metrics for each MR-system output pair. We split the data per dataset and system in order to make valid pairwise comparisons. To handle outliers within human ratings, we use the median score of the three human raters. GBMs, on the other hand, show greater diversity. This lets us conclude that WBMs and GBMs are sensitive to different systems and datasets. Bordered area shows correlations between human ratings and automatic metrics, the rest shows correlations among the metrics. That is, we compute the score of each metric for two system output sentences corresponding to the same MR. The prediction of a metric is correct if it orders the sentences in the same way as median human ratings (note that ties are allowed). Again, the performance of the metrics is dataset-specific: Metrics perform best on BAGEL data; for SFHOTEL, metrics show mixed performance; while for SFREST, metrics perform worst. Also, we observe a mismatch between the ordinal human ratings and the continuous metrics. In this section, we attempt to uncover why automatic metrics perform so poorly. It also explains why the RNNLG system, which contains very few instances of low user ratings, shows poor correlation between human ratings and automatic metrics. BAGEL has significantly shorter references both in terms of number of characters and words compared to the other two datasets. Although being shorter, the words in BAGEL references are significantly more often polysyllabic. Furthermore, BAGEL only consists of utterances generated from inform MRs, while SFREST and SFHOTEL also have less complex MR types, such as confirm, goodbye, etc. In other words, BAGEL is the most complex dataset to generate from, but also the one where metrics perform most reliably (note that the correlation is still only weak). Corpus-based methods may pick up these errors, and word-based metrics will rate these system utterances as correct, whereas we can expect human judges to be sensitive to ungrammatical utterances. Grammatical errors raise questions about the quality of the training data, especially when being crowd-sourced. In this section, we present our new metric, which we call RAINBOW to reflect that it combines mul-tiple other metrics. This supports our hypothesis that a combination of WBMs and GBMs can overcome the weaknesses of both. it shows comparable performance to the RAINBOW GBM model, which combines more than twice as many features. These studies mainly considered WBMs, while we are the first study to consider GBMs. Some studies ask users to provide separate ratings for surface realisation (e.g. In general, correlations reported by previous work range from weak to strong. There is a general trend showing that bestperforming metrics tend to be the more complex ones, combining word-overlap, semantic similarity and term frequency weighting. Note, however, that most previous works do not report whether any of the metric correlations are significantly dif-ferent from each other.) This paper shows that state-of-the-art automatic evaluation metrics for NLG systems do not sufficiently reflect human ratings. Word-based metrics make two strong assumptions: They treat humangenerated references as a gold-standard, which is correct and complete. We argue that these assumptions are invalid for corpus-based NLG, especially when using crowd-sourced datasets. Grammarbased metrics, on the other hand, do not rely on human-generated references and are not influenced by their quality. However, these metrics can be easily manipulated with grammatically-correct and easily-readable output that is unrelated to the input. To merge the advantages of WBMs and GBMs, we present a combined model, RAINBOW, which significantly improves correlation with human ratings."
760,1,5.0,4.0,4.0,True,acl_2017,train,"Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classification to machine translation to automatic question answering. Despite their early promise, many recurrent models have to read the whole text sequentially, making it difficult to apply them to long documents. For example, it is difficult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text non-sequentially, thereby skipping irrelevant information if needed. The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text. We employ a standard policy gradient method to train the model to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q&A, our proposed model, a modified non-sequential LSTM, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.","An important trait of all these models is that they read all the text available to them. In this paper, we consider the problem of understanding long documents with partial reading, and propose a modification to the basic neural architectures that allows them to read input text nonsequentially. The main benefit of this approach is faster inference because it skips irrelevant information. An unexpected benefit of this approach is that it also helps the models generalize better. In our approach, the model is a recurrent network, which learns to predict the number of jumping steps after it reads one or several input tokens. Such a discrete model is therefore not fully differentiable, but it can be trained by a standard policy gradient algorithm, where the reward can be the accuracy or its proxy during training. The green softmax are for jumping predictions. We only show case a) in this figure. ing speeds up its sequential counterpart by two to six times. Surprisingly, we also observe our model beats the standard LSTM in terms of accuracy. In summary, the main contribution of our work is to design an architecture that learns to read text non-sequentially and show that it is both faster and more accurate in practical applications of text processing. Our model is simple and flexible enough that we anticipate it be able to incorporate to recurrent nets with more sophisticated structures to achieve even better performance in the future. In this section, we introduce the proposed model named LSTM-Jump. We will first describe its main structure followed by the difficulty of estimating part of the model parameters because of non-differentiability. To address this issue, we appeal to a reinforcement learning formulation and adopt a policy gradient method. Before training, the number of jumps N allowed, the number of tokens read between every two jumps R and the maximum size of jumping K are chosen ahead of time. While K is a fixed parameter of the model, N and R are hyperparameters that can vary between training and testing. In the following, we describe in detail how the model operates when processing text. After stopping, the latest hidden state is further used for predicting desired targets. How to leverage the hidden state depends on the specifics of the task at hand. Once obtained, they can be used for inference. Therefore, we are tempted to formulate it as a reinforcement learning problem and apply policy gradient method to train the model. Optimizing this objective numerically requires computing its gradient, whose exact value is intractable to obtain as the expectation is over high dimensional interaction sequences. But in our case, we only consider final reward. It is equivalent to a special case that all intermediate rewards are identical and without discount. During inference, we can either use sampling or greedy evaluation by selecting the most probable jumping step suggested by the jump softmax and follows that path. In the our experiments, we will adopt the sampling scheme. In this section, we present our empirical studies to understand the efficiency of the proposed model in reading text. The tasks under experimentation are: synthetic number prediction, sentiment analysis, news topic classification and automatic question answering. Those are representative tasks in text reading involving different sizes of datasets and various levels of text processing, from character to word and to sentence. hierarchical structure to achieve higher accuracy than those presented below. However, this is orthogonal to the main focus of this work and would be left as an interesting future work. While K is fixed during both training and testing, we would fix R and N at training but vary their values during test to see how the change of parameters affects the result. Besides, the reported test time is measured by running one pass of the whole test set and the speedup is over the base LSTM model. First of all, we carry out the sanity check of whether LSTM-Jump is indeed able to learn how to jump if a very clear jumping signal is given in the text. We find that directly training the LSTM-Jump with full sequence is unlikely to converge, therefore, we adopt a curriculum training scheme. Whenever the training accuracy reaches a threshold, we shift to longer sequences. We also train an LSTM with the same curriculum training scheme to conduct the prediction. As the training size is huge, we do not need to worry about overfitting so dropout is not applied. In fact, we find that the training, validation and testing accuracies are almost the same. The first observation is that LSTM-Jump is faster than LSTM; the longer the sequence is, the more significant speedup LSTM-Jump can gain. The jumping level is word. quence. Thanks to this fact, the reading speed of LSTM-Jump is hardly affected by the length of sequence, but that of LSTM is linear with respect to length. Besides, LSTM-Jump also outperforms LSTM in terms of test accuracy under all cases. This is not surprising either, as LSTM has to read a large amount of tokens that are potentially not helpful and could interfere with the prediction. So in this section, we conduct sentiment analysis on two movie review datasets, both containing equal numbers of positive and negative reviews. In a nutshell, LSTM-Jump is always faster than LSTM under different combinations of R and N. At the same time, the accuracy is on par with that of LSTM. The jumping level is word. The various (R,N) combinations again display the trade-off between efficiency and accuracy. If one cares more about accuracy, then allowing LSTM-Jump to read and jump more times is a good choice. Otherwise, shrinking either one would bring a significant speedup though at the price of losing some accuracy. We now present results on testing the character level jumping with a news article classification problem. Not surprisingly, LSTM-Jump outperforms LSTM in terms of both efficiency and accuracy, although the advantage in speedup is not as significant as that in the previous tasks. This is mainly due to the fact that the embedding size and hidden are both much smaller than those used previously, and accordingly the processing of a token is much faster. By this cross-tasks comparison, we can see that the larger the recurrent neural network and the embedding are, the more speedup LSTM-Jump can gain, which is also confirmed by the task below. The last task is automatic question answering, in which we aim to test the sentence level skimming of LSTM-Jump. The models, LSTM or LSTM-Jump, firstly read the whole query, then the context sentences and finally output the predicted word. While LSTM reads everything, our jumping model would decide how many context sentences should skip after reading one sentence. The performance of LSTM-Jump is superior to LSTM in terms of both accuracy and efficiency under all settings in our experiments. The jumping level is sentence. The dominant performance of LSTM-Jump over LSTM might be interpreted as follows. After reading the query, both LSTM and LSTM-Jump know what the question is. On the contrary, the question can guide LSTM-Jump on how to read selectively and stop early when the answer is clear. Below is an example of how the model reads a test context given a query (bold face sentences are those read by our model in the increasing order). XXXXX is the missing word we want to fill. However, a major difference between those work and ours is that we have to sample from discrete jumping distribution, while they can sample from continuous distribution such as Gaussian. The difference is mainly due to the inborn characteristics of text and image. At the high-level, our model can be viewed as a simplified trainable Turing machine, where the controller can move on the input tape. It is worth noting that Zaremba and Sutskever report difficulty in using policy gradients to train their model. In this paper, we focus on learning how to skim text for fast reading. Such jumping behavior is modeled as a discrete decision making process, which can be trained by reinforcement learning algorithm such as REINFORCE. In four different tasks with six datasets, we test the efficiency of the proposed method on various levels of text jumping, from character to to word and to sentence. The results indicate our model is several times faster than, while the accuracy is on par with the baseline LSTM model. As an important future work, we hope to extend our model to a bidirectional jumping network, such that it can jump back and forth and possibly pay more attention to the important part of text."
326,2,3.57,5.0,4.0,True,acl_2017,train,"Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS). Most existingmethods focus on improve the performance for each single criterion. However, it is interesting to exploit these different criteria and mining their common underlying knowledge. In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge frommultiple heterogeneous segmentation criteria. Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each corpus obtains a significant improvement, compared to single-criterion learning.","Although there have been great achievements in building CWS corpora, they are somewhat incompatible due to different segmentation criteria. In a sense, it is a waste of resources if we fail to fully exploit these corpora. These methods adopted stacking or multi-task architectures and showed that heterogeneous corpora can help each other. However, most of these model adopt the shallow linear classifier with discrete features, which makes it difficult to design the shared feature spaces, usually resulting in a complex model. In this paper, we propose an adversarial multicriteria learning for CWS by integrating shared knowledge from multiple segmentation criteria. Finally, we exploit the eight segmentation criteria on the five simplified Chinese and three traditional Chinese corpora. Experiments show that our models are effective to improve the performance for CWS. The contributions of this paper could be summarized as follows. Chinese word segmentation task is usually regarded as a character based sequence labeling problem. There are lots of prevalent methods to solve sequence labeling problem such as maximum entropy Markov model (MEMM), conditional random fields (CRF), etc.     The role of feature layers is to extract features, which could be either convolution neural network or recurrent neural network. In neural models, the first step usually is to map discrete language symbols to distributed embedding vectors. We adopt bi-directional long short-term memory (Bi-LSTM) as feature layers. LSTM LSTM introduces gate mechanism and memory cell to maintain long dependency information and avoid gradient vanishing. Bi-LSTM In order to incorporate information from both sides of sequence, we use bi-directional LSTM (Bi-LSTM) with forward and backward directions. Although neural models are widely used on CWS, most of them cannot deal with incompatible criteria with heterogonous segmentation criteria simultaneously. Formally, assume that there areM corpora with heterogeneous segmentation criteria. The feature layers of these three models consist of a private (criterion-specific) layer and a shared (criterioninvariant) layer. The difference between three models is the information flow between the task layer and the shared layer. Besides, all of these three models also share the embedding layer. In the feature layer of Model-I, we regard the private layer and shared layer as two parallel layers. In the feature layer of Model-II, we arrange the shared layer and private layer in stacked manner. The private layer takes output of shared layer as input. The yellow blocks are the shared BiLSTM layer, while the gray block are the private Bi-LSTM layer. The yellow circles denote the shared embedding layer. The red information flow indicates the difference between three models. In the feature layer of Model-III, the shared layer and private layer are in stacked manner as ModelII. Additionally, we send the outputs of shared layer to CRF layer directly. The Model III can be regarded as a combination ofModel-I andModel-II. The parameters of the network are trained to maximize the log conditional likelihood of true labels on all the corpora. Although the shared-private model separates the feature space into shared and private spaces, there is no guarantee that sharable features do not exist in private feature space, or vice versa. We use a criterion discriminator which aims to recognizewhich criterion the sentence is annotated by using the shared features. Specifically, given a sentence X with length n, we refer to h(s)X as shared features for X in one of the sharing models. Therefore, we maximize the entropy of predicted criterion distribution when training shared parameters. Finally, we combine the task and adversarial objective functions. Nw and Nc indicate numbers of tokens and characters respectively. Dw and Dc are the dictionaries of distinguished words and characters respectively. Ns indicates the number of sentences. these datasets, AS, CITYU and CKIP are traditional Chinese, while the remains, MSRA, PKU, CTB, NCC and SXU, are simplified Chinese. Since the scale of each datasets varies, we use different training batch sizes for datasets. Only the performance on MSRA drops slightly. Compared to the baseline results (Bi-LSTM and stacked Bi-LSTM), the proposed models boost the performance with the help of exploiting information across these heterogeneous segmentation criteria. Although various criteria have different segmentation granularities, there are still some underlying information shared. By introducing adversarial training, the performances are further boosted, and Model-I is slightly better than Model-II and Model-III. The adversarial training tries to make shared layer keep criteria-invariant features. The reason may be that the shared parameters bias to other segmentation criteria and introduce noisy features into shared parameters. When we additionally incorporate the adversarial strategy, we observe that the performance on MSRA is improved and outperforms the baseline results. We could also observe the improvements on other datasets. However, the boost from the adversarial strategy is not significant. The main reason might be that the proposed three sharing models implicitly attempt to keep invariant features by shared parameters and learn discrepancies by the task layer. Traditional Chinese and simplified Chinese are two similar languages with slightly difference on character forms and usages on grammar. We investigate that if datasets in traditional Chinese and simplified Chinese could help each other. Specifically, we firstly train the model on simplified Chinese datasets, then we train traditional Chinese datasets independently with shared parameters fixed. To further explore the convergence speed, we plot the results on development sets through epochs. There are three blocks. The first block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of our proposed three models without adversarial training. The third block consists of our proposed three models with adversarial training. AS CKIP CITYU Avg. Here, we conduct Model-I without incorporating adversarial training strategy. same efficient. However, the time consumption of training process varies from model to model. Each point denotes a sentence, with the (x, y) values of each point denoting the F-measure scores of the two models, respectively. a) is comparison between Bi-LSTM andModel-I. b) is comparison between Bi-LSTM and Model-I with adversarial training. from creeping into shared space. For instance, the segmentation granularity of personal name is often different according to heterogenous criteria. With the help of adversarial strategy, our models could correct a large proportion of mistakes on personal name. There are many works on exploiting heterogeneous annotation data to improve various NLP tasks. structure-based stacking model to reduce the approximation error, which makes use of structured features such as sub-words. These models are unidirectional aid and also suffer from error propagation problem. These methods adopt the shallow classifiers, therefore suffering from the problem of defining shared features. Our proposed models use deep neural networks, which can easily share information with hidden shared layers. Unlike the above models, we design three sharing-private architectures and keep shared layer to extract criterion-invariance features by introducing adversarial training. Moreover, we fully exploit eight corpora with heterogeneous segmentation criteria to model the underlying shared information. In this paper, we propose adversarial multi-criteria learning for CWS by fully exploiting the underlying shared knowledge across multiple heterogeneous criteria. Experiments show that our proposed three shared-private models are effective to extract the shared information, and achieve significant improvements over the single-criterion methods."
12,2,4.0,4.0,3.43,True,acl_2017,train,"Extracting time expressions from free text is a fundamental task for many applications. We analyze the time expressions from four datasets and observe that only a small group of words are used to express time information, and the words in time expressions demonstrate similar syntactic behaviour. Based on the observations, we propose a type-based approach, named SynTime, to recognize time expressions. Specifically, we define three main syntactic types, namely time token, modifier, and numeral, to group time-related regular expressions over tokens. On the types we design simple heuristic rules to recognize time expressions. In recognition, SynTime first identifies the time tokens from raw text, then searches their surroundings for modifiers and numerals to form time segments, and finally merges the time segments to time expressions. As a light-weight rule-based tagger, SynTime runs in real time, and can be easily expanded by simply adding keywords for the text of different types and of different domains. Experiment results show that SynTime outperforms state-of-the-art methods on benchmark datasets and tweets data.","Rulebased time expression taggers could recognize most time expressions with carefully designed rules, while they could not recognize the time expressions that are not matched in any explicit rule. Machine learning based methods require training data for good performance, and may not recognize less frequent time expressions. From the analysis, we make four observations. Second, the vocabulary used to express time information is very small, with a small group of keywords. The last observation is that words in time expressions demonstrate similar syntactic behaviors. Time expression is part of language and acts as an interface of communication. Short expressions, small vocabulary, occurrence, and similar syntactic behaviors all reduce the cost of energy required to communicate. From free text SynTime first identifies time tokens, and then recognizes modifiers and numerals. Naturally, SynTime is a rule-based tagger. The key difference between SynTime and other rulebased taggers lies in the way of defining types and the way of designing rules. Other rule-based taggers define types for tokens based on their semantic meaning. Accordingly, other rule-based taggers design rules for each type based on their meanings and deal with each type separately. SynTime designs rules based on the token types and their relative positions in time expressions. That is why we call SynTime a type-based approach. More importantly, other rule-based taggers design rules in a fixed way, including fixed length and fixed position. In contrast, SynTime designs rules in a heuristic method based on the idea of boundary expansion. The heuristic rules are quite simple that it makes SynTime much more flexible, expansible, and extremely light-weight, leading SynTime to run in real time. We evaluate SynTime against three state-of-theart baselines, namely, HeidelTime, SUTime, and UWTime, on three datasets, namely, TimeBank, WikiWars, and Tweets. TimeBank and WikiWars are benchmark datasets for time expression extraction. On WikiWars, SynTime achieves comparable results. More importantly, SynTime achieves the best recalls on all three datasets and exceptionally good results on Tweets dataset. To sum up, we make the following contributions. We design SynTime based on the observations. SynTime provides an idea to simplify rule-based time tagger. The task is divided into two subtasks: Recognition and normalization. Rule-based Time Expression Recognition. It first identifies individual words, then expands them to chunks, and finally to time expressions. Rule-based taggers achieve very good results in TempEval exercises. SynTime is also a rule-based tagger while the key differences between SynTime and other rulebased taggers are the way of defining types and of designing rules. SynTime defines types for tokens according to their syntactic behaviors and designs rules in a heuristic way. Machine Learning based Method. Machine learning based methods extract features from the text and apply statistical models on the features for recognizing time expressions. The two methods explicitly use linguistic information. They use five time types and assign one of them to each word, which is similar to SynTime in the way of defining types over tokens. However, they focus only on the type of date, while SynTime can recoginize all the time expressions and does not involve learning and runs in real time. Time Expression Normalization. We conduct analysis on four datasets: TimeBank, Gigaword, WikiWars, and Tweets. From the four datasets, we analyze their time expressions and make four observations. Although the texts are collected from different sources (i.e., news articles, Wikipedia articles, and tweets) and vary in sizes, the length of time expressions follow a similar distribution. In informal communication people tend to use words in minimum length to express time information. On average, time expressions contain about two words. From the time expressions in all four datasets, we observe that the group of keywords used to express time information is small. Despite the different sizes in the four datasets, the numbers of distinct time tokens are comparable. This indicates that POS could not provide enough information to distinguish time expressions from common words. This observation indicates that for the time expressions, their similar constituents behave in similar syntactic way. When seeing this, we realize that this is exactly how linguists define part-of-speech for language. Time expression is part of language and acts as an interface of communication. Short expressions, small vocabulary, occurrence, and similar syntactic behaviors all reduce the cost of energy required to communicate. Shown in the left-hand side of the figure, SynTime is initialized with regular expressions over tokens. After initialization SynTime can be directly applied on text, without training. On the other hand, SynTime can be easily expanded by adding timerelated token regular expressions from training text. The expansion enables SynTime to recognize time expressions in text from different types and from different domains. Left-hand side shows the construction of SynTime, with initialization using token regular expressions, and optional expansion using training text. Right-hand side shows the main steps that SynTime recognizes time expressions. the time tokens from POS-tagged raw text. Then around the time tokens SynTime searches for modifiers and numerals to form time segments. In the last step, SynTime transforms the time segments to time expressions. Time Token. Modifier. Thus SynTime does not collect those prepositions. Numeral. SynTime Initialization. Specifically, we collect from SUTime only the tokens and the regular expressions over tokens, and discard its other rules of recognizing full time expressions. On the types, SynTime designs simple heuristic rules to recognize time expressions. Identifying Time tokens is simple and straightforward, through matching of string and regular expression. Some words might cause ambiguity. To filter out the ambiguous words, we use POS information. The task of time segment identification is to search the surrounding of each time token identified in previous step for the modifiers and numerals, then gather the time token with its modifiers and numerals to form a time segment. At first, each time token is a time segment. If it is either a PERIOD or DURATION, then no need to further search. Otherwise, search its left and its right for modifiers and numerals. For the left searching, if encounter a PREFIX or NUMERAL or IN ARTICLE, then continue searching. The labels above are from time segment identification; the labels below are for time expression extraction. searching, if encounter a SUFFIX or NUMERAL, then continue searching. The left searching will not exceed the previous time token; the right searching will not exceed the next time token. A special kind of time segments do not contain any time token; they depend on other time segments next to them. The task of time expression extraction is to extract time expressions from the identified time segments in which the core step is to determine whether to merge two adjacent or overlapping time segments into a new time segment. We scan the time segments in a sentence from beginning to the end. A stand-alone time segment is a time expression. The main focus is to deal with two or more time segments that are adjacent and overlapping. According to our time segment identification, the shared boundary could be a modifier or a numeral. SynTime expansion requires the words to be added to be annotated manually. We apply the initial SynTime on the time expressions from training text and list the words that are not covered. Whether the uncovered words are added to SynTime is manually determined. The rule for determination is that the added words can not cause ambiguity and should be generic. According to our observations, not many words are used to express time information, the manual addition of keywords thus will not cost much. In addition, we find that even in tweets people tend to use formal words. For SynTime we report the results of its two versions: SynTime-I and SynTime-E. SynTime-I is the initial version, and SynTime-E is the expanded version by adding keywords to SynTime-I. Datasets. We use two benchmark datasets, TimeBank and WikiWars, and one manually labeled Tweets dataset. The Tweets dataset is collected from Twitter. Baseline Methods. HeidelTime and SUTime both are rule-based methods, and UWTime is a learning method. The second setting achieves slightly better result and we report that result. Evaluation Metrics. This indicates that SynTime covers most of time tokens. On Tweets dataset, SynTime-I and SynTime-E achieve exceptionally good performance. and even in tweets people tend to use formal words. For precision, SynTime achieves comparable results in strict match and performs slightly poorer in relaxed match. It outperforms all the baseline methods. The reason is that for the rulebased time taggers, their rules are designed in a fixed way, lacking flexibility. Time expressions involve quite many changing numbers which in themselves affect the pattern recognition. One suggestion is to consider a type-based learning method that could use type information. POS is a kind of type information. But according to our analysis, POS could not distinguish time expressions from common words. Features need carefully designing. On WikiWars, SynTime-I achieves competitive results in both matches. Time expressions in WikiWars include lots of prepositions and quite a few descriptive time expressions. SynTime could not fully recognize such kinds of time expressions because it follows TimeML and TimeBank. The best results are in bold face and the second best are underlined. Some results are borrowed from their original papers and the papers are indicated by the references. Dataset Method Strict Match Relaxed Match Pr. Re. Re. This confirms the small size of time words and the high coverage of SynTime. On WikiWars, relatively more tokens are added, SynTime-E performs much better than SynTimeI, especially in recall. This indicates that with more words added from specific domains (e.g., WikiWars about war), SynTime can improve the performance. SynTime assumes that words are tokenized and POS tagged correctly. In reality, however, the tokenized and tagged words are not that perfect, due to the limit of used tools. The incorrect tokens and POS tags affect the result. We conduct an analysis on the time expressions from four datasets, and observe that time expressions in general are very short and expressed by a small vocabulary, and words in time expressions demonstrate similar syntactic behavior. Inspired by part-of-speech, based on the observations, we define a syntactic type system for the time expression, and propose a type-based time expression tagger, named by SynTime. SynTime defines syntactic types for tokens and on the types it designs simple heuristic rules based on the idea of boundary expansion. Experiments on three datasets show that SynTime outperforms the state-of-the-art baselines, including rule-based time taggers and machine learning based time tagger. As an extremely light-weight rule-based tagger, SynTime runs in real time. SynTime provides an idea to simplify the rule-based time tagger. In the future we will try our analytical method on other languages and other parts of language."
214,3,2.55,2.18,1.91,False,acl_2017,train,"Discourse structure analysis is helpful for the machine to identify different types of discourse writing styles, and lays the foundation for the study of discourse automatic generation. In this paper, after studying the difference and the relationship between micro and macro discourse structures, we explore the macro discourse structure, and put forward a Macro Chinese Discourse Treebank (MCDTB) on the top of existing Chinese Discourse Treebank (CDTB), by unifying micro and macro discourse structures. Specifically, at the micro level, we put forward the primary-secondary relationship from the logical semantic perspective, while at the macro level, we put forward the primarysecondary relationship from the pragmatic function perspective. Preliminary experimentation shows that our macro-micro unified schema is appropriate for the discourse structure analysis.","Here, the primary content refers to the dominant position in the discourse, and plays a decisive role in the part, while the secondary content refers to the auxiliary position, and does not play a decisive role in the discourse. In principle, the primary-secondary relationship plays a critical role in natural language processing, since the recognition of the discourse primarysecondary relationship not only helps to understand the discourse structure and semantics, but also provides strong support for deep natural language processing applications. recognition of the discourse primary-secondary relationship has become the bottleneck in discourse structure analysis recently, largely due to the ignorance of its critical role in discourse structure analysis by viewing it only as a dispensable component in the analysis of the discourse rhetorical structure. Generally speaking, there exist two hierarchical levels of discourse structures: micro level and macro level. While micro structure refers to the relationship between the internal structures in a sentence or two consecutive sentences, the macro structure refers to the relationship between sentences, paragraphs and chapters, at a higher level. Although RST establishes two different types of units, where nucleus are considered as the central parts, and satellites as peripheral ones. Although Van Dijk explores the macrostructure from both semantics and pragmatics, he focuses on the semantic aspect of the macrostructure, emphasizing that without the semantic macrostructure, it is unable to explain the meaning of the overall discourse and understand the partial coherence in the macrostructure. Unfortunately, no matter whether at micro level or at macro level, existing theories focus on logical semantics, with few from the pragmatic function perspective. In this paper, we regard the recognition of primary-secondary relationship as an independent task from the discourse rhetorical structure analysis, and integrate the overall discourse with the primary-secondary relationship as the unified carrier. For this purpose, a macro discourse structure representation schema is proposed. At the micro level, we lay on the logical semantic relationship between the discourse units based on the rhetorical structure, while at the macro level, we lay on the pragmatic function of the content based on the schematic structure. Specifically, our work expands the discourse analysis from intra-paragraph to the overall discourse. Preliminary experiments on the recognition of discourse relation and primarysecondary relation show that our macro-micro unified representation schema is appropriate for the discourse structure analysis tasks. In this section, we first summarize theories wildly employed in discourse structure, then we present existing approaches to discourse structure analysis. Nuclei are considered as the most important parts of text whereas satellites contribute to the nuclei and are secondary. The arrows point to the nuclei in the structure. A discourse can be expressed to a complete discourse structure tree based on the CDT representation schema. In this marcostructure, Pi represents a lower layer proposition, and Mkj represents a marcostructure unit, in which the superscript k represents the hierarchy, while the subscript j indicates the unit order on current layer. Two probabilistic models were employed that use syntactic and lexical information to segment and parse text. On the contrast of the researches at the micro level, very few corpus resources and computational models are proposed at the macro level. The overall discourse structure is relevant to the discourse genre and discourse pattern, thus discourse structures vary if the genres are different. For example, the news articles are commonly described in summary-story structure, and academic papers are consist of abstract, introduction, related work, experimentation, conclusion etc. while the court documents are recorded in the structure of in what way, for what reason, where, according to what inference etc. There are also different ways of discourse expression for the same genre, that is, different discourse patterns. The discourse pattern is the macro structure of text organization, and related to the specific environment and use habits, after repeated use. According to linguists, the common discourse patterns include Problem-Response Pattern, QuestionAnswer Pattern, Claim-Counterclaim Pattern, General-Particular Pattern etc. There are two main parts in the structure: summary and story. Summary contains headline and lead, while story includes situation and comments, and all these parts together constitute the macrostructure of news discourse through a top-down hierarchical structure. Such a schematic structure allows readers to quickly analyze and understand the discourse function in the communicative situation. We construct a top-down structure with multiple layers, composed of title, chapter, paragraph, microstructure, and elementary discourse unit. In general, there is only one layer of the title and the chapter, while the other structural layers are composed of multiple layers. The arrow direction indicates the primary unit of discourse relation. We expand the discourse analysis from intraparagraph to the overall discourse on the basis of original discourse structure analysis. At the micro level, we analyze the logical semantic relationship between the discourse units based on the rhetorical structure; at the macro level, we ana-lyze the function of the discourse content based on the schematic structure. In schematic structure, we annotate the function of each paragraph and unit group, while in rhetorical structure we focus on the rhetorical relation between discourse units. Unlike the definition in microstructure (the elementary units are treated as leaf nodes), on the macro layer, we directly treat the paragraphs which are natural segmented in the discourses as leaf nodes. Discourse relations connect discourse units, which are treated as non-leaf nodes in our macro discourse structure. A discourse relation generally includes two or more discourse units, these discourse units belong to the same relation layer. If one of the discourse units can generalize the intention and content of the relation layer, and can represent the relation layer connect to other layers, this discourse unit is a primary unit, while others are secondary ones. If the two or more discourse units are equally important, this relation is a multi-primary relation. For instance, in statementillustration relation, one of the discourse units is statement, while the other is illustration, the illustration unit is a service for the statement, so the statement-illustration relation is P-S relation; another example, in joint relation, there can be two or more discourse units, one or more discourse units may act as primary parts, so joint relation may be a P-S or S-P or MP relation. From the micro perspective, the primarysecondary relationship represents the relation between sentences or sentence groups, while from the macro perspective, the primary-secondary relationship represents the relation between paragraphs, chapters or discourses. Based on the arrow pointing in the discourse structure tree can also get the same conclusion. In the representation schema, the discourse is organized as a tree structure, in which paragraphs appear in the leaf nodes and the discourse relations appear in the non-leaf nodes. The tree structure is an appropriate representation of discourse structure, which not only expresses the hierarchical relationship of the discourse, but also expresses the primary-secondary relationship between the discourse units. Essentially, the depth of the hierarchical structure indicates the depth of the corresponding discourse semantic. Based on the micro rhetorical discourse structure, we convert the logical semantic relationship to pragmatic function relationship, and establish the schematic structure. We analyze and understand the discourse from a global perspective, and add the pragmatic function to both the leaf nodes and non-leaf nodes, which means all the discourse units on each layer. The functions of every discourse unit form a completed schematic structure tree. It is necessary to construct a micro and macro unified discourse corpus for discourse structure analysis. Preliminary experiments show that the presentation schema and corpus resource are appropriate to the macro discourse structure analysis. Because the discourse units are not isolated from the overall discourse, its difficult to judge whether the discourse units are important or not simply from the units themselves. It is necessary to have a comprehensive understanding of the overall article when judging the primarysecondary relations and functional roles. In determining primary-secondary relations, we should have a full understanding of the discourse intention and main content, and grasp the principle of local and global. Local principle refers to the primary units could summarize the main content and intention of its relation layer, and represent the relation layer relating to the context; global principle refers to the primary discourse units should conform to the main intention of the overall article. We annotate the discourse topic, lead, abstract, paragraph topics, discourse relations, primarysecondary relations, paragraph segments, and explore the relationship between microstructure and macrostructure. We employ a combination of top-down and bottom-up strategy in the annotation work. Such a top-down strategy can easily grasp the overall discourse structure, which consistent with the reading habit of human beings. Our annotation work shows the strategy is effective. Its very difficult to achieve high consistence because the judgments of relation and structure are very subjective. Our measurement data is only taken on the layer of leaf nodes. The statistics shows the relationship between discourse relation and primary-secondary relation is strong dependence, we can make further exploration on this phenomenon later. P-S: PrimarySecondary; S-P: Secondary-Primary; MP: MultiPrimary structure tree construction. In this section, we evaluated our Macro Chinese Discourse Treebank with the tasks of recognition of discourse relation and primary-secondary relation. We solve the imbalance data problem of primary-secondary relation recognition by resampling strategy, that is, re-sampling the data set of nucleus in latter to ten times. All our classifiers are trained using nltk package with default parameters, and experimented with maximum entropy classifier. The experimental results show that our corpus is helpful to the discourse structure analysis tasks. We explore to recognize the primary-secondary relationship which plays a critical role in discourse structure analysis, and view it as an independent task from the discourse rhetorical structure analysis. In particular, we expand the discourse structure analysis from intra-paragraph to the overall discourse, and propose a macro-micro unified discourse structure representation scheme, and describe the scheme in detail. Evaluation of the corpus resource on discourse relation recognition and primary-secondary relation recognition justifies the effectiveness of the representation schema and corpus resource. In the future work, we will enlarge the scale of the corpus and explore the various discourse patterns."
193,3,5.0,4.0,4.31,True,acl_2017,train,"We present the first parser for UCCA, a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work, and supports rapid annotation. UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. To our knowledge, the conjunction of these formal properties is not supported by any existing parser. Our transition-based parser, which uses a novel transition set and features based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle more general graph structures will inform the development of parsers for other semantic DAG structures, and in languages that frequently use discontinuous structures.","UCCA differs from syntactic schemes in terms of content and formal structure. It exhibits reentrancy, discontinuous nodes and non-terminals, which no single existing parser supports. We present the first UCCA parser, TUPA (Transition-based UCCA Parser), building on recent advances in discontinuous constituency and dependency graph parsing, and further introducing novel transitions and features for UCCA. We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. UCCA graphs are labeled, directed acyclic graphs (DAGs), whose leaves correspond to the tokens of the text. A node (or unit) corresponds to a terminal or to several sub-units (not necessarily contiguous) viewed as a single entity according to semantic or cognitive considerations. Edges bear a category, indicating the role of the sub-unit in the parent relation. Each scene contains one main relation (marked as either a Process or a State), as well as one or more Participants. Further categories account for interscene relations and the internal structure of complex arguments and relations (e.g. coordination, multi-word expressions and modification). One incoming edge for each non-root node is marked as primary, and the rest (mostly used for implicit relations and arguments) as remote edges, a distinction made by the annotator. The primary edges thus form a tree structure, whereas the remote edges enable reentrancy, forming a DAG. For instance, consider the following examples where the concept of a scene has a different rationale from the syntactic concept of a clause. First, non-verbal predicates in UCCA are represented like verbal ones, such as when they appear in copula clauses or noun phrases. One such property is reentrancy, namely the sharing of semantic units between predicates. Finally, unlike most dependency schemes, UCCA uses non-terminal nodes to represent units comprising more than one word. To our knowledge, no existing parser supports all structural properties required for UCCA parsing. We now turn to presenting TUPA. Building on previous work on parsing reentrancies, discontinuities and non-terminal nodes, we define an extended set of transitions and features that supports the conjunction of these properties. Some states are marked as terminal, meaning that G is the final output. During training, an oracle creates training instances for the classifier, based on gold-standard annotations. Transition Set.   wn, we predict a UCCA graph G over the sequence. Parsing starts with a single node on the stack (an artificial root node), and the input tokens in the buffer. LEFT-EDGEX and RIGHT-EDGEX create a new primary X-labeled edge between the first two elements on the stack, where the parent is the left or the right node, respectively. As a UCCA node may only have one incoming primary edge, EDGE transitions are disallowed if the child node already has an incoming primary edge. LEFTREMOTEX and RIGHT-REMOTEX do not have this restriction, and the created edge is additionally marked as remote. We distinguish between these two pairs of transitions to allow the parser to create remote edges without the possibility of producing invalid graphs. REDUCE pops the stack, to allow removing a node once all its edges have been created. Finally, FINISH pops the root node and marks the state as terminal. Classifier. To investigate the impact of the type of transition classifier in UCCA parsing, we experiment with three different models. The embeddings and classifier are trained jointly. It performed worse than the sparse perceptron model and was hence discarded. We write the stack with its top to the right and the buffer with its head to the left. i(x) is a running index for the created nodes. In addition to the specified conditions, the prospective child in an EDGE transition must not already have a primary parent. The BiLSTM runs on the input tokens in forward and backward directions, yielding a vector representation that is then concatenated with dense features representing the parser state (e.g., existing edge labels and previous parser actions; see below). This representation is then fed into a feedforward network similar to TUPAMLP. The feedforward layers, BiLSTM and embeddings are all trained jointly. Features. TUPASparse uses binary indicator features representing the words, POS tags, syntactic dependency labels and existing edge labels related to the top four stack elements and the next three buffer elements, in addition to their children and grandchildren in the graph. These embeddings are initialized randomly. We do not apply word dropout to the external word embeddings. Training. For example, the oracle would predict a NODE transition if the stack has on its top a parent in the gold graph that has not been created, but would predict a RIGHT-EDGE transition if the second stack element is a parent of the first element according to the gold graph and the edge between them has not been created. The transition predicted by the classifier is deemed correct and is applied to the parser state to reach the subsequent state, if the transition is included in the set of optimal transitions. G After L graduation P H Transition classifier After LSTM LSTM LSTM LSTM graduation LSTM LSTM LSTM LSTM to LSTM LSTM LSTM LSTM Paris LSTM LSTM LSTM LSTM.               Top: parser state (stack, buffer and intermediate graph). Bottom: TUPABiLTSM architecture. Vector representation for the input tokens is computed by two layers of bidirectional LSTMs. The vectors for specific tokens are concatenated with embedding and numeric features from the parser state (for existing edge labels, number of children, etc. and fed into the MLP for selecting the next transition. Data. Implementation. Unless otherwise noted, we use the default values provided by the package. See Appendix C for the hyperparameter values we found by tuning on the development set. Evaluation.   Performance on remote edges is of pivotal importance in this investigation, which focuses on extending the class of graphs supported by statistical parsers. These are identical to the original graphs, apart from the removal of remote edges. We note that the measure collapses to the standard PARSEVAL constituency evaluation measure if Gp and Gg are trees. Punctuation is excluded from the evaluation, but not from the datasets. Comparison to bilexical graph parsers. As no direct comparison with existing parsers is possible, we compare TUPA to bilexical dependency graph parsers, which support reentrancy and discontinuity but not non-terminal nodes. Comparison to tree parsers. In our setting, the conversion to trees consists simply of removing remote edges from the graph, and then to bilexical trees by applying the same procedure as for bilexical graphs. Baseline parsers. Default settings are used in all cases. The other parsers are greedy. DAGParser and UPARSE are most directly comparable to TUPASparse, as they also use a perceptron classifier with sparse features. TUPASparse considerably outperforms both, where DAGParser does not predict any remote edges in the out-ofdomain setting. TurboParser fares worse in this comparison, despite somewhat better results on remote edges. Using a feedforward NN and embedding features, TUPAMLP obtains higher scores than TUPASparse, but is outperformed by the LSTM parser on primary edges. Other configurations yielded lower scores. Columns correspond to labeled precision, recall and F-score, for both primary and remote edges. F-score upper bounds are reported for the conversions. For the tree approximation experiments, only primary edges scores are reported, as they are unable to predict remote edges. TUPABiLSTM obtains the highest F-scores in all metrics, surpassing the bilexical parsers, tree parsers and other classifiers. on both primary and remote edges, both in the in-domain and out-of-domain settings. Grammar-Based Parsing. Depending on the grammar and the implementation, such semantic parsers can support some or all of the structural properties UCCA exhibits. Nevertheless, this line of work differs from our grammarless approach in two important ways. First, the representations are different. UCCA does not attempt to model the syntax-semantics interface and is thus less coupled with syntax. Second, while grammar-based parsers explicitly model syntax, grammarless approaches, as presented here, directly model the relation between tokens and semantic structures. Broad-Coverage Semantic Parsing. Like UCCA parsing, SDP addresses a wide range of semantic phenomena, and supports discontinuous units and reentrancy. It also differs from UCCA in the type of distinctions it makes, which are more tightly coupled with syntactic considerations, where UCCA aims to capture purely semantic cross-linguistically applicable notions. UCCA reflects the difference between these constructions. Abstract Meaning Representation. Events in AMR may also be evoked by non-verbal predicates, including possessive constructions. Unlike in UCCA, the alignment between AMR concepts and the text is not explicitly marked. UCCA parsing does not require separately aligning the input tokens to the graph. TUPA creates non-terminal units as part of the parsing process. Furthermore, existing transition-based AMR parsers are not general DAG parsers. They are thus limited to a sub-class of AMRs in particular, and specifically cannot produce arbitrary DAG parses. Evaluated in in-domain and out-of-domain settings, we show that coupled with a NN classifier and BiLSTM feature extractor, it accurately predicts UCCA graphs from text, outperforming a variety of strong baselines by a margin. Our contribution to this literature is a general grammarless parser that supports multiple parents, discontinuous units and non-terminal nodes."
557,1,5.0,4.0,4.0,True,acl_2017,train,"Neural networks have recently shown promising results for relation extraction. State-of-the-art models cast the task as an end-to-end problem, solved incrementally using a local classifier. Yet previous work using statistical models have demonstrated that global optimization can achieve better performances compared to local classification. We build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn representations. Experiments show that our model is highly effective, achieving the best performances on two standard benchmarks.","State-of-the-art models cast the task as an end-to-end problem, solved incrementally using a local classifier. Yet previous work using statistical models have demonstrated that global optimization can achieve better performances compared to local classification. We build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn representations. Experiments show that our model is highly effective, achieving the best performances on two standard benchmarks. End-to-end learning prevents error propagation in the pipeline approach, and allows cross-task dependencies to be modeled explicitly for entity recognition. As a result, it gives better relation extraction accuracies compared to pipelines. This demonstrates the strength of neural representation learning for endto-end relation extraction. However, parsing errors can lead to encoding inaccuracies of tree-LSTMs, thereby hurting relation extraction potentially. Pretrained for parsing, such hidden layers contain rich syntactic information on each word, yet do not explicitly represent parsing decisions, thereby avoiding potential issues caused by incorrect parses. Our method is also free from a particular syntactic formalism, such as dependency grammar, constituent grammar or CCG, requiring only hidden representations on word that contain syntactic information. It consists of two sub-tasks, namely entity detection, which recognizes valid entities, and relation classification, which determines the relation categories over entity pairs. We follow recent studies and recognize entities and relations as one single task. Only the upper triangular table is necessary for indicating the relations. For example, the previous entity boundary label can be helpful to deciding the boundary label of the current word. During relation classification, the types of the entities involved can indicate the relation category between them. We exploit the diagonal label sequence of partial table T, which denotes entity boundaries of words, to enhance the representation learning. Each LSTM derives a sequence of hidden vectors for inputs. The segment representations can reflect entities in a sentence, and thus can be potentially useful for both entity detection and relation extraction. We use separate feature representations for entity detection and relation classification, both of which are induced according to the above three LSTM structures. In particular, we first extract a set of base neural features, and then concatenate them and feed them into a non-linear neural layer for entity detection and relation classification, respectively. The differences between relation classification with entity detection lie in the range of hidden layers from LSTMs. For relation classification between i and j, we split each LSTM into five segments according to the two entities ended with i and j. For the word LSTMs, we extract all five segment features, while the entity label LSTM, we only use the segment features of entityi and entityj. Here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures. The decoder can also leverage partially-parsed results, such as features from partial syntactic trees. Our method is to dump the encoder source representations of state-of-the-art parsers, and then use them directly as part of input embeddings in our proposed model. Features are extracted from the two new LSTMs in the same way as from the basic bi-directional word LSTMs. Our method can be easily generalized to the use of other parsers, which are potentially useful for our task as well. For example, we can use a constituent parser in the same way. We refer this training method as local optimization, because it maximizes the score of the gold-standard label at each step locally. During the decoding phase, the greedy search strategy is applied in consistence with the training. We study such models here under the neural setting. By this definition, we maximize the scores of all gold-standard partial tables. Again cross-entropy loss is used to perform model updates. The major challenge is to compute pT gi, because we cannot traverse all partial tables that are valid at step i, since their count increases exponentially by the step number. When each action of table filling is taken, all hypotheses in the agenda are expanded by enumerating the next labels, and the B highest-scored resulting tables are used to replace the agenda for the next step. Search begins with the agenda containing an empty table, and finishes when all cells of the tables in the agenda have been filled. All the hyper-parameters are tuned by development experiments. We select the best iteration model according to the development results. We consider the baseline system with no syntactic features using local training. Feature ablation experiments are conducted for the two types of features. The results demonstrate that the two types of new features we use are useful for relation extraction. We study the influence of training strategies for the relation extraction model without using syntactic features. Scheduled sampling achieves improved F-measure scores for the local model. With the same greedy search strategy, the globally normalized model gives slightly better results than the local model with scheduled sampling. The performance of the global model increases with a larger beam size. We examine the effectiveness of the proposed syntactic features. Our final model achieves the best performances on both datasets. We focus on two major contributions by our model, first examining the influences of global optimization, and then studying the gains by using the proposed syntactic features. Global optimization aims to find the best label sequences, rather than the best label locally at each step. Thus intuitively global optimization should give better accuracies at the sentence level. We verify this by examining the sentence-level accuracies, where one sentence is regarded as correct when all the labels in the resulted table are correct. level accuracies of the globally normalized model are consistently better than the local model. On the other hand, the accuracy decreases sharply as the sentence length increases, with the local model suffering more severely from larger sentences. To understand the effectiveness of the proposed syntactic features, we examine the relation Fscores with respect to entity distances. We verify whether the proposed syntactic features can benefit our model similarly. Several studies find that extracting entities and relations jointly can benefit both tasks. We exploit this segmental representation for relation extraction. Our work is in line with these efforts. To our knowledge, we are the first to apply globally optimized neural models for end-to-end relation extraction, achieving the best results on standard benchmarks. Feature representations are learned from several LSTM structures over the inputs, and a simple method is used to integrate syntactic information into our model without the need of parser outputs. In addition, global optimization is taken to make use of structural information more effectively. Compared with previous work, our final model achieved the best performances on two benchmark datasets."
107,2,4.0,4.0,3.43,True,acl_2017,train,"The state-of-the-art named entity recognition (NER) systems are supervised machine learning models that require large amounts of human-annotated data to achieve high accuracy. However, annotating NER data by human is expensive and time-consuming, and can be quite difficult for a new language. In this paper, we present two approaches for weakly supervised cross-lingual NER with no human annotation in a target language. The first approach is to create weakly labeled NER data for a target language via annotation projection on comparable corpora, where we develop a heuristic scheme that effectively selects good-quality weakly labeled data from noisy data. The second approach is to project distributed representations of words (word embeddings) from a target language to a source language, so that the source-language NER system can be applied to the target language without re-training. We also design two co-decoding schemes that combine the outputs of the two projection-based approaches in a smart way. We evaluate the performance of the proposed approaches on both in-house and open NER data sets for a number of target languages. The results show that the combined systems outperform two state-of-the-art cross-lingual NER approaches on the CoNLL test data.","NER provides essential inputs for many information extraction applications, including relation extraction, entity linking, question answering and text mining. Building fast and accurate NER systems is a crucial step towards enabling large-scale automated information extraction and knowledge discovery on the huge volumes of electronic documents existing today. To achieve high accuracy, an NER system needs to be trained with a large amount of human-annotated data, and is often supplied with language-specific resources (gazetteers, word clusters). Annotating NER data by human is rather expensive and time-consuming, and can be quite difficult for a new language. This places a big challenge in building NER systems of multiple languages for supporting multilingual information extraction applications. The difficulty of acquiring human-annotated multilingual NER data raises the following question: given a well-trained NER system in a source language (e.g., English), how can one go about extending it to a new language with decent performance and no human annotation in the target language? There are mainly two types of approaches for building weakly supervised cross-lingual NER systems. These approaches require parallel corpora between a source language (usually English) and a target language with alignment information. Note that the quality of the projection-labeled data heavily depends on the alignment accuracy of the parallel sentences, while this factor was not considered in the previous work when determining which sentence pairs should be selected for projection. The basic idea is to train a single NER system in the source language with language-independent features, so the system can be applied to other languages using those universal features. The crosslingual word clusters (together with part-of-speech tags) are then used to generate universal features. In this paper, we make the following contributions to weakly supervised cross-lingual NER with no human annotation in the target languages. First, for the annotation projection approach, we develop a heuristic data selection scheme that seeks to select good-quality projection-labeled NER data from comparable corpora. Experimental results show that the data selection scheme can significantly improve the accuracy of the targetlanguage NER system (compared with no data selection) when the alignment accuracy is low and the projection-labeled data are noisy. Second, we propose a new approach for direct NER model transfer based on representation projection. It projects word representations in vector space (word embeddings) from a target language to a source language, to create a universal representation of the words in different languages. Under this approach, the NER system trained for the source language can be directly applied to the target language without the need for re-training. Finally, we design two co-decoding schemes that combine the outputs (views) of the two projection-based systems to produce an output that is more accurate than the outputs of individual systems. We evaluate the performance of the pro-posed approaches on both in-house and open NER data sets for a number of target languages. We organize the paper as follows. This benefit, however, comes at a price. In contrast, the training time of order-o MEMMs is linear (O(M)) with respect to M independent of o, so it can handle larger training data with higher order of dependency. We have implemented a linear-chain CRF model and a general-order MEMM model. We use a locally normalized model (the conditional distribution is normalized per token as in MEMMs) and introduce context dependency by conditioning on the previously assigned tags. We use the target word and surrounding context as features. We do not employ other common features such as gazetteers or character-level representations as such features might not be readily available or might not transfer to other languages. We have deployed two neural network architectures. Both networks have one hidden layer, with sigmoid and softmax activation functions on the hidden and output layers respectively. Traditional annotation projection approaches require parallel corpora between a source language and a target language with alignment information. In this paper, we develop a heuristic scheme that can effectively select good-quality projectionlabeled NER data from noisy comparable corpora (not necessarily parallel corpora). We use English as the source language. Project the NER tags to the target-language sentence y using the alignment information. One can use those projection-labeled data to train an NER system in the target language. If some of the projection-labeled data have bad quality and we use all the data for weakly supervised learning, the accuracy of the target-language NER system will be adversely affected by those bad-quality data. We would like to design effective data selection schemes that can select goodquality projection-labeled data from noisy data, to improve the accuracy of the annotation projection approach for cross-lingual NER. We first design a metric to measure the annotation quality of a projection-labeled sentence in the target language. We construct a frequency table T which includes all the entities in the projectionlabeled target-language sentences. We use the frequency T(e, l) to measure the reliability of labeling entity e with tag l in the target language. Estados Unidos means United States. The correct NER tag for Estados Unidos is GPE which has the highest frequency in the weakly labeled data. tags in the projection-labeled data, it is more likely that the annotation is correct, because the source (English) NER system and the alignment system are independent statistical models and it is less likely that both systems make consistent errors simultaneously generating the same wrong tag. We use q(y) to measure the annotation quality of sentence y. In addition, we use n(y) to measure the amount of annotation information contained in sentence y. We can tune the two parameters to make tradeoffs among the annotation quality of the selected sentences, the annotation information contained in the selected sentences, and the total number of sentence selected. For different target languages, we use the same source (English) NER system for annotation projection, so the differences in the improvements are mainly due to the alignment accuracy of the comparable corpora between English and different target languages. In this section, we present a new approach for direct NER model transfer based on representation projection. Under this approach, we train a single English NER system that uses only word embeddings as input representations. We create mapping functions which can map words in any language into English and we simply use the English NER system to decode. In particular, by mapping all languages into English, we are using one universal NER system and we do not need to re-train when a new language is added. We first build vector representations of words (word embeddings) for a language from monolingual data. For a target language f, we first extract a small training dictionary from a phrase table that includes word-to-word alignments between English and the target language. Using Mfe, for any new word in the target language with vector representation v, we can project it into English as the vector Mfev. The training dictionary plays a key role in finding an effective cross-lingual embedding mapping. To control the size of the dictionary, we only include word pairs with a minimum frequency threshold. Model transfer is achieved simply by projecting the target language word embeddings into the English vector space and decoding these using the English NER system. The English NER system is then applied on the projected input to produce the NER tags. Words which are not in the target vocabulary are projected into their English embeddings if found in the English vocabulary, or into an NER-trained unk vector otherwise. Many ways of obtaining cross-lingual embeddings have been proposed in the literature, following one of two main approaches. Another line of work builds inter-lingual representations simultaneously, often by generating mixed language corpora using the supervision at hand (aligned sentences, documents, etc.) We opt for the first solution in this paper because of its flexibility: we can map all languages to English rather than requiring separate embeddings for each language pair. Additionally we are able to easily add a new language without any constraints on the type of data needed. Note that although we do not specifically create inter-lingual representations, by training mappings to the common language, English, we are able to map words in different languages to a common space. We expect these ideas to carry over to the NER task, in particular due to the semantic nature of the task which may allow for more natural embeddingbased cross-lingual transfer. We have developed two weakly supervised systems for cross-lingual NER, which are trained with different data using different models (MEMM model for annotation projection and neural network model for representation projection). We would like to design a co-decoding scheme that can combine the outputs (views) of the two systems to produce an output that is more accurate than the outputs of individual systems. Since both systems are statistical models and can produce confidence scores (probabilities), a natural co-decoding scheme is to compare the confidence scores of the NER tags generated by the two systems and select the tags with higher confidences scores. However, confidence scores of two weakly trained systems may not be directly comparable, especially when comparing O tags with non-O tags. We consider an exclude-O confidence-based co-decoding scheme which we find to be more effective empirically. It is similar to the pure confidence-based scheme, with the only difference that it prefers a non-O tag to an O tag independent of the confidence scores. The representation projection system tends to have a more balanced precision and recall. Based on this observation, we develop a rank-based co-decoding scheme that gives higher priority to the annotation projection system. The scheme will first include all the entities detected by the annotation projection system. We have used various NER data sets for evaluation. The first group includes in-house humanannotated NER data for four languages: Japanese, Korean, German and Portuguese. The main motivation of deploying such a fine-grained entity type set is to build cognitive question answering applications on top of the NER systems. The development data are used for tuning the parameters of the models. The results show that the annotation projection approach (AP) has a relatively high precision and low recall. The rank-based codecoding scheme is more effective for combining the two projection-based approaches. We also provide the performance of supervised learning where the NER system is trained with human-annotated NER data in the target language (with size shown in the bracket). Our systems outperform the previous two approaches, especially for German, and are closer to supervised learning. In this paper we developed two weakly supervised approaches for cross-lingual NER based on effective annotation and representation projection. We also designed two co-decoding schemes that combine the two projection-based systems in a smart way. Experimental results show that the combined system achieves higher accuracy than both systems and outperforms two state-of-the-art crosslingual NER approaches, providing a strong baseline for building cross-lingual NER systems with no human annotation in the target languages."
384,3,4.1,2.8,3.3,True,acl_2017,train,"We present a method for populating fine-grained classes (e.g., “1950s American jazz composers”) with instances (e.g., Charles Mingus). While stateof-the-art methods tend to treat class labels as single lexical units, the proposed method considers each of the individual modifiers in the class label relative to the head. An evaluation on the task of reconstructing Wikipedia category pages demonstrates a >10 point increase in AUC, over a strong baseline relying on widely-used Hearst patterns.","This assumption has several significant weakness. First, in order to recognize an instance of a class, these patternbased methods require that the entire class label be observed verbatim in text. The probability that a given label will appear in its entirety within one of the expected patterns is very low, even in large amounts of text. As a result, the number of meaning representations to be learned is exponential in the length of the class label, and quickly becomes intractable. Thus, compositional models of taxonomic relations are necessary for better language understanding. We introduce a compositional approach for reasoning about fine-grained class labels. Noun Phrase Interpretation. They focus exclusively on providing good paraphrases for an input noun compound. To our knowledge, ours is the first attempt to use these interpretations for the downstream task of IsA relation extraction. Semantic Taxonomy Induction. These IsA extraction techniques provide a key step for the more general task of knowledge base population. These approaches all share the limitation that, in practice, in order for a class to be populated with instances, the entire class label has to have been observed verbatim in text. This requirement limits the ability to handle arbitrarily fine-grained classes. Our work addresses this limitation by modeling fine-grained class labels compositionally. In formal semantics, modification is modeled as function application. Specifically, let MH be a class label consisting of a head H, which we assume to be a common noun, preceded by a modifier M. The properties entailed by the modifier are independent of the particular state of the world. Second, the modifier is a function that can be applied in a truth-theoretic setting. Computational Approaches. While the notion of modifiers as functions has been incorporated into computational models previously, prior work focuses on either assigning an intrinsic meaning to M or on operationalizing M in a truth-theoretic sense, but not on doing both simultaneously. That is, given a set of instances H and a modifier M, their method could return the subset MH. A contribution of our work is to model the semantics of M intrinsically, but in a way that permits application in the model theoretic setting. The supplementary material gives additional details. We instantiate O with an IsA repository constructed by applying Hearst patterns to the Web documents. Classes are represented as (non-disambiguated) natural language strings. The predicates are extracted as natural language strings. Subjects and objects may be either disambiguated entity references or natural language strings. Every tuple is included in both the forward and the reverse direction. E.g. These inverted predicates simplify the following definitions. Properties. Relating M to H Directly. In practice, when building property profiles, we do not require that the object of the fact tuple match the modifier exactly, as suggested in Eq. Specifically, rather than looking only at tuples in D in which the object matches M, we consider all tuples, but discount the weight proportionally to the similarity between M and the object of the tuple. the case. Instance finding. After finding properties that relate a modifier to a head, we turn to the task of identifying instances of fine-grained classes.   From here on, we use Mods to refer to our method which generates lists of instances for a class using Eq. Weakly Supervised Reranking. Eq. Thus, instances of H with overall higher counts in D receive high weights for every MH. We therefore train a simple logistic regression model to predict the likelihood that e belongs to MH. Evaluation Sets. We evaluate our models on their ability to return correct instances for arbitrary class labels. curated list of links to other pages which fall under the category. We collect the titles of all Wikipedia category pages, removing those in which the last word is capitalized or which contain fewer than three words. These heuristics are intended to retain compositional titles in which the head is a single common noun. We also remove any titles which contain links to sub-categories. The first sample is chosen uniformly at random (denoted UniformSet).  MkH is proportional to the total number of class labels in whichH appears as the head. Baselines. Our simplest baseline ignores modifiers altogether, and simply assumes that any instance of H is an instance of MH, regardless of M. We refer to this baseline simply as Baseline.  MkH. We refer to this baseline as Hearst. Compositional Models. As a baseline compositional model, we augment the Hearst baseline via set intersection.  MkH, if each of the MiH appears in O independently, we take the instances of C to be the intersection of the instances of each of the MiH. We assign the weight of an instance e to be the sum of the weights associated with each independent modifier. We also try using the proposed methods to extend rather than replace the Hearst baseline. the score of an instance is the inverse of the sum of its ranks in each of the input lists. If an instance does not appear at all in an input list, its rank in that list is set to a large constant value. Precision and Coverage. Lists shown are from ModsI. Strikethrough denotes incorrect. non-instances. We report total coverage, the number of labels for which the method returns any instance, and correct coverage, the number of labels for which the method returns a correct instance. For precision, we compute the average precision (AP) for each class label. We report mean average precision (MAP), which is the mean of the APs across all the class labels. MAP is only computed over class labels for which the method returns something, meaning methods are not punished for returning empty lists. the proposed method does not cause a precision drop on classes covered by the base-line. In addition, there are many classes for which the baseline is not able to extract any instances, but the proposed method is. Therefore, going forward, we only report results using the reranking model (i.e. ModsH and ModsI will refer to ModsH RR and ModsI RR, respectively). Manual Re-Annotation. It possible that true instances of a class are missing from our Wikipedia reference set, and thus that our precision scores underestimate the actual precision of the systems. We choose class labels for which Hearst was able to return at least one instance, in order to ensure reliable precision estimates. b) Weighted random sample (WeightedSet). Given a ranked list of instances, ROC curves plot true positives vs. false positives retained by setting various cutoffs. original Wikipedia list of instances and against our manually-augmented list of gold instances. The overall ordering of the systems does not change, but the precision scores increase notably after re-annotation. We continue to evaluate against the Wikipedia lists, but acknowledge that reported precision is likely an underestimate of true precision. Precision-Recall Analysis. We next look at the precision-recall tradeoff in terms of the area under the curve (AUC) achieved when each method attempts to rank the complete list of candidate instances.  MkH, proposes every instance of the head H as a candidate). The requirement by Hearst that class labels appear in full in a single sentence results in very low recall, which translates into very low AUC when considering the full set of candi-date instances. AUC captures tradeoff between true and false positives. We have presented an approach to IsA extraction which takes advantage of the compositionality of natural language. Existing approaches often treat class labels as atomic units which must be observed in full in order to be populated with instances. As a result, current methods are not able to handle the infinite number of classes describable in natural language, most of which never appear in text. Our method reasons about each modifier in the label individually, in terms of the properties that it implies about the instances. This approach allows us to harness information that is spread across multiple sentences, significantly increasing in the number of finegrained classes which we are able to populate."
691,1,2.0,4.0,2.0,True,acl_2017,train,"Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive embeddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.","When pre-trained on a large corpus of unlabeled text, they provide an effective mechanism for generalizing statistical models to words which do not appear in the labeled training data for a downstream task. In this paper, we make the following distinction between types and tokens: By word types, we mean the surface form of the word, whereas by tokens we mean the instantiation of the surface form in a context. However, a fundamen-tal flaw in this design is their inability to distinguish between different meanings and abstractions of the same word. However, it is still not clear how to use a lexical ontology to derive context-sensitive token embeddings. In addition to providing context-sensitive token embeddings, the proposed method implicitly regularizes the embeddings of related words by forcing related words to share similar concept embeddings. As a result, the representation of a rare word which does not appear in the training data for a downstream task benefits from all the updates to related words which share one or more concept embeddings. Solid arrows represent possible senses and dashed arrows represent hypernym relations. as WordNet, but it does not require a tagger for word senses. In this section, we focus on defining our contextsensitive token embeddings. We first describe our grounding of word types using WordNet concepts. Then, we describe our model of contextsensitive token-level embeddings as a weighted sum of WordNet concept embeddings. We use WordNet to map each word type to a set of synsets, including possible generalizations or abstractions. To ground a word type, we identify the the set of (direct and indirect) hypernyms of the WordNet senses of that word. This grounding is key to our model of token embeddings, to be described in the following subsections. Our goal is to define a context-sensitive model of token embeddings which can be used as a dropin replacement for traditional type-level word embeddings. Notation. Embedding model. Unlike most embedding models, the token embeddings ui are not parameters. Note that this is defined for each word type (wi), and is shared across all tokens which have the same word type. The parameterization of the sense prior is similar to an exponential distribution since WordNet senses are organized in descending order of their frequency. It scores each concept in the WordNet grounding of wi by feeding the concatenation of the concept embedding and a dense vector that summarizes the textual context into a multilayer perceptron (MLP) with two tanh layers followed by a softmax layer. In the following section, we describe our model for predicting PP attachments, including our definition for context. The accuracy of a explicitly represent the importance of each item in a sequence, it can also be applied to non-sequential items.   At test time, we predict the candidate head with the highest probability according to the model in Eq. The main difference is that we replace the input features for each token with the output bi-RNN vectors. We now describe the difference between the proposed model and the baseline. Baseline model. In the baseline model, we use type-level word embeddings to represent the input vector lstm in(ti) for a token ti in the sequence. The word embedding parameters are initialized with pre-trained vectors, then tuned along with the parameters of the bi-LSTM and MLPattach. We call this model LSTM-PP. Proposed model. The context used for the attention component is simply the hidden state from the previous timestep. However, since we use a bi-LSTM, the model essentially has two RNNs, and accordingly we have two context vectors, and associated attentions. We call this model OntoLSTM-PP. Dataset and evaluation. The head words are either nouns or verbs and the dependent is always a noun. All examples in this dataset have at least two candidate head words. The RRR dataset is a binary classification task with exactly two head word candidates in all examples. The context for each example in the RRR dataset is also limited which defeats the purpose of our context-sensitive embeddings. Model specifications and hyperparameters. For each word type, we use a maximum of S senses and H indirect hypernyms from WordNet. We also used the development set to tune the number of layers in MLPattach separately for the OntoLSTM-PP and LSTM-PP, and the number of layers in the attention MLP in OntoLSTM-PP. When a synset has multiple hypernym paths, we use the shortest one. Finally, words types which do not appear in WordNet are assumed to have one unique sense per word type with no hypernyms. Since the POS tag for each word is included in the dataset, we exclude WordNet synsets which are incompatible with the POS tag. Baselines. In contrast, we do not use syntactic context, VerbNet and distance information, and do not explicitly encode POS information. This result illustrates that our approach of dynamically choosing a context sensitive distribution over synsets is a more effective way of making use of WordNet. Effect on dependency parsing. We note there is a relatively small difference in unlabeled attachment accuracy for all dependencies (not only PP attachments), even when gold PP attachments are used as additional features to the parser. an effective approach. Effect of context sensitivity and sense priors. We now show some results that indicate the relative strengths of two components of our contextsensitive token embedding model. but still grounded in WordNet. As it can be seen, removing context sensitivity has an adverse effect on the results. This illustrates the importance of the sense priors and the attention mechanism. This result illustrates the regularization behavior of sharing concept embeddings across multiple words, which is especially important for rare words. Effect of training data size. To test this hypothesis, we trained the two models with different amounts of training data and measured their accuracies on the test set. As expected, the gap tends to be larger at lower data sizes. The fact that LSTM-PP is overfitting the training data more, indicates the regularization capability of OntoLSTM-PP. Qualitative analysis. To better understand the effect of WordNet grounding, we took a sample of sentences from the test set whose PP attachments were correctly predicted by OntoLSTM-PP but not by LSTM-PP. Words that are not candidate heads or dependents are shown in brackets. Model PPA Acc. In both cases, the weights assigned by OntoLSTM-PP to infrequent words are also shown. The word types soapsuds and buoyancy do not occur in the training data, but OntoLSTMPP was able to leverage the parameters learned for the synsets that contributed to their token representations. Another important observation is that the word type buoyancy has four senses in WordNet (we consider the first three), none of which is the metaphorical sense that is applicable to markets as shown in the example here. Selecting a combination of relevant hypernyms from various senses may have helped OntoLSTM-PP make the right prediction. This shows the value of using hypernymy information from WordNet. Moreover, this indicates the strength of the hybrid nature of the model, that lets it augment ontological information with distributional information. Parameter space We note that the vocabulary sizes in OntoLSTM-PP and LSTM-PP are comparable as the synset types are shared across word types. Since the biggest contribution to the parameter space comes from the embedding layer, the complexities of both the models are comparable. Implementation and code availability. Future work. This approach may be extended to other NLP tasks that can benefit from using encoders that can access WordNet information. Moreover, the methods described in this paper can be extended to other kinds of structured knowledge sources like Freebase which may be more suitable for tasks like question answering. However, the target of all these approaches is obtaining multi-sense word vector spaces, either by incorporating sense tagged information or other kinds of external context. The number of vectors learned is still fixed, based on the preset number of senses. In contrast, our focus is on learning a context dependent distribution over those concept representations. These approaches do not make use of lexical ontologies and is not amenable for joint training with a downstream NLP task. In contrast, our work focuses on estimating token-level word embeddings as context sensitive distributions of concept embeddings. There is a large body of work that tried to improve word embeddings using external resources. In contrast to previous work that was aimed at improving type level word representations, we propose an approach for obtaining context-sensitive embeddings at the token level, while jointly optimizing the model parameters for the NLP task of interest. In this paper, we proposed a grounding of lexical items which acknowledges the semantic ambiguity of word types using WordNet and a method to learn a context-sensitive distribution over their representations. We also showed how to integrate the proposed representation with recurrent neural networks for disambiguating prepositional phrase attachments, showing that the proposed WordNetgrounded context-sensitive token embeddings outperforms standard type-level embeddings for predicting PP attachments. We provided a detailed qualitative and quantitative analysis of the proposed model."
150,2,3.5,4.0,3.5,False,acl_2017,train,"Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models and conventional character-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Moreover, the final BLEU score of our model is comparable to the state-of-the-art systems. Further analyses show that our model is able to learn morphology.","Consider that a language itself is an evolving system. So it is impossible to cover all words in the language. The problem of rare words that are out of vocabulary (OOV) is a critical issue which can effect the performance of neural machine translation. However, the training becomes much harder and the vocabulary is often filled with many similar words that share a lexeme but have different morphology. There are many approaches to dealing with the out-of-vocabulary issue. Intuitively, it is elegant to directly model pure characters. However, as the length of sequence grows significantly, character-level translation models have failed to produce competitive results compared with word-based models. Especially, it is much difficult to train the attention component. However, they found it slow and difficult to train the character-level models, and one has to resort to layer-wise training the neural network and applying supervision for the attention component. In fact, such RNNs often struggle with separating words that have similar morphologies but very different meanings. In order to address the issues mentioned earlier, we introduce a novel architecture by exploiting the structure of words. It is built on two recurrent neural networks: one for learning the representation of preceding characters and another for learning the weight of this representation of the whole word. To decode at character level, we devise a hierarchical decoder which sets the state of the second-level RNN (character-level decoder) to the output of the first-level RNN (word-level decoder), which will generate a character sequence until generating a delimiter. In this way, our model almost keeps the same encoding length for encoder as word-based models but eliminates the use of a large vocabulary. Furthermore, we are able to efficiently train the deep model which consists of six recurrent networks, achieving higher performance. We show that the model achieves a high translation performance which is comparable to the state-ofthe-art neural machine translation model on the task of En-Fr, En-Cs and Cs-En translation. The experiments and analyses further support the statement that our model is able to learn the morphology. Neural machine translation is often implemented as an encoder-decoder architecture.             We consider two problems in the word-level neural machine translation models. First, how can we map a word to a vector? It is usually done by a lookup table (embedding matrix) where the size of vocabulary is limited. Second, how do we map a vector to a word when predicting? It is usually done via a softmax function. However, the large vocabulary will make the softmax intractable computationally. Accordingly, we propose a deep character-level neural machine translation model (DCNMT). Different combinations of morphemes lead to different meanings. Based on these facts, we introduce a word encoder to learn the morphemes and the rules of how they are combined. Thus learning morphology in a word encoder might speedup training. Each rt contains information about the preceding characters. We can regard the weight wi as the energy that determines whether ri is a representation of a morpheme and how it contributes to the representation of the word. Compared with an embedding lookup table, the decoupled RNNs learn the representation of morphemes and the rules of how they are combined respectively, which may be viewed as learning distributed representations of words explicitly. To decode at the character level, we introduce a hierarchical decoder. The first-level decoder is similar to RNNsearch which contains the information of the target word. Specifically, st in Eqn. The second-level decoder either continues the character-level states or resets using word-level states at boundaries. However, it will be intractable or inefficient to conditionally pick outputs from the the first-level decoder when training in batch manner. HGRU has a settable state and generates character sequence based on the given state until generating a delimiter. In our model, the state is initialized by the output of the first-level decoder.       In our model, we use a matrix to unfold the outputs of the first-level decoder, which makes the batch training process more efficient. After this procedure, we can compute the probability of each target character by the second-level decoder according to Eqns. It is possible to use multilayer recurrent neural networks to make the model deeper. The third layer is the first-level decoder. It takes the representation of previous target word as a feedback, which is produced by the target word encoder in our model. As the feedback is less important, we use an ordinary RNN to encode the target word. With such a hierarchical architecture, we can train our character-level neural translation model perfectly well in an end-to-end fashion. We first encode the source sequence as in the training procedure, then we generate the target sequence character by character based on the output st of the first-level decoder. The generation procedure will terminate once an end of sentence (EOS) token is produced. First we evaluate our model on English-to-French translation task where the languages are morphologically poor. In order to show the strengths of our model, we conduct on the English-to-Czech and Czech-to-English translation tasks where Czech is a morphologically rich language. The HGRUs with red border indicate that the state should be set to the output of the first-level decoder. We use the parallel corpora for two language pairs from WMT: En-Cs and En-Fr. In terms of preprocessing, we only apply the usual tokenization. We do not use any monolingual corpus. In our DCNMT model, it is reasonable to search directly on character level to generate a translation. Apart from measuring translation quality, we analyze the efficiency of our model and effects of character-level modeling in more details. The only difference between CNMT and DCNMT is CNMT uses an ordinary RNN to encode source words (takes the last hidden state). For each test set, the best scores among the models per language pair are bold-faced. Obviously, character-level models are better than the subword-level models, and our model is comparable to the start-of-the-art character-level models. Though our model consists of six RNNs, our model is the simplest and the smallest one in terms of the model size. In this section, we investigate whether our model could learn morphology. First we want to figure out the difference between an ordinary RNN word encoder and our word encoder. In contrast, the representations produced by our encoder are more reasonable and the words with similar meaning are closer. In order to obtain such meaningful representations of words, we need the explicit segmentation which indicates the boundary of words. Then we analyze how our word encoder learns morphemes and the rules of how they are combined. We report the BLEU scores of DCNMT when trained after one epoch in the above line and the final scores in the following line. The training efficiency of each model is compared in terms of training epochs and training days. More interestingly, we find that not only the ending letter has high energy, but also the beginning letter is important. Since there are no explicit delimiters, it may be more difficult to detect the subword units. Another advantage of our model is the ability to translate the misspelled words or the nonce words. The character-level model has a much better chance recovering the original word or sentence. a) Misspelled words Source For the time being howeve their research is unconclusive. LISA Pour le moment UNK leur recherche est UNK. b) Nonce words (morphological change) Source Then we will be able to supplement the real world with virtual objects in a much convenienter form. Reference Ainsi, nous pourrons complter le monde rel par des objets virtuels dans une forme plus pratique. Google translate Ensuite, nous serons en mesure de complter le monde rel avec des objets virtuels dans une forme beaucoup plus pratique. LISA Ensuite, nous serons en mesure de complter le vrai monde avec des objets virtuels sous une forme bien UNK. DCNMT Ensuite, nous serons en mesure de complter le monde rel avec des objets virtuels dans une forme beaucoup plus pratique. The word-level model is unable to recognize the misspelled words. Our model has a much better chance to recover the original word. correctly based on the morphemes and the rules. More sample translations are provided in the supplementary material. In this paper we have proposed an hierarchical architecture to train the deep character-level neural machine translation model by introducing a novel word encoder and a multi-leveled decoder. We have demonstrated the efficiency of the training process and the effectiveness of the model in comparison with the word-level and other characterlevel models. The BLEU score implies that our deep character-level neural machine translation model likely outperforms the word-level models and is competitive with the state-of-the-art character-based models. As a result of the character-level modeling, we have solved the out-of-vocabulary (OOV) issue that word-level models suffer from, and we have obtained a new functionality to translate the misspelled or the nonce words. More importantly, the deep character-level model is able to learn the similar embedding of the words with similar meanings like the word-level models. Finally, it would be potentially possible that the idea behind our approach could be applied to many other tasks such as speech recognition and text summarization."
516,2,4.5,3.5,4.0,True,acl_2017,train,"Interactive topic models are powerful tools for those seeking to understand large collections of text. However, existing sampling-based interactive topic modeling approaches scale poorly to large data sets. Anchor methods, which use a single word to uniquely identify a topic, offer the speed needed for interactive work but lack both a mechanism to inject prior knowledge and lack the intuitive semantics needed for user-facing applications. We propose combinations of words as anchors, going beyond existing single word anchor algorithms—an approach we call “Tandem Anchors”. We begin with a synthetic investigation of this approach then apply the approach to interactive topic modeling in a user study and compare it to interactive and non-interactive approaches. Tandem anchors are faster and more intuitive than existing interactive approaches. Topic models distill large collections of text into topics, giving a high-level summary of the thematic structure of the data without manual annotation. In addition to facilitating discovery of topical trends (Gardner et al., 2010), topic modeling is used for a wide variety of problems including document classification (Rubin et al., 2012), information retrieval (Wei and Croft, 2006), author identification (Rosen-Zvi et al., 2004), and sentiment analysis (Titov and McDonald, 2008). However, the most compelling use of topic models is to help users understand large datasets (Chuang et al., 2012). Interactive topic modeling (Hu et al., 2014) allows non-experts to refine automatically generated topics, making topic models less of a “take it or leave it” proposition. Including humans input during training improves the quality of the model and allows users to guide topics in a specific way, custom tailoring the model for a specific downstream task or analysis. The downside is that interactive topic modeling is slow—algorithms typically scale with the size of the corpus—and requires non-intuitive information from the user in the form of must-link and cannot-link constraints (Andrzejewski et al., 2009). We address these shortcomings of interactive topic modeling by using an interactive version of the anchor words algorithm for topic models. The anchor algorithm (Arora et al., 2013) is an alternative topic modeling algorithm which scales with the number of unique word types in the data rather than the number of documents or tokens (Section 1). This makes the anchor algorithm fast enough for interactive use, even in web-scale document collections. A drawback of the anchor method is that anchor words—words that have high probability of being in a single topic—are not intuitive. We extend the anchor algorithm to use multiple anchor words in tandem (Section 2). Tandem anchors not only improve interactive refinement, but also make the underlying anchor-based method more intuitive. For interactive topic modeling, tandem anchors produce higher quality topics than single word anchors (Section 3). Tandem anchors provide a framework for fast interactive topic modeling: users improve and refine an existing model through multiword anchors (Section 4). Compared to existing methods such as Interactive Topic Models (Hu et al., 2014), our method is much faster.","Including humans input during training improves the quality of the model and allows users to guide topics in a specific way, custom tailoring the model for a specific downstream task or analysis. We address these shortcomings of interactive topic modeling by using an interactive version of the anchor words algorithm for topic models. This makes the anchor algorithm fast enough for interactive use, even in web-scale document collections. Tandem anchors not only improve interactive refinement, but also make the underlying anchor-based method more intuitive. Anchor words make computing the topic matrix A tractable because the occurrence pattern of the anchor word mirrors the occurrence pattern of the topic itself.   Once we have the set of anchor words, we can compute the probability of a topic given a word (the inverse of the conditioning in A). Solving each row of C is fast and is embarrassingly parallel. The construction of Q requires only a single pass over the data and can be pre-computed for interactive use-cases. Once Q is constructed, topic inference scales with the size of Q which, in turn, depends on the square of the vocabulary size V. In contrast, traditional topic model inference typically requires multiple passes over the entire data. entire data. However, even if such techniques were to be adapted to incorporate human guidance, a single pass is not tractable for interactive use. Single word anchors can be opaque to users. Bizarre, low-to-mid frequency words are often anchors because anchor words must be unique to a topic; intuitive or high-frequency words cannot be anchors if they have probability in any other topic. Unfortunately, because these words might appear in multiple topics, individually they are not suitable as anchor words. Instead, we need to use sets of representative terms as an interpretable, parsimonious description of a topic. This section discusses strategies to build anchors from multiple words and the implications of using multiword anchors to recover topics. This extension not only makes anchors more interpretable but also enables users to manually construct effective anchors in interactive topic modeling settings. If we interpret the anchor algorithm geometrically, each row of Q represents a word as a point in V-dimensional space. Instead of individual anchor words (one anchor word per topic), we use anchor facets, or sets of words that describe a topic. While these new points do not correspond to words in the vocabulary, we can express nonanchor words as convex combinations of pseudowords. To construct these pseudowords from their facets, we combine the co-occurrence profiles of the facets. These pseudowords then augment the original cooccurrence matrix Q with K additional rows corresponding to synthetic pseudowords forming each of K multiword anchors. As before, V is the number of token types in the data and K is the number of topics. The first V rows of S correspond to the V token types observed in the data, while the additionalK rows correspond to the pseudowords constructed from anchor facets. We now describe more concretely how to combine an anchor facets to describe the cooccurrence pattern of our new pseudoword anchor. In tandem anchors, we create vector representations that combine the information from anchor facets.  GK, where Gk is a set of anchor facets which will form the kth pseudoword anchor.   gK, where gk is the pseudoword from Gk. Vector Average An obvious function for com-puting the central tendency is the vector average. Or-operator An alternative approach is to consider a cooccurrence with any anchor facet in Gk. Increasing the volume of the simplex spanned by the anchors explains more words. Element-wise Min Vector average and oroperator are both sensitive to outliers and cannot account for polysemous anchor facets. Using the intersection, the cooccurrence pattern of our anchor facet will only include terms relevant to camera bags. Harmonic Mean Leveraging the intuition that we should use a combination function which is both centralizing (like vector average) and ignores large outliers (like element-wise min), the final combination function is the element-wise harmonic mean. After constructing the pseudowords of S we then need to find the coefficients Ci,k which describe each word in our vocabulary as a convex combination of the multiword anchors. Our purpose is twofold. In addition, we examine running time of tandem anchors and compare to traditional model-based interactive topic modeling techniques. We cannot assume that we will have metadata available to build tandem anchors, but we use them here because they provide a high water mark without the variance introduced by study participants. To seed the tandem anchors, we use the titles of newsgroups. To build each multiword anchor facet, we split the title on word boundaries and expand any abbreviations or acronyms. We do not fully specify the topic; the title gives some intuition, but the topic modeling algorithm must still recover the complete topic-word distributions. This is akin to knowing the names of the categories used but nothing else. Critically, the topic modeling algorithm has no knowledge of document-label relationships. We do not aim for state-of-the-art accuracy, but the experiment shows title-based tandem anchors yield topics closer to the underlying classes than GramSchmidt anchors. Since the topics are fixed, this inference is very fast and can be parallelized on a perdocument basis. Finally, we infer topic assignments in the test data and evaluate the classification using those topicword features. For both training and test, we exclude words outside the LDA vocabulary. This is true regardless of the combiner function. For all metrics, the unsupervised Gram-Schmidt anchors do worse than creating anchors based on Newsgroup titles (for all metrics except VI, higher is better). For coherence, Gram-Schmidt does better than two functions for combining anchor words, but not the element-wise min or harmonic mean. Consequently, after building a confusion matrix between the predicted and true classes, external clustering metrics reveal confusion between classes. Adjusted Rand index also accounts for chance groupings of documents. Next we use Fmeasure, which also considers pairwise groups, balancing the contribution of false negatives, but without the true negatives. Finally, we use variation of information. Since we are measuring the amount of information lost, lower variation of information is better. As with accuracy, this is true regardless of which combination function we use. A smoothing parameter prevents zero logarithms. Although title-based anchor facets produce better classification features, topics from Gram-Schmidt anchors have better coherence than title-based anchors with the vector average or the or-operator. Both the vector average and the or-operator are swayed by large outliers, making them sensitive to ambiguous terms in an anchor facet. Element-wise min also has this robustness, but harmonic mean is also able to better characterize anchor facets as it has more centralizing tendency than the min. Tandem anchors will enable users to direct topic inference to improve topic quality. However, for the algorithm to be interactive we must also consider runtime. Longer waits can increase the cognitive load on the user and harm the user interaction. Only tandem anchors is fast enough to be interactive. Fortunately, the runtime of tandem anchors is amenable to interactive topic modeling. Furthermore, larger datasets typically have a sublinear increase in distinct word types, so we can expect to see similar run times, even on much larger datasets. Compared to other interactive topic modeling algorithms, tandem anchors has a very attractive run time. Utopian is much slower than tandem anchors. Even on the small InfoVisVAST dataset, Utopian takes nearly a minute to converge. While each of these interactive topic modeling algorithms do achieve reasonable topics, only our algorithm fits the run time requirements for interactivity. Furthermore, since tandem anchors scales with the size of the vocabulary rather than the size of the data, this trend will only become more pronounced as we increase the amount of data. Given high quality anchor facets, the tandem anchor algorithm can produce high quality topic models (particularly when the harmonic mean combiner is used). Furthermore, the tandem anchor algorithm is fast enough to be interactive (as opposed to model-based approaches such as the Interactive Topic Model). We now turn our attention to our main experiment: tandem anchors applied to the problem of interactive topic modeling. We compare both single word and tandem anchors in our study. We do not include the Interactive Topic Model or Utopian, as their run times are too slow for our users. To show that interactive tandem anchor words are fast, effective, and intuitive, we ask users to understand a dataset using the anchor word algorithm. For this user study, we recruit twenty participants drawn from a university student body. The student median age is twenty-two. Seven are female, and thirteen are male. However, this does not change our result. We give each a participant a high level overview of topic modeling. We also describe common problems with topic models including intruding topic words, duplicate topics, and ambiguous topics. Users are instructed to use their best judgement to determine if topics are useful. The task is to edit the anchor words to improve the topics. We asked that users spend at least twenty minutes, but no more than thirty minutes. Based on our results with title-based anchors, we use the harmonic mean combiner in our analysis. As before, we report not only accuracy, but also multiple clustering metrics using the confusion matrix from the classification task. Finally, we report topic coherence. While we only compare user generated anchors in our analysis, we include the unsupervised Gram-Schmidt anchors as a baseline. Some of the data violate assumptions of normality. Significant topics are likely skewed towards a few related words, so we measure the distance of each topic-word distribution from the uniform distribution over words. Topics which are close to the underlying word distribution of the entire data are likely to be vacuous, so we also measure the distance of each topic-word distribution from the underlying word distribution. Finally, background topics are likely to appear in a wide range of documents, while meaningful topics will appear in a smaller subset of the data. With single word anchors, topics with good coherence may still be too general. Tandem anchors enables users to produce topics with more specific word distributions which are better features for classification. We examine the qualitative differences between how users select multiword anchor facets versus single word anchors. Users who prefer editing multiword anchors over single word anchors often report that multiword anchors make it easier to merge similar topics into a single focused topic by combining anchors. For example, by combining multiple words related to Christianity, users were able to create a topic which is highly specific, and differentiated from general religion themes which included terms about Atheism and Judaism. This is in harmony with our quantitative measurements of topic coherence, and may be the result of our stopping criteria: when users judged the topics to be useful. Grahm-Schmidt anchors are provided as a baseline. For all metrics except VI, higher is better. Except for coherence, multiword anchors are best. In all cases higher is better. Multiword anchors produce topics which are more significant than single word anchors. Users are able to combine words to create more specific topics with tandem anchors. This was true regardless of whether we used tandem anchors or single word anchors. Our participants also produce fewer topics when using multiword anchors. In follow up interviews, participants indicate that the easiest way to resolve an ambiguous topic with single word anchors was to create a new anchor for each of the ambiguous terms, thus explaining the proliferation of topics for single word anchors. In contrast, fixing an ambiguous tandem anchor is simple: users just add more terms to the anchor facet. Tandem anchors extend the anchor words algorithm to allow multiple words to be combined into anchor facets. For interactive topic modeling, using anchor facets in place of single word anchors produces higher quality topic models and are more intuitive to use. Furthermore, our approach scales much better than existing interactive topic modeling techniques, allowing interactivity on large datasets for which interactivity was previous impossible."
239,2,2.14,3.0,2.57,False,acl_2017,train,"Maybe the single most important goal of representation learning is making subsequent learning faster. Surprisingly, this fact is not well reflected in the way embeddings are evaluated. In addition, recent practice in word embeddings points towards importance of learning specialized representations. We argue that focus of word representation evaluation should reflect those trends and shift towards evaluating what useful information is easily accessible. Specifically, we propose that evaluation should focus on data efficiency and simple supervised tasks, where the amount of available data is varied and scores of a supervised model are reported for each subset (as commonly done in transfer learning). In order to illustrate significance of such analysis, a comprehensive evaluation of selected word embeddings is presented. Proposed approach yields a more complete picture and brings new insight into performance characteristics, for instance information about word similarity or analogy tends to be non–linearly encoded in the embedding space, which questions the cosine–based, unsupervised, evaluation methods. All results and analysis scripts are available online.","Surprisingly, this fact is not well reflected in the way embeddings are evaluated. In addition, recent practice in word embeddings points towards importance of learning specialized representations. We argue that focus of word representation evaluation should reflect those trends and shift towards evaluating what useful information is easily accessible. Specifically, we propose that evaluation should focus on data efficiency and simple supervised tasks, where the amount of available data is varied and scores of a supervised model are reported for each subset (as commonly done in transfer learning). In order to illustrate significance of such analysis, a comprehensive evaluation of selected word embeddings is presented. All results and analysis scripts are available online. The importance of word representation learning has lead to developing multiple algorithms, but lack of principled evaluation hinders moving the field forward, which motivates developing more principled ways of evaluating word representations. Word embeddings are not only hard to evaluate, but also challenging to train. In the former approach embeddings are used in a downstream task (eg. POS tagging), while in the latter embeddings are tested directly for preserving syntactic of semantic relations. The most popular intrinsic task is Word Similarity (WS) which evaluates how well dot product between two vectors reproduce score assigned by human annotators. Intrinsic evaluations always assume a very specific model for recovering given property. Motivated by this, we propose an evaluation focused on data efficiency. To quantify precisely accessible information, we additionally propose focusing only on simple (supervised) tasks, as complex downstream tasks are challenging to interpret. Our main goal is to better align evaluation of word embeddings with their transfer application. Future performance is correlated with the amount of easily accessible and useful information. By easily accessible information, we mean information that model can quickly learn to use. Useful information is defined as one that correlates well with the final task. fer learning point of view. Therefore, measuring performance after seeing just a subset of the supervised dataset is crucial for comparing word embeddings. Another argument is the empiricial difference between how easily accessible is the information in various embeddings. As our experiment show, commonly used dense representations achieve different learning speeds. having enough training data makes embeddings dispensable. Second part of the proposal is to focus on simple supervised tasks to directly evaluate useful information content. We also confirm empirically that word embeddings trade off capacity between different information. In this work we pose hypothesis, that specialization of word embeddings can be best evaluated by checking what simple information is most easily recoverable. Importance of simple supervised tasks can be also seen in the light of algebraic structure that is encoded in word representation space. Thus, simple supervised tasks are closely aligned with the actual use of word embeddings and allow to quantify how quickly model can extract the most salient subspace (which leads to faster learning in general). Our final remark is about diversity of models. model which does not learn from the data. We argue that such evaluation is not generally informative. Each box plot represents a single tested embedding. problem, we should in theory fit all possible models and pick the one that has the best generalization capabilities. While this is impractical, it illustrates that fixing one specific model gives answer to a different question, thus drawing general conclusions from it can be highly biased. A good rule of thumb might be to include representatives of typical model classes, or at least match the model with class of models we are interested in (which rarely will be constant), which concludes our guidelines for a correct evaluation. We leave out details from the proposal how to order embeddings, as this is determined by the specific research question given evaluation should answer. A sensible default is to report AUC of learning curve for each task, and pick set of tasks that are most interesting to the researcher. examples following training data distribution. Further theoretical analysis is included in Appendix, and in the rest of the paper we present practical arguments for the proposed evaluation scheme. In this section we define specific metrics and tasks and perform exemplary evaluation of several pretrained embeddings in the advocated setting. The first question will aid understanding how useful are simple supervised tasks coupled with data efficiency. Second question shows that ranking of embeddings do change under transfer learning evaluation. In this paper we report only a subset of results with the most interesting conclusions, all results (along with code) are also made available online for further analysis. Rightmost plot deptics performance of the traditionally used constant model. While NMT has been reported to have strong performance on SimLex (as shown on the rightmost plot), its relative gains diminish under supervised version of the benchmark (leftmost and central plot). Analogy datasets are composed of quadruples (two pairs of words in a specific relation, for instance (king, queen, man, woman)). Similarity datasets are composed of pairs of words and assigned mean rank by human annotators. Sentence and Single word datasets have binary targets. Models for each datasets include both nonlinear and linear variants. WordRep dataset is a set of pairs which we split into disjoint sets. linear, for robustness we include in search a fallback to a simpler linear or constant model. Additionally, in the case of Similarity and Analogy we include commonly used constant models. Our objective was to cover representatives of embeddings emerging from both shallow and deeper architectures. Vectors were normalized to a unit length. Third column is rank computed by recommended default AUC of curve. As expected, when averaging over many tasks (of different dataset sizes), data efficiency is not changing final ordering (third column). On average rank increases as embeddings are becoming significantly different (as determined by ANOVA during rank computation). For each dataset we first randomly select test set and run evaluation for increasing sizes of training dataset, thus scores approximate generalization error after seeing increasing amounts of data. additional insights. Positive answer to this question motivates introduction of simple tasks with varying dataset size, ideally defined on single or pair of words. For solving Analogy we implemented a shallow neural network. Having learnable models for Similarity and Analogy datasets enables reusing many publicly available datasets in the new context. Also, we can robustly evaluate if given information about relation between two words is present in the embedding based on analogy questions. In the case of Similarity dataset the best performing model was Support Vector Regression, similarly as in Analogy datasets we also improve over the constant models. What is more, we can draw novel conclusions. Interesting example is NMT performance on SimLex. Third column is rank computed by recommended default AUC of curve. Interestingly, information stored in Single word datasets is easily accessible even with small amount of data. Second question was how stable are the orderings under growing dataset. This means that usually an embeddings has a changed rank after training, which establishes usefulness of measuring data efficiency for the tested embeddings. Interestingly, when averaged over many experiments, final ordering of embeddings tends to be similar, see Tab. none of embeddings is consistently more data efficient than others. Measuring data efficiency is crucial for a realistic (i.e. as close to application as possible) evaluation of representations. Another interesting point is that rank change after training on full dataset is relatively low for Single word datasets, which suggests that simple information about words like noun or verb is always quickly accessible to models, but more complicated information like relationships or similarities between pair of words are not, see Tab. Our last question was how stable is the ordering under changing model type. More specifically, are there embeddings especially fitted for use with linear models? Clearly some embeddings in fact are, see Fig. This is an important empirical fact for practictioners, which motivates including such evaluation in experiments. This is also visible when averaged over all categories, see Tab. Performed eperiments show usefulness of the additional and more granular level of analysis enabled. Unfortunately, there is already a large volatility of final embeddings ordering when using standard evaluation, and our proposed scheme at times makes it even more challenging to decide which embeddings are optimal. This hints, that purely unsupervised large scale pretraining might not be suitable for NLP applications. Most importantly, evaluation should be more targeted, either at some specific application area, or at specific properties of representation. Task categories from left to right are: Sentence (blue), Single word (green), Similarity (red) and Analogy (purple). Theoretically, tasks defined on single words should test for existence of such concepts, but in our case including (supervised) Analogy tasks was very useful, as those tasks are still very challenging for current embeddings. For Analogy tasks (see Fig. These higher order (or subspace) focused tasks are also well aligned with the application of word embeddings, because empirically models tend to focus on a small subspace in the vector space. As exemplified by experiments, proposed evaluation reveals differences between embeddings along usually overlooked dimensions: data efficiency, non-linearity of downstream model and simple supervised tasks (including recovery of higher order relations between words). Interesting new conclusions can be reached, including differences between different size GloVe embeddings or performance of non-linear models on similarity benchmarks. Additionally, obtained results reinforce conclusions from other published studies that there are no universally good embeddings and finding such might not be achievable, or a well posed problem. One should take great care when designing evaluation and specify what is the main focus. For instance, if the main goal of the word embeddings is to be useful in transfer, one should include advocated data efficiency metrics. New word embedding algorithms are moving away from typical pretraining scheme, with increasing focus on specialized word embeddings and applications under very limited dataset size, where fast learning is crucial. We hope that proposed evaluation methodology will help advance research in these scenarios. Let us first try to answer the question what is the information about the task and how can it be measured given some data representation. For simplicity let us assume that task is a binary classification, but the same reasoning applies to multiclass, multilabel, regressions etc. A quite natural, machine learning perspective is to define information stored as a Bayes risk of optimal model trained to perform this task. Obviously, raw representation already has some non-negative Bayes risk, which cannot be reduced during any embedding. Actually, it is quite easy to show, that nearly every embedding preserves all the information contained in the source representation. Every injective embedding preserves all the information about the task. Proof. Let us assume that Bayesian optimal classifier (the one obtaining the Bayes risk RX) on the input space X be called o. This remains an open question how frequent in general are injective embeddings. However in case of natural language processing (and many other fields), the input space is actually discrete or even finite. In such case, non-injective embeddings are rare phenomenon. In particular, for any finite set, probability of selecting at random linear projection which gives non-injective embedding is zero. The above reasoning is in some sense trivial, yet still worth underlying, as it gives an important notion of what should be measured when evaluating embeddings. Thus, to really distinguish various embeddings, we should rather ask what is the best achievable performance under limited amount of data or under constrained class of models, which is theoretical argument for data efficiency oriented evaluation. Let D be the dimension of a given word embedding. The model was trained with gradient descent optimization on minibatches. Hyperparameters: learning rate, number of epochs, optimizer, batch size and (boolean) fallback to constant model were chosen using cross-validation. It should also be noted that this approach turned out to be very computationally intensive."
444,3,3.67,2.67,2.33,False,acl_2017,train,"Language generation tasks that seek to mimic human ability to use language creatively are difficult to evaluate, since one must consider creativity, style, and other non-trivial aspects of the generated text. The goal of this paper is to develop evaluation methods for one such task, ghostwriting of rap lyrics, and to provide an explicit, quantifiable foundation for the goals and future directions of this task. Ghostwriting must produce text that is similar in style to the emulated artist, yet distinct in content. We develop a novel evaluation methodology that addresses several complementary aspects of this task, and illustrate how such evaluation can be used to meaningfully analyze system performance. We provide a corpus of lyrics for 13 rap artists, annotated for stylistic similarity, which allows us to assess the feasibility of manual evaluation for generated verse.","However, human beings also use language creatively, and for the language generation tasks that seek to mimic this ability, determining how accurately the generated text represents its target is insufficient, as one also needs to evaluate creativity and style. We believe that one of the reasons such tasks receive little attention is the lack of sound evaluation methodology, without which no task is well-defined, and no progress can be made. The goal of this paper is to develop an evaluation methodology for one such task, ghostwriting, or more specifically, ghostwriting of rap lyrics. Ghostwriting is ubiquitous in politics, literature, and music. The goal of ghostwriting is to present something in a style that is believable enough to be credited to the performer. The following are the main contributions of this paper. We present a comprehensive manual evaluation methodology of the generated verses along three key aspects: fluency, coherence, and style matching. Additionally, we believe that the annotation method we propose for manual style evaluation can be used for other similar generation tasks. One of the drawbacks of such work is a lack of systematic evaluation. The presence of a systematic formal evaluation process would lead to a clearer comparison between models and facilitate progress in this area of research. With this in mind, we make the interface used for style evaluation in this work available for public use. Our evaluation results highlight the truly multifaceted nature of the ghostwriting task. While having a single measure of success is clearly desirable, our analysis shows the need for complementary metrics that evaluate different components of the overall task. The coherence evaluation demonstrates the difficulty of incorporating large amounts of training data into the LSTM model, which intuitively would be desirable to create a flexible ghostwriting model. However, most of this work focuses on simple tasks, such as referring expressions generation. Perplexity is arguably the most popular automated metric for language models. Previous work that explores text generation for artistic purposes, such as poetry and lyrics, generally uses either automated or manual evaluation. Some heuristic-based automated approaches have also been used. Note that none of the work cited above provide a comprehensive evaluation methodology, but rather focus on certain specific aspects of generated verses, such as rhyme density or syntactic correctness. Furthermore, none of the aforementioned works implement models that generate complete verses at the token level (including verse structure), which is the goal of the models we aim to evaluate. In contrast to previous approaches that evaluate whole verses, our evaluation methodology uses a fine-grained, lineby-line scheme, which makes it easier for human annotators, as they no longer need to evaluate the whole verse at once. In addition, despite the fact the each line is annotated using a discrete scale, our methodology produces a continuous numeric score for the whole verse, enabling better comparison. Since the preprocessing was done heuristically, the resulting dataset may still contain some text that is not actual verse, but rather dialogue or chorus lines. We believe that adequate evaluation for the ghostwriting task requires both manual and automatic approaches. The automated evaluation methodology enables large-scale analysis of the generated verse. vocabulary, tone, and themes preferred by a particular artist. We have designed two annotation tasks for manual evaluation. The first task is to determine how fluent and coherent the generated verses are. The second task is to evaluate manually how well the generated verses match the style of the target artist. To assess fluency, we ask to what extent a given line can be considered a valid English utterance. Since a language model may produce highly disjointed verses as it progresses through the training process, we offer the annotator three options for grading fluency: strongly fluent, weakly fluent, and not fluent. The grade of not fluent is reserved for highly unintelligible text. To assess coherence, we ask the annotator how well a given line matches the preceding line. That is, how believable is it that these two lines would follow each other in a rap verse. We offer the annotators the same choices as in the fluency evaluation: strongly coherent, weakly coherent, and not coherent. During the training process, a language model may output the same line repeatedly. We account for this in our coherence evaluation by defining the consecutive repetition of a line as not coherent. This is important to define because the line on its own may be strongly fluent, however, a coherent verse cannot consist of a single fluent line repeated indefinitely. Style Matching The goal of the style matching annotation is to determine how well a given verse captures the style of the target artist. In this annotation task, a user is presented with an evaluation verse and asked to compare it against four other verses. The goal is to pick the verse that is written in a similar style. is always a verse from the same artist that was used to generate the verse being evaluated. The other three verses are chosen from the remaining artists in our dataset. Each verse is evaluated in this manner four times, each time against different verses, so that it has the chance to get matched with a verse from each of the remaining twelve artists. The generated verse is considered stylistically consistent if the annotators tend to select the verse that belongs to the target artist. To evaluate the difficulty of this task, we also perform style matching annotation for authentic verse, in which the evaluated verse is not generated, but rather is an actual existing verse from the target artist. Uniqueness of Generated Lyrics We use a modified tf-idf representation for verses, and calculate cosine similarity between generated verses and the verses from the training data to determine novelty (or lack thereof). In order to more directly penalize generated verses that are primarily the reproduction of a single verse from the training set, we calculate the maximum similarity score across all training verses. That is, we do not want generated verses that contain text from a single training verse, which in turn rewards generated verses that draw from numerous training verses. The point of an effective system is not to produce arbitrary rhymes: it is to produce rhyme types and rhyme frequency similar to the target artist. Rhyme density is able to capture this in a single metric, since the tool we use is able to detect these various forms of rhymes. Therefore, an objective automatic methodology is desirable. However, the rhyme detection tool is not designed to deal with highly repetitive text, which the LSTM model produces often in the early stages of training. Unfortunately, This approach is clearly not scalable, as it is not fully automatic. In order to fully automate this method, we propose to handle highly repetitive text by weighting the rhyme density of a given verse by its entropy. More specifically, for a given verse, we calculate entropy at the token level and divide by total number of tokens in that verse. Verses with highly repetitive text will have a low entropy, which results in down-weighting the rhyme density of verses that produce false positive rhymes due to their repetitive text. Merging Uniqueness and Similarity Since ghostwriting is a balancing act of the two opposing forces of textual uniqueness and stylistic similarity, we want a low correlation between rhyme density (stylistic similarity) and maximum verse similarity (lack of textual uniqueness). However, our goal is not to have a high rhyme density, but rather to have a rhyme density similar to the target artist, while simultaneously keeping the maximum similarity score low. As the model overfits the training data, both the value of maximum similarity and the rhyme density will increase, until the model generates the original verse directly. Therefore, our goal is to evaluate the value of the maximum similarity at the point where the rhyme density has the value of the target artist. We use regression lines for these points to identify the value of the maximum similarity line at the point where the rhyme density line has the value of the target artist. We give more detail below. The main generative model we use in our evaluation experiments is an LSTM. We refer the reader to the original work for a detailed description. Therefore, since each artist has a different training set, the number of iterations that constitutes a full epoch also varies. LSTM model generates a verse. At every value of n, we take the average rhyme density and maximum similarity score of the five verses that we generate to create a single data point for rhyme density and maximum similarity score, respectively. To enable comparison, we also create nine data points from the verses generated by the LSTM as follows. Each line is annotated by two annotators. For each annotated verse, we report the percentage of lines annotated as strongly fluent, weakly fluent, and not fluent, as well as the corresponding percentages for coherence. Coherence is calculated in a similar manner. Note that while the agreement is relatively low, it is expected, given the subjective nature of the task. The numbers above the bars reflect the total score of the artist (higher is better). The numbers above the bars reflect the total score of the artist (higher is better). Each page was annotated twice, by native Englishspeaking rap fans. We present two different views of the results. For each artist, we calculate their average rhyme density across all verses. We then use this value to determine at which iteration this rhyme density is achieved during generation (using the regression line for rhyme density). Next, we use the maximum similarity regression line to determine the maximum similarity score at that iteration. Low maximum similarity score indicates that we have maintained stylistic similarity while producing new, previously unseen lyrics. As a result, the rhyme density regression line hits the average rhyme density on a negative iteration. The lack of strong correlation supports the notion that different aspects of verse quality should be addressed separately. Moreover, the metrics are in fact complementary. Interestingly, the number of verses a rapper has in our dataset has a strong negative correlation with coherence score (cf. Therefore, it is easier for the former to produce more coherent lyrics since it saw more of the same patterns. As a result, models trained on a larger number of verses have a lower coherence score. Note that the fluency score does not have this negative correlation with the number of verses. However, these lines will not necessarily form a coherent verse if the artist has a large number of verses. The bold indicates the system with a lower similarity at the target rhyme density. Since verses vary in length, this correlation is not observed for verses. Finally, the lack of strong correlation with vocabulary richness suggests that token uniqueness is not as important as the sheer volume. In this paper, we have presented a comprehensive evaluation methodology for the task of ghostwriting rap lyrics, which captures complementary aspects of this task and its goals. We developed a manual evaluation method that assesses several key properties of generated verse, and created a data set of authentic verse, manually annotated for style matching. A previously proposed semiautomatic evaluation method has now been fully automated, and shown to replicate results of the original method. We have illustrated how the proposed evaluation methodology can be used to inspect an LSTM-based ghostwriter model. We believe our evaluation experiments also clearly demonstrate that complementary evaluation methods are required to capture different aspects of the ghostwriting task. Lastly, our evaluation provides key insights into future directions for generative models."
501,3,3.67,3.33,2.25,False,acl_2017,train,"We introduce a new multi-modal task for computer systems, posed as a combined vision-language comprehension challenge: identifying the most suitable text describing a scene, given several similar options. Accomplishing the task entails demonstrating comprehension beyond just recognizing “keywords” (or key-phrases) and their corresponding visual concepts. Instead, it requires an alignment between the representations of the two modalities that achieves a visually-grounded “understanding” of various linguistic elements and their dependencies. This new task also admits an easy-to-compute and wellstudied metric: the accuracy in detecting the true target among the decoys. The paper makes several contributions: an effective and extensible mechanism for generating decoys from (human-created) image captions; an instance of applying this mechanism, yielding a large-scale machine comprehension dataset (based on the COCO images and captions) that we make publicly available; human evaluation results on this dataset, informing a performance upper-bound; and several baseline and competitive learning approaches that illustrate the utility of the proposed task and dataset in advancing both image and language comprehension. We also show that, in a multi-task learning setting, the performance on the proposed task is positively correlated with the end-to-end task of image captioning.","This new task also admits an easy-to-compute and wellstudied metric: the accuracy in detecting the true target among the decoys. The paper makes several contributions: an effective and extensible mechanism for generating decoys from (human-created) image captions; an instance of applying this mechanism, yielding a large-scale machine comprehension dataset (based on the COCO images and captions) that we make publicly available; human evaluation results on this dataset, informing a performance upper-bound; and several baseline and competitive learning approaches that illustrate the utility of the proposed task and dataset in advancing both image and language comprehension. We also show that, in a multi-task learning setting, the performance on the proposed task is positively correlated with the end-to-end task of image captioning. There has been a great deal of interest in multimodal artificial intelligence research recently, bringing together the fields of Computer Vision and Natural Language Processing. This interest has been fueled in part by the availability of many large-scale image datasets with textual annotations. A central theme underlying these efforts is the use of natural language to identify how much visual information is perceived and understood by a computer system. In this paper, we argue for directly measuring how well the semantic representations of the visual and linguistic modalities align (in some abstract semantic space). Consequentially, computer systems that can learn to maximize and exploit such alignment should outperform those that do not. We take a two-pronged approach for addressing this issue. What makes the DMC task even more appealing is that it admits an easy-tocompute and well-studied performance metric: the accuracy in detecting the true target among the decoys. To this end, we render the DMC task as a classification problem, and incorporate it in a multitask learning framework for end-to-end training of joint objectives. Our empirical study shows that performance on the DMC task positively correlates with per-formance on the Image Captioning task. Both suggest the DMC task as a fruitful direction for future research. Image understanding is a long-standing challenge in computer vision. There has recently been a great deal of interest in bringing together vision and language understanding. Particularly relevant to our work are image captioning (IC) and visual question-answering (VQA). Both have instigated a large body of publications, a detailed exposition of which is beyond the scope of this paper. In IC tasks, systems attempt to generate a fluent and correct sentence describing an input image. IC systems are usually evaluated on how well the generated descriptions align with human-created captions (ground-truth). To provide visual grounding, image features are extracted and injected into the language model. Note that language generation models need to both decipher the information encoded in the visual features, and model natural language generation. In VQA tasks, the aim is to answer an input question correctly with respect to a given input image. Our DMC task is related but significantly different. Thus, to perform well, a computer system needs to understand both complex scenes (visual understanding) and complex sentences (language understanding), and be able to reconcile them. The DMC task admits a simple classificationbased evaluation metric: the accuracy of selecting the true target. While both share the idea of selecting captions from a large set, our framework has some important and distinctive components. First, we devise an algorithm for smart selection of candidate decoys, with the goal of selecting those that are sufficiently similar to the true targets to be challenging, and yet still be reliably identifiable by human raters. Second, we have conducted a thorough human evaluation in order to establish a performance ceiling, while also quantifying the level to which current learning systems underperform. We propose a new multi-modal machine comprehension task to examine how well visual and textual semantic understanding are aligned. Given an image, human evaluators or machines must accurately identify the best sentence describing the scene from several decoy sentences. Accuracy on this task is defined as the percentage that the true targets are identified. Thus, for any given image, it appears that one just needs to use the captions corresponding to other images as decoys. Specifically, our desideratum is to recruit challenging decoys that are sufficiently similar to the targets. Since we are also interested in human performance on this task, it is thus impractical to increase the number of decoys to raise the difficulty level of the task at the expense of demanding humans to examine tediously and unreliably a large number of decoys. In short, we need an automatic procedure to reliably create difficult sets of decoy captions that are sufficiently similar to the targets. We describe such a procedure in the following. While it focuses on identifying decoy captions, the main idea is potentially adaptable to other settings. Due to space limits, we omit a detailed introduction of the PV model. It suffices to note that the model outputs a continuouslyvalued embedding for a sentence, a paragraph, or even a document. The output of the algorithm is the dataset MCIC which is used for the DMC task. For an image with multiple groundtruth captions, we split it to multiple instances with the same image and one unique groundtruth caption per instance. The first two steps of the algorithm tune several hyperparameters. These hyperparameters are dataset-specific. Details are discussed in the next section. It accomplishes this by first extracting N candidates from the PV neighborhood of the ground-truth caption, excluding those that belong to the same image. In the inner for loop, it computes the similarity of each candidate to the ground-truth and stores them in a list A. If enough candidates are generated, the list is sorted in descending order of score. The embedding similarity, simPV, is computed as the cosine similarity between the two in the PV embedding space. The dataset, called MCIC, is made publicly available (address anonymized). We describe the details of this dataset below. The hyperparameters of the PV model, dim (embedding dimension) and epochs (number of training epochs), are optimized in the OPTIMIZE-PV step of the MC-IC Algorithm. The main idea is to learn embeddings such that ground-truth captions from the same image have similar embeddings. Details are in the Suppl. Details are in the Suppl. Material.   For each such instance, there is one and only one j such that the label is true. To ensure diversity, raters were prohibited from evaluating more than six instances or from responding to the same task instance twice. Raters were shown one instance at a time. They were shown the image and the five caption choices (ground-truth and four decoys) and were instructed to choose the best caption for the image. To supplement the instructions, raters were initially shown a few examples from the training set with the ground-truth caption highlighted, to illustrate how to discern the most appropriate caption for the image (see the Suppl. Material for details). Due to space limitation, more discussions about the human raters disagreement is available in the Suppl. Material. We describe several learning methods for the dual machine comprehension (DMC) task with the MCIC dataset. Regression. Linear classifier. Our next approach BaselineLinM is a linear classifier learned to discriminate true targets from the decoys. FFNN Model. We first employ the standard feedforward neural-network models to solve the MCIC task. DNN takes an image and outputs an image embedding vector. Our architecture uses a two hidden-layer fully connected network with Rectified Linear hidden units, and a softmax layer on top. The formula in Eq. We describe here a hybrid neural-network model that combines a recurrent neural-network with a feedforward one. We encode the image into a single-cell RNN encoder, and the caption into an RNN decoder. Material for an architecture illustration). In addition to the classification loss (Eq. In this sense, our model is an extension of the Show-and-Tell model with a single attention state representing the entire image, used as image memory representation for all decoder decisions. The hyper-parameter choices are decided using the hold-out development portion of the MCIC set. Evaluation metrics The metrics we use to measure performance come in two flavors. First, the accuracy in detecting (the index of) the true target among the decoys provides a direct way of measuring the performance level on the comprehension task. We use this metric as the main indicator of comprehension performance. We report the accuracies (and their standard deviations) for random choice, baselines, and neural network-based models. Nonlinear neural networks overcome these modeling limitations. The results clearly indicate their superiority over the baselines. In the Suppl. Material, we show the impact on performance of the mebedding dimension and neural-network sizes. That is, the ability to perform the comprehension task (as measured by the accuracy metric) positively correlates with the ability to perform other tasks that require machine comprehension, such as caption generation. However, our empirical results suggest that there is value in training models with a multi-task setup, in which both the comprehension side as well as the generation side are carefully tuned to maximize performance. We have proposed and described in detail a new multi-modal machine comprehension task (DMC). The underlying hypothesis is that computer systems that can be shown to perform increasingly well on this task will do so by constructing a visually-grounded understanding of various linguistic elements and their dependencies. The empirical results validate that improvements in comprehension and generation happen in tandem."
805,1,5.0,4.0,4.0,True,acl_2017,train,"Text similarity measures are used in multiple tasks such as plagiarism detection, information ranking and recognition of paraphrases and textual entailment. While recent advances in deep learning highlighted further the relevance of sequential models in natural language generation, existing similarity measures do not fully exploit the sequential nature of language. Examples of such similarity measures include ngrams and skip-grams overlap which rely on distinct slices of the input texts. In this paper we present a novel text similarity measure inspired from a common representation in DNA sequence alignment algorithms. The new measure, called TextFlow, represents input text pairs as continuous curves and uses both the actual position of the words and sequence matching to compute the similarity value. Our experiments on eight different datasets show very encouraging results in paraphrase detection, textual entailment recognition and ranking relevance.","While a big part of this content consists of visual information such as pictures and videos, texts also continue growing at a very high pace. These tasks also often target dedicated document collections for domain oriented research where text similarity measures can be directly applied. However, most of the recent advances focused on designing high performance classification methods, trained and tested for specific tasks and did not offer a standalone similarity measure that could be applied (i) regardless of the application domain and (ii) without requiring training corpora. The proposed method is tailored to this specific task, and the training models are not expected to perform well on different kinds of data such as sentences, questions or paragraphs. While such representation takes into account the actual positions of the words, it does not allow detecting subsequence matches and takes into account missing words only by omission. The sequential nature of natural language is taken into account mostly through word n-grams and skip-grams which capture distinct slices of the analysed texts but do not preserve the order in which they appear. In this paper, we use intuitions from a common representation in DNA sequence alignment to design of a new standalone similarity measure called TextFlow (XF). The proposed measure uses at the same time the full sequence of input texts in a natural sub-sequence matching approach together with individual token matches and mismatches. We use a similar dot matrix representation for text pairs and draw a curve oscillating around the diagonal (cf. The area under the curve is considered to be the distance between the two text pairs which is then normalized with the matrix surface. In this setting, the Y axis is the delta of positions of a word occurring in the two texts being compared. The semantics are: the bigger the area under curve is, the lower the similarity between the compared texts. With this representation, we are able to take into account all matched words and sub-sequences at the same time. XF computation is performed in O(nm) in the worst case where we have to check all tokens in the target text Y for all tokens in the input text X. XF is an asymmetric similarity measure. Its asymmetric aspect has interesting semantic applications as we show in the example below (cf. The minimum value of XF provided the best differentiation between positive and negative text pairs when looking for semantic equivalence (i.e., paraphrases), the maximum value was among the the top three for the textual entailment example. We conduct this comparison at a larger scale in the evaluation section. The best value per column is highlighted. The second best is underlined. Worst and second worst values are followed by one and two stars. We designed a neural network to perform this task, with a hidden layer dedicated to compute the exact XF value. We use identity as activation function in the dedicated XF layer in order to have a correct comparison with the other similarity measures, including canonical XF where the similarity value is provided in the input layer (cf. Datasets. Positive examples were collected from the titles and first sentences. Negative examples were collected from the same source by selecting consecutive sentences and random sentences. We discarded the contradiction pairs as they do not necessarily represent dissimilar sentences and are therefore a random noise w.r.t. our similarity measure evaluation. We combined the first two into the same positive category for our evaluation. Features. In the second part of the evaluation, we use neural networks to compare the efficiency of XFc, XFt and other similarity measures with in the same setting. Similarity Measures. Implementation. We also share the training sets used for both parameter training and evaluation. The initial parameters forXFt were chosen with a random function. Evaluation Measures. It also allows quantifying how consistently a system achieves high performance for the remaining systems. Canonical Text Flow. The best result is highlighted, the second best is underlined. The best result is highlighted, the second best is underlined. Trained Text Flow. Canonical XF was more consistent than trained XF on all dimensions except accuracy, for which XFt was optimized. These results also show that the actual positions difference is a relevant factor for text similarity. We explain it mainly by the natural flow of language where the important entities and relations are often expressed first, in contrast with a purely logical-driven approach which has to consider, for instance, that active forms and passive forms are equivalent in meaning. In additional experiments, we compared TFc and TFt with the other similarity measures when applied to bi-grams and tri-grams instead of individual tokens. This issue is not encountered with TextFlow as it relies on the token level, and such an insertion will not cancel or reduce significantly the gains from the correct ordering of the words. Also, in such setting, the impact of a sequence of N words will naturally increase or decrease according to the word weights (cf. We presented a novel standalone similarity measure that takes into account continuous word sequences. Evaluation on eight datasets show promising results for textual entailment recognition, paraphrase detection and ranking. Among the potential extensions of this work are the inclusion of different kinds of weights such as TF-IDF, embedding relatedness and semantic relatedness. We also intend to test other variants around the same concept, including considering the matched words and sequences to have a negative weight to balance further the weight of missing words."
741,2,5.0,4.0,4.0,True,acl_2017,train,"This paper presents a new graph-based approach that induces synsets using synonymy dictionaries and word embeddings. Firstly, we build a weighted graph of synonyms extracted from commonly available resources, such as Wiktionary. Secondly, we apply word sense induction to deal with ambiguous words. Finally, we cluster the disambiguated version of the ambiguous input graph into synsets. Our meta-clustering approach lets us use an efficient hard clustering algorithm to perform fuzzy clustering of the graph. Despite its simplicity, our approach shows excellent results outperforming five analogous state-of-the-art methods in terms of F-score on four different gold standard datasets for English and Russian derived from large-scale manually constructed lexical resources.","However, for most languages no manually-constructed resource comparable to the English WordNet in terms of coverage and quality is available. This lack of linguistic resources for many languages urges the development of new methods for automatic construction of WordNet-like resources. This problem stems from the fact that articles in Wiktionary and similar resources list undisambiguated synonyms. They are easy to disambiguate for humans while reading a dictionary article, but can be a source of errors for a language processing system. The contribution of this paper is a novel approach which resolves ambiguities of the input graph enabling fuzzy clustering. The method takes as an input synonymy relations between potentially ambiguous terms available in humanreadable dictionaries and transforms them into a disambiguated machine readable representation in the form of synsets. Our method, called WATSET, is based on a new meta-algorithm for fuzzy graph clustering. Besides, it outperforms analogous state-of-the-art methods for synset induction. In its core, BabelNet was obtained by mapping the Princeton WordNet and Wikipedia enhanced by the machine translation of the results. Later, other resources were mapped to this core, including Wiktionary and OmegaWiki. UBY has a similar architecture, but relies on similarity of dictionary definitions and existing cross-lingual links for mapping. Methods based on word sense induction try to induce sense representations without the need for any initial lexical resource by extracting semantic relations from text. Each cluster corresponds to a word sense. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters. In the case of WSI, such a network is a local neighbourhood of one word. Nodes of the ego network are the words which are semantically similar to the target word. In our approach, to induce synsets, we use word ego network clustering similarly as in word sense induction approaches, but apply them to the graph of semantically clean synonyms. Methods based on clustering of synonyms, such as our approach, induce the resource from an ambiguous graph of synonyms where edges were extracted from manually-created resources. According to the best of our knowledge, most experiments either used graph-based word sense induction applied to text-derived graphs or used a mapping-based method which already assumes availability of a WordNet-like resource. The algorithm starts by adding random noise to edge weights. Then, the approach launches Markov Clustering of this graph several times to estimate the probability of each word pair being in the same synset. In the nutshell, pairs of nodes are grouped if either node has a maximal affinity to the other. ing maximal nodes of each node. Next, all nodes are marked as root nodes. Finally, for each root node, the following procedure is repeated: all transitive children of this root form a cluster and marked as non-root nodes; a root node together with all its transitive children form a fuzzy cluster. MCL simulates random walks within a graph by alternation of two operators called expansion and inflation, which recomputes the class labels. At each iteration, the labels of all the nodes are updated according to the majority labels among the neighbouring nodes. While this method is only commonly used in social network analysis, we decided to add it to the comparison as synsets are essentially cliques of synonyms, and it is natural to try using an algorithm based on detection of cliques. The goal of the method is to induce a set of unambiguous synsets by grouping individual ambigu-ous synonyms. The method takes a dictionary of ambiguous synonymy relations and a text corpus as an input and outputs synsets. Note that the method can be used without a background corpus, yet as our experiments will show, corpusbased information improves the results, when utilizing it for weighting the word graph. A synonymy dictionary can be perceived as a graph, where the nodes correspond to lexical entries (words) and the edges connect pairs of the nodes when the synonymy relation between them holds. However, the hard clustering property of this algorithm does not handle polysemy: one word can have several senses but will be assigned to only one cluster. To deal with this limitation, a word sense induction procedure is used to induce senses for all words. Finally, the disambiguated word sense graph is clustered globally to induce the synsets from this disambiguated word graph. Since the different graph clustering algorithms are sensitive to edge weighing, we consider the distributional semantic similarity measures based on word embeddings as a possible edge weighing approach for our synonymy graph. As we show further, this approach yields the best results. The set of nodes V includes every lexeme appearing in the input synonymy dictionaries. As the graph G is likely to have polysemous words, the goal is to separate the individual word senses using graph-based word sense induction. In particular, removal of the nodes participating in many triangles tends to separate the original graph into several connected components. In our experiments, we test Chinese Whispers and Markov Clustering.     They are recovered by the disambiguation approach described below. The result of the previous step is splitting word nodes into (one or more) sense nodes. For recovering these sense labels of the neighbouring words, we employ the following sense disambiguation approach. As one may observe, disambiguation of the nearest neighbours is a necessity to be able to construct a global version of the sense-aware graph. We conduct our experiments on the data for two different languages. We evaluate our approach on two datasets for English to demonstrate its performance on a resource-rich language. Additionally, we evaluate it on two Russian datasets since Russian is a good example of an under-resourced language with a clear need for synset induction. streambank? riverbank? streamside? building? bank building? bank building? streambank? English. WordNet contains general vocabulary and appears to be de facto gold standard in similar tasks. Russian. The resource is constructed using crowdsourcing and mostly covers general vocabulary. Particularly, non-expert users are allowed to edit synsets in a collaborative way loosely supervised by a team of project curators. Due to the ongoing development of the resource, we selected as the gold standard only those synsets that were edited at least eight times in order to filter out noisy incomplete synsets. To evaluate the quality of the induced synsets, we transformed them into binary synonymy relations and computed precision, recall, and F-score on the basis of the overlap of these binary relations with the binary relations from the gold standard datasets. English. Russian. For each language, we constructed a synonymy graph using openly available language resources. Here, each bar corresponds to the top performance of each method. English. Only the words marked as English have been extracted. Russian. While these two latter resources are specific to Russian, Wiktionary is available for most languages. Note that the same input synonymy dictionary was used by authors of YARN to construct synsets using crowdsourcing. Therefore, results on the YARN dataset show how close an automatic synset induction method can approximate manually created synsets provided the same starting raw linguistic materials. In our experiments, we rely on our own implementation of MaxMax and ECO as reference implementations are not available. The first step being common for all of the tested synset induction methods is the graph construction. The results across various configurations and methods indicate that using the weights based on the similarity scores provided by word embeddings is the best strategy for all the methods but MaxMax on both English datasets. However, in these cases, its performance using the ones weighing does not exceed the other methods using the sim weighing. Therefore, we report all further results on the basis of the sim weights. All methods rely on the similarity edge weighting (sim); best configurations of each method in terms of F-scores are shown for each dataset. Results are sorted by F-score on WordNet, top three values of each metric are boldfaced. For each method, we show the best configurations in terms of F-score. Without this pruning, the MaxMax and CPM methods tend to discover giant components obtaining almost zero precision as we generate all possible pairs of nodes in such clusters. The other methods did not demonstrate such behavior. Also, it outperformed all other methods according to precision on both Russian datasets. The disambiguation of the input graph performed by the WATSET method splits nodes belonging to several local communities to several nodes significantly facilitating the clustering task otherwise complicated by the presence of the hubs that wrongly link semantically unrelated nodes. WATSET. Chinese Whispers, a simplified version of MCL, converges faster due to node label randomization which leads to a stricter stopping condition. CW thus does not amplify the hubs between the unrelated nodes and therefore produces smaller clusters in average. Using CW instead of MCL for word sense induction in WATSET expectedly produces finegrained senses. Interestingly, at the global clustering step, these senses erroneously tend to form coarse-grained synsets connecting unrelated senses of the ambiguous words. The MaxMax algorithm showed mixed results. On the one hand, it outputs large clusters uniting more than hundred nodes. The synsets produced on English datasets were even larger and did not pass, which resulted in low recall. MaxMax appeared to be extremely sensible to the edge weighing, which complicates its practical use. Such clusters have been automatically pruned, but the rest clusters connect virtually every node left after the pruning. This is confirmed by the high values of recall. As one increase the minimal number of elements in the clique k, precision effectively grows, but at the cost of a dramatic drop in recall. We suppose that the network structure assumptions exploited by CPM do not accurately model the structure of the synonymy graphs in the present task. Finally, the ECO method yielded the worst results because the most cluster candidates failed to pass through the constant threshold used for estimating whether a pair of words should be included in the same cluster. The remaining synsets for both languages have at most three words having been connected by a chance due to the edge noising procedure used in this method resulting in low precision. On difference in absolute scores. This difference stems from the difference of the gold standards. RuThes is more domain-specific in terms of vocabulary, so our input set of generic synonymy dictionaries has a limited coverage on this dataset. On the other hand, recall calculated on YARN reaching significantly higher levels as this resource was manually built on the basis of exactly the same initial resources. No single resource for Russian can obtain high precision scores on another one. On sparseness of the input dictionary. of various sizes for the best WATSET configuration on both languages. As one might observe, the quality of the results is highly plausible. However, one limitation of all approaches considered in this paper is the dependence on completeness of the input dictionary of synonyms. In some parts of the input synonymy graph, important bridges between words can be missing, leading to smallerthan-desired synsets. A promising extension of the present methodology is using the distributional models to enhance connectivity of the graph to further improve recall of the method by adding extra relations. In this paper, we presented a new robust approach to fuzzy graph clustering that relies on a hard clustering method. Using ego network clustering, the nodes belonging to several local communities are split into several nodes each belonging to one local community. The disambiguated graph contains fewer hubs connecting unrelated nodes from different communities and thus facilitates clustering. We apply this meta-clustering algorithm to the task of synset induction, obtaining the best results on two datasets for two different natural language in terms of precision and competitive results in terms of F-score, as compared to five state-ofthe-art graph clustering methods."
350,1,4.0,4.0,3.0,True,acl_2017,train,"Modern models of event extraction for tasks like ACE are based on supervised learning of events from small hand-labeled data. However, hand-labeled training data is expensive to produce, in low coverage of event types, and limited in size, which makes supervised methods hard to extract large scale of events for knowledge base population. To solve the data labeling problem, we propose to automatically label training data for event extraction via world knowledge and linguistic knowledge, which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically. The experimental results show that the quality of our large scale automatically labeled data is competitive with elaborately human-labeled data. And our automatically labeled data can incorporate with human-labeled data, then improve the performance of models learned from these data.","Although this paradigm was widely studied, existing approaches still suffer from high costs for manually labeling training data and low coverage of predefined event types. Therefore, for extracting large scale events, especially in open domain scenarios, how to automatically and efficiently generate sufficient training data is an important problem. This paper aims to automatically generate training data for EE, which involves labeling triggers, event types, arguments and their roles. Confidential Review Copy. DO NOT DISTRIBUTE. However, when we use DS for RE to EE, we meet following challenges: Triggers are not given out in existing knowledge bases. For example, Barack Obama plays a Spouse role in this marriage event instance. It seems that we could use an event instance and an argument to automatically generate training data for argument identification just like DS for RE. However, an event instance is a virtual node in existing knowledge bases and mentioned implicitly in texts. Following ACE, we can use trigger words to represent event instance, like married for people.marriage event instance. Unfortunately, triggers are not given out in existing knowledge bases. To resolve the trigger missing problem mentioned above, we need to discover trigger words before employing distant supervision to automatically label event arguments. Following DS in RE, we could naturally assume that a sentence contains all arguments of an event in the knowledge base tend to express that event, and the verbs occur in these sentences tend to evoke this type of events. However, arguments for a specific event instance are usually mentioned in multiple sentences. Simply employing all arguments in the knowledge base to label back in sentences will generate few sentences as training samples. To solve above problems, we propose an approach to automatically generate labeled data for large scale EE by jointly using world knowledge (Freebase) and linguistic knowledge (FrameNet). Confidential Review Copy. DO NOT DISTRIBUTE. ter that, we propose a Soft Distant Supervision (SDS) for EE to automatically label training data, which assumes that any sentence containing all key arguments in Freebase and a corresponding trigger word is likely to express that event in some way, and arguments occurring in that sentence are likely to play the corresponding roles in that event. Finally, we evaluate the quality of the automatically labeled training data by both manual and automatic evaluations. In addition, we employ a CNNbased EE approach with multi-instance learning for the automatically labeled data as a baseline for further research on this data. Moreover, we employ FrameNet to filter noisy triggers and expand more triggers. Also, our automatically labeled data can augment traditional humanannotated data, which could significantly improve the extraction performance. In this paper, we respectively use Freebase as our world knowledge containing event instance and FrameNet as the linguistic knowledge containing trigger information. The articles in Wikipedia are used as unstructured texts to be labeled. In this paper, we regard these CVTs as events, type of CVTs as event type, CVT instances as event instances, values in CVTs as arguments in events and roles of CVTs as the roles of arguments play in the event, respectively. Each frame has a set of lemmas with part of speech tags that can evoke the frame, which are called LUs. For example, appoint.v is a LU of Appointing frame in FrameNet, which can be mapped to people.appointment events in Freebase. And a LUs of the frame plays a similar role as the trigger of an event. Thus we use FrameNet to detect triggers in our automatically data labeling process. We use Wikipedia because it is relatively up-to-date, and much of the information in Freebase is derived from Wikipedia. Confidential Review Copy. DO NOT DISTRIBUTE. This section illustrates how to detect key arguments for each event type via Freebase. Intuitively, arguments of a type of event play different roles. Some arguments play indispensable roles in an event, and serve as vital clues when distinguishing different events. For example, compared with arguments like time, location and so on, spouses are key arguments in a marriage event. We call these arguments as key arguments. We propose to use Key Rate (KR) to estimate the importance of an argument to a type of event, which is decided by two factors: Role Saliency and Event Relevance. Role Saliency (RS) reflects the saliency of an argument to represent a specific event instance of a given event type. If we tend to use an argument to distinguish one event instance form other instances of a given event type, this argument will play a salient role in the given event type. Event Relevance (ER) reflects the ability in which an argument can be used to discriminate different event types. If an argument occurs in every event type, the argument will have a low event relevance. Then we choose top K arguments as key arguments. After detecting key arguments for every event types, we use these key arguments to label sentences that may express events in Wikipedia. Finally, we select sentences contains all key arguments of an event instance in Freebase as sentences expressing corresponding events. Then we use these labeled sentences to detect triggers. In a sentence, a verb tend to express an occurrence of an event. Intuitively, if a verb occurs more times than other verbs in the labeled sentences of one event type, the verb tends to trigger this type of event; and if a verb occurs in sentences of every event types, like is, the verb will have a low probability to trigger events. Thus we propose Trigger Candidate Frequency (TCF) and Trigger Event Type Frequency (TETF) to evaluate above two aspects. Finally, we choose verbs with high TR values as the trigger words for each event type. We can obtain an initial verbal trigger lexicon by above trigger word detection. However, this initial trigger lexicon is noisy and merely contains verbal triggers. The nominal triggers like marriage are missing. Because the number of nouns in one sentence is usually larger than that of verbs, it is hard to use TR to find nominal triggers. Confidential Review Copy. DO NOT DISTRIBUTE. noisy verbal triggers and expand nominal triggers. Specifically, we use the average word embedding of all words in i-th Freebase event type name ei and word embedding of k-th lexical units of j-th frame ej,k to compute the semantic similarity. And we use all nouns in the mapped frame to expand trigger lexicon. In this paper, event extraction is formulated as a two-stage, multi-class classification task. The first stage is called Event Classification, which aims to predict whether the key argument candidates participate in a Freebase event. If the key arguments participate a Freebase event, the second stage is conducted, which aims to assign arguments to the event and identify their corresponding roles. We call this stage as argument classification. We employ two similar Dynamic Multi-pooling Convolutional Neural Networks with Multi-instance Learning (DMCNNs-MIL) for above two stages. In order to alleviate the wrong label problem, we use Multi-instance Learning (MIL) for two DMCNNs. In stage of argument classification, we take sentences containing the same argument candidate and triggers with a same event type as a bag and all instances in a bag are considered independently. And the objective of multi-instance learning is to discriminate bags rather than instances. Thus, we define the objective function on the bags. In this section, we first manually evaluate our automatically labeled data. Then, we conduct automatic evaluations for our labeled data based on ACE corpus and analyze effects of different approaches to automatically label training data. Finally, we shows the performance of DMCNNs-MIL on our automatically labeled data. By using the proposed methods, a large set of labeled data could be generated automatically. Confidential Review Copy. DO NOT DISTRIBUTE. grid search respectively. However, these sentences miss labeling triggers. Thus, we leverage these rough labeled data and FrameNet to find triggers and use SDS to generate labeled data. We firstly manually evaluate the precision of our automatically generated labeled data. Each selected sample is a sentence with a highlighted trigger, labeled arguments and corresponding event type and argument roles. Annotators are asked to assign one of two labels to each sample. It is very easy to annotate a sample for annotators, thus the annotated results are expected to be of high quality. To prove the effectiveness of the proposed approach automatically, we add automatically generated labeled data into ACE dataset to expand the training sets and see whether the performance of the event extractor trained on such expanded training sets is improved. In our automatically labeled data, there are some event types that can correspond to those in ACE dataset. We mapped these types of events manually and we add them into ACE training corpus in two ways. We call this Expanded Data (ED) as ED Only. Then we use such data to train the same event extraction model (DMCNN) and evaluate them on the ACE testing data set. And we use the same evaluation metric P, R, F as ACE task defined. We select three baselines trained with ACE data. Confidential Review Copy. DO NOT DISTRIBUTE. This demonstrates that our automatically generated labeled data could expand human annotated training data effectively. This demonstrates that our large scale automatically labeled data is competitive with elaborately humanannotated data. In this section, we prove the effectiveness of KR to find key arguments and explore the impact of different numbers of key arguments to automatically generate data. We specifically select two methods as baselines for comparison with our KR method: ER and RS, which use the event relevance and role salience to sort arguments of each type of events respectively. Then we choose the same number of key arguments in all methods and use these key arguments to label data. After that we evaluate these methods by using above automatic evaluations based on ACE data. This demonstrates the effectiveness of our KR methods. Then we automatically evaluate the performance by using automatic evaluations proposed above. We specifically select two methods as baselines: TCF and TETF. TCF, TETF and TR respectively use the trigger candidate frequency, trigger event type frequency and trigger rate to sort trigger candidates of each type of events. Then we generate initial trigger lexicon by using all trigger candidates with high TCF value, TETF value or TR value. FrameNet was used to filter noisy verbal triggers and expand nominal triggers. Then we evaluate the performance of these methods by using above automatic evaluations. It demonstrates the effectiveness of our TR methods. Confidential Review Copy. DO NOT DISTRIBUTE. strates the effectiveness of the usage of FrameNet. Held-out Evaluation In the held-out evaluation, we hold out part of the Freebase event data during training, and compare newly discovered event instances against this heldout data. We can see that multi-instance learning is effective to alleviate the noise problem in our distant supervised event extraction. Human Evaluation Because the incomplete nature of Freebase, heldout evaluation suffers from false negatives problem. We also perform a manual evaluation to eliminate these problems. In the manual evaluation, we manually check the newly discovered event instances that are not in Freebase. Because the number of these event instances in the test data is unknown, we cannot calculate the recall in this case. Instead, we calculate the precision of the top n extracted event instances. We can see that DMCNNs-MIL achieves the best performance. Most of previous event extraction work focused on supervised learning paradigm and trained event extractors on human-annotated data which yield relatively high performance. However, these supervised methods depend on the quality of the training data and labeled training data is expensive to produce. But extracted events may not be easy to be mapped to events for a particular knowledge base. But DS for RE cannot directly use for EE. For the reasons that an event is more complicated than a relation and the task of EE is more difficult than RE. However, the method can only extract arguments of one plane crash type and need flight number strings as input. In other words, the approach cannot extract whole event with different types automatically. In this paper, we present an approach to automatically label training data for EE. The experimental results show the quality of our large scale automatically labeled data is competitive with elaborately human-annotated data. Also, we provide a DMCNN-MIL model for this data as a baseline for further research. In the future, we will use the proposed automatically data labeling method to more event types and explore more models to extract events by using automatically labeled data. Confidential Review Copy. DO NOT DISTRIBUTE."
33,2,4.11,3.56,3.56,True,acl_2017,train,"This paper deals with sentence-level sentiment classification. Though a variety of neural network models have been proposed recently, however, previous models either depend on expensive phrase-level annotation, most of which has remarkably degraded performance when trained with only sentence-level annotation; or do not fully employ linguistic resources (e.g., sentiment lexicons, negation words, intensity words). In this paper, we propose simple models trained with sentence-level annotation, but also attempt to model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are able to capture the linguistic role of sentiment words, negation words, and intensity words in sentiment expression.","In spite of the great success of these neural models, there are some defects in previous studies. Second, linguistic knowledge such as sentiment lexicon, negation words or negators (e.g., not, never), and intensity words or intensifiers (e.g., very, absolutely), has not been fully employed in neural models. The goal of this research is to developing simple sequence models but also attempts to fully employing linguistic resources to benefit sentiment classification. Firstly, we attempts to develop simple models that do not depend on parsing trees and do not require phrase-level annotation which is too expensive in real-world applications. Secondly, in order to obtain competitive performance, simple models can benefit from linguistic resources. Three types of resources will be addressed in this paper: sentiment lexicon, negation words, and intensity words. Sentiment lexicon offers the prior polarity of a word which can be useful in determining the sentiment polarity of longer texts such as phrases and sentences. Intensifiers change the valence degree of the modified text, which is important for fine-grained sentiment classification. We address the issue by imposing linguistic-inspired regularizers on sequence LSTM models. The rest of the paper is organized as follows: In the following section, we survey related work. There are many neural networks proposed for sentiment classification. Such recursive models usually depend on a tree structure of input text, and in order to obtain competitive results, usually require annotation of all subphrases. Linguistic knowledge and sentiment resources, such as sentiment lexicons, negation words (not, never, neither, etc.) or negators, and intensity words (very, extremely, etc.) or intensifiers, are useful for sentiment analysis in general. Negation words play a critical role in modifying sentiment of textual expressions. Sentiment intensity of a phrase indicates the strength of associated sentiment, which is quite important for fine-grained sentiment classification or rating. Intensity words can change the valence degree (i.e., sentiment intensity) of the modified text. Long Short-Term Memory has been widely adopted for text processing. In LSTM, the hidden state of each position (ht) only encodes the prefix context in a forward direction while the backward context is not considered. Particularly, parameters in the two LSTMs are shared. In this way, the forward and backward contexts can be considered simultaneously. The central idea of the paper is to model the linguistic role of sentiment, negation, and intensity words in sentence-level sentiment classification by regularizing the outputs at adjacent positions of a sentence. Though this is not always true (e.g., soap movie), this assumption holds at most cases. We approach this phenomenon with a sentiment class specific shifting distribution. The negation regularizer models this linguistic phenomena with a negator-specific transformation matrix. Modeling this effect is quite important for finegrained sentiment classification, and the intensity regularizer is designed to formulate this effect by a word-specific transformation matrix. More formally, the predicted sentiment distribution (pt, based on ht, see Eq. Note that we do not consider the modification span of negation and intensity words to preserve the simplicity of the proposed models. Instead, we resort to sequence LSTMs for encoding surrounding contexts at a given position. The sentiment regularizer constrains that the sentiment distributions of adjacent positions should drift accordingly if the input word is a sentiment word. This is the issue of sentiment drift. For instance, a sentiment lexicon may have class labels like strong positive, weakly positive, weakly negative, and strong negative, and for each class, there is a shifting distribution which will be learned by the model. Note that in this way all words of the same sentiment class share the same drifting distribution, but in a refined setting, we can learn a shifting distribution for each sentiment word if large-scale datasets are available. The negation regularizer approaches how negation words shift the sentiment distribution of the modified text. The former changes the polarity to negative, while the latter changes to neutral instead of positive. The regularizer assumes that if the current position is a negation word, the sentiment distribution of the current position should be close to that of the next or previous position with the transformation. In total, we train m transformation matrixs for m negation words. Sentiment intensity of a phrase indicates the strength of associated sentiment, which is quite important for fine-grained sentiment classification or rating. Intensifier can change the valence degree of the content word. The intensity regularizer models how intensity words influence the sentiment valence of a phrase or a sentence. The formulation of the intensity effect is quite the same as that in the negation regularizer, but with different parameters of course. For each intensity word, there is a transform matrix to favor the different roles of various intensifiers on sentiment drift. For brevity, we will not repeat the formulas here. However, we can alleviate the problem by leveraging bidirectional LSTM. For a single LSTM, we employ a backward LSTM from the end to the beginning of a sentence. This is because, at most times, the modified words of negation and intensity words are usually at the right side of the modified text. But sometimes, the modified words are at the left side of negation and intensity words. To better address this issue, we employ bidirectional LSTM and let the model determine which side should be chosen. Due to the same consideration, we redefine L (NSR) t and L (SR) t with bidirectional LSTM similarly. The formulation is the same and omitted for brevity. Our models address these linguistic factors with mathematical operations, parameterized with shifting distribution vectors or transformation matrices. In the sentiment regularizer, the sentiment shifting effect is parameterized with a classspecific distribution (but could also be wordspecific if with more data). In the negation and intensity regularizers, the effect is parameterized with word-specific transformation matrices. This is to respect the fact that the mechanism of how negation and intensity words shift sentiment expression is quite complex and highly dependent on individual words. We partially address the modification scope issue by applying the minimization operator in Eq. Note that SST has provided phrase-level annotation on all inner nodes, but we only use the sentence-level annotation since one of our goals is to avoid expensive phrase-level annotation. Due to the length limit, we present the implementation details and a full list of resources in the supplementary file. Negation word no, nothing, never, neither, not, seldom, scarcely, etc. Intensity word terribly, greatly, absolutely, too, very, completely, etc. Very strong results are reported. demonstrating the effectiveness of the linguistic regularizers. Second, LR-LSTM and LR-Bi-LSTM perform slightly better than Tree-LSTM but Tree-LSTM leverages a constituency tree structure while our model is a simple sequence model. As future work, we will apply such regularizers to tree-structured models. Last, on the MR dataset, our model is comparable to or slightly better than CNN. We have the following observations: First, linguistically regularized LSTM and BiLSTM are better than their counterparts. That means, LR-Bi-LSTM can avoid the heavy phrase-level annotation but still obtain comparable results. Second, our models are comparable to Tree-LSTM but our models are not dependent on a parsing tree and more simple, and hence more efficient. Last, on the SST dataset, our model is better than CNN, DAN, and NCSL. Phraselevel means the models use phrase-level annotation for training. And sentence-level means the models only use sentence-level annotation. In order to reveal the effect of each individual regularizer, we conduct ablation experiments. Each time, we remove a regularizer and observe how the performance varies. First of all, we conduct this experiment on the entire datasets, and then we experiment on sub-datasets that only contain negation words or intensity words. NSR, SR, NR and IR denotes Non-sentiment Regularizer, Sentiment Regularizer, Negation Regularizer, and Intensity Regularizer respectively. Method Neg. Sub. Int. Sub. Sub.) that only contains negators, and intensity sub-dataset (Int. Sub.) that only contains intensifiers. To further reveal the linguistic role of negation words, we compare the predicted sentiment distributions of a phrase pair with and without a negation word. The experimental results performed on MR are shown in Fig. Typical phrases include not very good, not too bad. Second, the dots at the up-left and bottom-right respectively indicates the negation effects: changing negative to positive and positive to negative. Typical phrases include never seems hopelessly (up-left), no good scenes (bottom-right), not interesting (bottom-right), etc. Last, the dots located at the center indicate that neutral phrases maintain neutral sentiment with negators. Typical phrases include not at home, not here, where negators typically modify nonsentiment words. We show the matrix that indicates how the sentiment shifts after being modified by intensifiers. Each number in a cell (mij) indicates how many phrases are predicted with a sentiment label i but the prediction of the phrases with intensifiers changes to label j. most irresponsible picture), positive to very positive (eg. most famous author), neutral to negative (eg. most plain), and neutral to positive (eg. most closely), respectively. There are also many phrases retaining the sentiment after being modified with intensifiers. For the left phrases, they fall into three categories: first, words modified by intensifiers are non-sentiment words, such as most of us, most part; second, intensifiers are not strong enough to shift sentiment, such as most complex (from neg. to neg. most traditional (from pos. to pos. third, our models fail to shift sentiment with intensifiers such as most vital, most resonant film. We present linguistically regularized LSTMs for sentence-level sentiment classification. The proposed models address the sentient shifting effect of sentiment, negation, and intensity words. Furthermore, our models are sequence LSTMs which do not depend on a parsing tree-structure and do not require expensive phrase-level annotation. Results show that our models are able to address the linguistic role of sentiment, negation, and intensity words. To preserve the simplicity of the proposed models, we do not consider the modification scope of negation and intensity words, though we partially address this issue by applying a minimization operartor (see Eq. As future work, we plan to apply the linguistic regularizers to tree-LSTM to address the scope issue since the parsing tree is easier to indicate the modification scope explicitly."
777,1,4.0,4.0,3.0,True,acl_2017,train,"This paper presents an approach for modeling inter-topic preferences of Twitter users: for example, those who agree TPP also agree free trade. This kind of knowledge is useful not only for stance detection across multiple topics but also for various real-world applications including public opinion survey, election prediction, election campaign, and online debate. In order to extract users’ preferences on Twitter, we design high-quality linguistic patterns (e.g., “A is completely wrong”) in which people agree and disagree topics. By applying the linguistic patterns to a collection of tweets, we extract statements agreeing and disagreeing various topics. Inspired by the work on item recommendation, we formalize the task of modeling inter-topic preferences as matrix factorization: representing users’ preference as a user-topic matrix and mapping both users and topics into a latent feature space that abstracts the preferences. The experimental results demonstrate that the presented approach is useful for predicting missing preferences of users and that the latent vector representations of topics encode inter-topic preferences.","Ironically, the cutting-edge technology of social media promotes ideological groups even with its potential to deliver diverse information. However, stance detection across different topics is extremely difficult. In addition, stance detection encounters the difficulty with different user types. A ruins the future of our country. I support A A is necessary Welcome A We should introduce A. I disagree A A is completely wrong A ruins the future of our country. A good news. A good news. Establishing a bridge across different topics and users is a major challenge not only to stance detection but also to social media analytics. An important material for the bridge is commonsense knowledge about topics. We call this kind of knowledge inter-topic preference throughout this paper. within the same target through the use of n-gram features on a supervision data. In contrast, this paper directly acquires inter-topic preferences from an unlabeled corpus of tweets. The low-rank matrices provide latent vector representations of users and topics. The contributions of this paper are three-folds. This study uses a Japanese Twitter corpus because of its availability from the authors, but the core idea is applicable to any language. This section collects statements of users agreeing or disagreeing various topics on Twitter as source data for modeling inter-topic preferences. We removed retweets from the corpus. We use linguistic patterns to extract tuples (u, t, v) from the corpus. These regular expressions can find users who have strong preferences to topics. We regard the topics found in this procedure as the set of target topics T in this study. This paper uses the alphabetical expression sansei only for explanation; the actual pattern uses Chinese characters corresponding to sansei. This paper uses the alphabetical expression hantai only for explanation; the actual pattern uses Chinese characters corresponding to hantai. However, a Twitter user does not necessarily express preferences for all topics. In addition, it is by nature impossible to predict whether a new (non-existent in the data) user agree or disagree topics. In essence, matrix factorization maps both users and topics into a latent feature space that abstracts topic preferences of users. We call pu and qt the user vector and topic vector, respectively. How good is the low-rank approximation found by matrix factorization? What is the sweet spot for the number of dimension k of the latent space? We investigate the reconstruction error of matrix factorization with different values of k in order to answer these questions. We can observe that a reconstruction error decreases as the iterative method of libmf progresses. How accurately can the user and topic vectors predict missing topic preferences? In order to investigate this question, we evaluate the accuracy for predicting hidden preferences in the matrix R as follows. For comparison, we also include the majority baseline that predicts pro and con based on the majority of preferences about each topic in the training set. This result again indicates that the presented method reasonably utilize known preferences to complete missing preferences. In contrast, the majority baseline decreases its performance as it receives more information about the users. Because this result was counterintuitive, we examined the cause of this phenomenon. We can observe that the mean variance increases as we focus on vocal users. These results demonstrate the usefulness of user and topic vectors for predicting missing preferences. For example, the proposed method predicts that the user A, who is positive to regime change but negative to Okinawa US military base, may also be positive to vote of non-confidence to Cabinet but negative to construction of a new base. In addition, we removed topics that are too discriminatory or aggressive to other countries and races. Even though the experimental results of this paper do not necessarily reflect our idea, we do not think it is a good idea to distribute politically incorrect ideas through this paper. Crowdsourcing, a Japanese online service for crowdsourcing. We could observe a moderate correlation even though inter-topic preferences collected in this manner are highly subjective. Considering that people who support the LDP may also tend to favor its policies, we found these results reasonable. From these results, we conclude that topic vectors were able to capture inter-topic preferences. This section summarizes the related work that spreads across various research fields. Social science and political science A number of of studies analyze social phenomena regarding political activities, political thoughts, and public opinions on social media. Employing a single axis (e.g., liberal to conservative) or a few axes (e.g., political parties and candidates of elections), these studies provide intuitive visualizations and interpretations along the axes. In contrast, this study is the first attempt for recognizing and organizing various axes of topics on social media with no prior assumption about axes. Therefore, we think this study provides a new tool for computational social science and political science to analyze and interpret phenomena on social media. Here, we mention some work that acquires lexical knowledge about politics. The constructed cue lexicons are associated with ideologies such as left, center, and right. Representing each speech of a candidate with cue lexicons, they infer the proportions of ideologies of the candidate. The study requires a predefined set of labels and text data associated with the labels. They presented a generative model for users, subjects, and predicates to find a one-dimensional latent space, which corre-sponds to the political spectrum. Their paper reported that the generative model outperformed Principal Component Analysis (PCA), which is a method for matrix factorization. The empirical result probably reflected the underlying assumptions that PCA treats missing elements as zero (not as simply missing data). Although these previous studies have a potential to improve the quality of the user-topic matrix R, unfortunately, no corpus nor resource is available for Japanese language. We do not have a large collection of English tweets at this moment, but combining fine-grained opinion analysis with matrix factorization is an immediate future work. However, inter-topic preferences also include pairs of topics where causality relation hardly holds. For example, it is unreasonable to infer that nuclear plant and railroading of bills have causal relation, but those who dislike nuclear plant also oppose railroading of bills because (they think) the governing political parties rush the bill for resuming a nuclear plant. This study models this kind of inter-topic preferences based on the preferences of the public. Having said that, it is a promising future direction of this work to incorporate the approach for acquiring causality knowledge. This paper presents a novel approach for modeling inter-topic preferences of users on Twitter. An immediate future work is to embed the topic and user vectors to a cross-topic stance detector. Thus, we believe that this work will bring a new application of NLP to other disciplines. Removed for the reviewing process)"
331,2,4.43,3.29,3.0,False,acl_2017,train,"Concept maps can be used to concisely represent important information and bring structure into large document collections. Therefore, we study a variant of multidocument summarization that produces summaries in the form of concept maps. However, suitable evaluation datasets for this task are currently missing. To close this gap, we present a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. It was created using a novel crowdsourcing approach that allows us to efficiently determine important elements in large document collections. We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization.","However, when exploring a set of documents manually, humans rarely write a fully-formulated summary for themselves. Therefore, we believe that the study of summarization with similarly structured outputs is an important extension of the traditional task. Labels can be freely defined. A concept can be an entity, abstract idea, event or activity, designated by a unique label. Good maps should be propositionally coherent, meaning that every relation together with the two connected concepts form a meaningful proposition. For summarization, concept maps allow to represent a summary concisely and clearly reveal relations. Moreover, we see a second interesting use case beyond the capabilities of textual summaries: When concepts and relations are linked to corresponding locations in the documents, the graph can be used to navigate in a document collection, similar to how a table-of-contents is used in a single document. The corresponding task that we propose is concept-map-based MDS: Given a set of related documents, create a concept map that represents its most important content, satisfies a specified size limit and is connected. Further, to ease the use of automatic evaluations, we focus on the extractive variant of the task in this work, requiring that all labels have to be taken from the documents. The proposed task is complex, consisting of several interdependent subtasks. tract appropriate labels for concepts and relations and recognize different expressions that refer to the same concept across multiple documents. Further, one has to select the most important concepts and relations for the summary and finally organize them in a graph satisfying the connectedness and size constraints. In order to develop and evaluate methods for the task, gold-standard corpora are necessary, but no suitable corpus is available. The manual creation of such a dataset is very timeconsuming, as annotators need to perform all subtasks described above. In particular, an annotator would need to manually identify all potential concepts in the documents, while only a few of them will eventually end up in the summary. To overcome these issues, we developed a corpus creation method that effectively combines automatic preprocessing, scalable crowdsourcing and high-quality expert annotations. The document clusters were built from a web crawl, comprising a variety of different genres, text styles and document lengths. These resources are publicly available under a permissive license. These approaches extract concept and relation labels from syntactic structures and connect them to build a concept map. However, common task definitions and comparable evaluations are missing. Our newly proposed task and the created large-cluster dataset fill these gaps as they emphasize the summarization aspect of the task. resources are attached for review. How useful would the following statements be for you? The corpus we present combines these aspects, as it has large clusters of heterogeneous documents, and provides a necessary benchmark to evaluate the proposed task. In contrast to our corpus, it only deals with single documents, requires a two orders of magnitude smaller amount of compression and is not publicly available. However, they both differ in important aspects: Whereas concept maps follow an open label paradigm and are meant to be interpretable by humans, knowledge bases and ontologies are usually more strictly typed and made to be machine-readable. Moreover, approaches to automatically construct them from text typically try to extract as much information as possible, while we want to summarize a document. As a result, approaches from these fields cannot be directly applied. They discovered several challenges, including poor work quality and the subjectiveness of the annotation task, indicating that crowdsourcing is not useful for this purpose. To overcome these is-sues, we introduce a new task design, low-context importance annotation, to determine summaryworthy parts of documents. We break the importance annotation down to single propositions. The goal of our crowdsourcing scheme is to obtain a score for each proposition indicating its importance in a document cluster, such that a ranking according to the score would reveal what is most important and should be included in a summary. In preliminary tests, we found that this design, despite the minimal context, works reasonably well as long as the topic is something the worker can relate to. We randomly group five propositions into a task. Comparison Tasks As an alternative, we use a second task design based on pairwise comparisons. strict than exact label agreement and can account for close labels and high-or low-scoring workers. This shows that the approach, despite the high subjectiveness of the task, allows us to collect reliable annotations. Peer Evaluation In addition to the reliability studies, we extrinsically evaluated the annotations in the task of summary evaluation. Conclusion Based on the pilot study results, we conclude that the proposed crowdsourcing scheme allows us to obtain proper annotations for the importance of propositions. Thus, the approach can be used to construct gold-standard summaries, evaluate systems or generate training data. To carry out comparable evaluations, the proposed task requires a gold-standard corpus. bullying, homeschooling, drugs). It was created from a large web crawl using state-of-the-art information retrieval. Web pages are boilerplate-cleaned, segmented and all sentences have binary labels for topic relevance. Therefore, we resort to an automatic approach for this step. For instance, from Specifically, eligible students may borrow the Direct Stafford Loans and eligible parents may borrow the Direct PLUS Loan on behalf of their dependent students. Open IE systems extract tuples of two arguments and a relation phrase representing propositions: (eligible students, may borrow, Dir. Stafford Loan) (eligible parents, may borrow, Direct PLUS Loan) While the relation phrase is similar to a relation in a concept map, many arguments in these tuples represent useful concepts. Since we want to create a gold-standard corpus, we have to ensure that we produce high-quality data. We therefore made use of the confidence assigned to every extracted tuple to filter out low quality ones. Despite the similarity of the Open IE paradigm, not every extracted tuple is a suitable proposition for a concept map. To reduce the effort in the subsequent steps, we therefore want to filter out unsuitable ones. To better understand these criteria, we performed a small annotation study. Based on a set of examples, we created a guideline explaining when to label a tuple as suitable for a concept map. Due to the high number of tuples we decided to automate the filtering step. We trained a linear SVM on the majority voted annotations. To ensure that the automatic classification does not remove suitable propositions, we tuned the classifier to avoid false negatives. In particular, we introduced class weights, improving precision on the negative class at the cost of a higher fraction of positive classifications. Further, we assumed that we can manually verify a certain number of the most uncertain negative classifications. We found this to be a reasonable trade-off of automation and data quality and applied the model to the full dataset. We also corrected positive classifications for certain types of tuples for which we knew the classifier to be imprecise. We use the topic descriptions from the underlying DIP corpus. These figures show that the crowdsourcing approach works on this dataset as reliably as on the TAC documents. Having a manageable number of propositions per topic, an expert annotator then applied a few straightforward transformations that correct common errors of the Open IE system. First, we break down propositions with conjunctions in either of the arguments into separate propositions per conjunct, which the Open IE system sometimes fails to do. And second, we correct span errors that might occur in the arguments or relation phrase, especially when sentences were not properly segmented. In this final step, we connect the set of important propositions to form a graph. For instance, given the following two propositions (student, may borrow, Stafford Loan) (the student, does not have, a credit history) one can easily see, although the first arguments differ slightly, that both labels describe the concept student, allowing us to build a concept map with three concepts. The annotation task thus involves deciding which of the available propositions to include in the map, which of their concepts to join and which of the available labels to use. As these decisions highly depend upon each other and require context, we decided to use expert annotators rather than crowdsource the subtasks. Annotators were given the topic description and the most important, ranked propositions. They could connect them step by step in a simple annotation tool that visualized the map constructed so far. Further, they should prefer more important propositions and ensure connectedness. When connecting two propositions, they were asked to keep the concept label that was appropriate for both propositions. Size Rel. Std. between Montessori teacher and Montessori education). To assess the reliability of this annotation step, we had the first three maps created by two annotators. In this section, we describe our newly created corpus, which, in addition to having summaries in the form of concept maps, differs from traditional summarization corpora in several aspects. In addition, the documents are also more variable in terms of length, as the (length-adjusted) standard deviation is twice as high as in the other corpora. ments from a variety of genres. Systems summarizing the corpus can therefore not rely on genre-specific characteristics and have to deal with the full variety of text types. Textual Heterogeneity In addition to the variety of genres, the documents also differ in terms of language use. The higher this value is, the more the language differs between documents. To obtain a better picture of what kind of text spans have been used as labels, we automatically tagged them with their part-of-speech and determined their head with a dependency parser. Analyzing the graph structure of the maps, we found that all of them are connected. including less important propositions, and we decided against it. In this section, we briefly describe a baseline and evaluation scripts that we release, with a detailed documentation, along with the corpus. Baseline Method We implemented a simple approach inspired by previous work on concept map generation and keyphrase extraction. Extract all NPs as potential concepts. Merge potential concepts whose labels match after stemming into a single concept. For each pair of concepts co-occurring in a sentence, select the tokens in between as a potential relation if they contain a verb. If a pair of concepts has more than one relation, select the one with the shortest label. Rank all concepts by importance. At inference time, the classifier provides an importance score for each concept. Strict Match compares them after stemming and only counts complete matches. These automatic measures might be complemented with a human evaluation. An analysis of the single pipeline steps revealed major bottlenecks of the method and challenges of the task. Hence, content selection is a major challenge, stemming from the large cluster sizes. The propagation of these errors along the pipeline contributes to overall low scores. In this work, we presented low-context importance annotation, a novel crowdsourcing scheme that we used to create a new benchmark corpus for concept-map-based MDS. The corpus has largescale document clusters of heterogeneous web documents, posing a challenging summarization task. Together with the corpus, we provide implementations of a baseline method and evaluation scripts and hope that our efforts facilitate future research on this variant of summarization."
433,3,5.0,3.58,4.0,True,acl_2017,train,"Singlish can be interesting to the ACL community both linguistically as a major creole based on English, and computationally for information extraction and sentiment analysis of regional social media. We investigate dependency parsing of Singlish by constructing a dependency treebank under the Universal Dependencies scheme, and then training a neural network model by integrating English syntactic knowledge into a state-ofthe-art parser trained on the Singlish treebank. Results show that English knowledge can lead to 36% relative error reduction, resulting in a parser of around 85% accuracies. We make both our annotation and parser available for further research.","When major languages such as English or French are adopted in another culture as the primary language, they often mix with existing languages or dialects in that culture and evolve into a stable language called a creole. While the majority of the natural language processing (NLP) research attention has been focused on the major languages, little work has been done on adapting the components to creoles. Correspondingly, fundamental English NLP components such as POS taggers and dependency parsers perform poorly on such Singlish texts based on our observations. To address this issue, we start with investigating the linguistic characteristics of Singlish and specifically the causes of difficulties for understanding Singlish with English syntax. We adopt this model as the basis for our Singlish parser. Seminal work employed statistical models. The basic idea is to map the word embedding spaces between different languages into the same vector space, by using sentence-aligned bilingual data. Our work is similar to these methods in using a neural network model for knowledge sharing between different languages. However, ours is different in the use of a neural stacking model, which respects the distributional differences between Singlish and English words. This empirically gives higher accuracies for Singlish. Since English is the major genesis of Singlish, we choose English as the source of lexical feature transfer to assist Singlish dependency parsing. With the aligned initiatives for creating transfer-learning-friendly treebanks, we adopt the Universal Dependencies protocol for constructing the Singlish dependency treebank, both as a new resource for the low-resource languages and to facilitate knowledge transfer from English. On top of the general Universal Dependencies guidelines, English-specific dependency relation definitions including additional subtypes are employed as the default standards for annotating the Singlish dependency treebank, unless augmented or redefined when necessary. We find empirically that all these deviations can be accommodated by applying the existing English dependency relation definitions, which are explained with examples as follows. meanings using existing UD-Eng POS tags and dependency relations. This type can be regarded as a variant of the English it-extraposition sentence structure with the extraposition moved to the front and the clause marker removed. The only possibility of a copular root is when the copula has a clausal argument or adjunct. However, such copula is typically not deleted in either Singlish or Chinese, and we have not encountered such cases in our treebank. If they appear by any chance, feasible solutions can be either recovering the copula with a special symbol or promoting the head of the clausal argument or adjunct as the sentence root. NP deletion: Noun-phrase (NP) deletion often results in null subjects or objects. The former simply involves a change of word orders and thus requires no special treatments. On the other hand, tag questions should be carefully analyzed in two scenarios. Data Source: Singlish is used in written form mainly in social media and local Internet forums. This contrast indicates the degree of lexical deviation of Singlish from English. A full summary of the number of occurences of each POS tag and dependency labels is included in Appendix A. Both the English and Singlish models consist of an input layer, a feature layer, and an output layer. Output layer: This is a CRF layer to predict the POS tags for the input words by maximizing the conditional probability of the sequence of tags given input sentence. During training, loss is back-propagated to all trainable parameters in both the Singlish Tagger and the pre-trained feature layer of the base English Tagger. At test time, the input sentence is fed to the integrated tagger model as a whole for inference. However, due to limited amount of training data, the tagging accuracy is not satisfactory even with a large dropout rate to avoid overfitting. Both the base and neural stacking models consist of an input layer, a feature layer, and an output layer. Input Layer: This layer encodes the current input word by concatenating a pre-trained word embedding with a trainable word embedding and POS tag embedding from the respective lookup tables. Feature Layer: The two recurrent vectors produced by the multi-layer bi-LSTM network from each input vector are concatenated and mapped to multiple feature vectors in lower-dimension space by a set of parallel multilayer perceptron (MLP) layers. Output Layer: This layer applies biaffine transformation on the feature vectors to calculate the score of the directed arcs between every pair of words. The inferred trees for input sentence are formed by choosing the head with the highest score for each word and a cross-entropy loss is calculated to update the model parameters. Loss is back-propagated by reversely traversing all forward paths to all trainable parameter for training and the whole model is used collectively for inference. We apply the same settings for a baseline Singlish parser. We attempt to choose a better configuration of the number of bi-LSTM layers and the hidden dimension based on the development set performance, but the default settings turn out to perform the best. When using the neural stacking model, we fix the model configuration for the base English parser model and choose the size of the hidden vector and the number of bi-LSTM layers stacked on top based on the development set. This demonstrate the distributional differences between Singlish and English tokens, even though they share a large vocabulary. This demonstrates that knowledge from English can be successfully incorporated to boost the Singlish parser. This significant improvement is further explained below. This suggests that the base English parser mainly contributes to analyzing basic English syntax, while the Singlish dependency treebank models unique Singlish grammars better. The main reason can be that these two types of grammars preserve a high percentage of English syntax, with only alterations in word orders and deletions of one word. An interesting finding is that feeding English distributed lexical semantic information to the base Singlish parser undermines the performance even over basic English syntax, which again suggests the differences in distributed lexical semantics. We release the annotated Singlish dependency treebank, the trained model and the source code for the parser with free public access. Possible future work include expanding the investigation to other regional languages such as Malay and Indonesian."
68,3,3.69,1.31,1.31,False,acl_2017,train,"Text readability has an important role in text drafting and document selecting. Researches on the readability of the text have been made long ago for English and some common languages. There are few researches in Vietnamese text readability and most of them are performed from more than two decades ago on very small corpora. In this paper, we build a new and larger corpus and use it to create a newer formula to predict the difficulty of Vietnamese text. The experimental results show that the new formula can predict the readability of Vietnamese documents with over 80% accuracy.","Base on the readability, readers can determine whether a text is suitable for their reading ability or not. The text author(s) can also use the readability of the draft to guide readers object or have some adjustments to make it fit the toward reader. Building a model to analyze text readability has meant a lot in the scientific and practical: help scientists writing research reports more readable; support educators drafting textbooks and curricula to suit each age of students; support publishers in shaping the audience; help governments drafting legal documents to suit the majority of citi-zens; or to assist manufacturers in preparing user guide for their products.   In addition, text readability can effectively support in choosing appropriate curriculums when teaching language for foreigners.   Both these two researches focus on examining relations between statistical characteristics at words level and at sentences level and text readability. Since these studies, there is almost no other publication on Vietnamese text readability. In this paper, we mainly focus on creating a new formula for Vietnamese text readability assessment base on a self-built corpus with a large number of documents. Up to now, there are thousands of works in this field. In this section, we will describe some famous formulas for English text readability assessment and two formulas for Vietnamese. This is a very famous readability formula that provides the comprehension difficulty score of an English document. Words that not in this list are considered as difficult words. This is an easy formula for measuring English text readability and is used by many US government agencies like US Department of Defense.   Like the name of the formula, the higherRE, the easier the document. This formula is best suited for education and is also integrated into the Microsoft Word. Because this formula determines the grade-school level of the text so the higher FKRA, the more difficult the text. Similar to the Flesch-Kincaid grade level, the higher Gunning Fog index, the more difficult the text. This formula estimates the years of education a person needs to understand a specific text and is widely used in checking health messages. The higher SMOG value, the more difficult the text. Since these studies, there is almost no other publication on Vietnamese text readability. So in our research, we built another corpus with a larger amount of document for examining. Easy documents: including documents written for children or by children or just need people who are studying at primary schools or having maximum primary education to read and understand. These documents were mainly collected from primary school textbooks, primary sample essays, fairy-tales, stories for babies.   Normal documents: they are documents written for middle and high school students, or documents which only need people with high school education to be readable and understandable. Most documents in this category were collected from textbooks and general newspapers. Difficult documents: including documents written for college students, specialized documents, scientific paper.   which need high or specialized education to be readable and understandable. These documents were collected from university textbooks, specialized documents, political theory articles, language and literary articles, law and legal documents.   Ten experts were asked to evaluate collected documents. They are Vietnamese language specialists, current or former Vietnamese literature teacher-who has much knowledge and experiment in using and teaching Vietnamese. Easy Normal Difficult Overall No. This is a statistical measure for assessing the agreement between a fixed number of raters when classifying items. In this part, we will describe some features that are commonly used in text readability assessment. Average sentence length: the average sentence length of a text is one of the simplest and common characteristic when measuring text readability. Percentage of difficult words: in many studies, the percentage of difficult words is an important feature when evaluating text readability. However, create the easy or difficult word list needs a lot of effort, so most researches used frequent word list as a replacement: if a word does not appear in the frequent list, it will be considered as a difficult word. Not only the percentage of difficult words, but also the percentage of difficult syllables was examined in our study. lation analysis. However, they are also high correlated with some others. To choose which features to put in the formula, we select sequentially features from the highest correlation with text readability to the lowest and remove features have high correlation coefficient with selected features. The features AWLS and ASLS are high correlated with AWLC and ASLC so they were not chosen. The selected features were used as predictors to perform multiple regression analysis with Text Readability as the criterion. The purpose is to find coefficient values of these features to form a formula for predicting text readability. In this study, we divided our corpus into five equal parts for cross-fold analyzing. The predicted values were rounded to the nearest unit and compared to the expert evaluated readability for accuracy assessment. Finally, the average coefficient value of each predictor was used to form the final text readability formula. The main reason is the lists of Dien and Hao were statistically analyzed from the corpus mainly collected from newspapers, which are mostly texts with normal readability. This is a good result and can be applied in practice. In this paper, we have presented our work on creating a new large corpus for Vietnamese text readability assessing. We also used the corpus to create a new formula for predicting Vietnamese text readability. Experiments performed on the corpus using created formula shows that the formula can predict the readability of Vietnamese text with high accuracy. For the future works, other corpora will be built with more detailed levels of difficulty and for more specific domains. Other deeper features like part-of-speech, sentence structure, discourse.   will be examined to create more precise formula(s). Some machine learning methods will be examined to create some classifier for automatically Vietnamese text readability assessment."
130,3,2.58,2.83,2.58,True,acl_2017,train,"Mild Cognitive Impairment (MCI) is a mental disorder difficult to diagnose. Although language impairment is an important marker it is frequently undervalued in cognitive assessments. Linguistic features, mainly from parsers, have been used to detect MCI. However, MCI disfluencies produce agrammatical speech impacting in parsing results; manually correcting transcripts of patient’s speech is not a solution to large scale assessments. In this paper, we use complex network features to automatically identifying MCI in transcripts, using several classification algorithms, as it a lightweight and language independent representation. We modeled transcripts into complex networks and enriched them with word embeddings to better represent short texts produced in assessments. We evaluate our model in three datasets: one from the DementiaBank; Cinderella and Arizona-Battery in Portuguese were produced by assessments applied at University of São Paulo Medical School. The results show that complex networks are suitable to detect MCI and outperform linguistic features in all datasets. We also combined the classifiers through ensemble and multi-view learning. The ensemble of algorithms and multi-view method achieved 10% of improvement in Cinderella dataset compared to the best individual classifier. For the Arizona dataset, the multi-view achieved the highest accuracy (80%), a 4.71% of improvement.","Although language impairment is an important marker it is frequently undervalued in cognitive assessments. Linguistic features, mainly from parsers, have been used to detect MCI. In this paper, we use complex network features to automatically identifying MCI in transcripts, using several classification algorithms, as it a lightweight and language independent representation. We modeled transcripts into complex networks and enriched them with word embeddings to better represent short texts produced in assessments. The results show that complex networks are suitable to detect MCI and outperform linguistic features in all datasets. We also combined the classifiers through ensemble and multi-view learning. Mild Cognitive Impairment (MCI) can affect one or multiple cognitive domains (e.g. Language is one of the most efficient information sources for assessing cognitive functions. Changes in language usage are frequent in patients with dementia and are normally first recognized by the patients themselves or their family members. The discourse production (mainly narratives) is attractive because it allows for the analysis of linguistic microstructures, including phonetic-phonological, morphosyntactic and semantic-lexical components, as well as semanticpragmatic macrostructures. A variety of features are required for this analysis, including Part-of-Speech (PoS), syntactic complexity, lexical diversity and acoustics features. Even when traditional statistical techniques (Bag of Words (BoW) or ngrams) are applied they also have problems to deal with disfluencies. In this paper, we show that speech transcripts (narratives or descriptions) can be modeled into complex networks that are enriched with word embeddings. We modeled narratives and short descriptions into complex networks and enriched them with word embeddings to better represent short texts produced in these assessments. When applied to a machine learning classifier, the complex network features were able to distinguish between control participants and mild cognitive impairment participants. Discrimination of the two classes could be improved by combining complex networks with linguistic and traditional statistical features; we also identified the best scenario for application of network features extracted from transcripts of neuropsychological assessments. Detection of memory impairment has been based on linguistic, acoustic, and demographic features, in addition to scores of neuropsychological tests. Some studies used short animated films to evaluate immediate and delayed recall in MCI patients which were asked to talk about the first film shown, then about their previous day, and finally about another film shown last. For the Portuguese language, machine learning algorithms were used to identify subjects with AD and MCI. PLN tools with high precision are needed to compute these metrics, which is a problem for Portuguese since there is no robust dependency or constituency parser. Therefore, the transcriptions had to be manually revised; they were segmented in sentences, following a semantic-structural criterion and capitalization was added afterwards. The authors also removed disfluencies and inserted omitted subjects when they were hidden, in order to reduce parsing errors. Each distinct word becomes a node and words that are adjacent in the text are connected by an edge. Before modeling texts into complex networks, it is often necessary to do some preprocessing in the raw text. words and punctuation marks) and then stopwords and punctuation marks are removed, since they have little semantic meaning. One last step we decided to eliminate from the preprocessing pipeline is lemmatization, which transforms each word into its canonical form. Another problem with transcriptions in our work is their size. Following this methodology, in our model we added new edges to the co-occurrence networks considering similarities between words, that is, for all pairs of words in the text that were unconnected, an edge was created if their vectors (from word embedding) had a cosine similarity higher than a given threshold. Note that (b) is a more informative network than (a), since (a) is practically a linear network. The first dataset is composed of short English descriptions, while the second contains longer Brazilian Portuguese narratives. A third dataset had very short narratives, also in Portuguese. Below, we describe in more detail the datasets, participants, and the task in which they were used. During the interview, patients were given the picture and were told to discuss everything they could see happening in the picture. We extracted the word-level transcript patient sentences from the CHAT files and discarded the annotation, as our goal was to create a fully automated system that does not require the input of a human annotator. We automatically removed filled pauses such as uh, um, er, and ah (e.g. uh it seems to be summer out), short false starts (e.g. just t the ones), and repetition (e.g. The Avg. Education is given in years. tricians, neurologists, neuropsychologists, speech pathologists, and occupational therapists, by a criterion of consensus. Inclusion criteria for the control group were elderly people with no cognitive deficits and preservation of functional capacity in everyday life. The exclusion criteria for the normal group were: poorly controlled clinical diseases, sensitive deficits that are not being compensated for and interfere with the performance in tests, other neurological or psychiatric diagnoses that are associated with dementia or cognitive deficits and use of medications in doses that affect cognition. The time was recorded, but there was no limit imposed to the narrative length. However, other disfluencies (revisions, elaboration, paraphasias and comments about the task) were kept. Interviews were conducted in Portuguese and the subject listened to the examiner read a brief narrative. The Avg. Education is given in years. rics of co-occurrence networks, linguistic features and statistics of bag of words representation. Each transcription was mapped into a cooccurrence network, and then enriched via word embedding using the cosine similarity of words. This method extends the skipgram model to use character-level information, with each word being represented as a bag of character n-grams. Betweenness: is a centrality measurement that considers a node as relevant if it is highly accessed via shortest paths. Clustering Coefficient: measures the probability that two neighbors of a node are connected. The metrics affected by constituency and dependency parsing were not used because they are not robust to deal with disfluencies. Metrics based on manual annotation (such as proportion short pauses, mean pause duration, mean number of empty words, and others) were also discarded. In this work term frequency was used. In order to quantify the ability of the topological characterization of networks, linguistic metrics and BoW features to distinguish subjects with MCI from those without, we employed four machine learning algorithms to induce classifiers from a training set. The techniques are Gaussian Naive Bayes (GaussianNB), k-Nearest Neighbor (K-NN), Support Vector Machine (SVM), linear and radial bases functions (RBF), and Random Forest (RF). We also combine these classifiers through ensemble and multi-view learning. In multi-view learning, multiple classifiers are trained in different feature spaces and thus combined to produce a single result. This approach is an elegant solution rather than combining all features in the same vector or space, primarily because the combination is not a straightforward step, which can lead to noise insertion, since the data have different natures. CN, CNE, LM, and BoW denote, respectively, complex networks, complex network enriched with embeddings, linguistic metrics and Bag-of-Words, and CNE-LM, CNE-BoW, LM-BoW and CNE-LM-BoW refer to combinations of the feature spaces (multiview learning), using the majority vote. The last line represents the use of an ensemble of machine learning algorithms, in which the combination used was the majority voting in both ensemble and multiview learning. The results for the three datasets show that characterizing transcriptions into complexity networks is competitive with other traditional methods, such as with the use of linguistic metrics. In fact, among the three types of features, using enriched networks (CNE) provided the highest accuracies, and in general CNE is better than using only complex networks. SVM gives better accuracy in most cases compared to other machine learning algorithms. As for ensemble and multi-view learnings there are some good results. In this study, we employed metrics of topological properties of CN in a machine learning classification approach to distinguish between healthy controls and patients with MCI. To the best of our knowledge, these metrics have never been used before to detect MCI in speech transcripts; CN were enriched with word embeddings to better represent short texts produced in neuropsychological assessments. We have shown that the topological properties of CN outperfom traditionally linguistic metrics, in individual classifiers results. Furthermore, we found that combining machine and multi-view learning can improve accuracy. The comparison with our results is not straightforward, though, because the databases used in the studies are different. There is a clear need of publicly available datasets to compare different methods, which would allow for optimizing detection of MCI in elderly people. As future work, we intend to explore other methods to enrich CN, such as Recurrent Language Model language, and other metrics to characterize an adjacency network. The pursue of these strategies is relevant because language is one of the most efficient information sources for assessing cognitive functions, commonly used in neuropsychological assessments. You just want me to start talking? And the mother has an apron on. It must be summer or spring. And the curtains are pulled back. And they have a nice walk around their house. And the house with the kitchen has a lot of cupboard space under the sink board and under the cabinet from which the cookie you know cookies are being removed."
87,1,5.0,4.0,4.0,True,acl_2017,train,"The large and growing amounts of online scholarly data present both challenges and opportunities to enhance knowledge discovery. One such challenge is to automatically extract a small set of keyphrases from a document that can accurately describe the document’s content and can facilitate fast information processing. In this paper, we propose PositionRank, an unsupervised model for keyphrase extraction from scholarly documents that incorporates information from all positions of a word’s occurrences into a biased PageRank. Our model obtains remarkable improvements in performance over PageRank models that do not take into account word positions as well as over strong baselines for this task. Specifically, on several datasets of research papers, PositionRank achieves improvements as high as 29.09%.","On one hand, these rapidly-growing scholarly document collections offer benefits for knowledge discovery, and on the other hand, finding useful information has become very challenging. Keyphrases associated with a document typically provide a high-level topic description of the document and can allow for efficient information processing. Various feature sets and classification algorithms yield different extraction systems. These graph-based techniques construct a word graph from each target document, such that nodes correspond to words and edges correspond to word association patterns. Since their introduction, many graph-based extensions have been proposed, which aim at modeling various types of information. Two of the most popular approaches are based on matrix factorization (MF) and Markov chains (MC). MF methods learn the general taste of a user by factorizing the matrix over observed user-item preferences. Our method is based on personalized transition graphs over underlying Markov chains. Red bold phrases represent the gold-standard keyphrases for the document. ing to its textually-similar documents, computed using the cosine similarity between the tf-idf vectors of documents. Keyphrases are then ranked by aggregating the topic-specific scores obtained from several topicbiased PageRanks. We posit that other information can be leveraged that has the potential to improve unsupervised keyphrase extraction. For example, in a scholarly domain, keyphrases generally occur on positions very close to the beginning of a document and occur frequently. The author input keyphrases are marked with red bold in the figure. We specifically address this question using research papers as a case study. The result of this extraction task will aid indexing of documents in digital libraries, and hence, will lead to improved organization, search, retrieval, and recommendation of scientific documents. The rest of the paper is organized as follows. We summarize related work in the next section. Graph-based ranking methods and centrality measures are considered stateof-the-art for unsupervised keyphrase extraction. In particular, they decomposed a document into multiple topics, using topic models, and applied a separate topic-biased PageRank for each topic. The PageRank scores from each topic were then combined into a single score, using as weights the topic proportions returned by topic models for the document. More precisely, thresholding on the frequency of phrases is applied, where the thresholds are estimated from the data. The candidate phrases are then ranked using the tf-idf model in conjunction with a boosting factor which aims at reducing the bias towards single word terms. Phrases and their initial weights are then incorporated into a graph-based algorithm which produces the final ranking of keyphrase candidates. In contrast to the above approaches, we propose PositionRank, aimed at capturing both highly frequent words or phrases and their position in a document. Our model assigns higher probabilities to words found early on in a document instead of using a uniform distribution over words. In this section, we describe PositionRank, our fully unsupervised, graph-based model, that simultaneously incorporates the position of words and their frequency in a document to compute a biased PageRank score for each candidate word. These steps are detailed below. Let d be a target document for extracting keyphrases. Hence, in this work, we build undirected graphs. Formally, letG be an undirected graph constructed as above and let M be its adjacency matrix. The PageRank score of a node vi is recursively computed by summing the normalized scores of nodes vj, which are linked to vi (as explained below). By recursively applying Eq. The idea of PositionRank is to assign larger weights (or probabilities) to words that are found early in a document and are frequent. We weigh each candidate word with its inverse position in the document before any filters are applied. If the same word appears multiple times in the target document, then we sum all its position weights. Summing up the position weights for a given word aims to grant more confidence to frequently occurring words by taking into account the position weight of each occurrence. Candidate words that have contiguous positions in a document are concatenated into phrases. The top-scoring phrases are output as predictions (i.e., the predicted keyphrases for the document). In order to evaluate the performance of PositionRank, we carried out experiments on three datasets. In experiments, we use the title and abstract of each paper to extract keyphrases. The author-input keyphrases are used as gold-standard for evaluation. Evaluation Metrics. We use mean reciprocal rank (MRR) curves to illustrate our experimental findings. For comparison purposes, we used Porter Stemmer to reduce both predicted and gold keyphrases to a base form. Our experiments are organized around several questions, which are discussed below. How sensitive is PositionRank to its parameters? One parameter of our model that can influence its performance is the window size w, which determines how edges are added between candidate words in the graph. As can be seen from the figure, the performance of our model does not change significantly as w changes. In this experiment, we analyze the influence that position-weighted frequent words in a document would have on the performance of PositionRank. of a word, referred as PositionRank-fp. Note that the weights of words are normalized before they are used in the biased PageRank. As we can see from the figure, the performance of PositionRank-full model consistently outperforms its counterpart that uses the first position only, on all datasets. We can conclude from this experiment that aggregating information from all occurrences of a word acts as an important component in PositionRank. Hence, we use PositionRank-full model for further comparisons. How well does position information aid in unsupervised keyphrase extraction from research papers? In this experiment, we compare our position-biased PageRank model (PositionRank) with two PageRank-based models, TextRank and SingleRank, that do not make use of the position information. PositionRank can successfully harness this information in an unsupervised setting to obtain good improvements in the extraction performance. How does PositionRank compare with other existing state-of-the-art methods? In TF-IDF, we calculate the tf score of each candidate word in the target document, whereas the idf component is estimated from all three datasets. We performed experiments with various numbers of textually-similar neighbors and present the best results for each dataset. In TPR, we build an undirected graph using information from the target paper. of a document and to compute the probability of words in these topics. For all models, the score of a phrase is obtained by summing the score of the constituent words in the phrase. As can be seen from the table, PositionRank outperforms all baselines, on all datasets. From the table, we can also see that ExpandRank is generally the best performing baseline on all datasets. However, it is interesting to note that, unlike PositionRank that uses information only from the target paper, ExpandRank adds external information from a textually-similar neighborhood of the target paper, and hence, is computationally more expensive. PositionRank-first position only (fp) typically performs worse than PositionRank-full model, but it still outperforms the baseline methods for most top k predicted keyphrases, on all datasets. Best results are shown in bold blue. A striking observation is that PositionRank outperforms TPR on all datasets. Compared with our model, TPR is a very complex model, which uses topic models to learn topics of words and infer the topic proportion of documents. Additionally, TPR has more parameters (e.g., the number of topics) that need to be tuned separately for each dataset. PositionRank is much less complex, it does not require an additional dataset (e.g., to train a topic model) and its performance is better than that of TPR. TF-IDF and ExpandRank are the best performing baselines, on all datasets. We proposed a novel unsupervised graph-based algorithm, called PositionRank, which incorporates both the position of words and their frequency in a document into a biased PageRank. To our knowledge, we are the first to integrate the position information in novel ways in unsupervised keyphrase extraction. Specifically, unlike supervised approaches that use only the first position information, we showed that modeling the entire distribution of positions for a word outperforms models that use only the first position."
649,1,4.0,4.0,4.0,True,acl_2017,train,"Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality (Liu et al., 2016). Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem. We present an evaluation model (ADEM) that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model’s predictions correlate significantly, and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue models unseen during training, an important step for automatic dialogue evaluation.","These models are trained in an end-to-end manner to optimize a single objective, usually the likelihood of generating the responses from a fixed corpus. One of the challenges when developing such systems is to have a good way of measuring progress, in this case the performance of the chatbot. The Turing test provides one solution to the evaluation of dialogue systems, but there are limitations with its original formulation. The test requires live human interactions, which is expensive and difficult to scale up. Furthermore, the test requires carefully designing the instructions to the human interlocutors, in order to balance their behaviour and expectations so that different systems may be ranked accurately by performance. Although unavoidable, these instructions introduce bias into the evaluation measure. In the case of chatbots designed for specific conversation domains, it may also be difficult to find sufficient human evaluators with appropriate background in the topic (e.g. Despite advances in neural network-based models, evaluating the quality of dialogue responses automatically remains a challenging and understudied problem in the non-task-oriented setting. While human evaluation should always be used to evaluate dialogue models, it is often too expensive and time-consuming to do this for every model specification (for example, for every combination of model hyperparameters). We believe this is sufficient for making progress as current dialogue systems often generate inappropriate responses. We also find empirically that asking evaluators for other metrics results in either low inter-annotator agreement, or the scores are highly correlated with appropriateness (see supp. material). Thus, we collect a dataset of appropriateness scores to various dialogue responses, and we use this dataset to train an automatic dialogue evaluation model (ADEM). The model is trained in a semi-supervised manner using a hierarchical recurrent neural network (RNN) to predict human scores. Each example is in the form (context, model response, reference response, human score). We show that ADEM scores correlate significantly with human judgement at both the utterance-level and system-level. For example, the responses should include both relevant and irrelevant responses, both coherent and non-coherent responses and so on. To achieve this variety, we use candidate responses from several different models. It should be noted that the humangenerated candidate responses are not the reference responses from a fixed corpus, but novel human responses that are different from the reference. In addition to increasing response variety, this is necessary because we want our evaluation model to learn to compare the reference responses to the candidate responses. To train evaluation models on human judgements, it is crucial that we obtain scores of responses that lie near the distribution produced by advanced models. Finally, since it does not require domain specific knowledge (e.g. technical knowledge), it should be easy for AMT workers to annotate. Recurrent neural networks (RNNs) are a type of neural network with time-delayed connections between the internal units. LSTMs add a set of gates to the RNN that allow it to learn how much to update the hidden state. One of the most popular approaches for automatically evaluating the quality of dialogue responses is by computing their word overlap with the reference response. In particular, the most popular metrics are the BLEU and METEOR scores used for machine translation, and the ROUGE score used for automatic summarization. We briefly describe BLEU here, and provide a more detailed summary of word-overlap metrics in the supplemental material. It computes the n-gram precision for the whole dataset, which is then multiplied by a brevity penalty to penalize short translations. Drawbacks One of the major drawbacks of word-overlap metrics is their failure in capturing the semantic similarity between the model and reference responses when there are few or no common words. This problem is less critical for machine translation; since the set of reasonable translations of a given sentence or document is rather small, one can reasonably infer the quality of a translated sentence by only measuring the word-overlap between it and one (or a few) reference translations. Further, word-overlap scores are computed directly between the model and reference responses. As such, they do not consider the context of the conversation. While this may be a reasonable assumption in machine translation, it is not the case for dialogue; whether a model response is an adequate substitute for the reference response is clearly context-dependent. We call this evaluation model ADEM. ADEM learns distributed representations of the context, model response, and reference response using a hierarchical RNN encoder. The model gives high scores to responses that have similar vector representations to the context and reference response after this projection. The model is end-to-end differentiable; all the parameters can be learned by backpropagation. The simplicity of our model leads to both accurate predictions and fast evaluation (see supp. material), which is important to allow rapid prototyping of dialogue systems. The lower-level RNN, the utterance-level encoder, takes as input words from the dialogue, and produces a vector output at the end of each utterance. The context-level encoder takes the representation of each utterance as input and outputs a vector representation of the context. Following previous work, we take the last hidden state of the context-level encoder as the vector representation of the input utterance or context. An important point is that the ADEM procedure above is not a dialogue retrieval model: the fundamental difference is that ADEM has access to the reference response. Pre-training with VHRED We would like an evaluation model that can make accurate predictions from few labeled examples, since these examples are expensive to obtain. We therefore employ semi-supervised learning, and use a pre-training procedure to learn the parameters of the encoder. In particular, we train the encoder as part of a neural dialogue model; we attach a third decoder RNN that takes the output of the encoder as input, and train it to predict the next utterance of a dialogue conditioned on the context. The dialogue context is encoded into a vector using our hierarchical encoder, and the VHRED then samples a Gaussian variable that is used to condition the decoder (see supplemental material for further details). We use representations from the VHRED model as it produces more diverse and coherent responses compared to HRED. Two context vectors produced by the VHRED encoder are similar if the contexts induce a similar distribution over subsequent responses; this is consistent with the formulation of the evaluation model, which assigns high scores to responses that have similar vector representations to the context. The skip-thought-vector model takes as input a single sentence and predicts the previous sentence and next sentence. On the other hand, VHRED takes as input several consecutive sentences and predicts the next sentence. This makes it particularly suitable for learning long-term context representations. When training ADEM, we also employ a subsampling procedure based on the model response length. In particular, we divide the training examples into bins based on the number of words in a response and the score of that response. We then over-sample from bins across the same score to ensure that ADEM does not use response length to predict the score. However, we found the ADEM model learned more effectively when this embedding size was reduced. When training our models, we conduct early stopping on a separate validation set. the contexts in the test set are unseen during training). The results are detailed in the supplemental material. ADEM at initialization). C-and R-ADEM represent the ADEM model trained to only compare the model response to the context or reference response, respectively. tain reasonable but inferior performance compared to using VHRED embeddings. Each point in the scatterplots represents a dialogue model; humans give low scores to TFIDF and DE responses, higher scores to HRED and the highest scores to other human responses. This renders them completely deficient for dialogue evaluation. seen during training. Thus, it is crucial that ADEM correlates with human judgements for new models. For each dialogue model that was the source of response data for training ADEM (TF-IDF, Dual Encoder, HRED, humans), we conduct an experiment where we train on all model responses except those from the chosen model, and test only on the model that was unseen during training. We observe that the ADEM model is able to generalize for all models except the Dual Encoder. This is particularly surprising for the HRED model; in this case, ADEM was trained only on responses that were written by humans (from retrieval models or human-generated), but is able to generalize to responses produced by a generative neural network model. Each point represents the average scores for the responses from a dialogue model (TFIDF, DE, HRED, human). Human scores are shown on the horizontal axis, with normalized metric scores on the vertical axis. The ideal metric has a perfectly linear relationship. There are several instances where ADEM predicts accurately: in particular, ADEM is often very good at assigning low scores to poor responses. There are also several instances where the model assigns high scores to suitable responses, as in the first two contexts. One drawback we observed is that ADEM tends to be too conservative when predicting response scores. This is the case in the third context, where the model assigns low scores to most of the responses that a human rated highly. This behaviour is likely due to the squared error loss used to train ADEM; since the model receives a large penalty for incorrectly predicting an extreme value, it learns to predict scores closer to the average human score. We provide many more experiments, including a failure analysis, in the supplemental material. Their approach differs from ours as, in the dialogue domain, we must additionally condition our score on the context of the conversation, which is not necessary in translation. Several recent approaches use hand-crafted reward features to train dialogue models using reinforcement learning (RL). These metrics are based on hand-crafted features, which only capture a small set of relevant aspects; this inevitably leads to suboptimal performance, and it is unclear whether such objectives are preferable over retrieval-based crossentropy or word-level maximum log-likelihood objectives. Furthermore, many of these metrics are computed at the conversation-level, and are not available for evaluating single dialogue responses. Our models do not attempt to model task completion, and thus fall outside this domain. The left two columns show performance on the entire test set, and the right two columns show performance on responses only from the dialogue model not seen during training. Context Reference response Model responses Human score ADEM score photo to see my television debut go to-some. i thought ppl were recognizing someone who looked like you! were the oysters worth the wait? yeah it was me. get yours before theres no more! i am a right chatter tweetbox on sundays. same happened last sunday lol any news on meeting our user? We use the Twitter Corpus to train our models as it contains a broad range of non-task-oriented conversations and has has been used to train many stateof-the-art models. However, our model could easily be extended to other general-purpose datasets, such as Reddit, once similar pre-trained models become publicly available. Such models are necessary even for creating a test set in a new domain, which will help us determine if ADEM generalizes to related dialogue domains. We leave investigating the domain transfer ability of ADEM for future work. The evaluation model proposed in this paper favours dialogue models that generate responses that are rated as highly appropriate by humans. It is likely that this property does not fully capture the desired end-goal of chatbot systems. For example, one issue with building models to approximate human judgements of response quality is the problem of generic responses. Since humans often provide high scores to generic responses due to their appropriateness for many given contexts, a model trained to predict these scores will exhibit the same behaviour. An important direction for future work is modifying ADEM such that it is not subject to this bias. In this case, a model that generates generic responses will easily be distinguishable and obtain a low score. An important direction of future research is building models that can evaluate the capability of a dialogue system to have an engaging and meaningful interaction with a human. Compared to evaluating a single response, this evaluation is arguably closer to the end-goal of chatbots. However, such an evaluation is extremely challenging to do in a completely automatic way. We view the evaluation procedure presented in this paper as an important step towards this goal; current dialogue systems are incapable of generating responses that are rated as highly appropriate by humans, and we believe our evaluation model will be useful for measuring and facilitating progress in this direction."
654,2,4.5,4.5,4.0,True,acl_2017,train,"We introduce a new deep learning model for semantic role labeling (SRL) that significantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations. We use a deep highway BiLSTM architecture with constrained decoding, while observing a number of recent best practices for initialization and regularization. Our 8-layer ensemble model achieves 83.2 F1 on the CoNLL 2005 test set and 83.4 F1 on CoNLL 2012, roughly a 10% relative error reduction over the previous state of the art. Extensive empirical analysis of these gains show that (1) deep models excel at recovering long-distance dependencies but can still make surprisingly obvious errors, and (2) that there is still room for syntactic parsers to improve these results. All code and models will be publicly released.","We also present careful empirical analysis to determine what works well and what might be done to progress even further. Our model combines a number of best practices in the recent deep learning literature. All code and models will be publicly released. Formally, the goal of our task is to predict a sequence y given a sentence-predicate pair (w, v) as input. Each yi belongs to a discrete set of BIO tags T. Words outside argument spans have the tag O, and words at the beginning and inside of argument with role r have the tags Br and Ir respectively. To incorporate additional information (e.g. Part of our model is extended from his unpublished implementation. The approach described so far does not model any dependencies between the output tags. Starting with an empty sequence, the tag sequence is built from left to right. Syntactic Constraints We can enforce consistency with a given parse tree by rejecting or penalizing arguments that are not constituents. Both datasets provide gold predicates (their index in the sentence) as part of the input. Tokens that are not covered by GloVe are replaced with a randomly initialized UNK embedding. We report results of our best single and ensemble (PoE) model. Only the BIO hard constraints significantly improve over the ensemble model. The combination of highway layers, orthonormal parameter initialization and recurrent dropout is crucial to achieving strong performance. The numbers shown here are without constrained decoding. We ablate our full model by removing highway connections, RNN-dropout and orthonormal initialization independently. The analysis shows that while our model makes a similar number of labeling errors to traditional syntax-based systems, it has far fewer missing arguments (perhaps due to parser errors making some arguments difficult to recover for syntax-based systems). colorado.edu. Move an unique core argument to itscorrect position. Drop a predicted argument that does notoverlap with any gold span. Add a gold argument does not overlapwith any predicted span. All the operations are permitted only if they do not cause any overlapping arguments. We only count predicted arguments that match gold span boundaries. Results show the major cause of these errors is inaccurate prepositional phrase attachment. Interestingly, the gap between shallow and deep models becomes much larger for the long-distance predicate-argument structures. Surprisingly, the neural model performance deteriorates less severely on long-range dependencies than traditional syntax-based models. We can quantify two types of structural consistencies: the BIO constraints and the SRL-specific constraints. Via our ablation study, we show that deeper BiLSTMs are better at enforcing these structural consistencies, although not perfectly. BIO Violations The BIO format requires argument spans to begin with a B tag. Any I tag directly following an O tag or a tag with different label is considered a violation. The number of BIO violations decreases when we use a deeper model. To understand the reason behind it, we compare the average entropy between tokens involved BIO violations with the averaged entropy of all tokens. This suggests that BIO inconsistencies occur when there is some ambiguity. Using BIO-constrained decoding can resolve this ambiguity and result in a structurally consistent solution. With our constrained decoding algorithm, it is straightforward to enforce the unique core roles (U) and continuation roles (C) constraints during decoding. Although the violations are eliminated, the performance does not significantly improve. A natural question follows: are neural SRL models implicitly learning syntax? Constrained Decoding with Syntax The above analysis raises a further question: would improving consistency with syntax provide improvements for SRL? The penalty of disagreeing with the parse tree is a single parameter dictating how much the model should trust the provided syntax. These results suggest that high-quality syntax can still make a large impact on SRL. A known challenge for syntactic parsers is robustness on out-of-domain data. The best existing parser gives a small improvement, but the improvement from gold syntax shows that there is still potential for syntax to help SRL. Dev. These improvements suggest that while decoding with hard constraints is beneficial, joint training or multi-task learning could be even more effective by leveraging full, labeled syntactic structures. Our experiments show that off-the-shelf neural methods have a remarkable ability to learn long-range dependencies, syntactic constituency structure, and global constraints without coding task-specific mechanisms for doing so. An alternative line of work has attempted to reduce the dependency on syntactic input for semantic role labeling models. However, their best system fell short of traditional feature-based systems. However, we also show that there is potential for syntax to further improve performance. Extensive error analysis sheds light on the strengths and limitations of our deep SRL model, with detailed comparison against shallower models and two strong non-neural systems. Finally, we posed the question of whether deep SRL still needs syntactic supervision. Despite recent success without syntactic input, we found that our best neural model can still benefit from accurate syntactic parser output via straightforward constrained decoding."
382,2,4.0,2.5,3.5,True,acl_2017,train,"In this paper, we focus on how to create data-to-text corpora which can support the learning of wide-coverage microplanners i.e., generation systems that handle lexicalisation, aggregation, surface realisation, sentence segmentation and referring expression generation. We start by reviewing common practice in designing training benchmarks for Natural Language Generation. We then present a novel framework for semi-automatically creating linguistically challenging NLG corpora from existing Knowledge Bases. We apply our framework to DBpedia data and compare the resulting dataset with (Wen et al., 2016)’s dataset. We show that while (Wen et al., 2016)’s dataset is more than twice larger than ours, it is less diverse both in terms of input and in terms of text. We thus propose our corpus generation framework as a novel method for creating challenging data sets from which NLG models can be learned which are capable of generating text from KB data.","To train Natural Language Generation (NLG) systems, various input-text corpora have been developed which associate (numerical, formal, linguistic) input with text. We show that the performance of this neural model is much lower on the new data set than on the existing ones. We thus propose our corpus generation framework as a novel method for creating challenging data sets from which NLG adequate match between data and text. As such, this dataset is ill-suited for training microplanners. Moreover, since its texts contain both missing and additional information, it cannot be used to train joint models for content selection and micro-planning either. models can be learned which are capable of generating complex texts from KB data. Domain Specific Benchmarks. Several domain specific data-text corpora have been built by researchers to train and evaluate NLG systems. An important limitation of these datasets is that, because they are domain specific, systems learned from them are restricted to generating domain specific, often strongly stereotyped text (e.g., weather forecast or soccer game commentator reports). Arguably, training corpora for NLG should support the learning of wide-coverage generators. By nature however, domain specific corpora restrict the lexical and often the syntactic coverage of the texts to be produced and thereby indirectly limit the expressivity of the generators trained on them. Contrary to the domain-specific data sets just mentioned, these corpora have a wider coverage and are large enough for training systems that can generate linguistically sophisticated text. One main drawback of these benchmarks however is that their construction required massive manual annotation of text with complex linguistic structures (parse trees for the SR task and Abstract Meaning Representation for the AMR corpus). Moreover because these structures are complex, the annotation must be done by experts. It cannot be delegated to the crowd. In short, the creation of such benchmark is costly both in terms of time and in terms of expertise. That is, these benchmarks give very limited support for learning models that can handle micro-planning NLG subtasks such as lexicalisation, aggregation, sentence segmentation and referring expression generation. Crowdsourced Benchmarks. More recently, data-to-text benchmarks have also been created by associating data units with text using crowdsourcing. They then use crowdsourcing to associate each data unit with a text. The crowdsourcing approach to creating inputtext corpora has several advantages. First, it is low cost in that the data is produced automatically and the text is authored by a crowdworker. This is in stark contrast with the previous approach where expert linguists are required to align text with data. Second, because the text is crowd-sourced from the data (rather than the other way round), there is an adequate match between text and data both semantically (the text expresses the information contained in the data) and computationally (the data is sufficiently different from the text to require the learning of complex generation operations such as sentence segmentation, aggregation and referring expression generation). In this case, the generation task is limited to determining (i) the linear ordering and (ii) the full form of the word in the input. Third, by exploiting small hand-written ontologies to quickly construct meaningful artificial data, the crowdsourcing approach allows for the easy creation of a large dataset with data units of various size and bearing on different domains. This, in turn, allows for better linguistic coverage and for NLG tasks of various complexity since typically, inputs of larger size increases the need for complex microplanning operations. Another limitation concerns the shape of the input data. More generally, allowing for trees of deeper depth is necessary to indirectly promote the introduction in the benchmark of a more varied set of syntactic constructs to be learned by generators. To address these issues, we introduce a novel method for creating data-to-text corpora from large knowledge bases such as DBPedia. Our method combines (i) a content selection module designed to extract varied, relevant and coherent data units from DBPedia with (ii) a crowdsourcing process for associating data units with human authored texts that correctly capture their meaning. First, it can be used to create a data-to-text corpus from any knowledge base where entities are categorised and there is a large number of entities belonging to the same category. Second, as crowdworkers are required to enter text that matches the data and a majority vote validation process is used to eliminate mis-matched pairs, there is a direct match between text and data. This allows for a clear focus on the non content selection part of generation known as microplanning. Third, because data of increasing size is matched with texts ranging from simple clauses to short texts consisting of several sentences, the resulting benchmark is appropriate for exercising the main subtasks of microplanning. This data is stored as RDF (Resource Description Format) triples of the form (subject, property, object) where the subject is a URI (Uniform Resource Identifier), the property is a binary relation and the object is either a URI or a literal value such as a string, a date or a number. This method can be summarised as follows. Next, category graphs are used to learn bigram models of DBPedia properties which specify the probability of two properties co-occuring together. Three types of bi-gram models are extracted from category graphs using the SRILM toolkit: one model (S-Model) for bigrams occurring in sibling triples (triples with a shared subject); one model (C-Model) for bigrams occurring in chained triples (the object of one triple is the subject of the other); and one model (M-Model) which is a linear interpolation of the sibling and the chain model. The intuition is that these sibling and chain models capture different types of coherence, namely, topic-based coherence for the S-Model and discourse-based coherence for the CModel. An input is a set of triples produced by the content selection module. The number of input is thus the number of distinct sets of triples produced by this module. In contrast, input patterns are inputs where subject and object have been abstracted over. That is, the number of input patterns is the number of distinct sets of properties present in the set of inputs. The number of properties is the number of distinct RDF properties occurring in the dataset. Similarly, the number of entities is the number of distinct RDF subjects and objects occurring in each given dataset. We do this in four main steps as follows. Clarifying Properties. Getting Verbalisations for Single Triples. single triples consisting of a subject, a property and an object. For each such input, crowdworkers were asked to produce a sentence verbalising its content. We used both a priori automatic checks to prevent spamming and a posteriori manual checks to remove incorrect verbalisations. We also monitored crowdworkers as they entered their input and banned those who tried to circumvent our instructions and validators. The exact match between a triple and its verbalisation was also prohibited. In addition, after data collection was completed, we manually checked each data-text pair and eliminated from the data set any pair where the text either did not match the information conveyed by the triple or was not a well-formed English sentence. Getting Verbalisations for Input containing more than one Triple. The verbalisations collected for single triples were used to construct input with bigger size. In such a way, we diminish the risk of having misinterpretations of the original semantics of a data unit. Contributors were also encouraged to change the order, and the wording of sentences, while writing their texts. For each data unit, we collected three verbalisations. Verifying the Quality of the Collected Texts. Then the crowd was asked to assess its fluency, semantic adequacy, and grammaticality. Those criteria were checked by asking the following three questions: Does the text sound fluent and natural?, Does the text contain all and only the information from the data?, Is the text good English (no spelling or grammatical mistakes)?. We collected five answers per verbalisation. A verbalisation was considered as bad, if it received three negative answers in at least one criterion. The DBPNLG dataset has been uploaded with this submission. We now investigate the impact of this difference on the two datasets (DBPNLG and RNNLG). To assess the degree to which both datasets support the generation of linguistically varied text requiring complex microplanning operations, we examine a number of data and text related metrics. We also compare the results of an out-of-the-box sequence-to-sequence model as a way to estimate the complexity of the learning task induced by each dataset. Terminology. Similarly, in DBPNLG, DBpedia RDF properties relate a subject entity to an object which can be either an entity or a datatype value. In what follows, we refer to both as attributes. Number of attributes. A dataset with a larger number of attributes is therefore more likely to induce texts with greater syntactic variety. Number of Input Patterns. Hence a higher number of input patterns will favour a higher number of syntactic realisations. That is, the relative variety in input patterns is higher in DBPNLG. The ratio between number of inputs and the number of input patterns has an important impact both in terms of linguistic diversity and in terms of learning complexity. While this facilitates learning, this also reduces linguistic coverage (less combinations of structures can be learned) and may induce overfitting. Note that because datasets are typically delexicalised when training NLG models (cf. The two datasets markedly differ on this ratio which is five times lower in DBPNLG. From a learning perspective, this means that the RNNLG dataset facilitates learning but also makes it harder to assess how well systems trained on it can generalise to handle unseen input. Input Shape. In contrast, the trees extracted by the DBPNLG content selection procedure may be of depth five and therefore allow for further syntactic constructs such as object relative clause and passive participials (cf. We can show this empirically as well that DBPNLG is far more diverse than RNNLG in terms of input shapes. Another difference between the two datasets is that DBPNLG contains a higher number of text per input thereby providing a better basis for learning paraphrases. The size and the content of the vocabulary is another important factor in ensuring the learning of wide coverage generators. While a large vocabulary makes the learning problem harder, it also allows for larger coverage. DBPNLG exhibits a higher corrected type-token ratio (CTTR), which indicates greater lexical variety, and higher lexical sophistication (LS). Lexical sophistication measures the proportion of relatively unusual or advanced word types in the text. Typetoken ratio (TTR) is a measure of diversity defined as the ratio of the number of word types to the number of words in a text. To address the fact that this ratio tends to decrease with the size of the corpus, corrected TTR can be used to control for corpus size. Richer and more varied datasets are harder to learn from. As a proof-of-concept study of the comparative difficulty of the two datasets with respect to machine learning, we compare the performance of a sequence-to-sequence model for generation on both datasets. As RNNLG is bigger in size than DBPNLG, we constructed a balanced sample of RNNLG which included equal number of instances per category (tv, laptop, etc). The training was done in two delexicalisation modes: fully and name only. In case of fully delexicalisation, all entities were replaced by their generic terms, whereas in name only mode only subjects were modified in that way. The delexicalisation in sentences was done using the exact match between entities and tokens. In both modes, RNNLG yielded lower scores than DBPNLG. This is inline with the observations made above concerning the higher data diversity, larger vocabulary and more complex texts of DBPNLG. We presented a framework for building NLG datato-text training corpora from existing knowledge bases. However the code is geared toward dialog acts and modifying it to handle RDF triples is non trivial. Since the comparison aims at examining the relative performance of the same neural network on the two datasets, we used the tensor flow implementation instead. verbalisers for RDF knowledge bases. In this context, our framework is useful both for creating training data from RDF KB verbalisers and to increase the number of datasets available for training and testing NLG. Another important feature of our framework is that it permits creating semantically and linguistically diverse datasets which should support the learning of lexically and syntactically, wide coverage micro-planners. We applied our framework to DBpedia data and showed that although twice smaller than the largest corpora currently available for training data-to-text microplanners, the resulting dataset is more semantically and linguistically diverse. Despite the disparity in size, the number of attributes is comparable in the two datasets. The ratio between input and input patterns is five times lower in our dataset thereby making learning harder but also diminishing the risk of overfitting and providing for wider linguistic coverage. Conversely, the ratio of text per input is twice higher thereby providing better support for learning paraphrases. We are currently working on further extending the DBPNLG dataset and once completed, will make it available as part of a shared task for evaluating data-to-text micro-planners. Recently, several sequence-to-sequence models have been proposed for generation. Our experiments suggest that these are not optimal when it comes to generate linguistically complex texts from rich data. More generally, they indicate that the data-to-text corpora built by our framework are challenging for such models. We hope that the DBPNLG dataset which we will make available in the shared task will drive the deep learning community to take up this new challenge and work on the development of neural generators that can handle the generation of linguistically rich texts."
18,1,5.0,4.0,5.0,True,acl_2017,train,"Cloze-style reading comprehension is a representative problem in mining relationship between document and query. In this paper, we present a simple but novel model called attention-over-attention reader for better solving cloze-style reading comprehension task. Our model aims to place another attention mechanism over the document-level attention and induces “attended attention” for final answer predictions. One advantage of our model is that it is simpler than related works while giving excellent performance. We also propose an N-best re-ranking strategy to double check the validity of the candidates and further improve the performance. Experimental results show that the proposed methods significantly outperform various state-of-the-art systems by a large margin in public datasets, such as CNN and Children’s Book Test.","Reading comprehension is a general problem in the real world, which aims to read and comprehend a given article or context, and answer the questions based on it. Recently, the cloze-style reading comprehension problem has become a popular task in the community. To teach the machine to do cloze-style reading comprehensions, large-scale training data is nec-essary for learning relationships between the given document and query. In this paper, we present a novel neural network architecture, called attention-over-attention model. As we can understand the meaning literally, our model aims to place another attention mechanism over the existing document-level attention. To sum up, the main contributions of our work are listed as follows. In this section, we will give a brief introduction to the cloze-style reading comprehension task at the beginning. And then, several existing public datasets will be described in detail. Note that the answer is usually a single word in the document, which requires the human to exploit context information in both document and query. The type of the answer word varies from predicting a preposition given a fixed collocation to identifying a named entity from a factual illustration. Large-scale training data is essential for training neural networks. Several public datasets for the cloze-style reading comprehension has been released. Here, we introduce two representative and widely-used datasets. They construct these datasets with web-crawled CNN and Daily Mail news data. One of the characteristics of these datasets is that the news article is often associated with a summary. So they first regard the main body of the news article as the Document, and the Query is formed by the summary of the article, where one entity word is replaced by a special placeholder to indicate the missing word. Apart from releasing the dataset, they also proposed a methodology that anonymizes the named entity tokens in the data, and these tokens are also re-shuffle in each sample. The motivation is that the news articles are containing limited named entities, which are usually celebrities, and the world knowledge can be learned from the dataset. So this methodology aims to exploit general relationships between anonymized named entities within a single document rather than the common knowledge. So they proposed another way to extract query from the original data. In the CBTest datasets, there are four types of sub-datasets available which are classified by the part-of-speech tag of the answer word, containing Named Entities (NE), Common Nouns (CN), Verbs and Prepositions. In their studies, they have found that the answering of verbs and prepositions are relatively less dependent on the content of document, and the humans can even do preposition blank-filling without the presence of the document. As the aim of reading comprehension is to exploit relations between document and query, most of the following studies are only focusing on the NE and CN datasets. In this section, we will give a detailed introduction to the proposed Attention-over-Attention Reader (AoA Reader). of query representation is necessary, and it should be paid more attention to utilizing the information of query. Now, we will give a formal description of our proposed model. Contextual Embedding. We first transform every word in the documentD and queryQ into one-hot representations and then convert them into continuous representations with a shared embedding matrix We. The motivation of using shared embedding weights is that the length of the query is shorter than the document, and thus the embedding weights will not be fully learned by only using a small amount of training data. By embedding sharing, both the document and query can participate in the learning of embedding and both of them will benefit from this mechanism. After that, we use two bi-directional RNNs to get contextual representations of the document and query individually, where the representation of each word is formed by concatenating the forward and backward hidden states. Pair-wise Matching Score. After obtaining the contextual embeddings of the document hdoc and query hquery, we calculate a pair-wise matching matrix, which indicates the pair-wise matching degree of one document word and one query word. Formally, when given ith word of the document and jth word of query, we can compute a matching score by their dot product. Individual Attentions. After getting the pair-wise matching matrixM, we apply a column-wise softmax function to get probability distributions in each column, where each column is an individual document-level attention when considering a single query word. Instead of using naive heuristics (such as summing or averaging) to combine these individual attentions into a final attention, we introduce another attention mechanism to automatically decide the importance of each individual attention. We apply a row-wise softmax function to the pair-wise matching matrix M to get query-level attentions. Our motivation is to exploit mutual information between the document and query. However, most of the previous works are only relying on query-to-document attention, that is, only calculate one document-level attention when considering the whole query. the attention-over-attention mechanism. Intuitively, when we do cloze-style reading comprehensions, we often refill the candidate into the blank of the query to double-check its appropriateness, fluency and grammar to see if the candidate we choose is the most suitable one. If we do find some problems in the candidate we choose, we will choose the second possible candidate and do some checking again. To mimic the process of double-checking, we propose to use N-best re-ranking strategy after generating answers from our neural networks. The procedure can be illustrated as follows. N-best decoding. Refill the candidate into query. As a characteristic of the cloze-style problem, each candidate can be refilled into the blank of the query to form a complete sentence. This allows us to check the candidate according to its context. Feature scoring. The candidate sentences can be scored in many aspects. In this paper, we exploit three features to score the N-best list. This model is trained on the document part of training data. It should be noted that the local LM is trained sample-by-sample, it is not trained on the entire test set, which is not legal in the real test case. This model is useful when there are many unknown words in the test sample. The word class can be obtained by using clustering methods. Weight Tuning. Re-scoring and Re-ranking. After getting the weights of each feature, we calculate the weighted sum of each feature in the N-best sentences and then choose the candidate that has the lowest cost as the final answer. The general settings of our neural network model are listed below in detail. The best baseline results are depicted in italics, and the overall best results are in bold face. The results are reported with the best model, which is selected by the performance of validation set. The ensemble model is made up of four best models, which are trained using different random seed. When it comes to ensemble model, our AoA Reader also shows significant improvements over previous best ensemble models by a large margin and set up a new state-of-the-art system. To investigate the effectiveness of employing attention-over-attention mechanism, we also compared our model to CAS Reader, which used predefined merging heuristics, such as sum or avg etc. As we have seen that the re-ranking approach is effective in cloze-style reading comprehension task. Each row includes all of the features from previous rows. LMglobal denotes the global LM, LMlocal denotes the local LM, LMwc denotes the word-class LM. Generally speaking, in NE category, the performance is mainly boosted by the LMlocal feature. However, on the contrary, the CN category benefit from LMglobal and LMwc rather than the LMlocal. NN denotes the feature (probability) produced by baseline neural network model. The LMglobal and LMwc are all trained by training set, which can be seen as Global Feature. However, the LMlocal is only trained within the respective document part of test sample, which can be seen as Local Feature. Because it is much more likely to meet a new named entity than a common noun in the test phase, so adding the local LM provides much more information than that of common noun. However, on the contrary, answering common noun requires less local information, which can be learned in the training data relatively. In this section, we will give a quantitative analysis to our AoA Reader. The following analyses are carried out on CBTest NE dataset. First, we investigate the relations between the length of the document and corresponding accuracy. As we can see that the AoA Reader shows consistent improvements over AS Reader on the different length of the document. The bar below the figure indicates the number of samples in each interval. Not surprisingly, we found that both models do a good job when the correct answer appear much frequent in the document than the other candidates. The bar below the figure indicates the number of samples in each rank. high or too low). One possible reason is that the model is hard to choose a candidate that has a neutral frequency as the correct answer, because of its ambiguity (neutral choices are hard to made). Cloze-style reading comprehension tasks have been widely investigated in recent studies. We will take a brief revisit to the related works. Along with the release of cloze-style reading comprehension dataset, they also proposed an attention-based neural network to handle this task. Experimental results showed that the proposed neural network is effective than traditional baselines. To handle the reading comprehension task, they proposed a window-based memory network, and self-supervision heuristics is also applied to learn hard-attention. A restriction of this model is that the answer should be a single word and appear in the document. Results on various public datasets showed that the proposed model is effective than previous works. They first applied the reading comprehension model into Chinese zero pronoun resolution task with automatically generated large-scale pseudo training data. Also, we find that our model is typically general and simple than the recently proposed model, and brings significant improvements over these cutting edge systems. We present a novel neural architecture, called attention-over-attention reader, to tackle the clozestyle reading comprehension task. The proposed AoA Reader aims to compute the attentions not only for the document but also the query side, which will benefit from the mutual information. Then a weighted sum of attention is carried out to get an attended attention over the document for the final predictions. Among several public datasets, our model could give consistent and significant improvements over various state-of-theart systems by a large margin. The future work will be carried out in the following aspects. We believe that our model is general and may apply to other tasks as well, so firstly we are going to fully investigate the usage of this architecture in other tasks. In this context, we are planning to investigate the problems that need comprehensive reasoning over several sentences."
117,1,4.0,5.0,4.0,True,acl_2017,train,"Relation detection is a core component for many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning that detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different hierarchies of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to enable one enhance another. Experimental results evidence that our approach achieves not only outstanding relation detection performance, but more importantly, it helps our KBQA system to achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.","The main focus of this work is to improve the relation detection subtask and further explore how it can contribute to the KBQA system. As a result, there is a significant gap between general relation detection studies and KB-specific relation detection. Second, relation detection for KBQA often becomes a zero-shot learning task, since some golded test relations may not appear in the training data. This increases the number of target relation types and the sizes of candidate relation pools, further increasing the difficulty of KB relation detection. Owing to these reasons, KB relation detection is significantly more challenging compared to general relation detection tasks. This paper improves KB relation detection to cope with the above mentioned problems. First, in order to deal with the unseen relations, we propose to break the relation names into word sequences for question-relation matching. a) A single relation example. We first identify the topic entity with entity linking and then detect the relation asked by the question with relation detection (from all relations connecting the topic entity). b) A more complex question containing two entities. to match longer question contexts, we propose to build both relation-level and word-level relation representations. Third, we use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations in order to match the different levels of relation information. Finally, we propose a residual learning method for sequence matching, which makes the model training easier and results in more abstract (deeper) question representations, thus improves hierarchical matching. In order to assess how the proposed improved relation detection could benefit the KBQA end task, we also propose a simple KBQA implementation composed of two-step relation detection. This step is important to deal with the ambiguities normally present in entity linking results. The above steps are followed by an optional constraint detection step, when the question cannot be answered by single relations (e.g. containing multiple entities). Finally the highest scored query from the above steps is used to query the KB for answers. Our main contributions include: (i) An improved relation detection model by hierarchical matching between questions and relations with residual learning; (ii) We demonstrate that the improved relation detector enables our simple KBQA system to achieve state-of-the-art results on both single-relation and multi-relation KBQA tasks. Relation Extraction Relation extraction (RE) is an important sub-field of information extraction. General research in this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. All are much fewer than thousands of relations in KBQA. As a result, few work in this field focuses on dealing with large number of relations or unseen relations. Different KBQA data sets have different levels of requirement about the above open-domain capacity. For example, most of the gold test relations in WebQuestions can be observed during training, thus some prior work on this task adopted the close domain assumption like in the general RE research. While for data sets like SimpleQuestions and ParaLex, the capacity to support large relation sets and unseen relations becomes more necessary. Such factorization works because that the relation names usually comprise meaningful word sequences. Another difference between relation detection in KBQA and general RE is that general RE research assumes that the two argument entities are both available. entity types or entity embeddings). However, while the questions are natural word sequences, how to represent relations as sequences remains a challenging problem. In this case, each relation name is treated as a unique token. The problem with this approach is that it suffers from the low relation coverage due to limited amount of training data, thus cannot generalize well to large number of opendomain relations. In this case, the relation is treated as a sequence of words from the tokenized relation name. It has better generalization, but suffers from the lack of global information from the original relation names. The italics show the evidence phrase for each relation token in the question. Note that without the dotted arrows of shortcut connections between two layers, the model will only compute the similarity between the second-layer of questions representations and the relation, thus is not doing hierarchical matching. The two types of relation representation contain different levels of abstraction. Since the two levels of granularity both have their own pros and cons, we propose a hierarchical matching approach for KB relation detection: for a candidate relation, our approach matches the input question to both word-level and relation-level representations to get the final ranking score. This section describes our hierarchical matching approach. Relation Representations from Different Granularity. We provide our model with both types of relation representation: word-level and relationlevel. We initialize the relation sequence LSTMs with the final state representations of the word sequence, as a back-off for unseen relations. We apply one max-pooling on these two sets of vectors and get the final relation representation hr. Different Abstractions of Questions Representations. Usually relation names could match longer phrases in the question and relation words could match short phrases. Yet different words might match phrases of different lengths. As a result, we hope the question representations could also comprise vectors that summarize various lengths of phrase information (different levels of abstraction), in order to match relation representations of different granularity. We deal with this problem by applying deep BiLSTMs on questions. Since the second BiLSTM starts with the hidden vectors from the first layer, intuitively it could learn more general and abstract information compared to the first layer. Unlike the standard usage of deep BiLSTMs that employs the representations in the final layer for prediction, here we expect that two layers of question representations can be complementary to each other and both should be compared to the relation representation space (Hierarchical Matching). This is important for our task since each relation token can correspond to phrases of different lengths, mainly because of syntactic variations. We could perform the above hierarchical matching by computing the similarity between each layer of and hr separately and doing the (weighted) sum between the two scores. Our analysis shows that this naive method suffers from the training difficulty, evidenced by that the converged training loss of this model is much higher than that of a single-layer baseline model. Intuitively, the proposed method should benefit from hierarchical training since the second layer is fitting the residues from the first layer of matching, so the two layers of representations are more likely to be complementary to each other. This also ensures the vector spaces of two layers are comparable and makes the second-layer training easier. This section describes our KBQA pipeline system. We make minimal efforts beyond the training of the relation detection model, making the whole system easy to build. Compared to previous approaches, the main difference is that we have an additional entity reranking step after the initial entity linking. Having observed that different entity candidates usually connect to different relations, here we propose to help entity disambiguation in the initial entity linking with relations detected in questions. The KBQA end task, as a result, benefits from this process. In this step, we use the raw question text as input for a relation detector to score all relations in the KB with connections to at least one of the entity candidates in EL K (q). We call this step relation detection on entity set since it does not work on a single topic entity as the usual settings. We use the HR-BiLSTM as described in Sec. For each question q, after generating a score s rel (r; q) for each relation using HR-BiLSTM, we use the top l best scoring relations (Rl q) to re-rank the original entity candidates. This method can be viewed as exploiting entity-relation collocation for entity linking. This helps the model better distinguish the relative position of each word compared to the entity. Our method can be viewed as entitylinking on a KB sub-graph. We leave in future work beam-search and feature extraction on beam for final answer re-ranking like in previous research. The top shows performance of baselines. On the bottom we give the results of our proposed model together with the ablation tests. Each question in these datasets is labeled with the gold semantic parse. Hence we can directly evaluate the relation detection performance independently as well as evaluate on the KBQA end task. SimpleQuestions (SQ): It is a single-relation KBQA task. Therefore, our results can be compared with their reported results on both tasks. WebQSP (WQ): A multi-relation KBQA task. For this evaluation we use the full Freebase KB. The baseline BiLSTM with relation word sequence appears to be the best baseline on WebQSP and is close to the previous best result of AMPCNN on SimpleQuestions. This shows that the SimpleQuestions suffers more from unseen relations while WebQSP questions can usually be answered by a fixed small set of relations and relation chains. On SimpleQuestions, even removing the deep layers does not change much the performance. WebQSP benefits more from residual and deeper architecture, possibly because in this dataset it is more important to handle larger scope of context matching. Finally, on WebQSP, replacing BiLSTM with CNN in our hierarchical matching framework results in a large performance drop. Yet on SimpleQuestions the gap is small. We believe this is because the LSTM relation-encoder can better learn the composition of chains of relations in WebQSP, as it is better at dealing with longer dependencies. Remark: To verify that residual learning helps WebQSP because it helps the hierarchical architecture learn different levels of abstraction instead of only benefiting from combination of two BiLSTMs, we replace the deep BiLSTM question encoder with two single-layer BiLSTMs (both on words), with shortcut connections between their hidden states. This proves that the residual and deep structures both contribute to the good performance of HR-BiLSTM. Note that in contrast to previous KBQA systems, our system does not use joint-inference or featurebased re-ranking step, nevertheless it still achieves better or comparable results to the state-of-the-art. Since the reranking step relies on the relation detection models, this shows that our HR-BiLSTM model contributes to the good performance in multiple ways. Appendix C gives the detailed performance of the re-ranking step. KB relation detection is a key step in KBQA and has significant differences from general relation extraction tasks. We propose a novel KB relation detection model, HR-BiLSTM, that performs hierarchical matching between questions and KB relations. Our model outperforms the previous methods on KB relation detection tasks and allows our KBQA system to achieve state-of-the-arts. Constraint-based question answering with knowledge graph. Hannah Bast and Elmar Haussmann. More accurate question answering on freebase. Liang. Semantic parsing on Freebase from question-answer pairs. Jason Weston. Large-scale simple question answering with memory networks. Translating embeddings for modeling multirelational data. In Advances in Neural Information Processing Systems. Zihang Dai, Lei Li, and Wei Xu. Cfo: Conditional focused neural question answering with largescale knowledge bases. Classifying relations by ranking with convolutional neural networks. Paraphrase-driven learning for open question answering. question answering with attention. Improved relation extraction with feature-rich compositional embedding models. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Forbus, and Ni Lao. Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems. Thien Huu Nguyen and Ralph Grishman. Employing word representations and regularization for domain adaptation of relation extraction. Jakob Uszkoreit. A decomposable attention model for natural language inference. Bryan Rink and Sanda Harabagiu. Utd: Classifying semantic relations by combining lexical and semantic resources. Semi-supervised relation extraction with large-scale word clustering. Combining recurrent and convolutional neural networks for relation classification. Liu. Relation classification via multi-level attention cnns. Question answering on freebase via relation extraction and textual evidence. tweet entity linking. Van Durme. Freebase qa: Information extraction or semantic parsing? Xuchen Yao and Benjamin Van Durme. Information extraction over structured data: Question answering with freebase. Jianfeng Gao. Semantic parsing via staged query graph generation: Question answering with knowledge base. In Association for Computational Linguistics (ACL). Semantic parsing for single-relation question answering. Wen-tau Yih, Matthew Richardson, Chris Meek, MingWei Chang, and Jina Suh. The value of semantic parse labeling for knowledge base question answering. Simple question answering by attentive convolutional neural network. and Jun Zhao. Relation classification via convolutional deep neural network. Exploring various knowledge in relation extraction. In Association for Computational Linguistics. Li, Hongwei Hao, and Bo Xu. Attentionbased bidirectional long short-term memory networks for relation classification."
619,1,5.0,3.0,2.0,True,acl_2017,train,This paper presents a new corpus of annotated revisions of argumentative essays. This corpus analyzes between-draft revisions in the context of the full essay. The writer’s intention for each revision is labeled with categories analogous to those used for argument mining and discourse analysis. The corpus enables more advanced research in writing comparison and revision analysis. Applications of the corpus are demonstrated via a study on student revision behaviors and a study on automatic revision intention prediction.,"This work presents an annotated corpus to facilitate revision analysis for argumentative essays. Within the domain of argumentative essays, the corpus may be used for research and application of argument mining techniques and argumentative revision analysis. We expect the corpus to be useful for advanced writing comparison study (discourse level) that connects to the mainstream research on writing analysis (e.g., argument mining). Alice may become satisfied with the increase and move on. If Alice had intended her change as a way to add evidential support for her thesis, she would see that her attempt was not as successful as she hoped. The above scenario highlights the application of a revision analysis system. This paper is about creating a corpus to enable the development of such systems. Because this is a relatively new problem, there are many possible ways for us to design the corpus. Here, we discuss some of our decisions. First, we need to define the unit of revision. In the example above illustrates a phrase-aligned revision. While this offers a more precise definition of the scope of a revision, it may be difficult to achieve consistent annotations. For example, the changes may not adhere to any syntactic linguistic unit. For this first corpus, we define our unit of revision to be at the sentence level. In other words, even if a pair of sentences contain multiple edits, the entire sentence pair will be annotated as one sentence revision. Second, we need to define the quality we want to observe about the revision sentence pair. For this first corpus, we focus on recognizing the purpose of the revision, as in the example above. It is a useful property, and it has previously been studied by others in the literature. People have considered both binary purpose categories such as Content vs. Third, we not only have to decide on the an-notation format, we also need to decide on how to obtain the raw text: argumentative essays with multiple drafts. Comparing to high school students, college students are expected to have a better organization of the argument elements. We decided to give all subjects the same writing prompt and collect three drafts. The identical prompt minimizes the impact of topic difference for argumentation-related study. The collection of three drafts allows for a comparison of revision differences at different stages of rewriting. Finally, we need a method of eliciting two revised drafts from each writer. Ideally, an instructor would give formative feedback after each draft for each student, but we do not have the resources to carry out such an expensive project. We decided to simulate instructor feedback by asking students to add more examples after the first draft. Based on the above design decisions, we have developed a corpus of argumentative essays with three drafts and detailed annotations for sentencealigned revisions between each consecutive pair of drafts. b) Interface B. a) Interface A with the annotated revision purposes, (b) Interface B with a streamlined character-based diff. This time, they are not given additional instructional feedback. Instead, participants are shown a computer interface that highlights the differences between their first and second draft. They are asked to revise the third draft to improve the general quality of their essay. We experimented with two variations of elicitation. Both groups are asked to read a tutorial about their respective interfaces before beginning to revise. Additionally, participants in group A are also asked to verify the manually annotated revision purposes between their first and second draft. This world has no restrictions on whom one can talk to. The only aspects of communication that this new development improves are internet navigation and faux internet relatability. Being immersed in the sphere of new technologies can allow for complete isolation from the active, non-digital world. The sentences were aligned across the drafts and the revision purposes were labeled on the aligned sentence pairs. Two annotators (one is experienced, and the other is newly trained) annotated the files separately and discussed on the disagreed annotations to remove possible annotation errors due to misun-derstanding. The files labeled by the trained annotator were used as the gold standard annotation after the corrections. In addition to the raw text and the annotations, this corpus release also includes meta-data collected about the participants. This includes: a pre-study survey, a post-study survey, and demographic statistics. Pre-Study Survey The pre-study survey contains participant demographic information as well as their self-reported writing background, such as their confidence in their writing ability, the number of drafts they typically make, etc. This corpus will be freely available for research purposes, with the first release coordinated with the publication of this paper. This version release will include: the raw text plus revision annotations, and the meta-data. The revision annotations are stored as. Each spreadsheet file contains two sheets: Old Draft and New Draft. Each row in the sheet represents one sentence in the corresponding draft. The index of the aligned sentence row in the other draft and the type of the revision on the sentence are recorded. The meta-data are in. Information in the text files are stored as the JSON data format. While the development of a full fledged revision analysis system is outside the scope of this work, we demonstrate the potential applications of our corpus with two examples. The first performs statistical analyses on the collected revision data and meta-data to understand aspects of participant behaviors. The second uses the corpus to train a supervised classifier to automatically predict revision purposes. There is much we do not know about how to stimulate students to self-reflect and revise. Using the Argumentative Revision Corpus, we can begin to ask and address some questions about student revision habits and behaviors. Our first question is: How do different types of revision feedback impact student revision? To test the hypotheses, we will use both subjective and objective measures. Subjective measures are based on participant post-study survey answers. To compare differences between specific subgroups on the subjective and objective measures, we conduct ANOVA tests with two factors. For example, one factor is the native language of the participant, and another is the interface used. determine correlation between quantitative measures, we conduct Pearson and Spearman correlation tests. Additional information about revision purposes may elicit a stronger self-reflection response in Group A participants. This suggests that the character-based interface is ineffective when participants have to reflect on many changes. This suggests that giving feedback (from either interface) encourages native speakers to make more content revisions. Summary Experimental findings over the three hypotheses suggest that feedback on revisions do impact how students review and rewrite their drafts. Another application of the corpus is to serve as the gold standard for training and evaluation of an automatic revision analysis system. One subtask of such a system is to predict the intention of the revision purpose. They developed and reported the performance of a binary classifier for each individual revision category using features from the argument mining and discourse analysis researches. The availability of our corpus makes it possible for researchers to replicate their findings and conduct further studies. The same revision classification method works differently for first revision attempts and second revision attempts. Unweighted average F-score for each category is reported. We compared the results using unigram features and the results using all the features. We compared the improvement ratio brought by the advanced features over the unigram baseline. We would expect that the performance of the two trained classifiers is different on the same test data. Bold indicates larger than the number in the other row. Perhaps essays of native speakers are more similar to each other when revised along these dimensions. In this paper we present a new corpus for writing comparison research. In addition to three drafts of essays, we have compared and analyzed the drafts to align semantically identical sentences and assigned revision purposes for each aligned revision sentence pairs. We have also conducted two separate studies to demonstrate the application of the corpus for revision behavior analysis and for automatic revision purpose classification. We plan to further augment the corpus to advance research on revision analysis in the future."
588,3,4.0,3.69,2.62,False,acl_2017,train,"Reading comprehension in NLP refers to the ability of models to answer any question about a passage accurately. An important open problem is how to effectively use external knowledge to answer such questions. In this paper, we introduce a new task and derive new models to drive progress towards this goal. In particular, we propose the task of rare entity prediction: given a web document with several entities removed, models are tasked with predicting the correct missing entities conditioned on the document context and the lexical resources. This task is challenging due to the diversity of language styles and the extremely large number of rare entities. Our experiments show that models that make use of external knowledge in the form of lexical resources, particularly our model using hierarchical LSTMs, perform significantly better at rare entity prediction than those that do not.","A natural setting for testing the abilities of models to acquire knowledge through natural language is reading comprehension, where models must answer questions about a given document. In this work, we aim to move towards reasoning about specific instances of entities in context. This is a very difficult problem, as we have very few training samples per instance; thus, we demonstrate that we cannot simply rely on language modelling, and must leverage external sources of knowledge. These models show performance improvements on tasks such as knowledge base completion. However, such ideas are rarely examined for reading comprehension tasks. In this paper, we propose a novel task called rare entity prediction, where models must predict missing entities (e.g. proper names or places) in web articles by leveraging the available lexical resources. For this task, we provide a significantly enhanced version of the Wikilinks dataset, with entity descriptions extracted from Freebase serving as the lexical resources, which we call the Wikilinks Entity Prediction dataset. In addition to the task and dataset, we introduce several models that can be used to solve this task, using distributional semantics and recurrent neural networks. We show that language modelling baseline models that do not consider the Freebase descriptions are unable to achieve good performance on the task. Rare entity prediction task is unique because it is difficult for models that heavily rely on word cooccurrence statistics to achieve good performance. To solve the task, models need to have the abilities to incorporate external a priori knowledge with an understanding of the unstructured natural language. Some questions can be challenging to answer even for humans without proper prior knowledge about the candidate entities. We believe that being able to integrate multiple sources of knowledge is crucial for not only reading comprehension, but also other NLP systems. Related to our work is the task of entity prediction, also called link prediction or knowledge base completion, in the domain of multi-relational data. In entity prediction, either the head or tail entity is removed, and the model has to predict the missing entity. While this task necessitates understanding structured information in the form of relationships between objects, it does not require the processing of natural language text, which is crucial in rare entity prediction. Given an article, models are tasked with filling in blanks of one-sentence summaries of the article. While these tasks require the understanding of unstructured natural language, it does not require integration with external knowledge sources. The purpose of their model is to obtain more accurate word and phrase embeddings by combining lexical and phrasal semantics, and they achieve fairly good performance on a reverse dictionary and crossword QA tasks. The authors propose a WikiFacts dataset where Wikipedia descriptions are aligned with Freebase facts. While they also aim to integrate external knowledge with unstructured natural language, their task differs from ours in that it is primarily a language modeling problem. However, the nature of the unstructured data is different: in factoid QA, questions are often short and relate directly to the relations of a given entity, while rare entity prediction requires learning much longer-term dependencies in natural language, where relations between entities are not specifically referred to. The dataset consists of a list of non-Wikipedia web pages (discovered using the Google search index) that contain hyperlinks to Wikipedia, such as random blog posts or news articles. Every token with a hyperlink to Wikipedia is then marked and considered an entity mention in the dataset. Each entity mention is also linked back to a knowledge base through their corresponding Freebase IDs. To ensure the quality of data, the authors performed manual inspection on a small subset of the web pages. No incorrect hyperlinks were found. In particular, we parse the HTML texts of the web pages and extract their page contents to form our corpus. Additionally, we extract the lexical definitions of all entities that are marked in the corpus from Freebase using their Freebase IDs, which are available for all entities in the Wikilinks dataset. These lexical definitions will serve as the external knowledge to our models. This suggests that models that only rely on the surrounding context information may not be able to correctly predict the missing entities. This further motivates us to incorporate additional information into the decision process to improve the performance. In the experiments section, we show that the lexical resources are indeed necessary to achieve better results. For instance, we could define E so that it includes all entities found in the corpus. However, given the extremely large amount of unique entities found in the dataset, this would render the task difficult to solve from both a practical and computational perspective. In this section, we present two models that use the lexical definitions of entities to solve the proposed rare entity prediction problem. The basic building blocks of our models are recurrent neural networks (RNN) with long short-term memory (LSTM) units. This definition embedding is then fed as the blank input token of context Ci to the context encoder f (blue circles), which provides a context embedding hei. LSTMs are an extension of RNNs which include a memory cell ct alongside their hidden state representation ht. We denote the output (i.e. the last hidden state) of an RNN f operating on a sequence S as f(S), and subscript the t-th hidden state as ft(S). We use two jointly trained recurrent models, a lexical encoder g(.) and a context encoder f(. The lexical encoder converts the definition of an entity into a vector embedding, while the context encoder repeats the same process for a given context to obtain its context embedding. These two embeddings are then used by P to predict if the given entity-context pair is correct. Additionally, the blank in the context sentence is replaced by the encoded definition embedding to provide more information to f. For a given context Ci, we replace the embedding of the blank token with de. The cross term (hei) TWde is a dot product between hei and de that weighs the dimensions differently based on the learned parametersW. Note that while hei is a function of de, using de in the cross term (hei) TWde provides a much shorter gradient path from the loss to the lexical encoder through de. Entities in the candidate set are then ranked against each other according to their predicted probabilities. The entity with the highest probability is considered as the most plausible answer for the missing entity in the current context. HIERENC) The double encoder model above considers each context independently. However, since each document consists of a sequence of contexts, the knowledge carried by other contexts in C could also provide useful information for the decision process of Ci. Since a document is a sequence of Cis, each timestep of this network consists of one such context, and thus is indexed with i. Since we already have a context encoder f, we reuse the output of f(Cei) as the input of r at timestep i. Note that alternatively, one could aggregate information about the past predictions through other means like policies or soft attention. However, this would introduce extra complexities to the learning process. As such, we use averaging to that end. The entities in candidate set are again ranked against each other based on their probabilities. We randomly partition our dataset into training, validation and test sets. The lexical definitions for each entity are the first sentences of their Freebase descriptions. We have experimented with different configurations of defining contexts and entity definitions, such as expanding the context window by including the sentences before and after the blank, as well as taking more than one sentence out of the entity description. However, results on the validation set show that increasing the context window size and definition size has very little impact on the evaluation metrics and drastically increases the training time of all models. To train our models, we use the correct missing entity for each blank as the positive sample and all other entities in the candidate set as the negative samples. During the testing phase, we present each entity in the candidate set to our models and record the probabilities output by the models. The entity with the highest probability is chosen as the model prediction. For all gradient-based methods, including both baseline models and our proposed models, the learning objective is to minimize the binary cross-entropy of the training data. We measure the performance on our entity prediction task using the accuracy; that is, the number of correct entity predictions made by the model divided by the total number of predictions. RANDOM For each context in a given document, this baseline simply selects an entity from the candidate set uniformly at random as its prediction. FREQENT Under this baseline, we rank all entities in the candidate set by the number of times that they appear in the document. For each blank in the document, we always choose the entity with the highest frequency in that document as the prediction. Note that this model has access to extra information compared to the other models, in particular the total number of times each entity appears in the document. CONTENC Instead of using their definitions, entities are treated as regular tokens in our vocabulary. We feed the sequence Ci into the context encoder and as usual take the last hidden state as the context embedding hei. This model is essentially a language model baseline, that does not make use of the external a priori knowledge. The aggregations of contexts and definitions are treated as their own corpora, and two separate TF-IDF transformers are fitted. Candidate entities are ranked by the cosine similarity between their definition vectors and the context vector. The entity with the highest cosine similarity score is chosen as the prediction. pre-trained word embeddings. Same as above, the prediction is made by considering the cosine similarity between the context embedding and the entity embeddings. Models with the best performance on validation set are saved and used to test on the test set. The large discrepancy between the context encoder and the double encoder shows that lexical resources play a crucial role in solving the task. The best result is achieved by the hierarchical double encoder, which confirms that knowing about previous contexts is indeed beneficial to the prediction at the current timestep. She was in, Cyprus. Larnaca: Larnaca is a city on the southern coast of Cyprus and the capital of the eponymous district. Ben Macintyre: Ben Macintyre is a British author, historian, reviewer and columnist writing for The Times newspaper. HIERENC was able to successfully predict the correct missing entity, Larnaca. Although the context encoder was able to identify that the missing entity should be a city, it incorrectly predicted Istanbul. It seems that, although the context encoder was able to derive a strong association between Istanbul and Middle Eastern geolocations, such knowledge was not learned for Larnaca because of the lack of examples. Conversely, the hierarchical double encoder was able to take both the context and the external knowledge into account and successfully predicted the correct missing city. One interesting observation is the margin of difference in accuracy between the context encoder and the embedding average baseline. The context encoder, which is a relatively sophisticated context-only model, only slightly outperforms the simple embedding average baseline that has no learning component. This suggests that the lexical definitions are valuable in solving such tasks even when it is used in a rather simplistic way. Another interesting observation is that, in our experiments we found that using a large context window size (including the sentences before and after the sentence where the blank is found) does not have any significant positive impact on the results. This implies that the words that are most informative about the missing entity in the blank are generally found in the vicinity of the blank. It is likely that more sophisticated models will be able to use the surrounding context information more effectively, leading to greater performance increases. In this paper, we examine the use of external knowledge in the form of lexical resources for solving reading comprehension problems. Specifically, we propose the problem of rare entity prediction. In our Wikilinks Rare Entity Prediction dataset, the majority of the entities have very low frequencies across the entire corpus; thus, models that solely rely on co-occurrence statistics tend to underperform. We show that models leveraging the Freebase descriptions achieve large performance increases, particularly when this information is incorporated intelligently as in our double encoder-based models. For future work, it would be interesting to examine the effects of other knowledge sources. In this paper, we use entity definitions as the source of external knowledge. However, Freebase also contains other types of valuable information, such as relational information between entities. Thus, one potential direction for future work would be to incorporate the relational information, alongside the lexical definitions, to achieve better results. We have seen the crucial role that external knowledge plays in solving tasks with many rare entities. Thus we believe that incorporating external knowledge into other NLP systems, such as dialogue agents, should also see similar positive results. We plan to explore the idea of external knowledge integration further in our future research."
79,1,4.0,3.0,3.0,True,acl_2017,train,"Knowledge bases are important resources for a variety of natural language processing tasks but suffer from incompleteness. We propose a novel embedding model, ITransF, to perform knowledge base completion. Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts. Moreover, the learned associations between relations and concepts, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets— WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information.","The fundamental motivation behind these studies is that there exist some statistical regularities under the intertwined facts stored in the multirelational knowledge base. By discovering gener-alizable regularities in known facts, missing ones may be recovered in a faithful way. Due to its excellent generalization capability, distributed representations, a.k.a. Implicitly, TransE assumes both entity embeddings and relation embeddings dwell in the same vector space, posing an unnecessarily strong prior. Typically, these relation-dependent spaces are characterized by the projection matrices unique to each relation. As a benefit, different aspects of the same entity can be temporarily emphasized or depressed as an effect of the projection. Despite the superior performance of STransE compared to TransE, it is more prone to the data sparsity problem. Concretely, since the projection spaces are unique to each relation, projection matrices associated with rare relations can only be exposed to very few facts during training, resulting in poor generalization. For common relations, a similar issue exists. Without any restrictions on the number of projection matrices, logically related or conceptually similar relations may have distinct projection spaces, hindering the discovery, sharing, and generalization of statistical regularities. Since the number of paths grows exponentially with its length, as a side effect, path-based models enjoy much more training cases, suffering less from the problem. In this paper, we propose an interpretable knowledge transfer model (ITransF), which encourages the sharing of statistic regularities between the projection matrices of relations and alleviates the data sparsity problem. At the core of ITransF is a sparse attention mechanism, which learns to compose shared concept matrices into relation-specific projection matrices, leading to a better generalization property. In addition, the parameter sharing is clearly indicated by the learned sparse attention vectors, enabling us to interpret how knowledge transfer is carried out. To induce the desired sparsity during optimization, we further introduce a block iterative optimization algorithm. In summary, the contributions of this work are: (i) proposing a novel knowledge embedding model which enables knowledge transfer by learning to discover shared regularities; (ii) introducing a learning algorithm to directly optimize a sparse representation from which the knowledge transferring procedure is interpretable; (iii) showing the effectiveness of our model by outperforming baselines on two benchmark datasets for knowledge base completion task. Let E denote the set of entities and R denote the set of relations. As discussed above, a fundamental weakness in TransR and STransE is that they equip each relation with a set of unique projection matrices, which not only introduces more parameters but also hinders knowledge sharing. Intuitively, many relations share some concepts with each other, although they are stored as independent symbols in KB. Inspired by the existence of such lower-level concepts, instead of defining a unique set of projection matrices for every relation, we can alternatively define a small set of concept projection matrices and then compose them into customized projection matrices. Effectively, the relation-dependent translation space is then reduced to the smaller concept spaces. However, in general, we do not have prior knowledge about what concepts exist out there and how they are composed to form relations. Therefore, in ITransF, we propose to learn this information simultaneously from data, together with all knowledge embeddings. Following this idea, we first present the model details, then discuss the optimization techniques for training. We let each relation select the most useful projection matrices from the tensor, where the selection is represented by an attention vector. Hence our model space is a generalization of STransE. Note that we have omitted the dependence of N on (h, r, t) to avoid clutter. Sparse attention vectors In Eq. With a dense attention vector, it is computationally expensive to perform the convex combination of m matrices in each iteration. Moreover, a relation usually does not consist of all existing concepts in practice. Furthermore, when the attention vectors are sparse, it is often easier to interpret their behaviors and understand how concepts are shared by different relations. Motivated by these potential benefits, we further hope to learn sparse attention vectors in ITransF. Thus, we resort to an approximated algorithm in this work. Based on this notion, the high-level idea of the approximated algorithm is to iteratively optimize one of the two partitions while holding the other one fixed. Since all parameters in the dense partition, including the embeddings, the projection matrices, and the pre-softmax scores, are fully differentiable with the sparse partition fixed, we can simply utilize SGD to optimize the dense partition. Then, the core difficulty lies in the step of optimizing the sparse partition (i.e. Satisfying the two criterion seems to highly resemble the original problem defined in Eq. However, the dramatic difference here is that with parameters in the dense partition regarded as constant, the cost function is decoupled w.r.t. Note that, however, IHr and ITr are still coupled, without which we basically reach the situation in a backpack problem. To avoid adding another inner loop to our algorithm, we turn to a simple but fast approximation method based on the following single-matrix cost. Analogously, we can define the single-matrix cost LTr,i and the energy function fTr,i(h, t) on the tail side in a symmetric way. Then, the update rule for IHr follows the same derivation. Admittedly, the approximation described here is relatively crude. We leave the further improvement of the optimization method as future work. The distribution of negative triple is denoted by N(h, r, t). However, uniformly sampling corrupted entities may not be optimal. Often, the head and tail entities associated a relation can only belong to a specific domain. When the corrupted entity comes from other domains, it is very easy for the model to induce a large energy gap between true triple and corrupted one. As the energy gap exceeds, there will be no training signal from this corrupted triple. In comparison, if the corrupted entity comes from the same domain, the task becomes harder for the model, leading to more consistent training signal. We ran minibatch SGD until convergence. We use the same hyperparameters as used in STransE and no significant improvement is observed when we alter hyperparameters. Indeed, a lot of facts are easier to recover with the help of multi-step inference. For example, if we know Barack Obama is born in Honolulu, a city in the United States, then we easily know the nationality of Obama is the United States. We plan to extend our model to multi-step path in the future. To provide a detailed understanding why the proposed model achieves better performance, we present some further analysis in the sequel. Performance on Rare Relations In the proposed ITransF, we design an attention mechanism to encourage knowledge sharing across different relations. Naturally, facts associated with rare relations should benefit most from such sharing, boosting the overall performance. The first group contains intrinsic models without using extra information. The second group make use of additional information. Results in the brackets are another set of results STransE reported. Interpretability In addition to the quantitative evidence supporting the effectiveness of knowledge sharing, we provide some intuitive examples to show how knowledge is shared in our model. As we mentioned earlier, the sparse attention vectors fully capture the association between relations and concepts and hence the knowledge transfer among relations. For example, PhD is a hyponym of student and student is a hypernym of PhD. with the tail entities in its reverse relation. A typical example is (New York,instance hypernym, city). Finally, symmetric relations like spouse behave similarly as mentioned before. Model Compression A byproduct of parameter sharing mechanism employed by ITransF is a much more compact model with equal performance. Sparseness is desirable since it contribute to interpretability and computational efficiency of our model. We investigate whether enforcing sparseness would deteriorate the model performance and compare our method with another sparse encoding methods in this section. Dense Attention vs Sparse Attention Although sparsity usually enjoys many practical advantages, it may deteriorate the model performance when applied improperly. Here, we show that our model employing sparse attention can achieve similar results with dense attention with a significantly less computational burden. Nonnegative Sparse Encoding In the proposed model, we induce the sparsity by a carefully designed iterative optimization procedure. Apart from this approach, one may utilize sparse encoding techniques to obtain sparseness based on the pretrained projection matrices from STransE. Note that the X-axis denoting the number of matrices is not linearly scaled. Basically, A plays the same role as the attention vectors in our model. On both benchmarks, ITransF achieves significant improvement against sparse encoding on pretrained model. This performance gap should be expected since the objective function of sparse encoding methods is to minimize the reconstruction loss rather than optimize the criterion for link prediction. Further, they do not solve the data sparsity problem because there is no sharing of projection matrices which have a lot more parameters. Data sparsity is a common problem in many fields. However, the sorting operation in these works is not GPU-friendly. They allocate every word in the vocabulary in a table. A word is represented by a row vector and a column vector depending on its position in the table. They iteratively optimize embeddings and allocation of words in tables. In summary, we propose a knowledge embedding model which can discover shared hidden concepts, and design a learning algorithm to induce the interpretable sparse representation. Empirically, we show our model can improve the performance on two benchmark datasets without external resources, over all previous models of the same kind. In the future, we plan to enable ITransF to perform multi-step inference, and extend the sharing mechanism to entity and relation embeddings, further enhancing the statistical binding across parameters. In addition, our framework can also be applied to multi-task learning, promoting a finer sharing among different tasks."
96,3,4.7,4.0,3.4,True,acl_2017,train,"Sarcasm is a form of speech in which speakers say the opposite of what they truly mean in order to convey a strong sentiment. In other words, ”Sarcasm is the giant chasm between what I say, and the person who doesn’t get it.”. In this paper we present the novel task of sarcasm interpretation, defined as the generation of a non-sarcastic utterance conveying the same message as the original sarcastic one. We introduce a novel dataset of 3000 sarcastic tweets, each interpreted by five human judges. Addressing the task as monolingual machine translation (MT), we experiment with MT algorithms and evaluation measures. We then present SIGN: an MT based sarcasm interpretation algorithm that targets sentiment words, a defining element of textual sarcasm. We show that while the scores of n-gram based automatic measures are similar for all interpretation models, SIGN’s interpretations are scored higher by humans for adequacy and sentiment polarity. We conclude with a discussion on future research directions for our new task.1","user generated content, in environments such as Twitter, Facebook, Reddit and many more. A literal analysis of this sentence demonstrates a positive experience, due to the use of the word wonderful. However, if we knew that the sentence was meant sarcastically, wonderful would turn into a word of a strong negative sentiment. In spoken language, sarcastic utterances are often accompanied by a certain tone of voice which points out the intent of the speaker, whereas in textual communication, sarcasm is inherently ambiguous, and its identification and interpretation may be challenging even for humans. In this paper we present the novel task of interpretation of sarcastic utterances. We define the purpose of the interpretation task as the capability to generate a non-sarcastic utterance that captures the meaning behind the original sarcastic text. Our task is complex since sarcasm can be expressed in many forms, it is ambiguous in nature and its understanding may require world knowledge. Way to go California! Great, a choice between two excellent candidates, Donald Trump or Hillary Clinton. Instead, previous knowledge is required if one wishes to fully understand and interpret what went wrong with California, or who Hillary Clinton and Donald Trump are. Since sarcasm is a refined and indirect form of speech, its interpretation may be challenging for certain populations. Extracting the honest meaning behind the sarcasm may alleviate such issues. We conclude with a discussion on future research directions for our task, regarding both algorithms and evaluation. In computational work, the interest in sarcasm has dramatically increased over the past few years. Therefore, the following sections are dedicated to previous work from neighboring NLP fields which are relevant to our work: sarcasm detection, MT, paraphrasing and text summarization. Sarcasm Detection Recent computational work on sarcasm revolves mainly around detection. Due to the large volume of detection work, we survey only several representative examples. emojis and whether the utterance is a comment to another person, in order to train a classifier that distinguishes sarcastic utterances from tweets of positive and negative sentiment. Consequently, they described a bootstrapping algorithm that learns distinctive phrases connected to negative situations along with a positive sentiment and used these phrases to train their classifier. Moreover, it presents fundamental notions, such as the sentiment polarity of the sarcastic utterance and of its interpretation, that we adopt. Finally, when utterances are not marked for sarcasm as in the Twitter domain, or when these labels are not reliable, detection is a necessary step before interpretation. Machine Translation We approach our task as one of monolingual MT, where we translate sarcastic English into non-sarcastic English. Therefore, our starting point is the application of MT techniques and evaluation measures. For automatic MT evaluation, often an n-gram co-occurrence based scoring is performed in order to measure the lexical closeness between a candidate and a reference translations. Later we will show that these algorithms can be further improved and will explore the quality of the MT evaluation measures in the context of our task. Paraphrasing and Summarization Tasks such as paraphrasing and summarization are often addressed as monolingual MT, and so they are close in nature to our task. Work on paraphrasing and summarization is often evaluated using MT evaluation measures such as BLEU. As BLEU is precision-oriented, complementary recall-oriented measures are often used as well. PINC is often combined with BLEU due to their complementary nature: while PINC rewards n-gram novelty, BLEU rewards similarity to the reference. To properly investigate our task, we collected a dataset, first of its kind, of sarcastic tweets and their non-sarcastic (honest) interpretations. This data, as well as the instructions provided for our human judges, will be made publicly available and will hopefully provide a basis for future work regarding sarcasm on Twitter. Despite the focus of the current work on the Twitter domain, we consider our task as a more general one, and hope that our discussion, observations and algorithms will be beneficial for other domains as well. We employed ten Fiverr workers, half of them from the field of comedy writing, and half from the field of literature paraphrasing. We instructed the workers to translate each sarcastic tweet into a non sarcastic utterance, while maintaining the original meaning. We encouraged the workers to use external knowledge sources (such as Google) if they came across a subject they were not familiar with, or if the sarcasm was unclear to them. The table demonstrates the tendency of the workers to generally agree on the core meaning of the sarcastic tweets. Yet, since sarcasm is inherently vague, it is not surprising that the interpretations differ from one worker to another. For example, some workers change only one or two words from the original sarcastic tweet, while others rephrase the entire utterance. We regard this as beneficial, since it brings a natural, human variance into the task. This variance makes the evaluation of automatic sarcasm interpretation algorithms challenging, as we further discuss in the next section. As mentioned above, in certain cases world knowledge is mandatory in order to correctly evaluate sarcasm interpretations. Furthermore, we notice that transforming a sarcastic utterance into a non sarcastic one often requires to change a small number of words. This is not typical for MT, where usually the entire source sentence is translated to a new sentence in the target language and we would expect lexical similarity between the machine generated translation and the human reference it is compared to. This raises a doubt as to whether n-gram based MT evaluation measures such as the aforementioned are suitable for our task. Automatic Measures We use BLEU and ROUGE as measures of n-gram precision and recall, respectively. The scores are: adequacy: the degree to which the interpretation captures the meaning of the original tweet; and fluency: how readable the interpretation is. As our task is about the generation of one English sentence given another, a natural starting point is treating it as monolingual MT. We hence begin with utilizing two widely used MT systems, representing two different approaches: Phrase Based MT vs. Neural MT. We then analyze the performance of these two systems, and based on our conclusions we design our SIGN model. Henceforth we refer to this system as RNN. We train Moses and the RNN on the training set and tune their parameters on the development set. Moses scores much higher in terms of BLEU and ROUGE, meaning that compared to the RNN its interpretations capture more n-grams appearing in the human references while maintaining high precision. While the interpretations generated by the RNN are readable, they generally do not maintain the meaning of the original tweet. We believe that this is the result of the neural network overfitting the training set, despite regularization and dropout layers, probably due to the relatively small training set size. In order to deal with such cases, we wish to utilize a property typical of sarcastic language. used to convey a certain emotion by using strong sentiment words that express the exact opposite of their literal meaning. Hence, many sarcastic utterances can be correctly interpreted by keeping most of their words, replacing only sentiment words with expressions of the opposite sentiment. If we transform this word into a word of the opposite sentiment, such as worst, then we get a non-sarcastic utterance with the correct sentiment. We next present the Sarcasm SIGN (Sarcasm Sentimental Interpretation GeNerator), an algorithm which capitalizes on sentiment words in order to produce accurate interpretations. First, it clusters sentiment words according to semantic relatedness. Consequently, at test time the MT system outputs non-sarcastic utterances with clusters replacing sentiment words. Finally, SIGN performs a declustering process on these MT outputs, replacing sentiment clusters with suitable words. Upon receiving a sarcastic tweet, at both training and test, SIGN searches it for sentiment words according to the positive and negative sets. If such a word is found, it is replaced with its cluster. During training, this process is also applied to the non-sarcastic references. Moses is then trained on these new representations of the corpus, using the exact same setup as before. We choose these embeddings since they are believed to better capture the relations between a word and its context, having been trained on dependency-parsed sentences. cess takes place: the clusters are replaced with the appropriate sentiment words. We expect this process to improve the quality of sarcasm interpretations in two aspects. First, as mentioned earlier, sarcastic tweets often differ from their non sarcastic interpretations in as little as one sentiment word. SIGN should help highlight the sentiment words most in need of interpretation. Second, under the pre-processing SIGN performs to the input examples of Moses, the latter is inclined to learn a mapping from positive to negative clusters, and vice versa. This is likely to encourage the Moses output to generate outputs of the same sentiment as the original sarcastic tweet, but with honest sentiment words. For example, if the sarcastic tweet expresses a negative sentiment with strong positive words, the non-sarcastic interpretation will express this negative sentiment with negative words, thus stripping away the sarcasm. the tweet and its interpretation are not identical). In cases where one of our models presents significant improvement over Moses, the results are decorated with a star. While for fluency Moses and SIGN-context perform similarly, SIGN-context performs much better in terms of adequacy and the percentage of tweets with the correct sentiment. If for each of the algorithms we only regard to interpretations that differ from the original sarcastic tweet, the differences between the models are less substantial. Both tables consistently show that the contextbased selection strategy of SIGN outperforms the centroid alternative. This makes sense as, being context-ignorant, SIGN-centroid might produce non-fluent or inadequate interpretations for a given context. Nonetheless, even this naive de-clustering approach substantially improves adequacy and sentiment accuracy over Moses. Finally, comparison to SIGN-oracle reveals that the context selection strategy is not far from human performance with respect to both automatic and human evaluation measures. Still, some gain can be achieved, especially for the human measures on tweets that were changed at interpretation. This indicates that SIGN can improve mostly through a better clustering of sentiment words, rather than through a better selection strategy. The performance gap between Moses and SIGN may stem from the difference in their optimization criteria. Moses aims to optimize the BLEU score and given the overall lexical similarity between the original tweets and their interpretations, it therefore tends to keep them identical. SIGN, in contrast, targets sentiment words and changes them frequently. Consequently, we do not observe substantial differences between the algorithms in the automatic measures that are mostly based on n-gram differences between the source and the interpretation. Likewise, the human fluency measure that accounts for the readability of the interpretation is not seriously affected by the translation process. To further understand the relationship between the automatic and the human based measures we computed the Pearson correlations for each pair of (automatic, human) measures. We believe this indicates that these automatic measures do not provide appropriate evaluation for our task. Designing automatic measures is hence left for future research. In some cases, however, SIGN struggles with producing correct interpretations. Notice that the sarcasm in this tweet is not expressed by specific sentiment words. Moreover, world knowledge (who Lebron is, what kind of help he requires) is crucial in order to comprehend and interpret this sarcastic utterance. Further Improving SIGN so that it properly interprets such tweets is a major future direction."
727,2,4.57,4.0,3.57,True,acl_2017,train,"Framing is a political strategy in which politicians carefully word their statements in order to control public perception of issues. Previous works exploring political framing typically analyze frame usage in longer texts, such as Congressional speeches. We present a collection of weakly supervised models which harness collective classification to predict the frames used in political discourse on the microblogging platform, Twitter. Our global probabilistic models show that by combining both lexical features of tweets and network-based behavioral features of Twitter, we are able to increase the average, unsupervised F1 score by 21.52 points over a lexical baseline alone.","In recent U.S. presidential elections, Twitter was widely used by all candidates to promote their agenda, interact with supporters, and attack their opponents. Social interactions on such platforms allow politicians to quickly react to current events and gauge interest in and support for their actions. These dynamic settings both emphasize the importance of constructing automated tools for analyzing this content, but also the difficulty of constructing such tools as the language used to discuss new events and political agendas continuously changes. Consequently, the rich social interactions on Twitter can be leveraged to help support such analysis by providing alternatives to direct supervision. In this paper we focus on political framing, a very nuanced political discourse analysis task, on Twitter, a relatively unexplored domain for this task. For example, the debate around increasing the minimum wage can be framed as a quality of life issue or as an economic issue. Our dataset consists of the tweets authored by all members of the U.S. Congress from both parties, dealing with several policy issues (e.g., immigration, ACA, etc.). for Twitter. Twitter issue framing is a challenging multilabel prediction task. Instead of following a supervised path, our main goal in this paper is to evaluate whether the social and behavioral information available on Twitter is sufficient for constructing a reliable classifier for this task. We approach this task using a weakly supervised collective classification approach which leverages the dependencies between tweet frame predictions based on the interactions between their authors. We model these dependencies by connecting Twitter users that have social connections or behavioral similarity. Interestingly, these social connections capture the flow of influence within political parties; however, the number of connections that cross party lines is extremely low. Instead we rely on capturing behavioral similarity between users to provide us with this information. We construct a temporal histogram for each politician which captures their Twitter activity over time. Users whose Twitter activity peaks at similar times tend to discuss issues in similar ways, making the comparison between their frames usage easier. We apply our unsupervised model to our entire tweets dataset to analyze framing patterns over time by both party and individual politicians. In recent years there has been growing interest in analyzing political discourse. have shown how wording choices can affect message propagation on Twitter. We therefore dropped it from this study. annotators were allowed to label each tweet with multiple frames when one primary frame was not possible. For all such tweets, annotators repeated the annotation process together to determine if the tweets could be represented by a single frame or required more. Due to the dynamic nature of political discourse on Twitter, we design weakly supervised PSL models to require as little supervision as possible. The local models described in this section are data-dependent and used to extract and format information from tweets into input for PSL predicates and rules. PSL is a declarative modeling language which can be used to specify weighted, first-order logic rules. MLNs, in which rules are strictly true or false. the Expectation-Maximization algorithm in our unsupervised experiments. More important rules (i.e., those with larger weights) are given preference by the model. We expect that if a tweet and frame contain a matching unigram, then that tweet is likely expressed by that frame. The information that tweet T has expected unigram U of frame F is represented with the PSL predicate: HASUNIGRAMF (T, U). However, not every tweet will have a unigram that matches those in this list. Under the intuition that at least one unigram in a tweet should be similar to a unigram in the list, we designed the fol-lowing MaxSim metric to compute the maximum similarity between a word in a tweet and a word from the list of unigrams. Slogans are common catch phrases or sayings that people typically associate with different U.S. political parties. To visualize slogan usage by parties for different issues, we used the entire tweets dataset, including all unlabeled tweets, to extract the top bigrams and trigrams per party for each issue. Each model adds to the rules of the previous model. For brevity we show a subset of the rules and omit full model combinations. The full list of rules per model will be released with our dataset. prevent gun violence) as input to PSL predicates PARTYBIGRAMP (T, B) and PARTYTRIGRAMP (T, TG). These rules represent that tweet T has bigram B or trigram TG from the respective phrase lists of either party (i.e., P represents either Democrat or Republican in the rule instantiation). In addition to language based features of tweets, we also exploit the behavioral features of Twitter including similarities between temporal activity and network relationships. Temporal Similarity: When an event happens politicians are most likely to tweet about that event within hours of its occurrence. Similarly, most politicians tweet about the event most frequently the day of the event and this frequency decreases over time. We expect that the frames used the day of an event will be similar and change over time. Due to the compound nature of tweets, retweeting with additional comments can add more frames to the original tweet. Additionally, politicians on Twitter are more likely to follow members of their own party or similar non-political entities than those of the opposing party. Experimental Settings: We provide an analysis of our PSL models under both supervised and unsupervised settings. In the supervised experiments, we used five-fold cross validation with randomly chosen splits. Supervised, unigram model is the lowest score the models can achieve. Supervised, collective network model is the highest. generalize to future discussions. The labeled tweets are used for evaluation only. Evaluation Metrics: Since each tweet can have more than one frame, our prediction task is a multilabel classification task. There are a few interesting aspects of the unsupervised setting which differ from the supervised setting. This suggests that bigrams may not be as useful as trigrams in an unsupervised setting. These results suggest that retweet behaviors are not as useful as the follower network relationships in an unsupervised setting. Though the parties use similar frames, they are used to express different agendas. The highest prediction per frame is marked in bold. The highest prediction per frame is marked in bold. This seems to indicate that parties possibly adopt new frames simultaneously or in response to the opposing party, perhaps in an effort to be in control of the way the message is delivered through that frame. Individual Frames: In addition to entire party analysis, we were interested in seeing if frames could shed light on the behavior of aisle-crossing politicians. These are politicians who do not vote the same as the majority vote of their party (i.e., they vote the same as the opposing party). The most important descriptor we noticed was that all aisle-crossing politicians tweet less frequently on the issue than their fellow party members. This is true for both parties. In this example, these Republicans are considered aislecrossing votes because they have voted the same as Democrats on this issue. These latter two frames appear significantly less in the Republican tweets of our entire dataset as well. These results suggest that to predict aisle-crossing Republicans it would be useful to check for usage of typically Democrat-associated frames, especially if those frames are infrequently used by Republicans. This suggests that for aisle-crossing Democrats the use of additional frames not seen in their party might indicate potentially different voting behaviors. In this paper we present the task of collective classification of Twitter data for framing prediction. We provide an analysis of our approach in both supervised and unsupervised settings, as well as a real world analysis of framing patterns over time. Our global PSL models can be applied to other domains, such as politics in other countries, simply by changing the initial unigram keywords to reflect the politics of those countries."
818,3,3.67,3.67,3.67,True,acl_2017,train,"Learning commonsense knowledge from natural language text is nontrivial due to reporting bias: people rarely state the obvious, e.g., “my house is bigger than me”. However, while rarely stated explicitly, this trivial everyday knowledge does influence the way people talk about the world, which provides indirect clues to reason about the world. For example, a statement like “John entered his house” implies that his house is bigger than John. In this paper, we present an approach to infer relative physical knowledge of actions and objects along six dimensions (e.g., size, weight, and strength) from unstructured natural language text. We frame knowledge acquisition as joint inference over two closely related problems: learning (1) relative physical knowledge of object pairs and (2) physical implications of actions when applied to those object pairs. Empirical results demonstrate that it is possible to extract knowledge of actions and objects from language and that joint inference over different knowledge types improves performance.","Empirical results demonstrate that it is possible to extract knowledge of actions and objects from language and that joint inference over different knowledge types improves performance. Reading and reasoning about natural language text often requires trivial knowledge about everyday physical actions and objects. In fact, the potential use of such knowledge about everyday actions and objects can go beyond language understanding and reasoning. Ideally, an AI system should acquire such knowledge through direct physical interactions with the world. However, such a physically interactive system does not seem feasible in a foreseeable future. In this paper, we present an approach to acquire trivial physical knowledge from unstructured natural language text as an alternative knowledge source. In particular, we focus on acquiring relative physical knowledge of actions and objects organized along five dimensions: size, weight, strength, rigidness, and speed. While natural language text is a rich source to obtain broad knowledge about the world, compiling trivial commonsense knowledge from unstructured text is a nontrivial feat. In this work, we demonstrate that it is possible to overcome reporting bias and still extract the unspoken knowledge from language. The key insight is this: there is consistency in the way people describe how they interact with the world, which provides vital clues to reverse engineer the common knowledge shared among people. More concretely, we frame knowledge acquisition as joint inference over two closely related puzzles: inferring relative physical knowledge about object pairs while simultaneously reasoning about physical implications of actions. We consider five dimensions of relative physical knowledge in this work: size, weight, strength, rigidness, and speed. When considered in verb implications, size, weight, strength, rigidness generally concerns pre-conditions of the action, while speed concerns the post-condition of the action. Let us first consider the problem of representing relative physical knowledge between two objects. Moreover, we need to cope with uncertainties involved in knowledge acquisition. Therefore, we assume each knowledge piece is associated with a probability distribution. Above definition assumes that there is only a single implication relation for any given verb with respect to a specific knowledge dimension. We use this notation going forward. Thus, our work can potentially contribute to these resources by investigating new approaches to automatically recover richer frame knowledge from language. Because we consider implications over pairwise argument relations for each frame, there are sometimes multiple frame relations we consider for a single frame. Because the syntax ngram corpus provides only tree snippets without context, this way of enumerating potential frame patterns tend to overgenerate. We also add an additional crowdsourcing step where we ask crowd workers to judge whether a frame pattern with a particular verb and preposition could plausibly be found in a sentence. We use dependency parse to benefit from the Google Syntax Ngram dataset that provides language statistics over an extremely large corpus, which does not exist for SRL. We conduct a similar crowdsourcing step for the set of object pairs. We model knowledge acquisition as probabilistic inference using a factor graph. Connecting across substrates are factors that model inter-dependencies across different knowledge dimensions. In what follows, we describe each graph component. In the factor graph, we have two types of nodes in order to capture both classes of knowledge. The object first type of nodes are object pair nodes. Each object pair node is a random variable Oax,y which captures the relative strength of attribute a between objects x and y. The second type of nodes are frame nodes. Each frame node is a random variable F avt. This corresponds to the verb v used in a particular type of frame t, and captures the implied knowledge the frame vt holds along an attribute a. For an object pair node Oax,y, the value represents the belief about the relation between x and y along attribute a. For a frame node F avt, the value represents the belief about the relation along attribute a between any two objects that might be used in the frame vt. We denote the sets of all object pair and frame random variables O and F, respectively. The key aspect of our work is to reason about two types of knowledge simultaneously: relative knowledge on grounded object pairs, and implications of actions related to those objects. These factors encourage both random variables to agree on the same value. Some frames have relatively sparse text evidences to support their corresponding knowledge acquisition. Thus, we include several types of factors based on semantic similarities as described below. This gives us more evidence over a broader range of frames when textual evidence might be sparse. Given that each node represents a pair of objects, finding that x and y are similar yields two main cases in how to add factors (aside from the trivial case where the variable Oax,y is given a unary factor to encourage the value '). If nodes Ox,z and Oy,z exist, we expect objects x and y to both have a similar relation to z. We add a factor that encourages Ox,z and Oy,z to take the same value. The same is true if nodes Oz,x and Oz,y exist. On the other hand, if nodesOx,z andOz,y exist, we expect these two nodes to reach the opposite decision. For the case of ', if one prefers that value, then both should.) Performance on both learning relative knowledge about objects (right), as well as entailed knowledge from verbs (center) via realized frames (left), is improved by modeling their interplay (orange). Some knowledge dimensions, such as size and weight, have a significant correlation in their implied relations. These unary seed factors push the belief for its associated random variable strongly towards the seed label. Unary Factors For all frame and object pair random variables in the training set, we train a maximum entropy classifier to predict the value of the variable. We then use the probabilities of the classifier as potentials for seed factors given to all random variables in their class (frame or object pair). The same selection criteria and seed factors are applied to the crowdsourced object pairs. Our first experiment is to predict knowledge implied by new frames. Though the speed attribute has a skewed label distribution, giving the majority baseline high performance, our model outperforms the baselines on other attributes as well as overall. Metaphorical Language: While our frame patterns are intended to capture action verbs, our templates also match senses of those verbs that can be used with abstract or metaphorical arguments, rather than directly physical ones. Our second experiment is to predict the correct relations of new object pairs. Several works straddle the gap between IE, knowledge base completion, and learning commonsense knowledge from text. Our work focuses on specific domains of knowledge rather than general statements or occurrence statistics, and develops a frame-centric approach to circumvent reporting bias. While our work also focuses on commonsense knowledge, we attempt to directly learn a small number of specific types of knowledge from text without reasoning from an existing database or dataset of facts. A handful of works have attempted to learn the types of knowledge we address in this work. Another line of work addressed grounding verbs in the context of robotic tasks. These works are encouraging investigations into multimodal groundings of a small set of verbs. Our work instead grounds into a fixed set of attributes but leverages language on a broader scale to learn about more verbs in more diverse set of frames. We presented a novel take on verb-centric frame semantics to learn implied physical knowledge latent in verbs. We showed that by modeling changes in physical attributes entailed by verbs together with objects that exhibit these properties, we are able to better solve both tasks. Experiments on our novel dataset confirm that a model which takes advantage of physical relations as they arise from verbs and between objects outperforms baselines lacking such information. Our dataset and code will be made publicly available upon publication."
376,1,2.0,4.0,2.0,False,acl_2017,train,"In this article, we explore how text mining can help producing reliable, high-level numerical data for multi-document relation extraction applications. We combine neural network techniques, information aggregation and visualization methods to extract event-based relations with a good tolerance to noise. We describe a search engine based on these methods, identifying the evolution of alliance and opposition relations between countries.","As a result, a vast amount of content is available, which arguably creates more noise than knowledge at the end of the day. Without hierarchical organization and contextualization, users may lack perspective to understand and assimilate the multiplicity of events that they come accross every day, and to link them to related events in the past. In this context, we need tools for extracting, aggregating and visualizing knowledge in order to facilitate information analysis and access a variety of points of view. In this article, we take the example of the geopolitical alliances and their evolution; we extract relations from the texts and aggregate them in order to turn a set of very focused pieces of information into a broader knowledge and a bigger picture of a given situation. We show that adapting the models to the eventive nature of extracted relations helps existing recursive neural models. We use a precision-oriented cost function for a better later aggregation. We cope with the inevitable amount of noise to produce reliable numerical data from texts. We extract opposition (NEG), alliance (POS) or neutral (NEU) relations between two countries, explicitly expressed in news titles. Unlike for many knowledge extraction approaches, these relations are very changing, making the problem more difficult. At query time, we retrieve all titles relevant to the query, extract the candidate entities and their relations. For each day, POS and NEG relations are aggregated and a tendency of this day is obtained, as well as a trend over time. A graphical visualization of this trend is then proposed to the user. We chose to use only titles because we can consider that a title describes only one event, and that this event can be time-stamped by the document creation time, which is not true in article content. Most parsers are not trained on title sentences and will then experience a higher error rate. Major unions of countries (United Nations, European Union, ASEAN, African Union...) and important groups (ISIS...) are also considered. From this corpus, we randomly selected titles containing at least two entities. Each pair of entities is an instance of the classifier, which means that one sentence can lead to several instances. in order to refine the training. Only explicit relations were annotated POS or NEG, and no geopolitical knowledge was used. We describe now the models used for classifying relations at sentence level, as described in Sec. The root representation will then be used as features to classify each phrase with a softmax layer. This will allow us to manipulate the tree as desired in our Event-Centered (EC) model described in the following section. This guarantees that the model will really learn the relational structure rather than the specific connection between two countries. Only the path between entities is kept. Then (step), each branch node (agree, with and president) is extracted and converted into a leaf node, with an edge labeled MAIN going from this leaf node to the former position of the word. This label represents the fact that the word is the governor of its tree sibling in a dependency. Only three matrices (WMAIN,WLEFT,WRIGHT) are created, keeping the number of parameters reasonable. It may not be the case, either because of a specific structure of the sentence or because of a parser error, mainly due to the speficities of titles or to traditional bad prepositional attachments. We now describe how we force the path to contain an event word. The graph is then centered on this event so that we obtain a tree as described before: Using positional relations instead of real dependency names allows to twist the graph as desired to save what can be saved from an erroneous graph. Not doing so decreases dramatically the results. Our final application aggregates the sentencebased relations and uses a large corpus where the same relation on the same day will be expressed many times in different manners. We then bias the classical cross-entropy error function by adding a penalty to false positives (i.e. POS or NEG erroneous predictions). This can also be seen as an extra regularization parameter on the sum of all false positive predictions. The first step classifies between neutral and nonneutral relations, while the second step distinguishes between positive and negative relations. LSTM. TABARI. SP and EC models do not improve the results when only the top node of the structure is used as a feature for the softmax classifier. This can be explained by the fact that more sentence words are not represented at all in SP and EC structure, while FT contains all words between entities. However, with the extra content features, the EC accuracy is much higher, showing that a more focused tree structure can make the difference when the vocabulary gap is filled by representing missing words with simple average or concatenation. Even the unbiased cost function leads to a precision much higher than recall. The system TABARI does not perform well on this dataset. This can be due to the fact that the dataset addresses a very large variety of subjects and geographic areas; TABARI gets better performances on specific topics with dedicated rules. We describe here how we aggregate the sentencelevel relations. and rec. values would increase them artificially, making a NEU-only classifier the best one. Each sentence containing at least one relation is indexed with its stemmed content, the countries involved in relations, the relations themselves and the document creation date. A typical query is then composed of a few keywords representing the topic, a temporal interval (minimum of maximum dates) and zero or more country names on which the user wants to restrict relation extraction. All sentences returned by Solr, without any number limit, are aggregated. When zero or one country is specified by the user, we generate a graph of countries, where the distance between vertices (i.e., countries) reflects the opposition between the countries in a given time span (generally, larger that a single day). The problem here is that our classifier provides a relation between two countries independently of the others; it is therefore very unlikely that these values can be used as distance values in the whole graph (especially, triangle inequality will not be respected). For this algorithm, we need to transform our weights sw(d) into positive numbers (repulsive forces). We also need to damp the noise and the variations of the weights that are introduced by the volume of data. Circled numbers have been manually added to the screenshot to make some peaks clearer. Edge colors indicate the kind of relation (from dark red for strong alliance to dark blue for strong opposition), and vertice colors reflects proximity of countries with each other. Evaluating the relevance of trends suggested by a plot or a dynamic map is very subjective and would require a high level of expertise in each domain concerned by the tested queries. We selected queries having potentially a high density of extracted relations (e.g. For each of these peaks, we estimated whether the polarity of the peak was relevant or not, from a different news article collection. This is still a heavy task, which explains the low number of tested instances. TDT and Opinion Mining. These campaigns and the follow-up research aim at developing algorithms for identifying and organizing events described in documents. Our work contributes to this global research effort and explores the quantitative aspects of knowledge that can be extracted from textual documents. Different opinions must be aggregated, not only for getting more information, as in multidocument information extraction, but also for balancing positive and negative statements and conclude about the average opinion. International Relations. TABARI (a.k.a. This rule-based system makes extensive use of handcrafted dictionaries and patterns. Similar aspects with our work are the detection of conflicts, as well as the interest in the temporal evolution of these conflicts and a general approach for studying specific situations. However, the purpose and output (distributions of topical terms) are different. Their work is more directly related to traditional sentiment analysis and does not rely specifically on geopolitical events, even if the observed trends can follow these events. Several neural network models have been used for sentiment analysis and relation extraction. The results described in this paper confirm this conclusion. The advances in information extraction techniques and the increase of computing power make possible quite deep analysis of very large collections of texts. In this paper, we propose to take advantage of this progress to aggregate a large volume of extracted relations and to turn this knowledge into numerical data. We also propose a tool identifying the evolution of alliance and opposition relations between countries, on a specific subject. Most information conveyed every day remains textual and unstructured. With such applications, a wide new range of knowledge can become available, in many different fields, which would bring high added value to the final users, in terms of aggregation, contextualization and hierarchical organization of information. We see our international relations example as a use case study of this effort. We now intend to focus on reducing the necessary amount of supervision by using distant supervision, so that different subjects could be treated with less effort. Also, such study on country relationships should be multilingual in order to consider the cultural dimension as well. This article is accompanied with supplementary material: the annotated corpus, the FT, SP and EC versions of all relations (JSON format), the script converting text into each of these versions, and the source for our recursive models."
726,3,4.73,3.73,4.0,True,acl_2017,train,"We present an approach to rapidly and easily build natural language interfaces to databases for new domains, whose performance improves over time based on user feedback, and requires minimal intervention. To achieve this, we adapt neural sequence models to map utterances directly to SQL with its full expressivity, bypassing any intermediate meaning representations. These models are immediately deployed online to solicit feedback from real users to flag incorrect queries. Finally, the popularity of SQL facilitates gathering annotations for incorrect predictions using the crowd, which is directly used to improve our models. This complete feedback loop, without intermediate representations or database specific engineering, opens up new ways of building high quality semantic parsers. Experiments suggest that this approach can be deployed quickly for any new target domain, as we show by learning a semantic parser for an online academic database from scratch.","We present a robust approach to quickly and easily learn and deploy semantic parsers from scratch, whose performance improves over time based on user feedback and requires minimal intervention. We demonstrate the effectiveness of the complete approach by successfully learning a semantic parser for an academic domain by simply deploying it online for three days. This type of interactive learning is related to a number of recent ideas in semantic parsing, including batch learning of models that directly produce programs (e.g. Our experiments measure the performance of these learning advances, both in batch on existing datasets and through a simple online experiment for the full interactive setting. For the batch evaluation, we use sentences from the benchmark GeoQuery and ATIS domains, converted to contain SQL meaning representations. Our neural learning with data augmentation achieves near state-ofthe-art accuracies, despite the extra complexities of mapping directly to SQL. Finally, we do a small scale online experiment for a new domain, academic paper metadata search, demonstrating that actual users can provide useful feedback and our full approach is an effective method for learning a high quality parser that continues to improve over time as it is used. Learning parsers directly from SQL queries has the added benefit that we can potentially hire programmers on skilled-labor crowd markets, such as UpWork, to further improve scalability, an exploration we leave to future work. All three of these languages are modeled after natural language to simplify parsing. However, none of them is used to query databases outside of the semantic parsing literature; therefore, they are understood by few people and not supported by standard database implementations. In contrast, we parse directly to SQL, which is a popular database query language with wide usage and support. In contrast, our approach can generate arbitrary SQL queries, only uses lexical matching for entity names, and does not depend on syntactic parsing. We use a neural sequence-to-sequence model to directly generate SQL queries from natural language questions. Our work extends these results to the task of SQL generation. A final direction of related work studies methods for reducing the annotation effort required to train a semantic parser. Our approach trains on fully labeled SQL queries to maximize accuracy, but uses binary feedback from users to reduce the number of queries that need to be labeled. Our feedback-based learning approach can be used to quickly deploy semantic parsers to create NLIDBs for any new domain. Is is a simple interactive learning algorithm that deploys a preliminary semantic parser, then iteratively improves this parser using user feedback and selective query annotation. A key requirement of this algorithm is the ability to cheaply and efficiently annotate queries for chosen user utterances. Our algorithm alternates between stages of training the model and making predictions to gather user feedback, with the goal of improving performance in each successive stage. If the user marks the result correct, the pair (n, q) is added to the training set. This procedure can be repeated indefinitely, ideally increasing parser accuracy and requesting fewer annotations in each successive stage. We further present two data augmentation techniques which use content from the database schema and external paraphrase resources. The decoder predicts a conditional probability distribution over possible values for the next SQL token given the previous tokens using a combination of the previous SQL token embedding, attention over the hidden states of the encoder network, and an attention signal from the previous time step.   Beam search is used for decoding the SQL queries after learning. To recognize entities in the utterances at test time, we build a search engine on all entities from the target database. We store these mappings and apply them to the generated SQL to fill in the entity names. These templates contain slots whose values are populated given a database schema. A template is instantiated by first choosing the entities and attributes. SQL queries generated in this manner are guaranteed to be executable on the target database. On the language side, an English name of each entity is plugged into the template to generate an utterance for the query. We use the one-one and one-many paraphrases from the large version of PPDB. To paraphrase a training utterance, we pick a random word in the utterance that is not a stop word and replace it with a random paraphrase. We perform paraphrase expansion on all examples labeled during learning, as well as the initial seed examples from schema templates. Our first set of experiments demonstrates that our semantic parsing model has comparable accuracy to previous work, despite the increased difficulty of directly producing SQL. We will release our processed versions of both datasets. Utterances were anonymized by replacing them with their corresponding types and all words that occur only once were replaced by UNK symbols. The development set is used for hyperparameter tuning and early stopping. We report test set accuracy of our SQL query predictions by executing them on the target database and comparing the result with the true result. The former only works on semantically tractable utterances where words can be unambiguously mapped to schema elements, while the latter uses a reranking approach that also limits the complexity of SQL queries that can be handled. However, it requires a lot of SQL specific engineering (for example, special nodes for argmax) and is hard to extend to more complex SQL queries. Our results demonstrate that these models are powerful enough to directly produce SQL queries. Thus, our methods enable us to utilize the full expressivity of the SQL language without any extensions that certain logical representations require to answer more complex queries. More importantly, it can be immediately deployed for users in new domains, with a large programming community available for annotation, and thus, fits effectively into a framework for interactive learning. To our knowledge, this is the first effort to learn a semantic parser using a live system, and is enabled by our models that can directly parse language to SQL without manual intervention. We developed a web interface for accepting natural language questions to an academic database from users, using our model to generate a SQL query, and displaying the results after execution. Several example utterances are also displayed to help users understand the domain. Together with the results of the generated SQL query, users are prompted to provide feedback which is used for interactive learning. Screenshots of our interface are included in our Supplementary Materials. Collecting accurate user feedback on predicted queries is a key challenge in the interactive learning setting for two reasons. Second, it can be difficult for users to determine if the presented results are in fact correct. The second assist is utterance paraphrasing, which shows the user another utterance that maps to the same SQL query. The Correct and Wrong Result options represent scenarios when the user is satisfied with the result, or the result is identifiably wrong, respectively. Wrong Types indicates incorrect entity identification, which can be determined from type highlighting. Incomplete Result indicates that the query is correct but the result is not; this outcome can occur because the database is incomplete. We considered results marked as either Correct or Incomplete Result as correct queries for learning. The remaining incorrect utterances were sent to a crowd worker for annotation and were used to retrain the system for the next stage. The Correct and Incomplete results options are erroneous if the SQL query is correct, and vice versa for incorrect queries. contributed and incorrect utterances are labeled. This result demonstrates that we can successfully build semantic parsers for new domains by using neural models to generate SQL with crowdsourced annotations driven by user feedback. We analyzed the feedback signals provided by the users in the final stage of the experiment to measure the quality of feedback. This erroneous feedback results in redundant annotation of already correct examples. The main cause of this erroneous feedback was incomplete data for aggregation queries, where users chose Wrong instead of Incomplete. It is important that this fraction be low, as these queries become incorrectly-labeled examples in the training set that may contribute to the deterioration of model accuracy over time. This quality of feedback is already sufficient for our neural models to improve with usage, and creating better interfaces to make feedback more accurate is an important task for future work. We release a new semantic parsing dataset for academic database search using the utterances gathered in the user study. We augment these labeled utterances with additional utterances labeled by crowd workers. Note that these additional utterances were not used in the online experiment). We also provide a database on which to execute these queries containing academic papers with their authors, citations, journals, keywords and datasets used. We randomly divide each training set into K batches and present these batches sequentially to our interactive learning algorithm. As in the live experiment, accuracy improves with successive batches. Nevertheless, templates are important in a live system to motivate users to interact with it in early stages. As observed before, paraphrasing improves performance at all stages. This result demonstrates that more frequent deployments of improved models leads to fewer mistakes. We describe an approach to rapidly train a semantic parser as a NLIDB that iteratively improves parser accuracy over time while requiring minimal intervention. Our approach uses an attentionbased neural sequence-to-sequence model, with data augmentation from the target database and paraphrasing, to parse utterances to SQL. This model is deployed in an online system, where user feedback on its predictions is used to select utterances to send for crowd worker annotation. We further demonstrate the effectiveness of our online system by learning a semantic parser from scratch for an academic domain. A key advantage of our approach is that it is not language-specific, and can easily be ported to other commonly-used query languages, such as SPARQL or ElasticSearch. Finally, we also release a new dataset of utterances and SQL queries for an academic domain."
419,1,5.0,4.0,4.0,True,acl_2017,train,"We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a lowresource language. In experiments on 21 language pairs from four different language families, we obtain up to 58% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge.","Furthermore, for most languages in the world, high-cost linguistic annotation and resource creation are unlikely to be undertaken in the near future. Transfer learning is one solution to this problem: it exploits annotations in a high-resource language to train a system for a low-resource language. In this work, we present a method for cross-lingual transfer of inflectional morphology using an encoder-decoder recurrent neural network (RNN). This allows for the development of tools for computational morphology with limited annotated data. In morphologically rich languages, individual lexical entries may be realized as distinct inflec-tions of a single lemma depending on the syntactic context. In many languages, a lemma can have hundreds of individual forms. However, these models require a large amount of data to achieve competitive performance; this makes them unsuitable for out-of-thebox application to paradigm completion in the low-resource scenario. To mitigate this, we consider transfer learning: we train an end-to-end neural system jointly with limited data from a lowresource language and a larger amount of data from a high-resource language. This technique allows the model to apply knowledge distilled from the high-resource training data to the low-resource language as needed. Our results demonstrate successful transfer of morphological knowledge. zero) in-domain language sample per target tag. We also show that the effectiveness of morphological transfer depends on language relatedness, measured by lexical similarity. The citation form of a lexical entry is referred to as the lemma and the collection of its possible inflections as its paradigm. Tab. In general, the presence of rich inflectional morphology is problematic for NLP systems as it greatly increases the token-type ratio and, thus, word form sparsity. It is infeasible to transfer morphological knowledge from Chinese to Portuguese as Chinese does not use inflected word forms. But even transferring inflectional paradigms from morphologically rich Arabic to Portuguese seems difficult as the inflections often mark dissimilar subcategories. Thus, we conjecture that transfer of inflectional morphology is only viable among related languages. We now offer a formal treatment of the crosslingual paradigm completion task and develop our notation.   The goal of cross-lingual paradigm completion is to populate paradigms in the low-resource target language with the help of data from the high-resource source language, using only few in-domain examples. We consider two tasks, training a paradigm completor (i) for a high-resource language and (ii) for a low-resource language. We want to train jointly so we reap the benefits of having related languages. For example, consider the related Romance languages Spanish and French; focusing on one term from each of the summands in Eq. In their setup, the RNN encodes the input and predicts the forms in a single language. In contrast, we force the network to predict two languages. For brevity, language and target tags are omitted from the input. Thickness of red arrows symbolizes the degree to which the model attends to the corresponding hidden state of the encoder. The decoder, a unidirectional RNN, uses attention: it computes a weight for each hi. Each weight reflects the importance given to that input position.       Each source form is represented as a sequence of characters; each character is represented as an embedding. In the same way, each source tag is represented as a sequence of subtags, and each subtag is represented as an embedding. Both input and output are padded with distinguished BOW and EOW symbols. To verify the applicability of our method to a wide range of languages, we perform experiments on example languages from several different families. We experiment on Catalan, French, Italian, Portuguese and Spanish. Tab. We hypothesize that the transferability of morphological knowledge between source and target corresponds to the degree of lexical similarity; thus, we expect Portuguese and Catalan to be more beneficial for Spanish than Italian and French. We experiment on Bulgarian, Macedonian, Russian and Ukrainian (Cyrillic script) and on Czech, Polish and Slovene (Latin script). languages, so we assign them the low-resource role. For Romance and for Uralic, we experiment with groups containing three or four source languages. To arrive at a comparable experimental setup for Slavic, we run two experiments, each with three source and one target language: (i) from Russian, Bulgarian and Czech to Macedonian; and (ii) from Russian, Polish and Slovene to Ukrainian. Thus, the use of two scripts in Slavic allows us to explore transfer across different alphabets. We further consider a non-Indo-European language family, the Uralic languages. While Finnish and Estonian are closely related (both are members of the Finnic subfamily), Hungarian is a more distant cousin. Estonian and Northern Sami are lowresource languages, so we assign them the lowresource role, resulting in two groups of experiments: (i) Finnish, Hungarian and Estonian to Northern Sami; (ii) Finnish, Hungarian and Northern Sami to Estonian. It is unrelated to all other languages used in this work. Both in terms of form (new words are mainly built using a templatic system) and categories (it has tags such as construct state), Arabic is very different. Thus, we do not expect it to support morphological knowledge transfer and we use it as a baseline for all target languages. We first discuss details common to all experiments. We keep hyperparameters during all experiments (and for all languages) fixed to the following values. Biases are initialized to zero. The two metrics differ in that accuracy gives no partial credit and incorrect answers may be drastically different from the annotated form without incurring additional penalty. In contrast, edit distance gives partial credit for forms that are closer to the true answer. In this experiment, we investigate to what extent our model transfers morphological knowledge from a high-resource source language to a low-resource target language. We experimentally answer three questions. i) Is transfer learning possible for morphology? ii) How much annotated data do we need in the low-resource target language? iii) How closely related must the two languages be to achieve good results? Data. Lemmata and inflections are randomly selected from all available paradigms. Results and Discussion. Tab. There are two baselines. More closely related source languages improve performance more than distant ones. For the target language Macedonian, Bulgarian provides most benefit. This can again be explained by similarity: Bulgarian is closer to Macedonian than the other languages in this group. Unlike Polish and Slowenian, Russian is the only language in this group that uses the same script as Ukrainian, showing the importance of the alphabet for transfer. Still, the results also demonstrate that transfer works across alphabets (although not as well); this suggests that similar embeddings for similar characters have been learned. Finnish is the language that is closest to Estonian and it again performs best as a source language for Estonian. For Northern Sami, transfer works least well, probably because the distance between sources and target is largest in this case. Learning curves for Romance and Arabic further support our finding that language similarity is important. In Fig. There are two baselines in the table. finally, Arabic. This corresponds to the order of lexical similarity with Spanish, except for the performance of Portuguese (cf. Tab. discussion in the next subsection. That the transfer learning setup improves performance for the unrelated language Arabic as source is at first surprising. Error Analysis for Romance. Some inflections all systems get wrong, mainly because of erroneously applying the inflectional rules of the source to the target. Finally, the output of the model trained on Portuguese contains a class of errors that are unlike those of other systems. Formatting is the same as for Tab. In the one-shot (resp. zero-shot) case, we observe exactly one form (resp. zero forms) for each tag in the target language at training time. We report one-shot accuracy (resp. never) during training. However, the low-resource data is created in the way specified above. Results and Discussion. Tab. This is not surprising as there is not enough Spanish data for the system to generalize well and Arabic does not contribute exploitable information. This shows that a single training example is sometimes sufficient for successful generation although generalization to tags never observed is rarely possible. Catalan and Portuguese show the best performance in both settings; this is intuitive since they are the languages closest to the target (cf. Tab. Overall, this experiment shows that with transfer learning from a closely related language the performance of zero-shot morphological generation improves over the monolingual approach, and, in the one-shot setting, it is possible to generate the right form nearly half the time. We would like to separate the effects of regularization that we saw for Arabic from true transfer. Data. We generate a random cipher and apply it to morphological tags and word forms for Portuguese and Arabic. The language tags are kept unchanged. Spanish is also not changed. For comparability with Tab. Results and Discussion. Tab. Just the knowledge that something is a subtag is helpful because subtags must not be generated as part of the output. Portuguese uses suffixation for inflection whereas Arabic is templatic and inflectional changes are not limited to the end of the word. This difference is not affected by ciphering. Perhaps even ciphered Portugese lets the model learn better that the beginnings of words just need to be copied. This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. The drawback is that machine translation errors cause errors in the target. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages. Each of these papers has both similarities and differences with our approach. ii) Let k and m be the number of different input and output languages. iii) Whereas training RNNs in MT is hard, we only experienced one difficult issue in our experiments (due to the low-resource setting): regularization. iv) Some work is word-or subword-based, our work is character-based. v) Similar to work in MT, we show that zero-shot (and, by extension, one-shot) learning is possible. We presented a cross-lingual transfer learning method for paradigm completion, based on an RNN encoder-decoder model. Our experiments showed that information from a high-resource language can be leveraged for paradigm completion in a related low-resource language. Our analysis showed that the degree to which the source language data helps for a certain target language depends on their relatedness."
462,2,4.0,3.57,2.0,True,acl_2017,train,"Volatility prediction—an essential concept in financial markets—has recently been addressed using sentiment analysis methods. We investigate the sentiment of annual disclosures of companies in stock markets to forecast volatility. We specifically explore the use of recent Information Retrieval (IR) term weighting models that are effectively extended by related terms using word embeddings. In parallel to textual information, factual market data have been widely used as the mainstream approach to forecast market risk. We therefore study different fusion methods to combine text and market data resources. Our word embedding-based approach significantly outperforms state-ofthe-art methods. In addition, we investigate the characteristics of the reports of the companies in different financial sectors.","Volatility forecasting has gained considerable attention during the last three decades. These reports are however long, redundant, and written in a style that makes them complex to process. also analyse the topics discussed in the reports and observe a constant increase over the years in both the length of the documents as well as the number of topics. They claim that the increase in length is not the result of economic factors but is due to verboseness and redundancy in the reports. They suggest that only the risk factors topic appears to be useful and informative to investors. Their analysis motivates us to study the effectiveness of the Risk Factors section for volatility prediction. In terms of use of the textual content for volatility prediction, this paper shows that state-ofthe-art Information Retrieval (IR) term weighting models, which benefit from word embedding information, have a significantly positive impact on prediction accuracy. The significant improvement of this approach for document retrieval by capturing the importance of the terms motivates us to apply it on sentiment analysis. We extensively evaluate various state-of-the-art sentiment analysis methods to investigate the effectiveness of our approach. In addition to text, factual market data (i.e. An emerging question is how to approach the combination of the textual and factual market information. We propose various methods for this issue and show the performance and characteristics of each. The financial system covers a wide variety of industries, from daily-consumption products to space mission technologies. It is intuitive to consider that the factors of instability and uncertainty are different between the various sectors while similar inside them. We therefore also analyse the sentiment of the reports of each sector separately and study their particular characteristics. We also show that performance can be further improved by effectively combining textual and factual market information. In addition, we shed light on the effects of tailoring the analysis to each sector: despite the reasonable expectation that domain-specific training would lead to improvements, we show that our general model generalizes well and outperforms sector-specific trained models. The remainder of the paper is organized as follows: in the next section, we review the state-ofthe-art and related studies. Market prediction has been attracting much attention in recent years in the natural language processing community. They use the Multi Kernel Learning (MKL) algorithm for combining the two features. The combination shows improvement in final prediction in comparison to using each of the features alone. Motivated by this study, we investigate the performance of the MKL algorithm as one of the methods to combine the textual with non-textual information. While the mentioned studies use short-length texts (sentence or paragraph level), approaching long texts (document level) for market prediction is mainly based on n-gram bag of words methods. We reproduce all the methods in these studies, and show the advantage of our sentiment analysis approach. For brevity, unless otherwise mentioned, we report the volatility of the first year by calculating the mean of the first four quartiles after the publication of each report. We first describe our text sentiment analysis methods, followed by the features obtained from factual market data, and finally explain the methods to combine textual and market feature sets. We refer to this keyword set as Lex. We also use this expanded set in our experiments and refer to it as LexExt. The feature vector generated by the weights of the Lex or LexExt lexicons is highly sparse, as the number of dimensions is larger than the number of data-points. We therefore reduce the dimensions by applying Principle Component Analysis (PCA). Given the final feature vector x with l dimensions, we apply SVM as a well-known method for training both regression and classification methods. Our initial experiments showed better performance of the Radial Basis Function (RBF) kernel in comparison to linear and cosine kernels and is therefore used in this paper. Due to lack of space, the implementation details are moved to supplementary materials. The feature is converted to numerical representation using onehot encoding. In the context of multi-model learning, the method is referred to as early fusion. Using the first portion, we train separate machine learning models for each of the text and market feature sets. Next, we predict labels of the second portion with the trained models and finally train another model to capture the combinations between the outputs of the base models. In our experiments, the final model is always trained with SVM with RBF kernel. Stacking is computationally inexpensive. However, due to the split of the training set, the base models or the meta model may suffer from lack of training data. A potential approach to learn both the feature sets in one model is the MKL method. However, the optimization can be computationally challenging. It has been shown to outperform recent multi kernel approximation approaches. We use RBF kernels for both the text and market feature sets. We report the parameters applied in various algorithms and describe the evaluation metrics. We remove HTML tags and extract the text parts. We extract the Risk Factors section using term matching heuristics. Finally, the texts are stemmed using the Porter stemmer. Baselines GARCH: although the GARCH model is of market factual information, we use it as a baseline to compare the effectiveness of text-based methods with mainstream approaches. Market: uses all the market features. For both the GARCH and Market baselines, we use an SVM learner with RBF kernel. They combine the textual features with current volatility using the early fusion method. However, especially when comparing models, applied on different test sets (e.g. In this section, first we analyse the contents of the reports, followed by studying our sentiment analysis methods for volatility prediction. Finally, we investigate the effect of sentiment analysis of the reports in different industry sectors. Let us start our experiment with observing changes in the feature vectors of the reports over the years. These considerable differences between the centroid reports in years across these three groups hints at probable issues when using the data of the older years for the more recent ones. We observe that by dropping the oldest years one by one (from left to right in the figure), the performance starts improving. In fact, although in machine learning in general using more data results in better generalization of the model and therefore better prediction, the reports of the older years introduce noise. This subset is also the most recent cluster and presumably more similar to the future reports. All weighting schemes are then combined with the market features with the introduced fusion methods. In general, the extended schemes (with hat) improve upon their normal forms. MKL however has better performance than early fusion while it has the highest computational complexity among the methods. Our method outperforms all state-ofthe-art methods both when using textual features only as well as a combination of textual and market features. Let us now take a closer look on the changes in the performance of the prediction in time. The X-axes show eight quartiles after the publication date of the report. For comparison, the GARCH and only market features are depicted with dashed lines. As shown, the performance of the GARCH method as well as that using only market features (Market) decrease faster in the later quartiles since the historical prices used for prediction become less relevant as time goes by. Using only text features (Text), we see a roughly similar performance between the first four quartiles (first year), while the performance, in general, slightly decreases in the second year. In comparison to using only market features, the combination of the features shows more stable results in the later quartiles. The dashed lines show the market-based baselines. b) Performance of volatility prediction of each year given the past data. The hashed areas show corresponding baselines. c) Performance per sector. While the above experiments are based on cross-validation, for the sake of completeness it is noteworthy to consider the scenarios of realworld applications where the future prediction is based on past data. Corporations in the same sector share not only similar products or services but also risks and instability factors. Considering the sentiment of the financial system as a homogeneous body may neglect the specific factors of each sector. We therefore set out to investigate the existence and nature of these differences. We observe considerable differences between the performance of the sectors, especially when using only sentiment analysis methods (i.e. only text features). We refer to these models as sector-specific in contrast to the general model, trained on all the data. This is to some extent surprising, as one would expect that domainspecific training would improve the performance of sentiment analysis in text. However, we need to consider the size of the training set. To verify the effect of the size of training data, we train a sector-agnostic model for each sector. The hashed area in (a) indicates the GARCH and in (b) the Market baseline. sector-agnostic and-specific show the existence of particular risk factors in each sector and their importance. Results also confirm the hypothesis that the data for training in each sector is simply too small, and as additional data is accumulated, we can further improve on the results by training on different sectors independently. We continue by examining some examples of essential terms in sectors. To address this, we have to train a linear regression method on all the reports of each sector, without using any dimensionality reduction. Linear regression without dimensionality reduction has the benefit of interpretability: the coefficient of each feature (i.e. term in the lexicon) can be seen as its importance with regards to volatility prediction. crisis, or delist constantly have high coefficient values in the sector-specific as well as general model. However, some keywords are particularly weighted high in specificsector models. For instance, the keyword fire has a high coefficient in the energy sector, but very low in the others. This later sense of word is however weighted as a low risk-sensitive keyword in the other sectors. Another example is an interesting observation on the word beneficial. The word is introduced as a positive sentiment in the lexicon while it gains highly negative sentiments in some sectors (health care, and basic industries). Our bag-ofwords sentiment analysis approach benefits from state-of-the-art models in information retrieval which use word embeddings to extend the weight of the terms to the similar terms in the document. GARCH prediction model, and current volatility. In addition, we studied the characteristics of each individual sector with regard to risk-sensitive terms. Our analysis shows that reports in same sectors considerably share particular risk and instability factors. However, despite expectations, training different models on different sectors does not improve performance compared to the general model. We traced this to the size of the available data in each sector, and show that there are still benefits in considering sectors, which could be further explored in the future as more data becomes available."
97,3,2.27,1.91,1.91,False,acl_2017,train,"We have developed an automated Japanese short-answer scoring and support machine for new National Center written test exams. Our approach is based on the fact that recognizing textual entailment and/or synonymy has been almost impossible for several years. The system generates automated scores on the basis of evaluation criteria or rubrics, and human raters revise them. The system determines semantic similarity between the model answers and the actual written answers as well as a certain degree of semantic identity and implication. Owing to the need for the scoring results to be classified at multiple levels, we use random forests to utilize many predictors effectively rather than use support vector machines. An experimental prototype operates as a web system on a Linux computer. We compared human scores with the automated scores for a case in which 3–6 allotment points were placed in 8 categories of a social studies test as a trial examination. The differences between the scores were within one point for 70–90 percent of the data when high semantic judgment was not needed.","The use of AI-based computers was proposed to stabilize the test scores efficiently. The required type of writing test is a short-answer test, where a correct answer is expected to exist. Therefore, the test is scored by judging agreement of the meaning with the correct answer. Another type of writing test is essay writing, where a correct answer does not exist. The written answers are evaluated based on the rhetoric, the connection expressions, and the content. Two characters in Japanese are generally equivalent to one word in English. A short-answer test is widely considered to be more authentic and reliable for measuring ability compared with a multiple-choice test. If technical problems related to the short-answer test are solved, the potential demand for its use, as well as that for the national center test, will be enormous. While a short-answer scoring system has been developed because of its importance, various technical problems remain unsolved. However, the concordance rate with human examiners was found to be small and impractical. To combine several methods and patch them ad hoc is the most that can be done if people feigned to answer. Therefore, we thought of a support system for short written tests where a human rater can correct the automated score by referring to the original scores. We chose to leave room for human raters to overwrite it without making it a perfect automated scoring system. The new examination, which was created by the NCTUEE, will utilize the written test for Japanese literature, whose scoring seems to be more difficult than the examinations of science and social studies, which are prepared basically using the facts written in their respective textbooks. The written test for Japanese literature needs reading skills rather than skills of information processing and pattern matching. In this test in particular, we have to detect the semantic difference in the compared written answers with the model sentences, though almost no difference exists in the vocabulary used. Therefore, we tried to tackle the scoring of short sentences in social studies, where precise judgments are less needed. This test item asks about the content, and the recognizing correct meaning is necessary for getting scores. The approach functions as follows. The system gives not only a temporary score based on the criterion-based judgment but also a prediction score offered by machine learning based on the understanding of other human raters or supervised data. A certain degree of semantic meaning is also used. A human rater can certify the prediction score by which a system presents this information as reference. The degree of fitness with the scoring guideline is also necessary. On the basis of these learning results, we set up a scoring engine to return the scores for new answers. b) The system generates a scoring screen written in the Hyper Text Markup Language. Then, a CGI program is activated. The recommended value as a result of the scoring engine of (a) is indicated here. The scoring result is stocked in a file or a database. The user repeats this mark operation. A human rater reviews these judgments and revises them if necessary. Tentative scores located in the lower part are based on the aforementioned alternative judgment. The right-hand window is to determine the final score. The initial mark is settled by which predictive probability based on the past learned results gives the maximum. The probability values are also indicated. When no learning data exist, that is to say, when no pre-scored data about the relevant test item exist, the message to that effect is shown in the top windows: no probability and no initial mark are naturally determined. It shows that the effect of the machine learning is functioning appropriately. Our system is a Web application. We built the mechanism to make this HTML file automatically from a plain scoring criterion file that a computer beginner can handle. Two or three elements are set for criteria. In order, the label, allotment of points, and correspondence are located. The tab is the delimiter. When it has semantically the same meaning, it is also permitted. Minus points indicate points to be deducted. Even when an Arab had land in a place of conquest, kharaj was imposed."" When using many predictor variables, the classification often functions effectively. The degree of contributions can be estimated to determine effective predictor variables quantitatively in the classification. We omitted cross matrices between human ratings and the estimate because of space limitations. It shows the performance of the classification was in the level available. RFs do not need cross validation to calculate the error rate. The error rate can be estimated internally during a run. The remaining one-third of cases is used for the test data. The error rate for the test set can be obtained at the same time. RFs evaluate the importance of variables in distinction using an index of the Gini coefficient. The bigger the coefficient, the more the classification is affected. A variable name that starts with a capital letter implies linguistic semantic meaning built by the vocabulary used in Japanese Wikipedia. The typical examples are cosine similarity, precision, recall, and F-measure in the semantic space. Variations that multiply the allotment are also included. However, the Fmeasure and cosine similarity between the model (correct) answer and the answer in the semantic space often appear in a higher position. Generally speaking, semantic variables are dominant compared with surficial lexical variables. Deep cases are often not determined easily, so we use surface cases instead of them. However, almost no agreement of case elements happened; the degree of agreement does not become one of the effective predictors of the classification. The ways in which sentences can be filled out to indicate the same meaning are numerous. Therefore, we did not add case grammar variables on the agreement of the case elements in this experiment. Recognizing textual entailment between a model (correct) answer and a written answer is still difficult technically because complicated collation needs to be determined under contractual and semantic levels. Our technique is based on the collation between the keywords in two answers, and it uses both predictors considering superficial and semantic aspects. Therefore, it can be judged as sufficiently realistic for an approach of the first step. A form that entrusts the last judgment to the human is most suitable. Our procedure will be applied to other subjects, such as Japanese literature, when many different transcriptions of a correct answer are prepared. Many different expressions in the same sense can be allowed, especially in Japanese. Our system has a mechanism to choose the biggest score among the same labels, so this can be prepared using the specifications of the current state. This function has already been implemented and will be helpful to understand the semantics. However, a sufficiently large number of human scores cannot be provided for supervised learning. Indeed, actual written answer scores are often zero because they are illogical or are off-topic. Obtaining suitable and well-balanced score data will be necessary to ensure proper estimation. Because a short written test has been proposed as a new common test for entering Japanese universities, our new scoring and support system is now being considered. We hope this system will provide researchers who study this field and practical businesspeople with useful information and suggestions. The authors would like to thank everybody in the Gakken group for offering the test sets, scoring criteria, and human scores. In particular, Mr. Tsugunao Matsuoka and Ms. Naoko Saigo gave us valuable comments."
395,3,3.67,3.33,3.0,False,acl_2017,train,"This paper proposes DRL-Sense—a multisense word representation learning model, to address the word sense ambiguity issue, where a sense selection module and a sense representation module are jointly learned in a reinforcement learning fashion. A novel reward passing procedure is proposed to enable joint training on the selection and representation modules. The modular design implements pure senselevel representation learning with linear time sense selection (decoding). We further develop a non-parametric learning algorithm and a sense exploration mechanism for better flexibility and robustness. The experiments on benchmark data show that the proposed approach achieves the state-of-the-art performance on contextual word similarities and comparable performance with Google’s word2vec while using much less training data.","However, most of applications usually utilize word-level embeddings to obtain semantics. Considering that natural language is highly ambiguous, the standard word embeddings may suffer from polysemy issues. Our modular design implements pure sense-level representation learning while maintaining linear time sense selection. Our reinforcement learning based approach can be loosely connected to clustering methods and probabilistic modeling methods. However, most of the above approaches separates the clustering procedure and the representation learning procedure, which may suffer from the error propagation issue. Instead, our reinforcement learning model utilizes a reward signal to propagate the statistical information from sense representations to optimize sense selection. In contrast, by exploiting embeddings in the sense selection module, our model performs linear time sense selection, while maintaining a pure senselevel representation learning module. The proposed approach incorporates a sense selection module, which can be learned together with the representation learning module through reinforcement learning. This work proposes a framework to learn two key modules for multi-sense word representations: a sense selection module and a sense representation module. The sense selection module decides which sense to use given a text context, whereas the sense representation module learns meaningful representation for sense by their statistical characteristics. Considering that the sense representation module requires the sense identity of each word from the sense selection module, and the sense selection module may also benefit from the semantics carried by sense representations, these two modules should be tangled. Hence, a naive two-stage algorithm or two separate learning algorithms proposed by prior work are not optimal. Two shared-weight sense selection modules surround a sense representation module. The sense selection module infers the target word sense given its context. The selected senses are passed to the sense representation module for learning sense embeddings. Then, the sense embedding collocation is passed back to the sense selection modules for reinforcement learning. to learn two modules in a reinforcement learning manner by introducing a novel reward passing procedure to enable joint training. In addition, a nonparametric learning algorithm is incorporated to enable automatic sense induction, where the number of senses for each word is not predefined but automatically learned for practical usage. Also, simple mechanisms are applied to allow the sense exploration while incorporating the sense selection prior. The sense selection module decides the sense for a word given its context. Assuming that a word sense is determined by the local context, we use the Markov property to formulate the sense selection module as a Markov Decision Process (MDP) to infer the most probable sense based on its local context. However, this formulation lacks a central element of MDP: a reward signal for measuring the fitness of the selected sense zik. Therefore, if we have meaningful sense representations containing statistical estimation, we can use the estimation as a surrogate reward to simulate the reward signal in the sense selection module. A successful sense selection module can be applied to each word in a corpus for obtaining its sense identity. Various techniques about word embeddings can be directly employed after mapping all words in a corpus to its sense identity. The typical method is to formulate the sense selection problem as a maximum likelihood estimation (MLE) problem for the collocation likelihood. Specifically, we first create input sense embedding matrixU and collocation estimation matrix V as the representations to be learned. In order to estimate the sense collocation likelihood, two senses, zik and zjl, should be determined, which leads a two-step MDP: one for the target word wi and the other for the collocated word wj within a context window. However, there are two major drawbacks of this formulation. The selected senses are passed to the sense representation module to optimize the sense collocation likelihood. Afterwards, the estimated collocation likelihood is passed back as a reward signal to optimize the sense selection module. There are two major contributions in our modular design. First, efficient sense selection with word embeddings and pure sense representation learning are simultaneously achieved. Second, reinforcement learning allows both modules to be jointly trained. Due to high ambiguity in natural language, a greedy sense selection strategy may not work well in the early training stage, because the sense selection module does not learn well. We evaluate our proposed DRL-Sense model and compare with other multi-sense word representation models for both quantitative and qualitative experiments. batch training for robustness. In real world applications, using multiple sense vectors for a word simultaneously may bring additionally computational overhead over conventional single word embedding scenarios, and also change the existing neural network architecture for word embeddings. In contrast, by simply replacing the most probable sense identity with each word identity (as in MaxSimC), the computation framework and cost for downstream NLP tasks remains the same as conventional word embedding approaches. In the experiments, our model selects the synonym of the question word wQ by the collocation likelihood as a proxy of their semantic similarity. We qualitatively evaluate non-parametric learning and sense representation learning performance. The largest challenge of multi-sense representation learning is that most words can be represented by a single embedding (non-polysemous or a single embedding can model multiple senses well). Hence, the feasible solution is to retain most words a single sense embedding, while leaving polysemous words multiple sense embeddings. To evaluate the quality of sense embeddings, we show the k-nearest neighbors (k-NN) for each sense. Our model implements non-parametric learning for word sense induction and exploration for word sense selection. The experiments show that our DRL-Sense model achieves the state-ofthe-art performance for the benchmark contextual word similarity task and most of synonym selection datasets under the same setting. In the future, we plan to investigate reinforcement learning methods to incorporate multi-sense word representations for downstream NLP tasks. Therefore, as long as the collocation log likelihood L is negative, the update formula is to minimize the likelihood of choosing l and k, despite the fact that l and k may be good choices. In this section, we conduct ablation experiments by removing each components separately in our system to test the efficacy of proposed mechanisms. In addition, we also test the efficacy of specific module according to their complexity. Consistent improvement in terms of MaxSimC from each component to the proposed DRL-Sense model can be observed."
792,1,4.0,3.0,2.0,False,acl_2017,train,"The Long Short-Term Memory (LSTM) architecture for recurrent neural networks has become the state-of-the-art model for a range of different Natural Language Processing (NLP) tasks, especially in language modeling and sequence to sequence learning. In this paper we leverage a bidirectional LSTM while at the same time taking advantage of other semantic resources in order to create a vector space model for words and senses that outperforms most popular algorithms for learning embeddings. We evaluate our approach on the most well-known benchmarks on vector space representations.","Embeddings represent lexical and semantic items in a low-dimensional continuous space. The resulting vectors capture useful syntactic and semantic information, such as regularities in language, where relationships are characterized by a relation-specific vector offset. However, when dealing with large vocabularies, LSTMs involve time-intensive matrix-matrix multiplications, making them prohibitively expensive. However, both these approaches speed up the training process at the cost of lowering the performance. These representations are extremely good at capturing syntactic and semantic regularities in language as well as relationships among items. GloVe, an alternative approach trained on aggregated global word-word co-occurrences, reached similar results. While these embeddings are surprisingly good for monosemous words, they fail to represent properly the non-dominant senses of words. For instance, the representations of bar and pub should be similar, as well as those of bar and stick, but having similar representations of pub and stick is undesired. retrofitted) in order to make them more similar to those which share a word type and less similar those which do not. They use large raw text corpora to learn a neural model that embeds entire sentential contexts and target words in the same low-dimensional space. In contrast with the above approaches, which aim to learn representations of single words, sense embeddings represent individual word senses as separate vectors. An important limitation of this approach was the inability to train both word and sense embeddings. The only word embeddings that the model was able to learn were the representations of words which were not annotated by the disambiguation tool. Those representations were of poor quality due to the fact that they are learnt from the occurrences in a ambiguous or unclear context, else the disambiguation should be able to annotate them. AutoExtend is based on an autoencoder, a network that mimics the input and output vectors. The model was built by exploiting large corpora and knowledge obtained from WordNet and BabelNet. In contrast to AutoExtend, the model learns word and sense embeddings in a shared space as an emerging feature, rather than via constraints on both representations. In marked contrast, LSTMEmbed aims to learn representations for both words and senses in a shared emerging space, handling word ordering, and improving those representations by injecting semantic knowledge via pretrained embeddings. At the core of LSTMEmbed is a bidirectional LSTM, a special kind of recurrent neural network (RNN). An RNN is a type of neural network architecture particularly suited for learning time series. The simplest RNN is called Elman network. These gates control the flow of information between states. The state at each time step in a BLSTM consists of the state of two LSTMs, one going left and one going right. Given a sense-annotated corpus and a set of pretrained embeddings, the objective is to predict the embedding of a single word or sense (embedding given by the pretrained set) given its context. The context is defined by a fixed window W of words and senses on each side. Then, the model compares outLSTMEmbed with emb(si), where emb(si) is the embedding vector of the target token given by the set of pretrained embeddings. The weights of the network are modified in order to maximize the similarity between outLSTMEmbed and emb(si). For error calculation the comparison is in terms of cosine similarity. The vanilla version of LSTMEmbed is able to learn either word embeddings or sense embeddings. Our model can exploit both raw and senseannotated corpora and different types of pretrained embeddings. Given a raw-text training corpus and a set of pretrained word embeddings (i.e. With a sense annotated corpus the model can exploit either word or sense embeddings and also it is able to learn word or sense embeddings. Language modelling aims to learn a function which defines probability distributions over sequences of words. While RNN-LMs learns to predict target words based on their context, our model learns to predict the embedding of the target word, and uses the learning process for deriving useful representations of words and senses. In the following we describe how we leverage our representations for the computation of word similarity and analogy tasks. LSTMEmbed is intended for improving word and sense representations by learning from their contexts. Word Similarity. We only report the MaxSim measurement, that is, without taking into account the contextual sentences. Synonym Identification. For synonym identification we include two datasets. Word Analogy. Corpora. We unify the sense annotations of all corpora used to learn our models into a single sense inventory. BabelNet is both a multilingual encyclopedic dictionary, with lexicographic and encyclopedic cover-age of terms, and a semantic network which connects concepts and named entities in a network of semantic relations. The resource is a merger of multiple lexical semantic resources such as WordNet and Wikipedia. Keras is a neural networks library written in Python and an output of the ONEIROS project (Open-ended Neuro-Electronic Intelligent Robot Operating System). We can see that the BabelWiki corpus achieves better representations than SEW. BabelWiki. Contrary to our expectations, our approach does not appear to be competitive in the task on word analogy. The relations in our model might be represented by a higher degree relationship. excluding all the annotations. As we can see, training with sense-annotated data outperforms the configurations based only on raw text. raw text but unlike the other configurations, it does not seems to take advantage of the sense annotated data. We hypothesize that the increment of the vocabulary, with the corresponding increment of parameters to learn, is responsible for this behavior. Additionally, the learning time of this configuration was substantally larger than the time spent by LSTMEmbed, which is the reason why we did not include it in the experiments with large corpora. In this section we study how we can inject semantic information through the set of pretrained embeddings. Our assumption is that richer embeddings should enhance the representation delivered by our model. We compared four sets of pretrained embeddings. The second set consists of the same vectors, retrofitted with PPDB using the default configuration. As we can see, using richer pretrained embeddings improves the resulting representations given by our model. The sense embeddings from SensEmbed, a priori the richest set of pretrained embeddings, achieved in fact the best performance. In this paper we introduced LSTMEmbed, a new model based on a bidirectional LSTM for learning embeddings of words and senses. Second, the introduction of an output layer which predicts pre-trained embeddings allow us to inject more semantic information while speed up the training."
19,2,4.0,3.43,4.0,True,acl_2017,train,"Most existing approaches for zero pronoun resolution are heavily relying on annotated data, which is often released by shared task organizers. Therefore, the lack of annotated data becomes a major obstacle in the progress of zero pronoun resolution task. Also, it is expensive to spend manpower on labeling the data for better performance. To alleviate the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution. Furthermore, we successfully transfer the cloze-style reading comprehension neural network model into zero pronoun resolution task and propose a two-step training mechanism to overcome the gap between the pseudo training data and the real one. Experimental results show that the proposed approach significantly outperforms the state-of-the-art systems with an absolute improvements of 3.1% F-score on OntoNotes 5.0 data.","However, a major obstacle for training the supervised learning models for ZP resolution is the lack of annotated data. Following these shared tasks, the annotated evaluation data can be released for the following researches. Despite the success and contributions of these shared tasks, it still faces the challenge of spending manpower on labeling the extended data for better training performance and domain adaptation. To address the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution. Inspired by data generation on cloze-style reading comprehension, we can treat the zero pronoun resolution task as a special case of reading comprehension problem. So we can adopt similar data generation methods of reading comprehension to the zero pronoun resolution task. In this way, tremendous pseudo training samples can be generated from the various documents, such as news corpus. Towards the shortcomings of the previous approaches that are based on feature engineering, we propose a neural network architecture, which is an attention-based neural network model, for zero pronoun resolution. To sum up, the contributions of this paper are listed as follows. In this section, we will describe our approach in detail. First, we will describe our method of generating large-scale pseudo training data for zero pronoun resolution. Then we will introduce twostep training approach to alleviate the gaps between pseudo and real training data. Finally, the attention-based neural network model as well as associated unknown words processing techniques will be described. We will introduce the details of generating the pseudo training data for zero pronoun resolution as follows. In our experiments, we used large-scale news data for training. In this way, given the queryQ and documentD, the target of the prediction is to recover the answer A. That is quite similar to the zero pronoun resolution task. Therefore, the automatically generated training samples is called pseudo training data. It should be noted that, though we have generated large-scale pseudo training data for neural network training, there is still a gap between pseudo training data and the real zero pronoun resolution task in terms of the query style. So we should do some adaptations to our model to deal with the zero pronoun resolution problems ideally. In this paper, we used an effective approach to deal with the mismatch between pseudo training data and zero pronoun resolution task-specific data. Generally speaking, in the first stage, we use a large amount of the pseudo training data to train a fundamental model, and choose the best model according to the validation accuracy. Then we continue to train from the previous best model using the zero pronoun resolution task-specific training data, which is exactly the same domain and query type as the standard zero pronoun resolution task data. zero pronoun resolution task data, is far more effective than using either of them alone. Though there is a gap between these two data, they share many similar characteristics to each other as illustrated in the previous part, so it is promising to utilize these two types of data together, which will compensate to each other. As we will see in the experiment section that the proposed two-step training approach is effective and brings significant improvements. Firstly, we project one-hot representation of document D and query Q into a continuous space with the shared embedding matrix We. Then we input these embeddings into different bidirectional RNN to get their contextual representations respectively. Then we calculate a weighted sum of all document tokens to get the attended representation of document. Note that, for zero pronoun resolution task, antecedents of zero pronouns are always noun phrases (NPs), while our model generates only one word (a noun or a pronoun) as the result. To better adapt our model to zero pronoun resolution task, we further process the output result in the following procedure. Then, we use our model to generate an answer (one word) for the zero pronoun. After that, we go through all the candidates from the nearest to the far-most. For an NP candidate, if the produced answer is its head word, we then regard this NP as the antecedent of the given zero pronoun. By doing so, for a given zero pronoun, we generate an NP as the prediction of its antecedent. Because of the restriction on both memory occupation and training time, it is usually suggested to use a shortlist of vocabulary in neural network training. But this may place an obstacle in real world test. In this paper, we propose to use a simple but effective way to handle unknown words issue. The idea is straightforward, which can be illustrated as follows. Note that, the same words are projected to the same unknown word tokens, and all these projections are only valid inside of current sample. a) The weather today is not as pleasant as the weather of yesterday. Training details of our neural network models are listed as follows. No pre-trained word embeddings are used. Same to the previous researches that are related to zero pronoun resolution, we evaluate our system performance in terms of F-score (F). The best results are marked with bold face. The number in the brackets indicate the number of AZPs. When observing the performances of different domains, our approach also gives relatively consistent improvements among various domains, except for BN and TC with a slight drop. All these results approve that our proposed approach is effective and achieves significant improvements in AZP resolution. In our quantitative analysis, we investigated the reasons of the declines in the BN and TC domain. A primary observation is that the word distributions in these domains are fairly different from others. The average document length of BN and TC are quite longer than other domains, which suggest that there is a bigger chance to have unknown words than other domains, and add difficulties to the model training. Also, we have found that in the BN and TC domains, the texts are often in oral form, which means that there are many irregular expressions in the context. Such expressions add noise to the model, and it is difficult for the model to extract useful information in these contexts. These phenomena indicate that further improvements can be obtained by filtering stop words in contexts, or increasing the size of task-specific data, while we leave this in the future work. To alleviate this issue, we proposed the UNK processing mechanism to recover the UNK tokens to the real words. using pseudo training data in the pre-training step and task-specific data for domain adaptation step. As we can see that, using either pseudo training data or task-specific data alone can not bring inspiring result. By adopting our domain adaptation method, the model could give significant improvements over the other models, which demonstrate the effectiveness of our proposed two-step training approach. An intuition behind this phenomenon is that though pseudo training data is fairly big enough to train a reliable model parameters, there is still a gap to the real zero pronoun resolution tasks. On the contrary, though task-specific training data is exactly the same type as the real test, the quantity is not enough to train a reasonable model (such as word embedding). However, as the original task-specific data is fairly small compared to pseudo training data, we also wondered if the large-scale pseudo training data is only providing rich word embedding information. To better evaluate our proposed approach, we performed a qualitative analysis of errors, where two major errors are revealed by our analysis, as discussed below. Especially for the words that are near the ZP, which play important roles when modeling context information for the ZP. Though our proposed unknown words processing method is effective in empirical evaluation, we think that more advanced method for unknown words processing would be of a great help in improving comprehension of the context. Also, our model makes incorrect decisions when the correct antecedents of ZPs are in long distance. As our model chooses answer from words in the context, if there are lots of words between the ZP and its antecedent, more noise information are introduced, and adds more difficulty in choosing the right answer. Although our model does not intend to fill the ZP gap only with the words near the ZP, as most of the antecedents appear just a few words before the ZPs, our model prefers the nearer words as correct antecedents. Hence, once there are lots of words between ZP and its nearest antecedent, our model can sometimes make wrong decisions. To correctly handle such cases, our model should learn how to filter the useless words and enhance the learning of longterm dependency. For Chinese zero pronoun (ZP) resolution, early studies employed heuristic rules to Chinese ZP resolution. Then, supervised approaches to this task have been vastly explored. Also, there have been many works on ZP resolution for other languages. These studies can be divided into rule-based and supervised machine learning approaches. In sum, most recent researches on ZP resolution are supervised approaches, which means that their performance highly relies on large-scale annotated data. Therefore, the advantage of our proposed approach is obvious. We are able to generate large-scale pseudo training data for ZP resolution, and also we can benefit from the task-specific data for fine-tuning via the proposed two-step training approach. Our neural network model is mainly motivated by the recent researches on cloze-style reading comprehension tasks, which aims to predict one-word answer given the document and query. These models can be seen as a general model of mining the relations between the document and query, so it is promising to combine these models to the specific domain. By using this method, a large number of training data can be obtained without much human intervention, and make it possible to train a reliable neural network. They used attention-based neural networks for this task. Firstly, though we both utilize the large-scale corpus, they require that the document should accompany with a brief summary of it, while this is not always available in most of the document, and it may place an obstacle in generating limitless training data. In our work, we do not assume any prerequisite of the training data, and directly extract queries from the document, which makes it easy to generate large-scale training data. Secondly, their work mainly focuses on reading comprehension in the general domain. We are able to exploit large-scale training data for solving problems in the specific domain, and we proposed two-step training method which can be easily adapted to other domains as well. In this study, we propose an effective way to generate and exploit large-scale pseudo training data for zero pronoun resolution task. The main idea behind our approach is to automatically generate large-scale pseudo training data and then utilize an attention-based neural network model to resolve zero pronouns. a pre-training and adaptation step, and this can be also easily applied to other tasks as well. The future work will be carried out on two main aspects: First, as experimental results show that the unknown words processing is a critical part in comprehending context, we will explore more effective way to handle the UNK issue. Second, we will develop other neural network architecture to make it more appropriate for zero pronoun resolution task."
481,1,4.0,4.0,4.0,True,acl_2017,train,"In this paper, we aim to understand whether current language and vision (LaVi) models truly grasp the interaction between the two modalities. To this end, we propose an extension of the MS-COCO dataset, FOIL-COCO, which associates images with both correct and ‘foil’ captions, that is, descriptions of the image that are highly similar to the original ones, but contain one single mistake (“foil word”). We show that current LaVi models fall into the traps of this data and perform badly on three tasks: a) caption classification (correct vs. foil); b) foil word detection; c) foil word correction. Humans, in contrast, have near-perfect performance on those tasks. We demonstrate that using language cues only is not enough to deal with FOILCOCO and that it challenges the state-ofthe-art by requiring a fine-grained understanding of the relation between text and image.","There is thus growing interest in combining information from language and vision in the NLP and AI communities. Whilst some models have seemed extremely successful on those tasks, it remains unclear how the reported results should be interpreted and what those models are actually learning. Such results indicate that in current datasets, language provides priors that make it possible for a LaVi model to be successful without truly understanding and integrating language and vision. But problems do not stop at biasing. The authors thus suggest the need for a diagnostic dataset. Our paper contributes to investigate these three weaknesses of current VQA and IC research by proposing an automatic method for creating a big dataset of real images with minimal language bias and some diagnostic abilities. Specifically, we replace one word in the original caption with an incorrect one. Given this data, we propose three tasks based on widely accepted evaluation measures. We will refer to it as FOIL-COCO. Image captioning (IC) and visual question answering (VQA) tasks are the most relevant to our work. Despite the successes of the state-of-the-art LaVi models, it is unclear whether they capture vision and language in a truly integrative fashion. This baseline simply concatenates the Bag of Words (BoW) features from the question and Convolutional Neural Networks (CNN) features from the image to predict the answer. We complement this research by introducing three new tasks with different levels of difficulty, on which LaVi models can be evaluated sequentially. ii) Need for diagnostics: To overcome the bias uncovered in previous datasets, several research groups have started proposing tasks which involve distinguishing distractors from a groundtruth caption for an image. Our efforts, however, differ in some substantial ways. First, their technique to create incorrect captions (using BLEU to set an upper similarity threshold) is so that many of those captions will differ from the gold description in more than one respect. For instance, the caption two elephants standing next to each other in a grass field is associated with the decoy a herd of giraffes standing next to each other in a dirt field (errors: herd, giraffe, dirt) or with animals are gathering next to each other in a dirt field (error: dirt; infelicities: animals and gathering, which are both pragmatically odd). We have not yet evaluated their system against our data. ier the task becomes. In contrast, the foil captions we propose only differ from the gold description by one word and are thus more challenging. Thirdly, their evaluation is a multiple-choice task, where the system has to compare all captions to understand which one is closest to the image. So evaluating precision on both gold and foil items is crucial. This dataset has been designed with the explicit goal of enabling detailed analysis of different aspects of visual reasoning, by minimizing dataset biases and providing rich groundtruth representations for both images and questions. iii) Lack of objective evaluation metrics: The evaluation of Natural Language Generation (NLG) systems is known to be a hard problem. It is further unclear whether the quality of LaVi models should be measured using the same metrics designed for language-only tasks. Their study revealed that most of those metrics were only weakly correlated with human judgements. They proposed a semantic evaluation metric called SPICE, that measures how effectively image captions recover objects, attributes and the relations between them. In MS-COCO, each image is described by at least five descriptions written by humans via Amazon Mechanical Turk (AMT). dog, elephant, bird,.   and car, bicycle, ariplane,.   Mining of the hardest foil caption for each image-original caption pair. We take target and foil words to be nouns, specifically the labels of MSCOCO categories, and we couple together words belonging to the same supercategory (e.g., bicycle::motorcycle, bicycle::car, bird::dog). traffic light). for the test set. To this end, given an image, we replaced only those target words that occur in more than one MS-COCO caption associated with that image. Moreover, we want to use foils which are visually not present, viz. that refer to visual content not present in the image. Hence, given an image, an original caption and a target word, we replace the latter only with foils that are not among the labels (objects) annotated in MS-COCO for that image. We have used the images from MS-COCO training and validation sets to generate our training and test sets, respectively. For this purpose, we need to model the visual-language bias of the dataset. Neuraltalk is based on an LSTM which takes as input an image and generates a sentence describing its content. We obtain a neural network N that implicitly represents the visual-language bias in its weights. None of the target::foil word pairs has been filtered out by this mining process. of datapoints nr. unique images nr. of tot. captions nr. The latter is created by replacing one of the nouns in the original caption (target word) with a foil noun. Left column: some of the original COCO captions associated with an image. In bold we highlight one of the target words (bicycle), chosen because it is mentioned by more than one annotator. Middle column: For each original caption and each chosen target word, different foil captions are generated by replacing the target word with all possible candidate foil replacements. Right column: A single caption is selected amongst all foil candidates. For computational reasons, we have instantiated this task by asking models to correct the foil word by selecting the correct word from the target words, instead of from the whole dataset vocabulary (viz. State-of-the-art IC models are trained to generate captions using Recursive Neural Network (RNN), this make them not suitable for our tasks since our foil captions contain only one wrong word. Therefore we have evaluated VQA models. This model uses a two stack Long-Short Term Memory (LSTM) to encode the questions and the last fully connected layer of VGGNet to encode the images. The combination of these two projected embeddings is performed by a point-wise multiplication. The multi-model representation so obtained is used for the classification, which is performed by multi-layer perceptron (MLP) classifier. In particular, we evaluated the alternate version, viz. the model that sequentially alternates between generating image and question attention, and does so in a hierarchical way by starting from the word-level, then going to the phrase and then to the entire question-level. These levels are combined recursively to produce the distribution over the foil vs. correct captions. For the foil word detection task, we have applied the occlusion method to the models above. occlude subsets of the language input, forward propagate the masked input through the model, and compute the change in the probability of the answer predicted with the unmasked original input. For the error correction task, we applied the linear regression method over all the target words and selected the target word which has the highest probability of making that wrong caption correct with respect to the given image. Baselines We compare the SoA models above against the following baselines. For the classification task, we used a Blind model, viz. This model only accepts captions as input to predict the answer. Apart from that, we have used the same number of parameters, viz. Annotators are nearly perfect in classifying captions and detecting foil words. Hence, though, we have collected human answers only on a rather small subset of the test set, we believe their results are representative of how easy are the tasks for humans. Confidential Review Copy. DO NOT DISTRIBUTE. Systems show a strong bias towards correct captions and poor overall performance. The mixed-effect model was performed to get rid of possible effects due to either object supercategory (indoor, food, vehicle, etc.) or target::foil pair (e.g., zebra::giraffe, boat::airplane, etc.). The higher the values of these variables, the more the models tend to provide the wrong output. That is, when the foil word (e.g. cat) is semantically very similar to the original one (e.g. The same holds for frequency values. In particular, the higher the frequency of both the original word and the foil one, the more the models fail. Confidential Review Copy. DO NOT DISTRIBUTE. The higher the values of these variables, the more the models tend to provide the wrong output. That is, when the foil word (e.g. cat) is semantically very similar to the original one (e.g. The same holds for frequency values. In particular, the higher the frequency of both the original word and the foil one, the more the odels fail. In particular, the longer the foil caption, the higher the pr bability that the model will wrongly label the caption as correct. Intuitively, the longer the caption, the harder for the model to spot that there is a foil word that makes the caption wrong. We also checked the average precision of several object d t ction models on the categories in our dataset. For each category (e.g. However, we cannot conclude that the systems performed badly for that reason only. While this is only an indicative result (because the models used for object detection are different from the endto-end VQA models tested here), we can assume that other challenges lower the performance of the tested systems. Finally, we checked whether there was any correlation between results and the position of the foil in the sentence, to ensure the models did not profit from any artefact of the data. We did not find any such correlation. RB: add comments on the example with accuracy per pairs. This is what we have done with the FOIL-COCO dataset. The mistakes have been carefully thought, but au-in HieCoAtt. In particular, the longer the foil caption, the higher the probability that the model will wrongly label the caption as correct. Intuitively, the longer the caption, the harder for the model to spot th th re is a foil word hat makes th caption wrong. As revealed by the fairly high variance explained by the random effect related to target::foil pairs in the regression analysis, both models perform very well on some target::foil pairs, but fail on some others. We also checked the average precision of several object detection models on the categories in our dataset. For each category (e.g. However, we cannot conclude that the systems performed badly for that reason only. While this is only an indicative result (because the models used for object detection are different from the endto-end VQA models tested here), we can assume that other challenges lower the performance of the tested syst ms. Finally, we checked whether there was any correlation between results and the position of the foil in the sentence, to ensure the models did not profit from any artefact of the data. We did not find any such correlation. We have introduced FOIL-COCO, a large dataset of images associated with both correct and foil captions. generated, but carefully thought out, making the task of spotting foils particularly challenging. By associating the dataset with a series of tasks, we allow for diagnosing various failures of current LaVi systems, from their coarse understanding of the correspondences between text and vision to their grasp of language and image structure. Our hypothesis is that systems which, like humans, deeply integrate the language and vision modalities, should spot foil captions quite easily. The state-of-the art LaVi models we have tested fall through that test, implying that they fail to integrate the two modalities. LaVi models are a great success of recent research, and we are impressed by the amount of ideas, data and models produced in this stimulating and attractive area. With our work, we would like to push the community to think of ways the models can better merge language and vision rather than merely use one as a supplement to the other."
706,3,4.27,4.0,4.0,True,acl_2017,train,"Our goal is to create a convenient language interface for performing well-specified but complex actions such as analyzing data, manipulating text, and querying databases. However, existing natural language interfaces for such tasks are quite primitive compared to the power one wields with a programming language. To bridge this gap, we seed the system with a core programming language and allow users to “naturalize” the core language incrementally by defining alternative syntax and increasingly complex concepts in terms of compositions of simpler ones. In a voxel world, we show that a community of users can simultaneously teach one system a diverse language and use it to build 240 complex voxel structures. Over the course of three days, these builders went from using only the core language to using the full naturalized language in 74.7% of the last 10K utterances.","To accomplish this, one route is to use a programming language, but this is inaccessible to most and can be tedious even for programmers. url). parsers is still quite primitive compared to the power one wields with a programming language. In this paper, we propose bridging this gap with a new interactive language learning process which we call naturalization. We seed a system with a core programming language, always available to the user. Through this process, the user gradually teaches the system to understand the language that they want to use, rather than the core language that users are forced to use initially. This process accommodates both the user preference and the computer action space, which tends to result in a combination of English, short forms, and precise commands suitable for the computer. Definitions equate a novel utterance to a sequence of utterances the system already understands. Unlike in programming, the user uses concrete values rather than variables, which makes creating our definitions more accessible. The onus is now on the system to produce the correct generalization, so we propose a grammar induction algorithm tailored for the definitions setting. We implemented a system called Voxelurn, which is a command interface for a voxel world initially equipped with a programming language supporting conditionals, loops, and variable scoping. All users teach the system at once, so that what is learned immediately generalizes across users. It is thus a community of users that evolves the language to becomes more efficient over time. World. In addition, the state contains a selection, which specifies a set of positions that subsequent actions are performed with respect to. Core language.  The grammar rules are grouped into four categories. From top to bottom: domain-general action compositions, actions using sets, lambda DCS expressions for sets, and domain-specific relations and actions. In addition to expressivity, the core language interpolates well with natural language. To enable the building of more complex structures in a more modular way, we introduce a notion of scoping. This allows one A to use the selection as a temporary variable without affecting the rest of the program. This allows A to only focus on the selection (e.g., one of the palm trees). Therefore, we let each expression generate three possible scoping interpretations, and let the model learn which one is intended based on the context. This is an example where we have preemptively started to naturalize the core language to reduce the burden on users. Each definition consists of a head utterance and a body, which is a sequence of utterances that the system understands. When the user types an utterance x, the system generates candidate parses. If the user selects one, then we simply execute the resulting program. If there are no parses or the user rejects all of the existing ones, the user is asked to provide the definition body for x. Any utterances in the body not yet understood can be defined recursively. Alternatively, the user can carry out a sequence of steps and mark these as the body of some head specified post-hoc. Rather than propagating this uncertainty forward, we force the user to commit to an interpretation. This demonstrates the utility of interactivity in stopping a combinatorial explosion of interpretations. Let us turn to how the system learns and predicts. This section contains prerequisites before we can describe definitions and grammar induction in the next section. Semantic parsing.   Social.Author ID of author Social.Friends (ID of author, ID of user) Social.Self rule is authored by user? The user u does not appear in previous work on semantic parsing, but we use it to personalize the semantic parser trained on the community. We use a standard chart parser to construct a chart. For each chart cell, indexed by the start and end indices of a span, we construct a list of partial derivations recursively by selecting children derivations from subspans and applying a grammar rule. The resulting derivations are sorted by model score and only the top K are kept. We use chart(x) to denote the set of all partial derivations across all chart cells. Features. Derivations are scored using a weighted combination of features. Rule features fire on each rule used to construct a derivation. ID features fire on specific rules (by ID). Type features track whether a rule is part of the core language or user-induced, whether it has been used again after it was defined, if it was used by someone other than its author, and if the user and the author are the same. Author features capture the fact that some users provide better, and more generalizable definitions that tend to be used. Friends features are cross products of author ID and user ID, which captures whether rules from a particular author is systematically preferred or not by the current user, due stylistic similarities or differences. These capture a weak form of context-dependence that are generally helpful. When the user types an utterance, the system generates a list of candidate next states. Recall that the main form of supervision is via user definitions, which allows naturalization of the core language and creation of user-defined concepts. In this section, we show how to turn these definitions into new grammar rules that can be used by the system to parse new utterances. Previous work on grammar induction for semantic parsing is given pairs of utteranceprogram pairs (x, z). Though unpromsing rules are immediately pruned, many spurious rules are undoubtedly still kept. In the interactive setting, we must keep the number of candidates small to avoid a bad user experience, our precision bar for new rules is much higher. Fortunately, the structure of definitions makes the grammar induction task easier.   The body X is fully parsed into a derivation d, while the x is likely only partially parsed into a set of partial derivations chart(x). A grammar rule is produced by substituting any set of nonoverlapping matches by their categories. The second rule contains compositional categories like actions (A), which requires some care. We now propose a grammar induction procedure that optimizes a more global objective and uses the learned semantic parsing model to choose substitutions. We say that a packing P is maximal if there is no other derivations may be added without creating an overlap.   l, where l is the length of x. We also attempt to generalize compositional categories like sets and actions. along with the score of its packings, relative to the score of the simple packing. If we align the head and body, then we would intuitively expect aligned phrases to correspond to the same derivations. We can then transplant these derivations from d to chart(x) to create some new matches. This is more constrained than the usual alignment problem (e.g., in machine translation) since we only need to consider spans of X which corresponds to derivations in desc(d). We only perform this extension when the body consists of one utterance, which tend to be paraphrases. Multiutterance definitions tend to be about constructing new concepts, for which extension is not useful. Setup. Our ultimate goal is to create a community of users who can build interesting structures in Voxelurn while naturalizing the core language. We created this community using Amazon Mechanical Turk (AMT) in two stages. First, we have qualifier tasks, in which an AMT worker was instructed to build an exact simple, fixed target that we provide. In addition, replicating a target ensures that the users are able to use basic constructs of the core language, which is the starting point of the naturalization process. This two stage process was designed to give users freedom while guarding against spam. Incentives. Finally, to incentivize more definitions, we also track citations.  utterance by another user, the rule (and its author) gets a citaion. We pay bonuses to top users according to their h-index. Statistics. Is naturalization happening? Yes! For syntactic variations, where many of the definition body are not in core. The definitions for high level concepts tend long and hierarchical. top: percentage of all utterances belonging to each type. mid: percentage of accepted utterances belonging to induced. bot: expressiveness measured by the average length of the program per token of the utterance. In these applications, instead of learning from a training set, we could start with the programming language. User interactions can provide us with very strong, usable supervision through definitions. We hope that naturalization can lead to better language interface technologies that strikes a more usable balance between precision and naturalness."
614,2,4.0,4.0,3.0,False,acl_2017,train,"Current state-of-the-art models for lexical substitution – the task of nominating substitutes for a word in context – ignore word sense, instead relying on powerful vector and embedded word representations to find good substitutes. We present a simple method for improving the lexical substitution rankings of existing models by integrating word sense inventories, filtering substitutes from the correct sense to the top of the rankings. To enable maximum coverage of our method, we also propose a novel method for clustering paraphrases by word sense with substitutability in mind. Our method results in sense clusters that are more substitutable and have wider coverage than existing sense inventories. They can be applied as a filter over lexical substitution rankings generated by existing vectorand embeddingbased ranking models to significantly improve their performance.","To meet this need, there have been several efforts to automatically acquire lexical and phrasal paraphrases. Since words and phrases can be polysemous, their paraphrases can be divided into subsets representing the different meanings. There is a clear relationship between the sense of a word or phrase in the context of some sentence, and our ability to replace it with one or more of its paraphrases. The task of automatically replacing a word with its meaning-equivalent paraphrases in context is called lexical substitution (lexsub). Accordingly, many early lexsub models approached the problem in two steps: first, identify the sense of the target word by reference to an existing sense inventory (e.g. Numbers after each substitute indicate the number of annotators who made that suggestion. In this paper we propose a novel method for clustering paraphrases in PPDB which improves performance on the lexical substitution task. It is generalizable to all parts of speech and languages. Our method results in sense clusters that are more substitutable than existing sense inventories as measured by their agreement with humangenerated lexsub annotations. They can be applied as a filter over lexsub rankings generated by existing vector-and embedding-based ranking models to significantly improve lexsub performance. A crucial question when evaluating a sense inventory for its fitness for the lexsub task is, how well does it match human judgments about the interchangeability of words?     Full details of our BCubed F-Score implementation are in the supplemental material. The Avg CoInCo Overlap indicates the average number of words appearing in both the sense inventory and CoInCo annotations, by target word. annotated judgements. This will establish a score that we aim to outperform with our new method for generating paraphrase sense clusters. For each CoInCo target word that appears in WordNet, we take its senses to be its synsets, with lemmas belonging to hypernyms and hyponyms of each synset included. PPDBClus. The resulting sense inventory has more than four times the coverage of WordNet, but is also quite noisy. TWSI. It was developed specifically with substitutability in mind using an iterative, bootstrapped process for word sense induction and lexical coverage. We find that the average substitutability scores for WordNet and TWSI are significantly higher than that of PPDBClus when aggregated over the entire CoInCo dataset. We propose a novel method for automatically generating a sense inventory that is tailored to a substitutability metric. Our method uses a multi-view clustering algorithm to automatically create sense clusters of paraphrases for a given target word. Nonnegative matrix factorization (NMF) approaches cluster an input matrix by finding two smaller matrices that approximately equal the input when multiplied together. K indicates the number of clusters.   Each Xi gives a different view or representation of the data. Full details of our multi-view NMF implementation are in the supplemental material. Using multi-view NMF allows us to incorporate multiple types of information about paraphrases into the clustering algorithm. We now give an overview of the views used in our experiments. Our ultimate goal is to find clusters of paraphrases that are mutually substitutable in context. Intuitively, paraphrases that fit well within the same subset of sentences should be clustered into the same sense. One option might be to use existing human-annotated lexsub data, or to crowdsource a new set of human annotations for each target word. But we want to create a method that is scalable and generalizable. Full details on our implementation of the AddCos metric are in the supplemental material. Each row in XS corresponds to a sentence containing the target word that we extract randomly from AGiga. We also experiment with incorporating the clean structure of WordNet to encourage words belonging to the same synset, and others similar to them, to be clustered into the same sense. Each entry in the matrix, xij gives the cosine similarity between the word embedding for paraphrase pj and a compositional synset embedding for synset ci. This is based on the idea that if the foreign translations of a single English lemma can be partitioned into clusters, then the the English lemma is likely polysemous, and the clusters of translations represent its different senses. Each row in XF corresponds to a foreign word f. We now apply our proposed sense-clustering method to generate substitutable sense clusters within PPDB. We will use the training annotations to tune a PPDB Score cutoff threshold to reduce noise, and we will use the test annotations to evaluate the substitutability of the resulting clusters. This reduces the coverage of our resulting sense inventory, but decreases its noise. We choose as the final clustering solution, that clustering which produces the highest Silhouette Coefficient. We experiment with the number of views used to produce the clusters. We first cluster the paraphrases using each view individually, and then combine views to see how they complement one another. Altogether we experiment with seven unique combinations of views. In the remaining discussion, we use the notation SubstClusX to denote a sense inventory derived from the view(s) included in the superscript. The superscript C denotes the Context Substitutability view, P denotes the Paraphrase Similarity View, T denotes the Shared Translations view, and W denotes the WordNet Synsets view. For example, SubstClusCP denotes the sense inventory resulting from clustering the Context Substitutability and Paraphrase Similarity views. Of the resulting sense inventories, all multiview inventories and the SubstClusP single-view inventory are more substitutable than WordNet and TWSI in terms of B-Cubed F-Score. Our general approach is to take a set of ranked substitutes generated by an existing lexsub model. We also evaluate how well the sense filtering method works in practice, by using a simple WSD method to predict the correct sense of the target in context. In both cases, if sense filtering successfully improves the quality of ranked substitutes, it indicates that the sense inventory captures substitutability well. Our approach requires a set of rankings produced by a high-quality lexsub model to start. Its vector features correspond to syntactic dependency triples extracted from the English Gigaword corpus. Each ranking model produces a score for each (target, sentence, substitute) tuple in the test set, and ranks substitutes based on the predicted score. GAP compares a set of predicted rankings to a set of gold standard rankings. For each sentence in the CoInCo test set, we consider the PPDB paraphrases for the target word to be the substitution candidates, and we set the CoInCo annotator frequency to be the gold score. The predicted rankings are given by each lexsub model (Syn.VSM and AddCos). Sense filtering is intended to boost the lexsub rank of substitutes that belong to the most appropriate sense of the target given the context. First, given a target and sentence from the CoInCo test set, we obtain the PPDB paraphrases for the target word and rank them using each lexsub model model. We calculate the overall unfiltered GAP score for each model as the average GAP over sentences in the CoInCo test set. Next, we evaluate the ability of a sense inventory to improve the GAP score through filtering. This elevates the words from the chosen sense to the top of the rankings, while preserving their relative order. If the sense inventory corresponds well to substitutability, we should expect this filtering to improve the ranking by eliminating proposed substitutes that do not fall within the correct sense cluster. Next, having estimated the maximum achievable improvement in GAP score with sense filtering, we apply a simple word sense disambiguation method to see how well this method could work in practice. Human-annotated (gold) substitutes for the target year in the given sentence are ranked. We filter the model-generated substitutes by elevating the ranks of words from the best SubstClusCPWT sense, in the same relative order (italic text). All baseline and clustered sense inventories can improve the GAP score of the Syn.VSM and AddCos ranking models when used as a filter, based on Oracle GAP scores. This shows that filtering with a sense inventory can improve lexsub results. Further, in most cases, all sense inventories except PPDBClus can improve the GAP score when a simple WSD method is applied (Best-Fit GAP). Thus the sense filtering method is generally effective in practice. Comparing the B-Cubed F-Score for the sense inventories to the Oracle GAP scores, we find that our substitutability metric is generally a good indicator of the potential for a sense inventory to improve lexsub rankings. In general, sense inventories that score highest in terms of B-Cubed FScore also score well in terms of Oracle GAP. The Best-Fit GAP scores are not perfectly correlated with Oracle GAP scores, however, indicating that our simple WSD method may introduce bias. The simple WSD method fails to find the best sense cluster in many cases; this is an area for improve-ment. Our automatically-generated sense inventories are well-suited to the task of sense filtering for lexical substitution, as indicated by their performance exceeding all baselines in terms of Oracle GAP and Best-Fit GAP when evaluated over all parts of speech. The area where our SubstClus inventories are weakest is in improving the lexsub rankings for nouns. The crowdsourced TWSI significantly beats all other sense inventories in the nouns-only rankings. This suggests that performance in this task may be part-of-speech specific, and different optimization methods may be required for each part of speech. tated dataset (e.g. This task is called candidate ranking. This task is very challenging and the substitutes proposed by the embedding models frequently correspond to rare words. An alternative is to use existing semantic resources. The generated clusters are high coverage but contain many erroneous paraphrases, as well as paraphrases linked by different types of (non-substitutable) relations. In the substitutability-focused clustering that we propose, the resulting paraphrase clusters are more substitutable. We have presented a novel method for clustering paraphrases by word sense that unites various paraphrase representations in a multi-view approach. By incorporating a view that encodes the mutual substitutability of paraphrases, our method generates paraphrase sense clusters that are more substitutable and coherent than previous results. Our paraphrase sense clusters outperform existing sense inventories in this application when evaluated over all parts of speech. B-Cubed F-score measures cluster quality in terms of precision and recall with respect to each item being clustered. And recall indicates the extent to which two items that belong to the same gold category are clustered together. Thus our setting is a good fit for multiview clustering approaches that incorporate data from multiple views, or representations, of the items to be clustered. Nonnegative matrix factorization (NMF) approaches cluster an input matrix by finding two smaller matrices that approximately equal the input when multiplied together. In the result, if K represents the number of clusters of items in X, then the coefficient matrix V provides a transformation from paraphrases in X to clusters. The basis matrix U also provides a mapping from features to clusters.   Each Xi gives a different view or representation of the data. Multi-view NMF assumes that the views are complementary, and that each view is likely to cluster the data the same way on its own. We use it first to measure the similarity between a paraphrase and a sentence in the paraphrase-sentence matrix used as the first view for clustering. We also use it as a ranking model for the CoInCo dataset, which we then improve upon using our sense-filtering approach. Here we provide details of our implementation."
182,1,5.0,4.0,4.0,True,acl_2017,train,"Multimodal sentiment analysis is a developing area of research, which involves identification of emotions and sentiments in videos. Current research considers utterances as independent entities, i.e., ignores the inter-dependencies and relations among utterances of a video. In this paper, we propose an LSTM based model which enables these utterances to capture contextual information from its surroundings in the same video, thus aiding the classification process. Our model shows 5 − 10% improvement over the state of the art and high robustness to generalizability.","Thanks to the high availability of computers and smartphones, and the rapid rise of social media, consumers tend to record their reviews and opinions about products or films and upload them on social media platforms, such as YouTube or Facebook. Such videos often contain comparisons, which can aid prospective buyers make an informed decision. The primary advantage of analyzing videos over text is the surplus of behavioral cues present in vocal and visual modalities. The vocal modulations and facial expressions in the visual data, along with textual data, provide important cues to better identify affective states of the opinion holder. However, there are major issues that remain unaddressed, such as the role of speaker-dependent versus speaker-independent models, the impact of each modality across the dataset, and generalization ability of a multimodal sentiment classifier. Leaving these issues unaddressed has presented difficulties in effective comparison of different multimodal sentiment analysis methods. An utterance is a unit of speech bound by breathes or pauses. Utterance-level sentiment analysis focuses on tagging every utterance of a video with a sentiment label (instead of assigning a unique label to the whole video). The true meaning of an utterance is relative to its surrounding utterances. In this paper, we consider such surrounding utterances to be the context, as the consideration of temporal relation and dependency among utterances is key in human-human communication. In this paper, we discard the oversimplifying hypothesis on the independence of utterances and develop a framework based on long short-term memory (LSTM) to extract utterance features that also consider surrounding utterances. Both works showed that a bimodal system yielded a higher accuracy than any unimodal system. While there are many research papers on audiovisual fusion for emotion recognition, only a few have been devoted to multimodal emotion or sentiment analysis using textual clues along with visual and audio modalities. Both approaches relied on a feature-level fusion. In this work, we propose a LSTM network that takes as input all utterances in a video and extracts contextual unimodal and multimodal features by modeling the dependencies among the input utterances. We experimentally show that this proposed framework improves the performance of utterance-level sentiment classification over traditional frameworks. Videos, comprising of its constituent utterances, serve as the input.    Below, we explain the textual, audio, and visual feature extraction methods.   We use varying kernel vectors and window sizes to obtain multiple features. The process of extracting textual features is as follows-First, we represent each sentence as the concatenation of vectors of the constituent words. The convolution kernels are thus applied to these word vectors instead of individual words. We use ReLU as the activation function. The convolution of the CNN over the sentence learns abstract representations of the phrases equipped with implicit semantic information, which with each successive layer spans over increasing number of words and ultimately the entire sentence. Voice normalization is performed and voice intensity is thresholded to identify samples with and without voice. Z-standardization is used to perform voice normalization. The features extracted by openSMILE consist of several low-level descriptors (LLD) and their statistical functionals. Some of the functionals are amplitude mean, arithmetic mean, root quadratic mean, etc. Its ability to achieve state-of-the-art results motivated us to use it. Next, we apply max pooling to convout to select only relevant features. The pooling will be applied only to the last three dimensions of the array convout. The activations of this dense layer are finally used as the video features for each utterance. This calls for a model which takes into account such inter-dependencies and the effect these might have on the current utterance. LSTM is a kind of recurrent neural network (RNN), an extension of conventional feed-forward neural network. Specifically, LSTM cells are capable of modeling long-range dependencies, which other traditional RNNs fail to do given the vanishing gradient issue. Each LSTM cell consists of an input gate i, an output gate o, and a forget gate f, which enables it to remember the error during the error propagation. In our case, the LSTM network serves the purpose of context-dependent feature extraction by modeling relations among utterances. We propose several architectural variants of it later in the paper. Categorical cross entropy loss is taken for training. The dense layer activations serve as the output features. This matrix Xi serves as the input to the LSTM. The activations of the dense layer zi,t are used as the contextdependent features of contextual LSTM. A dropout layer between the LSTM cell and dense layer is introduced to check overfitting. As the videos do not have same the number of utterances, padding is introduced to serve as neutral utterances. To avoid the proliferation of noise within the network, masking is done on these padded utterances to eliminate their effect in the network. After feeding the train set to the network, the test set is passed through it to generate their context-dependent features. As this is the simple variant of the contextual LSTM, we termed it as simple contextual LSTM (sc-LSTM) h-LSTM We also test on an architecture where the dense layer after the LSTM cell is omitted. Thus, the output of the LSTM cell hi,t provides our context-dependent features and the softmax layer provides the classification. We call this architecture hidden LSTM (h-LSTM). bc-LSTM Bi-directional LSTMs are two unidirectional LSTMs stacked together having opposite directions. Thus, an utterance can get information from other utterances occurring before and after itself in the video. We replaced the regular LSTM with a bi-directional LSTM and named the resulting architecture as bi-directional contextual LSTM (bc-LSTM). The training process of this architecture is similar to sc-LSTM. It should be noted that using a gated recurrent unit (GRU) instead of LSTM did not improve the performance. Individual LSTM networks are used for each modality. The performance of the second level banks on the quality of the features from the previous level, with better features aiding the fusion process. Note: d-dimension of hidden unit. k-dimension of input vectors to LSTM layer. c-number of classes. Most of the research in multimodal sentiment analysis is performed on datasets with speaker overlap in train and test splits. Because each individual has a unique way of expressing emotions and sentiments, finding generic, person-independent features for sentimental analysis is very tricky. V is passed through the learnt models to get the features and classification outputs. While testing, our models have to classify emotions and sentiments from utterances by speakers they have never seen before. It contains positive and negative classes as its sentiment labels. The utterances are labeled to be either positive, negative or neutral. However, we drop the neutral label to maintain consistency with previous work. This provides the speaker-independent setting. The proposed model being used on reviews from different languages allows us to analyze its robustness and generalizability. Hierarchical vs Non-hierarchical Fusion Framework-As expected, trained contextual unimodal features help the hierarchical fusion framework to outperform the non-hierarchical framework. Due to this fact, we provide all further analysis and results using the hierarchical framework. Nonhierarchical model outperforms the performance of the baseline uni-SVM. This further leads us to conclude that it is the context-sensitive learning paradigm which plays the key role in improving performance over the baseline. Comparison among Network Variants-It is to be noted that both sc-LSTM and bc-LSTM perform quite well on the multimodal emotion recognition and sentiment analysis datasets. Since, bcLSTM has access to both the preceding and following information of the utterance sequence, it performs consistently better on all the datasets over sc-LSTM. These results prove our initial hypothesis that modeling the contextual dependencies among utterances, which uni-SVM cannot do, improves the classification. In their method, they extracted features from each modality and fed to a multiple kernel learning (MKL) classifier. However, they did not conduct the experiment in speaker-independent manner and also did not consider the contextual relation among the utterances. However, for fusion they used SVM trees. As expected, in all kinds of experiments, bimodal and trimodal models have outperformed unimodal models. Overall, audio modality has performed better than visual on all the datasets. On MOSI and IEMOCAP datasets, textual classifier achieves the best performance over other unimodal classifiers. On IEMOCAP dataset, the unimodal and multimodal classifiers obtained poor performance to classify neutral utterances. Textual modality, combined with non-textual modes boosts the performance in IEMOCAP by a large margin. However, the margin is less in the other datasets. On the MOUD dataset, textual modality performs worse than audio modality due to the noise introduced in translating Spanish utterances to English. The table reports the accuracy of classification. The table reports the accuracy of classification. glish language. However, notably visual modality performs better than other two modalities in this experiment which signifies that in cross-lingual scenarios facial expressions carry more generalized, robust information than audio and textual modalities. We could not carry out the similar experiment for emotion recognition as no other utterance-level dataset apart from the IEMOCAP was available at the time of our experiments. In some cases the predictions of the proposed method are wrong given the difficulty in recognizing the face and noisy audio signal in the utterances. Also, cases where the sentiment is very weak and non contextual, the proposed approach shows some bias towards its surrounding utter-ances which further leads to wrong predictions. This can be solved by developing a context aware attention mechanism. In order to have a better understanding on roles of modalities for overall classification, we also have done some qualitative analysis. On the other textual classifier correctly detected the polarity as positive. Contextual relationship among the utterances is mostly ignored in the literature. In this paper, we developed a LSTM-based network to extract contextual features from the utterances of a video for multimodal sentiment analysis. The proposed method has outperformed the state of the art and showed significant performance improvement over the baseline. As a part of the future work, we plan to propose LSTM attention model to determine importance of the utterances and contribution of modalities in the sentiment classification."
