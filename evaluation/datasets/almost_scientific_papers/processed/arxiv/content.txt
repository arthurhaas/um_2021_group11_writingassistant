As previously mentioned, semi-supervised learning techniques are used when labeled data is scarce and unlabeled data is abundant. Every techniques tries to integrate the information yield by the unlabeled instances inside a learning model based on the available labeled data. In this section we give a brief and high-level description of the semi-supervised learning techniques that we have employed, namely: Self Training, Transductive SVM and Active Learning.
3A Repeated Affirmative Answer, but the additional content after the conjunction makes the NSU much longer. It is still a valid NSU since it does not have a full clausal structure.
4The NSU is a Repeated Acknowledgment. Repeating the words in the antecedent, it introduces a verb. It is still considered an NSU according to the definition of Fernández (2006).
33

Fig 3 illustrates our final framework which combines regression (formulation A (using TGP)) and domain transfer (formulation C) with additional constraints. This formulation combines the three learning components described in the beginning of this section. Each of these components contains partial knowledge about the problem. The question is how to combine such knowledge to predict a new classifier given a textual description. The new classifier has to be consistent with the seen classes. The new classifier has to put all the seen instances at one side of the hyperplane, and has to be consistent with the learned domain transfer function. This leads to the following constrained optimization problem
Φ(t∗) = ĉ(t∗) =argmin c,ζi
[ cTc− αt∗TWc− γ ln(preg(c|t∗))
+ C ∑ ζi ]
s.t. : −(cTxi) ≥ ζi, ζi ≥ 0, i = 1 · · ·N t∗
TWc ≥ l α, γ, C, l : hyperparameters
(8)
The first term is a regularizer over the classifier c. The second term enforces that the predicted classifier has high correlation with tT∗W; W is learnt by Eq 10. The third term favors a classifier that has high probability given the prediction of the regressor. The constraints −cTxi ≥ ζi enforce all the seen data instances to be at the negative side of the predicted classifier hyperplane with some missclassification allowed through the slack variables ζi. The constraint t∗TWc ≥ l enforces that the correlation between the predicted classifier and t∗TW is no less than l, this is to enforce a minimum correlation between the text and visual features.
Solving for ĉ as a quadratic program: According to the definition of preg(c|t∗) for TGP, ln p(c|t∗) is a quadratic term in c in the form
− ln p(c|t∗) ∝ (c− c̃(t∗))T(c− c̃(t∗)) = cTc− 2cTc̃(t∗) + c̃(t∗)Tc̃(t∗)
(9)
We reduce − ln p(c|t∗) to −2cTc̃(t∗)), since 1) c̃(t∗)Tc̃(t∗) is a constant (i.e.does not affect the optimization), 2) cTc is already included as regularizer in equation 8. In our setting, the dot product is a better similarity measure between two hyperplanes. Hence, −2cTc̃(t∗) is minimized. Given − ln p(c|t∗) from the TGP and W, Eq 8 reduces to a quadratic program on c with linear constraints. We tried different quadratic solvers, however the IBM CPLEX solver 4 gives the best performance in speed and optimization for our problem.
In this paper we presented a new lexical resource for the Italian language containing more than 277.000 words which have been manually tagged with their prior polarity values, i.e. a value indicating the sentiment which such words evoke when are out of any context. We also provide an additional lexical resource containing a set of more than 200 polarity modifiers which can be used for inducing the sentiment polarity of Italian compound terms. Future works will be devoted to test the effectiveness of such resource in opinion mining task.
Automatic emotion recognition is a field that has gained a lot of attention in the past few decades [5], [6], [7]. Much of the work in this area are through the exploration of diverse patterns drawn from physiological signals, and many of these signals and features are being extracted to train and test several supervised and unsupervised emotion classification methods [8], [9], [10]. Recent studies show that autonomic affective regulation in two direction of arousal and valance is indexed by these bodily signals such as skin conductance, respiration rate, and cardiac variables, which can be measured using standard psychophysiological methods [11]. In addition, there is good evidence that physiological activity associated with psychology or mental states can be distinguished and systematically organized [12]. For example, electro-cardiovascular (ECG), blood volume pressure (BVP), and electromyogram (EMG) activities have been used to examine the dimension of pleasure, or valence
ar X
iv :1
60 6.
00 37
0v 1
[ cs
.H C
] 1
J un
2 01
6
(i.e, positive and negative affect) of human subjects [13], [14]. Galvanic skin response (GSR) activity has been also shown to be associated with task engagement [15].
Although human physiological response to emotion has been subject of research for several years, a deeper understanding is needed to completely describe the relation between human emotional experience and each source of biosignals [16]. In addition, many efforts have also been focused on distinguishing and classifying human emotions across a 2-dimensional theoretical well-known model within the field of physiology [17], [9], [18]; however, rather than focusing effort towards that direction, if the classifier is robust enough to detect a selected variation of discrete emotions, the differentiation across any theoretical dimension could be also achieved as an implicit task.
On the other hand, extracting as many features as possible and focusing the effort to improve classification accuracy at any cost, might not be the optimal approach in all cases– for instance, when attempting to implement the designed classifier in a wearable platform with very limited computing/power resources for a specific HCI application [19]. Therefore, another interesting direction to explore within the automatic emotion recognition paradigm is to enhance the optimal feature selection, feature reduction, or feature transformation methods in order to cost efficiently (in terms of power, speed, storage, etc.) exploit the related information content of human physiological signals.
Lemma C.1. In Alg. 2, if we assume the 2-norm of gradients of the loss w.r.t. partial sums by G (i.e., ‖∇it‖ = ‖∇`t(yi−1t )‖ ≤ G), and assume that each weak learner Ai has regret R(T ) = o(T ), then we there exists a constant c = 1−γ+ √ 1−γ(1−R(T ) TG2 )
γ < 2 γ − 1 such that
T∑ t=1 ‖∆ti‖2 ≤ c2G2T and T∑ t=1 ‖hti(xt)‖2 ≤ (4− 2γ)(1 + c)2G2T + 2R(T ) ≤ 4c2G2T. (33)
Proof. We prove the first inequality by induction on the weak learner index i. When i = 0, the claim is clearly true since ∆t0 = 0 for all t. Now we assume the claim is true for some i ≥ 0, and prove it for i+ 1. We first note
that by the inequality 1T ∑T t=1 at ≤ √∑ t a 2 t T for all sequence {at}t, we have
1 T ( ∑ t ‖∆ti‖)2 ≤ ∑ t ‖∆ti‖2 ≤ c2G2T (34)
⇒( ∑ t ‖∆ti‖)2 ≤ c2G2T 2 (35)
⇒ ∑ t ‖∆ti‖ ≤ cGT (36)
Then by the assumption that weak learner Ai has an edge γ with regret R(T ), we have from step 14 of Alg. 2:∑ t ‖∆ti+1‖2 = ∑ t ‖∆ti +∇ti+1 − hti+1(xt)‖2 ≤ (1− γ) ∑ t ‖∆ti +∇ti+1‖2 +R(T ) (37)
≤ (1− γ) ∑ t ( ‖∆ti‖+G )2 +R(T ) (38)
≤ (1− γ) (∑ t ‖∆ti‖2 + 2G ∑ t ‖∆ti‖+G2T ) +R(T ) (39) ≤ (1− γ)(1 + c)2G2T +R(T ) (40) = c2G2T (41)
We have the last equality because c is chosen as the positive root of the quadratic equation: γc2 + (2γ − 2)c+ (γ − 1− R(T )TG2 ) = 0, which is equivalent to c 2G2T = (1− γ)(c+ 1)2G2T +R(T ). The second inequality of the lemma can be derived from a similar argument of Lemma B.1 by expanding ‖ ( ∆ti−1 +∇ti − hti(xt) ) − ( ∆ti−1 +∇ti ) ‖2 and then applying edge assumption.
We now use the above lemma to prove the performance guarantee of Alg. 2 as follows.
Proof of Theorem 5.2. We first define the intermediate predictors as: f t0(x) := h0(x), f̂ t i (x) := f t−1(x)− ηihti(x), and f ti (x) := P (f̂ t i (x)). Then for all i = 1, ..., N we have:
‖f ti (xt)− f∗(xt)‖2 ≤ ‖f̂ ti (xt)− f∗(xt)‖2 = ‖f ti−1(xt)− ηihti(xt)− f∗(xt)‖2 (42) = ‖f ti−1(xt)− f∗(xt)‖2 + η2i ‖hti(xt)‖2 − 2ηi 〈 f ti−1(xt)− f∗(xt), hti(xt)−∆ti−1 −∇ti 〉 − 2ηi 〈 f ti−1(xt)− f∗(xt),∆ti−1 +∇ti 〉 (43)
Rearanging terms we have:〈 f∗(xt)− f ti−1(xt),∇ti 〉 (44)
≥ 1 2ηi ‖f ti (xt)− f∗(xt)‖2 − 1 2ηi ‖f ti−1(xt)− f∗(xt)‖2 − ηi 2 ‖hti(xt)‖2
− 〈 f∗(xt)− f ti−1(xt), hti(xt)−∆ti−1 −∇ti 〉 − 〈 f∗(xt)− f ti−1(xt),∆ti−1 〉 (45)
Using λ-strongly convex of `t and applying the above equality and ∆ t i = ∆ t i−1 +∇ti − hti(xt), we have:
`t(f ∗(xt)) ≥ `t(f ti−1(xt)) + 〈 f∗(xt)− f ti−1(xt),∇ti 〉 + λ
2 ‖f∗(xt)− f ti−1(xt)‖2 (46)
≥`t(f ti−1(xt)) + 1
2ηi ‖f ti (xt)− f∗(xt)‖2 −
1
2ηi ‖f ti−1(xt)− f∗(xt)‖2 − ηi 2 ‖hti(xt)‖2
+ 〈 f∗(xt)− f ti−1(xt),∆ti 〉 − 〈 f∗(xt)− f ti−1(xt),∆ti−1 〉 + λ
2 ‖f∗(xt)− f ti−1(xt)‖2 (47)
Summing over t = 1, ..., T and i = 1, ..., N we have:
N T∑ t=1 `t(f ∗(xt))
≥ N∑ i=1 T∑ t=1 [ `t(f t i−1(xt)) + 〈 f∗(xt)− f ti−1(xt),∇ti 〉 + λ 2 ‖f∗(xt)− f ti−1(xt)‖2 ] (48)
= N∑ i=1 T∑ t=1 `t(f t i−1(xt))− N∑ i=1 T∑ t=1 ηi 2 ‖hti(xt)‖2
+ N∑ i=1 T∑ t=1 1 2ηi ‖f ti (xt)− f∗(xt)‖2 − N∑ i=1 T∑ t=1 ( 1 2ηi − λ 2 )‖f ti−1(xt)− f∗(xt)‖2
+ N∑ i=1 T∑ t=1 〈 f∗(xt)− f ti−1(xt),∆ti 〉 − N∑ i=1 T∑ t=1 〈 f∗(xt)− f ti−1(xt),∆ti−1 〉 (49)
= N∑ i=1 T∑ t=1 `t(f t i−1(xt))− N∑ i=1 T∑ t=1 ηi 2 ‖hti(xt)‖2
+ N∑ i=1 T∑ t=1 1 2ηi ‖f ti (xt)− f∗(xt)‖2 − N−1∑ i=0 T∑ t=1 ( 1 2ηi+1 − λ 2 )‖f ti (xt)− f∗(xt)‖2
+ N∑ i=1 T∑ t=1 〈 f∗(xt)− f ti−1(xt),∆ti 〉 − N−1∑ i=1 T∑ t=1 〈 f∗(xt)− (f ti−1(xt)− ηihti(xt)),∆ti 〉 −
T∑ t=1 〈 f∗(xt)− f t0(xt),∆t0 〉 (We switched index and apply ∆t0 = 0 next.) (50)
= N∑ i=1 T∑ t=1 `t(f t i−1(xt))− N∑ i=1 T∑ t=1 ηi 2 ‖hti(xt)‖2 − N−1∑ i=1 T∑ t=1 〈 ηih t i(xt),∆ t i 〉 +
N−1∑ i=1 T∑ t=1 1 2 ‖f ti (xt)− f∗(xt)‖2( 1 ηi − 1 ηi+1 + λ)− T∑ t=1 ( 1 2η1 − λ 2 )‖f t0(xt)− f∗(xt)‖2
+ T∑ t=1 [〈 f∗(xt)− f tN−1(xt),∆tN 〉 + 1 2ηN ‖f tN−1(xt)− ηNhtN (xt)− f∗(xt)‖2 ] (51)
(We next apply ηi = 1
λi and complete the squares for the last sum.)
= N∑ i=1 T∑ t=1 `t(f t i−1(xt))− N∑ i=1 T∑ t=1 ηi 2 ‖hti(xt)‖2 − N−1∑ i=1 T∑ t=1 〈 ηih t i(xt),∆ t i 〉 + 1
2ηN T∑ t=1 ‖ ( f tN−1(xt)− f∗(xt) ) + ηN (∆ t N − htN (xt))‖2
− ηN 2 T∑ t=1 ( ‖∆tN − htN (xt)‖2 − ‖htN (xt)‖2 ) (52)
(We next drop the completed square, and apply Cauchy-Schwarz)
≥ N∑ i=1 T∑ t=1 `t(f t i−1(xt))− N∑ i=1 T∑ t=1 ηi 2 ‖hti(xt)‖2 − N∑ i=1 ηi T∑ t=1 ‖hti(xt)‖‖∆ti‖ − ηN 2 T∑ t=1 ‖∆tN‖2 (53)
(We next apply Cauchy-Schwarz again.)
≥ N∑ i=1 T∑ t=1 `t(f t i−1(xt))− N∑ i=1 ηi 2 T∑ t=1 ‖hti(xt)‖2 − ηN 2 T∑ t=1 ‖∆tN‖2
− N∑ i=1 ηi √√√√ T∑ t=1 ‖hti(xt)‖2 T∑ t=1 ‖∆ti‖2 (54)
Now we apply Lemma C.1 and replace the remaining ηi = 1 λi . Using ∑N i=1 1 i ≤ 1 + lnN , we have:
N T∑ t=1 `t(f ∗(xt))
≥ N∑ i=1 T∑ t=1 `t(f t i−1(xt))− N∑ i=1 1 2iλ 4c2G2T − 1 2Nλ c2G2T − N∑ i=1 1 iλ 2c2G2T (55)
≥ N∑ i=1 T∑ t=1 `t(f t i−1(xt))− 4c2G2T λ (1 + lnN)− c 2G2T 2Nλ (56)
Dividing both sides by NT and rearrange terms, we get:
1
TN N∑ i=1 T∑ t=1 [ `t(y i t)− `t(f∗(xt)) ] ≤ 4c 2G2 Nλ (1 + lnN) + c2G2 2N2λ .
Using Jensen’s inequality for the LHS of the above inequality, we get:
1
T T∑ t=1 `t( 1 N N∑ i=1 yit)− `t(f∗(xt)) ≤ 4c2G2 Nλ (1 + lnN) + c2G2 2N2λ ,
which proves the first part of the theorem.
For stochastic setting, we can prove it by using similar proof techniques (e.g., take expectation on both sides of Eqn. 57 and use Jensen inequality) that we used for proving theorem 5.1.
In this section we present a partition of the infinite plane into a finite number of regions. All points in a given region will be treated, under this partition, as a single epipole. Therefore, all lines intersecting a given region will be members of its epipolar line set. As we explained below, the size of each region is proportional to its location with respect to the image.
12
Our plane partition is defined by concentric circles with a set of radii {ri}ki=1, where ri ≥ ri+1 for each 1 ≤ i ≤ k and r1 = ∞. The center of these circles is the image center. Let each region be defined by the 4-tuple, (θ, δθi, ri, ri+1), where θi and θi+δθi are two angles, and ri and ri+1 are two radii (see Figure 1). The parameters of this partition are the set of r′is and the set of δθ ′ is. For a given ring, there are two degrees of freedom to define the set of regions in that ring: the ring length, ri−ri+1 and the region width, δθi. These parameters are set so that the partition maintains the system resolution and the equal hit measure. Later we define these two properties. For simplicity, we assume a circular image with radius of one.
System resolution: Any vision system is limited by the accuracy of the measurements. We define system resolution to be γ if it does not discriminate between two image lines passing through a point when the difference in the line directions is γ0 ≤ γ.
We say that the partition maintains the system resolution when the system cannot discriminate between two candidate epipoles which are located in the same region. Formally, Assume the system resolution is . Let G be a given region and e1, e2 ∈ G be two points (see Figure 4 ). Let l1 and l2 be two lines connecting an image point q and the two points e1 and e2, respectively. When the angle between l1 and l2 is less than the system resolution γ, the system cannot discriminate between these lines. More generally, for a given region G we define αG(q) to be the maximal angle between the image point q and any two points in the region. Let αG = max
q∈Image αG(q) then the
system resolution is maintained for G. Note that the system resolution cannot be maintained in regions which overlap the image or are very close to the image. The closer the point q is to the region G the larger αG(q) becomes. In particular when q ∈ G then αG(q) = 2π.
Hit measure: Roughly speaking, the equal hit measure property guarantees that the number of epipolar lines considered for each region is probabilisticly equal. We define the hit measure of a region, HM(G), to be the probability of a random epipolar line intersecting a region G. Each candidate epipolar line we consider is defined by an
13
image point and a single direction. We assume that these epipolar lines are generated by a uniform distribution of points in the image and uniform distribution of directions. Formally, the following integral computes HM(G).
HM(G) = 1
2π
∫∫
x2+y2≤1
αG(x, y)dxdy (8)
By changing the integral variables from x and y to the polar coordinates of the image points, φ and r, we obtain:
HM(G) = 1
2π
∫ 1
0
∫ 2π
0
rαG(r cos(φ), r sin(φ))dφdr (9)
(r is the determinant of the Jacobian of the exchange variables). In order to have a probabilistically equal number of points in each region, we would
like HM(G) to be identical for all regions.
In the Appendix we describe how to set the partition parameters, ri and δθi, in order to maintain the above conditions. We distinguish between three types of regions which we analyze separately. They include infinite regions on the outermost ring, regions within the image and close to the image such that the system resolution condition cannot be maintained and intermediate regions. The parameters depend on the desired resolution of the system, γ.
Annotation is cost-sensitive to the size of the task. As such, attempting to annotate the whole corpus of over 15,000 articles is infeasible. We propose a method to reduce the size of our task while maintaining the quality of the underlying timeline. For our article selection process, we need to fulfil the following criteria:
• Coverage: Our set of articles should have good coverage. Timelines should cover a broad range of time-periods and events. As such, the dataset we derive our reference timelines from must also share this property.
• Manageability: Each entity-article pair will be subject to a number of crowd-judgments. As such, it’s important to balance coverage with total data-set size.
• Informativeness: Ideally we desire the articles to be of a high quality.
To meet these criteria, we scrape the external (non-Wiki) links from an entity’s Wikipedia page. We motivate this decision by first noting the Wikipedia guidelines on verifiability3:
Attribute all quotations and any material challenged or likely to be challenged to a reliable, published source using an inline citation. The cited source must clearly support the material as presented in the article.
These standards of verifiability are not universally followed. Nevertheless, where they are we expect reasonable entity coverage and informativeness. After removing invalid URLS, we identify 3,197 articles for annotation.
Input : Set of Cubes (c1...cM), Total Points Output : Median along a dimension n say mn Procedure : P : total points in space x=0; While x<P/2 Move to next cube , x = x+Cm.totalPoints #We stop at the cube that contains our median (or a close approximation if k is too low or too high) Find median of set of points in Cm.
We can see that cube creation is an O(n+logn) task while median creation is an O(n/M) task which looks very efficient . The approach should have worked in two passes over data plus a single pass on grid cells. However there were major design & implementation issues with this approach 1. The number of cubes is (y+1)n. Even if y is 2,
for a huge n, we get 2n cells. This grows closer to total number of data points. 2. Programmatically unfeasible in direct sense. Accessing n-dimensional arrays needs n loops, we don’t know n beforehand. Solution is to convert n-d cells to 1-d (resulting in very long 1-d array). Unable to allocate memory on stack for the 1-d array. 3. Too many cells, sparse cells, data distribution across cells not uniform at all. 4. Most of the partitions will be empty, even when the number of data points N is large, leading to extreme waste of memory and CPU time. 5. Hence we conclude that the method was not Suitable for > 2d data 6. Didn’t solve problem of bias but tends to worsen it with a higher cubes number
III. VORONOI DIAGRAM BASED PARTITIONING
Our second approach involves voronoi diagrams (Aurenhammer,1991). Our goal is to construct a partitioning scheme that handles highdimensionality as well and not just provide good performance by ignoring a lot of dimensions. It is also necessary that partitions should not hold too many or too few points. Number of passes on data should be as less as possible preferably ~1. An efficient way to satisfy problem 1 & 2 of kd-tree is a tree structure that needs O(log n) number of comparisons on average to distribute a
www.ijera.com 69 | P a g e
point and to determine the affected partitions where n is the number of nodes in the tree. One way to satisfy problem of considering all dimensions all together is to use a Voronoi diagram which partitions the space into Voronoi cells, directed by a set of split points Q = q1, q2, . . . such that for each cell corresponding to split point qi, the points x in that cell are nearer to qi than to any other split point in Q. Hence, by constructing a tree of Voronoi diagrams, we can satisfy our two major concerns. Let’s call such a structure as v_tree. The top or root node of a v_tree gives a brief summary of the whole data and is split to many Voronoi cells which are split as well and so on.
To finalize the evaluation of FAVOUR, its performance is compared with the alternative algorithm described in Sec. VI-D (referred to by the acronym ALT below). The performance of ALT, estimated by applying the same LOUOCV procedure used for FAVOUR, is shown by the dashed curve in Fig. 4. A statistically significant worse performance is observed for ALT w.r.t to FAVOUR. The difference is more pronounced with few training examples, while it decreases when additional training information is used. However, even with fifteen training examples, a 2.9% performance difference is still observed.
To investigate the reason for the different performances, the ALT algorithm has been re-executed by using the MPPs computed by the FAVOUR approach (Sec. V) as initial user preference model. By this modification, the ALT performance becomes comparable with the FAVOUR one. No statistically significant difference is observed between the results of FAVOUR and the modified version of ALT (to avoid cluttering, the curve for the modified ALT version is not depicted in Fig. 4, since it approximately overlaps the FAVOUR curve).
We argue that the lower accuracy of the priors computed by the ALT algorithm is due to the model selection performed during the estimation of the mixed logit models. Under normal circumstances, a model selection process is necessary to avoid overfitting. However, when a MPP is used as a start for a personalization, this model selection might actually harm the personalization process, since different users might use different variables in their selection process. However, the approach of FAVOUR of using the full variable set for the MPP does not produce a usable mixed logit model, since the parameter standard deviations would explode with the use of too many variables. To overcome this limitation, we use standard variable selection methods to get an initial model and include in it the discarded variables by assigning to them zero mean and standard deviation value similar to that of the selected parameters.
The component used is Punch Plate for the optimization process. The weight of the component is 90 𝑔𝑚𝑠. The material used for the component is SPCC steel which is a commercial quality cold rolled steel. Thickness of the material used for the process is 0.8 𝑚𝑚. Yield strength of the material used is 280 𝑀𝑃𝑎. The ultimate tensile strength of the material is 340 𝑀𝑃𝑎.
𝑀𝑖𝑛𝑖𝑚𝑖𝑧𝑒
𝑆𝐷𝑀 = 0.0488 − 0.000133 × 𝐵𝐻𝐹 − 0.0167 × 𝜇 + 0.00150 × 𝑅𝐷 + 0.00217 × 𝑅𝑃 (11)
Subject to 2.5 < 𝑅𝐷 < 8
3 × 𝑅𝐷 > 𝑅𝑝 > 6 × 𝑅𝐷
where
𝐵𝐻𝐹 = 𝜋
4 (𝑑𝑜
2 + 2𝑧)2 × 𝑃 (12)
where 𝑃 = 2.5 𝑁/𝑚𝑚2.
and 𝑅𝐷 = 0.035 [50 + (𝑑0 − 𝑑1)√𝑆0 (13)
𝑅𝑃 = (3 𝑡𝑜 6) × 𝑅𝐷
Breiman, Leo. "Bagging predictors." Machine learning 24, no. 2 (1996): 123-140. Breiman, Leo. "Random forests." Machine learning 45, no. 1 (2001): 5-32.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. "Online passive-aggressive algorithms." Journal of Machine Learning Research 7, no. Mar (2006): 551-585. Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. "LIBLINEAR: A library for large linear classification." Journal of machine learning research 9, no. Aug (2008): 1871-1874. Fix, Evelyn, and Joseph L. Hodges Jr. Discriminatory analysis-nonparametric discrimination: consistency properties . California Univ Berkeley, 1951. Freund, Yoav, and Robert E. Schapire. "Large margin classification using the perceptron algorithm." Machine learning 37, no. 3 (1999): 277-296. Friedman, Jerome H. "Greedy function approximation: a gradient boosting machine." Annals of statistics (2001): 1189-1232. Geurts, Pierre, Damien Ernst, and Louis Wehenkel. "Extremely randomized trees." Machine learning 63, no. 1 (2006): 3-42. Loh, Wei-Liem. "On linear discriminant analysis with adaptive ridge classification rules." Journal of Multivariate Analysis 53, no. 2 (1995): 264-278.
Draft only -- Not for circulation without authors’ permission
McCallum, Andrew, and Kamal Nigam. "A comparison of event models for naive bayes text classification." In AAAI-98 workshop on learning for text categorization , vol. 752, pp. 41-48. 1998. Schapire, Robert E. "The boosting approach to machine learning: An overview." In Nonlinear estimation and classification , pp. 149-171. Springer New York, 2003. Yu, Hsiang-Fu, Fang-Lan Huang, and Chih-Jen Lin. "Dual coordinate descent methods for logistic regression and maximum entropy models." Machine Learning 85, no. 1-2 (2011): 41-75. Zadrozny, Bianca, and Charles Elkan. "Transforming classifier scores into accurate multiclass probability estimates." In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , pp. 694-699. ACM, 2002. Zhu, Ji, Hui Zou, Saharon Rosset, and Trevor Hastie. "Multi-class adaboost." Statistics and its Interface 2, no. 3 (2009): 349-360.
Draft only -- Not for circulation without authors’ permission
In this section, a series of experiments were designed to demonstrate that our methods proposed are effective and applicable. Ten benchmark real-world data sets were chosen for experimental evaluation. All the data sets were obtained from the UCI Repository of Machine Learning databases [1]. These data sets have been widely used in literatures. The general information about the selected UCI data sets is summarized in Table 3, where |U | and |C| denote the number of objects and the condition attributes, respectively. |Vd| denotes the number of decision classes.
Since the data sets may contain missing values or continuous attributes, they would be handled in advance prior to attribute reduction. Missing values were filled with mean values for continuous attributes and mode values for nominal attributes. Continuous attributes were discretized using equal-frequency discretization method. All preprocessing methods were implemented by using WEKA filters [9].
4.1. The monotonicity experiments
In this subsection, several experiments were performed to verify the effectiveness of the proposed fitness functions in Section 3. In the experiments, we took the co-entropy (CE,Definition 12), knowledge granulation (KG,Definition 13) and combination granulation (CG,Definition 14) as examples of the expected
granularity. Hence, both of fitness functions Gη and Gµ (Definition 15) can adopt three types of implementations based on CE, KG and CG. The threshold parameters α and β are set to 0.6 and 0.4 respectively.
Figures 1 - 10 present the experimental results of the proposed fitness functions on ten data sets. In each of figures, the X-axis represents the size of condition attribute subset. The condition attribute subset is increased from one attribute to all attributes during the experiments. The Y-axis pertains to values of fitness functions. Furthermore, each figure has two subfigures. The subfigure (a) shows that the experimental results of the fitness functions η, Gη − CE, Gη − KG and Gη − CG, where Gη − CE, Gη −KG and Gη − CG represent the CE-based, KG-based and CG-based implementations of Gη respectively. The subfigure (b) shows that the experimental results of the fitness functions µ, Gµ−CE, Gµ−KG and Gµ−CG, where Gµ−CE, Gµ−KG and Gµ−CG represent the CE-based, KG-based and CG-based implementations of Gµ respectively. Moreover, we rescaled the values of them to the [0, 1] range in order to better visualize the data because the values of the fitness functions Gη−CE and Gµ− CE may be greater than 1.
It can be seen from these figures that the values of the fitness functions η, Gη−CE, Gη−KG and Gη−CG increase with the number of selected attributes becoming bigger, and the values of the fitness functions µ, Gµ−CE, Gµ−KG and Gµ−CG decrease with the number of selected attributes becoming bigger. It indicates that the proposed fitness functions are monotonic with respect to the set inclusion of attributes. The results are consistent with Corollary 1 and Corollary 3. However, it is easy to see that the values of the fitness function η are the same when the number of attributes increased from 1 to 2 on all the data sets except Zoo. Similarly, there is no change in the results of the fitness function µ as the number of attributes increases from 1 to 2 on some data sets, such as Horse-colic, Voting and Kr-vs-kp. In these cases, the fitness functions η and µ can not evaluate the significance of attributes effectively. For the same situation, the values of the fitness functions Gη−CE, Gη−KG and Gη−CG get
bigger, while the values of the fitness functions Gµ−CE, Gµ−KG and Gµ−CG get smaller when the number of attributes increased from 1 to 2. It shows that the fitness functions Gη and Gµ can evaluate the significance of attributes more accurately. The results show that the fitness functions Gη and Gµ can provide more information for evaluating the significance of attributes. In other words, the fitness functions Gη and Gµ have a better discrimination power than the fitness functions η and µ. Hence, we can evaluate the significance of attributes more effectively by using the fitness functions Gη and Gµ.
4.2. Significance of single attributes
Evaluating single attributes and ranking them are an important step in Algorithms 4 and 5. Hence, in this subsection, we compared the effectiveness of the proposed fitness functions in evaluating the significance of single attributes.
Two data sets Vehicle and Credit Approval were used in experiments. There are 18 attributes in Vehicle and 15 attributes in Credit Approval. The experimental results of the fitness functions η, Gη − CE, Gη − KG and Gη − CG on two data sets are shown Figures 11 and 13 respectively. The experimental results of the fitness functions µ, Gµ − CE, Gµ − KG and Gµ − CG on two data sets are shown Figures 12 and 14 respectively. Similar to the monotonicity experiments, the values of the fitness functions Gη − CE and Gµ − CE were rescaled to the [0, 1] range.
As to data set Vehicle, for each single attribute, the values of the fitness function η are equal to zero. It indicates that the fitness function η can not evaluate the single attributes effectively. In this situation, we can not rank attributes by the fitness function η. In comparison, the fitness functions Gη − CE, Gη − KG and Gη − CG can differentiate the different single attributes
effectively. In other words, we can rank attributes effectively by the fitness function Gη. Similarly, for the most single attributes, the values of the fitness function µ are equal to 1. Hence, we can not rank attributes by the fitness function µ, but we can rank attributes by the fitness functions Gµ−CE, Gµ− KG and Gµ − CG. As to data set Credit Approval, we can obtain a similar result. The above experimental results show that the fitness functions η and µ are not appropriate to evaluate the single attributes and rank them sometimes, and the fitness functions Gη and Gµ can evaluate the single attributes and rank them effectively.
4.3. The classification accuracy experiments
In this subsection, we took the (α, β) low distribute reduct as an example of the distribute reducts. Then we compared the classification accuracy of
the (α, β) low distribution reduct with three traditional definitions of attribute reduct, including the qualitative positive region-preserved reduct (QLPRP) [61], the quantitative positive region-preserved reduct (QNPRP) [61] and the positive region extension reduct (EXDPR) [21], in probabilistic rough set model. For each definition, the corresponding reducts are obtained by the addition-deletion method and the deletion method.
For the (α, β) low distribution reduct, in Algorithms 4 and 5, we considered three different implementations ofGη by taking CE, KG and CG as the examples of the expected granularity. In addition, for Algorithm 4 and 5, the (α, β) low distribution reducts obtained by three different implementations are denoted as LDRCE, LDRKG and LDRCG respectively.
In the experiments, two well-known used classifiers including BayesNet and linear SVM (SMO) were selected to evaluate the different definitions of attribute
reduct. 10-fold cross-validation scheme was used to assess the performance of the classifiers. All approaches were implemented based on the WEKA data mining software package [9], where the classifiers were implemented with default settings.
To make a fair comparison, we took the value of α from 0.1 to 1.0 with step 0.1. For each data set, the corresponding attribute reducts can be got according to the different α. Then we computed the classification accuracies of all attribute reducts using BayesNet and SMO based on 10-fold cross-validation. The average value and standard deviation were recorded as the final classification accuracies.
Tables 4 - 7 show the experimental results of classification accuracies. In each table, the average maximum classification accuracies are depicted in bold. The classification accuracies are performed on the raw data sets also.
It is observed from Tables 4 - 7 that, for addition-deletion method, LDRCE,
LDRKG and LDRCG exhibited the best average classification accuracy based on BayesNet in most cases. In detail, LDRCE and LDRCG achieved the highest average classification accuracy on six data sets, and LDRKG achieved the highest average classification accuracy on seven data sets. It should be emphasized that LDRCE, LDRKG and LDRCG jointly achieved the highest average
classification accuracy on five data sets. Moveover, EXDPR got the maximum average classification accuracy on data set Hypothyroid. Compared with the classification accuracy of raw data, LDRCE, LDRKG and LDRCG obtained better classification performance on all the data sets except the data sets Credit Approval and German. In most cases, QLPRPR, QNPRPR and PRER decrease
the classification accuracies of raw data to some extent. In addition, the classification accuracies of LDRCE, LDRKG and LDRCG was found to be similar
or consistent. As to SMO, LDRCE, LDRKG and LDRCG have better average classification accuracy than QLPRP, QNPRP and PRER on all the data sets. Compared with the classification accuracy of raw data, LDRCE, LDRKG and LDRCG obtained better classification performance on more than half of data sets. For the deletion method, the similar results can be obtained by the classification accuracy based on BayesNet and SMO.
Furthermore, from Tables 4 - 7, we can clearly notice that the standard deviations of classification accuracies derived from LDRCE, LDRKG and LDRCG equal to zero on all the data sets. It implies Algorithms 4 and 5 are insensitive to threshold parameters. This can be explained because the (α, β) low distribution reduct has a more stringent reduction condition that preserves the (α, β) low approximation of all decision classes. In fact, the (α, β) low distribution reduct is equivalent to reduct that keeps Pawlak positive region (Definition 2) unchanged when decision table is consistent (POSR(D) = U). It means that for the consistent decision talbe, the classification accuracies derived from LDRCE, LDRKG and LDRCG are the same for all threshold parameters. What’s more, for inconsistent decision table (POSR(D) 6= U), Algorithms 4 and 5 are also insensitive to threshold parameters when each decision class contains too few inconsistent objects because the (α, β) low distribution reduct must keep the (α, β) low approximation of all decision classes unchanged.
Table 8 and 9 outlined the average length of the derived reduct based on the addition-deletion method and the deletion method. From Table 8 and 9, it can be seen that the numbers of attributes obtained by QLPRPR, QNPRPR and PRER are less than those obtained by LDRCE, LDRKG and LDRCG. The reason should be attributed to the fact that the (α, β) low distribution reduct have more stringent reduction condition than three other definitions of attribute redcut. For length of the derived reduct, the standard deviations of LDRCE, LDRKG and LDRCG also equal to zero on all the data sets. It further indicates Algorithms 4 and 5 are insensitive to threshold parameters. In addition, for QLPRPR, QNPRPR and PRER, the relatively high standard deviations of reduct length also show that QLPRPR, QNPRPR and PRER
obtained by the addition-deletion method and the deletion method are sensitive to threshold parameters.
The most popular way to model the redundancy among feature vectors is correlation such as Pearson Correlation Coefficient (PCC). The correlation value is defined over two feature vectors, and it’s a pairwise measurement. However, there also exiting redundancy between one feature vector and a set of feature vectors according to the philosophy of MCFS algorithm. In this section, we present SFG, which model the redundancy not only between two feature vectors but also one feature vector and a set of feature vectors.
The basic idea of sparse feature graph is to looking for a sparse linear representation for each feature vector while using all other feature vectors as dictionary. For each feature vector fi in features set F = [f1, f2, · · ·, fd], SFG solves the following optimization problem:
min α∈Rd−1 ‖fi −Φ iαi‖ 2 2, s.t. ‖αi‖0 < L, (3)
where Φi = [f1, f2, · · ·, fi−1, fi+1, · · ·, fd] is the dictionary of fi and each column of Φi is a selected feature from data matrix X. L is a constraint to limit the number of nonzero coefficients. In SFG, we set it to the number of features d. The αi is the coefficient of each atom of dictionary Φ
i. This coefficient vector not only decides the edge link to fi but also indicates the weight of that connection. The resulted SFG is a weighted directed graph and may have multiple components.
To solve the optimization problem 3, we use Orthogonal Matching Pursuit (OMP) solver [25] here since the number of features in our datasets is larger than 1,000. We modify the stop criterion of OMP by checking the value change of residual instead of residual itself or the maximum number of supports. The reason is that we want the number of supports (or say, the number of edge connections) to follow the raw data property. Real world datasets are always noisy and messy. It’s highly possible that several feature vectors may fail to find a correct sparse linear representation through OMP. If we set residual or maximum of supports as criteria, we can not differentiate the successful representations and the failed ones.
The OMP solver and SFG algorithm can be described as following.
Considering the 3-classes comparison experiments, we used the traditional Precision, Recall and F1 measures for the automated classification.
Predicted Positive Neutral Negative
Positive a b c Actual Neutral d e f
Negative g h i
Each letter in the above table represents the number of instances which are actually in class X and predicted in class Y, where X;Y ∈ positive; neutral; negative. The recall (R) of a class X is the ratio of the number of elements correctly classified as X to the number of known elements in class X . Precision (P) of a class X is the ratio of the number of elements classified correctly as X to the total predicted as the class X . For example, the precision of the negative class is computed as: P (neg) = i/(c+f + i); its recall, as:R(neg) = i/(g+h+ i); and the F1 measure is the harmonic mean between both precision and recall. In this case, F1(neg) = 2P (neg)·R(neg)P (neg)+R(neg) .
We also compute the overall accuracy as: A = a+e+ia+b+c+d+e+f+g+h+i . It considers equally important the correct classification of each sentence, independently of the class, and basically measures the capability of the method to predict the correct output. A variation of F1, namely, macro-F1, is normally reported to evaluate classification effectiveness on skewed datasets. Macro-F1 values are computed by first calculating F1 values for each class in isolation, as exemplified above for negative, and then averaging over all classes. Macro-F1 considers equally important the effectiveness in each class, independently of the relative size of the class. Thus, accuracy and Macro-F1 provide complementary assessments of the classification effectiveness. Macro-F1 is especially important when the class distribution is very skewed, to verify the capability of the method to perform well in the smaller classes.
The described metrics can easily computed for the 2-classes experiments by just removing neutral columns and rows as per below. In this case, the precision of positive class is computed as: P (pos) = a/(a+ c); its recall as: R(pos) = a/(a+ b); while its F1 is F1(pos) = 2P (pos)·R(pos)P (pos)+R(pos)
As we have a large number of combination among base methods, metrics and datasets, a global analysis of the performance of all these combinations is not an easy task. We propose a simple but
ACM Transactions on Embedded Computing Systems, Vol. 9, No. 4, Article 39, Publication date: March 2010.
Predicted Positive Negative
Positive a b Actual Negative c d
informative measure to assess the overall performance ranking. The Mean Ranking is basically the sum of ranks obtained by a method in each dataset divided by the total number of datasets, as per below:
MR =
nd∑ j=1 ri
nd
where nd is the number of datasets and ri is the rank of the method for dataset i.It is important to notice that rank was calculated based on Macro F1.
The last evaluation metric we exploit is the Friedman’s Test [Berenson et al. 2014]. It allows to verify whether, in a specific experiment, the observed values are globally similar. In other words, are the methods presenting similar performance across different datasets? To exemplify the application of this test, suppose that n restaurants are each rated by k judges. The question that arises is: are the judges ratings consistent with each other or are they following completely different patterns? The application in our context is very similar: the datasets as the restaurants and the macro-F1 achieved by a method is the rating from the judges.
The Friedman’s Test is applied to rankings. Then, to proceed with this statistical test, we sort the methods for each dataset using for comparison the macro-F1 metric. In other words, the method with highest macro-F1 received rank ‘1’ while the slowest macro-F1 method was ranked as ‘21’ for each dataset.
More formally, the Friedman’s rank test is defined as:
FR = ( 12
rc(c+ 1) c∑ j=1 R2j )− 3r(c+ 1)
where R2j = square of the total of the ranks for group j (j = 1,2,..., c) r = number of blocks c = number of groups
In our case, the number of blockes corresponds to the number of datasets and the number of the number of groups is the number of methods evaluated. As the number of blocks increases, the statistical test can be aproximated by using the chi-square distribution with c−1 degrees of freedom. Then, if the FR computed value is greater than the critical value for the chi-square distribution the null hypothesis is rejected. This null hypothesis states that ranks obtained by judges are globally similar, then rejecting the null hypothesis means that there are significant differences in the judgment ranks (datasets). It is important to note that, in general, the critical value is obtained with significance level α = 0.05 and. Synthesizing, the null hypothesis should be rejected if FR > X2α, where X 2 α is the critical value verified in the chi-square distribution table with c − 1 degrees of freedom and α equals 0.05.

This paper examines the interdependence generated between two parent nodes with a common instantiated child node, such as two hypotheses sharing common evidence. The relation so generated has been termed "inter causal." It is shown by construction that inter-causal independence is possible for bi nary distributions at one state of evidence. For such "CICI" distributions, the two mea sures of inter-causal effect, "multiplicative synergy" and "additive synergy" are equal. The well known "noisy-or" model is an ex ample of such a distribution. This introduces novel semantics for the noisy-or, as a model of the degree of conflict among competing hy potheses of a common observation.
In a general Bayesian network, the relation between a pair of nodes can be predictive, meaning we are inter ested in the effect of a node upon its successors, or, oppositely, diagnostic, where we infer the state of a node from knowledge of its successors. \\1e can define yet a third relation between nodes that are neither suc cessors of each other, but share a common successor. Such a relation has been termed inter-causal. (Henrion and Druzel 1990, p.10] For example, in the simplest di agram with this property, nodes A and B in Figure one are inter-causally related to each other by their com mon evidence at node e. This relation is a property of the clique formed by "marrying the parents" of e, not by the individual effects of the arcs into e. In this paper I derive the quantitative inter-causal properties due to evidence nodes constructed from the noisy-or" model.
The interest in inter-causal relations occurs in the pro cess of abduction, that is, reasoning from evidence back to the hypotheses that explain the evidence. This arises in problems of interpretation, where more than one hypothesis may be suggested by a piece of evi dence. (Goldman and Charniak 1990] Having multiple
explanations denotes the ambiguity due to not having enough information to entirely resolve which hypothe sis offers the true explanation. This paper shows how to construct an evidence node that expresses this am biguity by the degree of conflict between hypotheses. We apply this elsewhere (Agosta 1991] as a compo nent in building a "recognition network" where rele vant hypotheses are created "on the fly" as possible interpretations of the evidence.
The implicit relation between A and B due to shared evidence has been extensively explored as the prop erty of one hypothesis to "explain away" another. These are cases where, given evidence and the asser tion of one hypothesis, the other hypothesis can be disqualified as a cause of the evidence. This paper ex plores how this dependency induced between hypothe ses changes with the evidence. Interestingly, with bi nary variables, the induced dependency may vary, and as shown by the noisy-or, disappear for certain states of evidence.
This paper characterizes quantitatively the depen dency between A and B that stems from the likelihood matrix at e. Capital letters such as A and B denote
10 Agosta
unobserved random variables and lower case letters de note variables when they have been observed: e+ for E = true and e- forE= false.
Dependencies between two hypotheses' existence can occur in two senses: they conflict, so as the proba bility of one hypothesis' existence increases, the other decreases-we say one tends to exclude the other; or, as one increases the other increases also. The latter relation shall be called collaboration. First I discuss some of the basic independence properties of the net work shown in figure one as it depends on the state of node e. Next I consider how the conditional distribu tion of node e leads to conditional dependence of its parents, using the "noisy-or" model as an example for node e. Finally I propose a quantitative parameteriza tion of the dependence generated between the parent nodes.
1.1 INTER-CAUSAL INDEPENDENCE
The definition of d-separation [Pearl 1988, p.117] pro vides general conditions about the conditional inde pendence of nodes that are parents of a common evi dence node. In figure one, nodes A and B must be in dependent when their common successor is uninstanti ated, or has any instantiated successors. The converse is not always true: it is possible to construct cases where A and B remain conditionally independent after e has been observed.1 The d-separation theorem ap plies to the structure of the network: this conditional case extends it to the property of the distributions for a common successor node.
To construct such an independence conserving node, consider first the case where all variables are binary valued. The likelihood matrix for node e is:
[ r s ] def { _1 } t u = p e A B such that r�r p{e-IA = a+ B = b+ },
s �r p{ e-1 A= a- B = b+} and so 011. Taking expectation over B, the likelihood ratio seen by A, p{ e-1 a+} / p{ e-1 a-}, will be in the range be tween r/s and tfu. It is evident that, if the likelihood ratios in each row are the same, then the likelihood ratio seen by the other parent, A, will be constant for any value of B. Thus the expected likelihood ratio for A will be independent of the distribution of the other parent, node B. The same argument applies to the columns, and so to the relation of B upon A.
This property generalizes to random variables with more than two states where each row in the likelihood
1W. Buntine has pointed out that this is also a well known property of the logistic distribution, which may be thought of as a continous version of the noisy-or.
matrix differs only by a ratio, so that the row space is of rank one. Using a well known result from lin ear algebra, the row rank equals the column rank, so the same argument applies to the columns' likelihood ratios. This suggests a way to construct such a matrix:
Proposition 1: Independence is preserved between direct predecessors A and B of a common successor node E for one state of the evidence e-, if the com bined likelihood matrix is proportional to the "outer product" of the vectors for each individual likelihood:
This is shown by solving for p{ AI Be-} for any p{ A}, with Bayes' rule:
p{AIBe-} p{ e-1 A B }p{ A}
EA[p{e IAB}p{A}] Substituting in the likelihood, and simplifying :
p{ e-1 A }p{ e-1 B }p{ A} EA[P{ e-1 A }p{ e-1 B }p{ A}]
p{ e-A} - {AI -} p{ e-} p e .
I will call this independence condition between pre decessor nodes conditional on one state of the com mon evidence "conditional inter-causal independence," or CICI. This condition on the likelihood distribu tion serves as a qualification on the conditions of d separation for specified states of evidence at E.
Since the likelihood matrix appears in both numera tor and denominator of Bayes' rule, scaling the like lihood by a constant affects neither l.h.s. nor r.h.s. Thus in the binary case, where the likelihoods are a = p{ e-1 a+}, b = p{ e-1 b+ }, the outer product of the two likelihood vectors with a scaling factor, c, is general form for a CICI relation matrix:
[ � � ] [ abc a(1-b)c (1 -a)bc ] (1-a)(1-b)c · I will call this the "singular matrix" model. The in dependence constraint removes one degree of freedom, leaving the matrix to be specified with three parame ters. For binary variables, this constraint is equivalent to the relation matrix having a determinant equal to zero. This follows from the proposition:
Corollary 1: The determinant of a likelihood ma trix of binary valued random variables, p{ el A B }, of rank one equals zero. Thus det p{ e I A B } = 0 im plies that p{ AI Be} = p{ AI e }. Multiplying out the determinant gives det p{ el A B} = ru-st, the quan tity referred to as "multiplicative synergy" by Ilenrion. [Henrion, Druzdzel 1990]
"Conditional Inter-Causally Independent" Node Distributions, a Property of "Noisy-Or" Models 11
This independence relation p{ AI BE} = p{ AI E} holds for CICI nodes at both certainty for one value of e = E as well as for complete ignorance of E. The next questions are 1) whether this independence is implied for all distributions p{ E}, and conversely 2) whether there are necessarily states of E for which CICI nodes do create conditional dependence. If 1) is true, CICI evidence nodes would be degenerate and serve no pur pose.
To answer the first question we test if is it possible to have a relation matrix that is rank one at each state of the evidence. In that case the relation matrix would be factorable for every state of the evidence. In the binary case, this pair of constraints for both E = e+ and E = e- can be shown, with some algebra, to im ply that the likelihood ratios for one of the two parents must be constant and equal to one. This means that effectively there is no arc from that parent to the evi dence. This independence is implied by a more general result of [Geiger and I-Ieckerman 1990] about "transi tive distributions" for which connectedness in graphi cal representations is equivalent to dependence among the distributions. Strictly positive binary distributions are one case of transitive distributions.
Now the converse, to show when the likelihood is fac torable at one state of evidence it creates dependencies among parents at others. Let the evidence be a binary node, factorable at E = e-. Then by Bayes rule, at the other state of the evidence:
p{BIAe+} p{ B}
=
p{e+jAB}
p{e+jA}
(1-p{e-IA}p{e-IB}) 1-p{ e I A}
The right side cannot be factored into A and B factors, and is dependent upon A.
1.2 The noisy or The noisy-or model is an example that illustrates the dependencies generated by CICI likelihoods:
Proposition 2: A "noisy-or" is a case of a CICI node. This can be shown by writing the noisy-or for evidence e+ as
where q; = 1 -p;, the reliability probabilities. It is evident that for evidence e- , the likelihood matrix is a matrix of ones minus this. Calculating its determi nant,
det II - p{ e+ I A B} I = det p{ e-1 A B} = 0.
With CICI nodes I will, by convention, label the evi dence e- at which independence occurs.
The other way to build a CICI node is from the "sin gular matrix model," mentioned in the previous sec tion, where the singular matrix represents the likeli hood p{ e-1 A B } . What is the relation between these two models? They both have three degrees of freedom. Equating and solving obtains c = q0, q2 = b/(1-b), q, = a/(1-a). Since all terms must be probabilities in the range of (0, 1), the noisy-or can be identified with the singular matrix model only when the singular ma trix parameters are restricted to 0 < a, b < 1/2. This is because the noisy-or model enforces a size ordering among matrix entries, the largest entry being in the upper left hand corner. There are three other cases, 0 < a < 1/2 :S b < 1 , 0 < b < 1/2 :S a < 1 and 1/2 :S a, b < 1. These are equivalent to the noisy-or matrix with the row terms switched, the column terms switched, or both switched. These four generalizations cover the range of binary CICI relation nodes.
1.3 THE DEGREE OF INTER-CAUSAL EFFECT
We have seen that inter-causal independence among a node's parents depends upon the common node's evidence. In the binary case, forcing inter-causal in dependence at one state of the evidence precludes it from the other state. We have also seen that, in the binary case, the rank one condition for independence is easily tested by looking for a zero determinant of the likelihood matrix. The next question is, what does the value of a non-zero determinant indicate about the ef fect of A upon B?
1.3.1 Qualitative effects The value of this determinant varies from minus unity to plus unity as the relation between parents goes from extreme exclusion to extreme collaboration. At each extreme the parents A and B are deterministically de pendent. Then either the parents are mutual exclu sive, a condition already discussed, or they are forced to have identical distributions. To force identity be tween parents, the relation matrix becomes an identity matrix. For exclusion it is one minus this matrix zeros on the diagonal and ones off-diagonal. Call these extremes "complete collaboration and "complete ex clusion." Thus a relation matrix with complete col laboration for e+ will have complete exclusion for e-. These two matrices and their linear combinations are not CICI matrices, except for the trivial case of a con stant matrix.
To be able to use the determinant measure-the mul tiplicative synergy-to characterize the relation be tween parents, I must first establish that the sign of this property of the likelihood matrix is invariant to Bayes' rule: The next theorem shows that the sign of
12 Agosta
the multiplicative synergy equals the sign of the CICI relation between parents not just for p{ e I A B } , but for p{ AI Be}, and all other permutations that may be generated by Bayes' rule.
Lemma: Multiplication of a likelihood matrix, L(X, Y);j by any positive probability vector v(X); does not change the sign of the likelihood's determi nant.
To show this: Multiplication by a row vector variable is equivalent to a term-by-term multiplication of matri ces where the vector is replicated to fill out the columns of its matrix. This, in turn, is equivalent to matrix multiplication where the vector values fill the diagonal of a matrix, with all other entries zero. Write this di agonal matrix derived from the vector as d( v );; . From linear algebra there is the result that the determinant of a product equals the product of each matrix's de terminant, thus
det d( v );; L(X, Y);i == det d( v );; det L(X, Y);i. The determinant of the diagonal matrix is merely the product of terms along the diagonal, a number between zero and one. We can now show:
Proposition 3: Exclusion or collaboration (the sign of the multiplicative synergy) is given by the sign of the determinant of p{ e I A B } and is invariant to all permutations derivable by Bayes' rule of this likelihood matrix for a given conditioning.
Bayes' rule consists of multiplying the likelihood ma trix by one probability vector, the prior, then divid ing it by another, the pre-posterior. By the previous lemma, multiplication by the prior multiplies the like lihood's determinant by a positive number. Division by the pre-posterior likewise multiplies it by the re ciprocal, another positive number. Both operations preserve the sign of the likelihood determinant. Note that since the conditioning of the likelihood must be preserved;det p{ e+ I A B} > 0 does not necessarily im ply that det p{ a+ IE B} > 0.
1.3.2 Comparision to other measures of diagnostic and inter-causal relations
Inter-causality has been examined as a qualitative re lation by Wellman. (Wellman 1988) In the tradition of non-numeric, automatic reasoning methods for plan ning, he has developed an abstraction of influence di agrams where each influence is described by its sign. These "qualitative probabilistic networks" can formu late decision tradeoffs by considering dominance rela tionships among alternatives. Such networks are con structed from two kinds of qualitative relations: the first, qualitative influences, describes the relation be tween two variables; the second is the relation between influences that he terms qualitative synergy, which cor responds to inter-causality. Here is his definition of
synergy, in our notation: (p. 74)
Definition:( Qualitative synergy) Variables A and Bare positively synergistic onE, written Y+(EI A B), or just y+ E, if and only if, for every x, a1, a2, b1, b2, eo, a1 ?: a2, b1 ?: b2 implies
p{ eo I a1 b1 x } - p{ eo I a2 b1 x} :S p{ eo I a1 b2 x } - p{ eo I a2 b2 x}.
Similarly in the last relation, substitute "?:" for nega tively synergistic and "==" for zero synergy.
Henrion has called this quantity "additive synergy" to distinguish it from the multiplicative synergy mea sure defined previously. In comparison to \Vellman, our definition of "quantitative additive synergy" takes the liberty of assigning a value toY whose sign corre sponds to the sign of the synergy:
Ye+ � r Y(E == e+IAB) == r+ u-s - t .
Wellman does recognize in his examples the implied inter-causal relation between A and B due to the syn ergistic properties of the likelihood. As a further dis tinction, Wellman takes pains to extend his definition over all states of conditioning variables x, which he calls the context. This would be useless for our quan titative definition; however it serves his purpose of de termining dominance relations. Unlike his definition however, I define aY e for each conditioning of E in the likelihood matrix. Since qualitative synergy is de rived from a stochastic dominance relation on contin uous variables, to apply it to the case of binary vari ables he introduces a sign ordering convention such that e+ > e-. In my framework, his definition is equivalent to just the case where E == e+. As such, the manner in which this relation depends upon the evidential support at E is not devdoped in his exam ples.
1.3.3 Relation between additive and multiplicative synergies
As seen, for purposes of characterizing the effects be tween inter-causal nodes, I have modified definitions of synergy to be conditional on the states of binary variables. The next part develops a constraint among determinants (multiplicative synergy measures) of the same relation matrix with different states of binary evidence.
Proposition 4: Additive synergy equals the sum of the determinant measures, det e, for both states of ev idence. Expressed as a formula,
Ye+ == det e+ - det e-,
"Conditional Inter-Causally Independent" Node Distributions, a Property of "Noisy-Or" Models 13
where det e+ is defined to equal determinant IP{ E = e+ I AB } I, and likewise det e- to equal the de terminant IP{ E = e-1 AB }1. Further, Y changes sign when the state of evidence is negated. To demonstrate, since E is a binary variable,
Ye- Y(E=e-IAB) 1-r + 1-u-(1-s)-(1-t) s+t-r-u=-Ye+.
To see the relation between multiplicative and additive synergies, write out
dete- (1-r)(1-u)- (1-s)(1-t) s+t-r-u+ru-st Ye-+ dete+,
or Y e+ = det e+ -det e-
Proposition 5: Multiplicative and additive synergy are equal for CICI relation matrices. If one of the states of evidence forces independence (e.g., is CICI) then the determinant for that state disappears. Thus for CICI nodes the relation between additive and mul tiplicative synergy is: det e+ = Y e+, that is, both measures are equivalent.
The additive-multiplicative synergy relation makes it easy to show the following:
Proposition 6: Noisy-or matrices are exclusionary nodes for E = e+.
Since det e- = 0, one can use the previous result to show Ye+ < 0. See [Agosta 1991).
A typical situation expressed by a noisy-or is the rela tion between seeing cat prints in someone's house and inferring which kind of cat they have as a pet. The ex clusionary property of noisy-or nodes is the essence of their ability to "explain away" one hypothesized cause as another cause becomes more likely. Thus upon see ing paw prints, one cause-a pet blue Persian-tends to exclude their being also a short haired red tabby in the house.2 If we comb the house and find no paw prints, the explanations remain independent: we are no wiser about relati vc probabili tics of the household's domestic animals, even though we may justifiably tend to doubt they own a pet.
How would collaborative nodes, e.g. Y e+ > 0 nodes, be constructed? Recall the result in Linear Algebra that switching a pair of rows or columns of a matrix switches the sign of a matrix's determinant. Thus they can be built from exclusionary nodes by switching the off-diagonal and on-diagonal elements.
2For the model to apply strictly, there should be no relation between lovers of different kinds of cats; that is, being a Persian owner should not, in itself, make the house hold more or less likely to own a short haired tabby. (This example is inspired by (M. Henrion, 1990].)
1.3.4 The range of inter-causal dependency
How can the dependency be described quantitatively? This inter-causal dependency is not just a consequence of the diagnostic dependencies between the parents, A and B, and the evidence; rather it may be thought of as the relation between these dependencies. At the ex tremes of complete inter-causal dependency, the indi vidual (marginal) likelihoods p{ El A} and p{ El B} are completely determined by the marginals of the other predecessor: there is no additional freedom in the diagnostic relation between hypothesis and evi dence. In comparison, when A and B are inter-causally conditionally independent, the diagnostic support be tween hypothesis and evidence for each can be speci fied independently.
As a consequence of proposition 3, there is a qualita tive correspondence, where the sign of the determinant of likelihood matrix terms p{ el A B} corresponds to the sign of the induced dependency of p{ AI B }. Their quantitative relation is not as obvious. Note that un like the determinant, det e+, p{ AI B} is homogeneous of zeroth order in the likelihood terms. That is, scaling the entries in the likelihood matrix does not change the dependence among parents, as can be seen from the following version of Bayes' rule:
{BI A } = p{ei AB}p{B} p e En[p{eiAB}]
This means that multiplying all terms of p{ el A B} by a constant changes the value of the determinant but leaves p{ Bl A e} unchanged, destroying the one to one correspondence between the multiplicative syn ergy and any quantitative characterization of the inter causal relation p{ e I A B } .
To explore the quantitative relation, the next section shows the construction of the algebraic solution for one parent's belief as a function of the rest of the clique's nodes.
Figure 2: The noisy-or belief surface
14 Agosta

3.1 Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.2 Uninformed search methods . . . . . . . . . . . . . . . . . . . . . 6 3.3 Informed Constructive Methods . . . . . . . . . . . . . . . . . . . 7 3.4 Informed Local Search . . . . . . . . . . . . . . . . . . . . . . . . 8
Highly interactive game-like virtual environment has gained increasing spotlight in academic and educational researches. Besides being an efficient and engaging educational tool, virtual environment also has the potential to be integrated with Educational Data Mining (EDM) to cater to emerging requirements of educational assessment. Nowadays, the traditional academic assessment approaches cannot thoroughly reflect the students’ learning competencies which are crucial for them to thrive in a fast-changing world. We propose an assessment system that seamlessly integrates EDM with functionality and affordance of a virtual environment to assess students’ learning competency through analysing their behavioural data and patterns. We also propose a set of metrics which can be used for judging students’ learning competency and how these metrics can be evaluated computationally by quantifying and capturing students’ behavioural data in a virtual environment. The field study, which is conducted in Xinmin Secondary School in Singapore, showed that the proposed assessment system is promising in identifying useful behaviour metrics for assessing learning competency. The system also exhibits more potential in terms of its quantitative and objective approach, comparing to traditional assessment methods.
To summarize, the key points of this section were as follows:
• We introduced a linear programming problem that was a relaxation of our original problem. The function L(u) was shown to be the dual of this linear programming relaxation.
• In cases where the optimal solution to the underlying LP is fractional, the subgradient method will still d-converge to minu L(u). However the primal solutions (y(k), z(k)) will alternate between different solutions that do not satisfy the y(i, t) = z(i, t) constraints.
• In practice, tightening methods can be used to improve convergence. These methods selectively introduce constraints in an effort to improve convergence of the method, with the cost of increased complexity in finding y(k) and/or z(k). The precise constraints to be added can be chosen by identifying constraints that are frequently violated during the subgradient method.
• Finally, we described methods that construct a compact linear program that is equivalent to the original LP relaxation. This linear program is often small enough to be solved by a generic LP solver; this can be useful in debugging dual decomposition or Lagrangian relaxation algorithms.
We have demonstrated that a convolutional deep neural network is capable of approaching the ideal binary mask
separation performance. Our convolutional DNN is relatively simple and small scale, and was trained with relatively little data; only two minutes of training audio was provided for each voice. We have also demonstrated that the objective separation quality measures are dependent upon the probabilistic interpretation of the convolutional predictions made by the model. In particular, we have demonstrated that the model may in principle be optimized for either sound quality or separation/suppression and that these goals are mutually exclusive. However, our results also suggest that there is a comfortable global optimum where separation and sound quality are near to their individual maxima.
This performance is starkly superior to performance reported for the same test audio using a time-domain convolutional deep transform (CDT) with probabilistic resynthesis approach [8]. This is not surprising when considering the short-time Fourier transform as a contributory stage of abstraction in a deep architecture. Essentially, the STFT we employ constitutes a layer of abstraction featuring both a filter and a demodulation stage [8], [9], and the inverse STFT constitutes a further filter and synthesis stage. Hence, although the present DNN only features 3 layers, it might be interpreted as featuring a depth of 5 layers if we include those stages of demodulation and synthesis of the STFT and the inverse STFT. Thus, it is not surprising that this network performs far better than the respective 3-layer time domain approach. In addition, this model is more constrained, hence is easier to train than the respective autoencoder [8] and, we note in passing, that (unlike the autoencoder based approach reported previously) the present model showed no sign of bias towards either the male or female voice.
The performance reported here also appears superior to previous methods based on non-negative matrix factorization (NMF) which incorporated deep neural networks as part of the NMF pipeline [4], [5]. While these previous results are not directly comparable with the present results, the ideal binary mask reference allows some comparison to be made; The previous NMF-based models did not reach as close to the ideal binary mask performance as the present model. The advantage of the present approach is likely due to a combination of 1) a relatively large scale network (larger than those reported in [4], [5]) and 2) the probabilistic convolution featured here. The advantage of scale is enhanced by the fact that the present audio data was decimated to a sample rate of 4 kHz, further increasing the effective advantage of scale. It may also be the case that this relation of scale to sampling rate accounts for some performance gains in terms of mitigated aliasing [10].
More generally, given that DNN are inspirsed by, and modeled upon, the neural circuits and function of the brain, it may be that some aspect of the present study offers insight into the possible neural signal processing that might be employed in the human auditory system during cocktail party listening. In principle, a system equivalent or similar to the probabilistic binary mask described here might be implemented in the auditory brain. Indeed, the two minutes of training data applied here is not far in scale from the learning
(adaptation) rate demonstrated in the human auditory perceptual system [11].
Automatic detection and recognition of the ongoing surgical process is a vital step on the way to a more context-sensitive and collaborative operating room of the future [1]. While it is generally a positive development, that an increasing number of medical and imaging devices are available during surgery, this unfortunately also increases the cognitive workload of the surgeon and the organizational complexity for the OR team. In order to provide the available information only when it is actually necessary for the procedure, devices must recognize the surgical context and workflow.
Different approaches exist in the field of surgical workflow recognition [2], which is also an aspect of the recently defined area of surgical data science. Some methods try to extract a structured model from recorded surgeries [3], while others directly try to recognize the surgical phases or activities through instrument and sensor data [4– 6], laparoscopic video [7–9], kinematics information [10], or a mixture thereof [11]. In this work we will apply both Random Forests (RF) [12] and Hidden Markov Models (HMM) [13], separately and combined, to recognize the surgical phase from instrument and sensor data.
We briefly describe the subregion model presented by us recently [10]. The basic idea is firstly presented, and then the implementation details are described.

This section describes the state of the art of existing approaches for solving ATSP with CP. We distinguish the structural filtering, which ensures that a solution is a Hamiltonian path, from cost based pruning, which mainly focus on the solution cost. Then, we study a few representative branching heuristics.
Given, a directed weighted graph G = (V,A, f), and a function f : A → R, the ATSP consists in finding a partial subgraph G′ = (V,A′, f) of G which forms a Hamiltonian circuit of minimum cost. A simple ATSP model in CP, involving a graph variable GV , can basically be stated as minimizing the sum of costs of arcs in the domain of GV and maintaining GV to be a Hamiltonian circuit with a connectivity constraint and a degree constraint (one predecessor and one successor for each node). However, it is often more interesting to convert such a model in order to find a path instead of a circuit [15,25]. Our motivation for this transformation is that it brings graph structure that is more likely to be exploited.
In this paper, we consider the ATSP as the problem of finding a minimum cost Hamiltonian path with fixed start and end nodes in a directed weighted graph. In the following, s, e ∈ V respectively denote the start and the end of the expected path. s and e are supposed to be known. They can be obtained by duplicating any arbitrary node, but it makes more sense to duplicate the node representing the salesman’s home.
F.1 Proof of part (a) We first prove f̂ ∈ Nm. Notice that f̂ = ∑T
t=1 B 2bT log(1−µt1+µt )∆̂t. Thus, if
T∑ t=1 B 2bT ∣∣∣∣log(1− µt1 + µt ) ∣∣∣∣ ≤ B, (39)
then we have f̂ ∈ Nm by the definition of Nm. The definition of bT makes sure that inequality (39) holds. Thus, we have proved the claim. The time complexity is obtained by plugging in the bound from Theorem 3.
It remains to establish the correctness of f̂ . We may write any function f ∈ Nm as
f(x) = d∑ j=1 wjσ(fj(x)) where wj ≥ 0 for all j ∈ [d].
The constraints wj ≥ 0 are always satisfiable, otherwise since σ is an odd function we may write wjσ(fj(x)) as (−wj)σ(−fj(x)) so that it satisfies the constraint. The function fj or −fj belongs to the class Nm−1. We use the following result by Shalev-Shwartz and Singer (2010): Assume that there exists f∗ ∈ Nm which separate the data with margin γ. Then for any set of non-negative importance weights {αi}ni=1, there is a function f ∈ Nm−1 such that ∑n i=1 αiσ(−yif(xi)) ≤ − γ B . This implies that, for every t ∈ [T ], there is f ∈ Nm−1 such that
Gt(f) = n∑ i=1 αt,iσ(−yif(xi)) ≤ − γ B .
Hence, with probability at least 1− δ, the sequence µ1, . . . , µT satisfies the relation
µt = Gt(ĝt) ≤ − γ
2B for every t ∈ [T ]. (40)
Algorithm 4 is based on running AdaBoost for T iterations. The analysis of AdaBoost by Schapire and Singer (1999) guarantees that for any β > 0, we have
1
n n∑ i=1 e−βI[−yifT (xi) ≥ −β] ≤ 1 n n∑ i=1 e−yifT (xi) ≤ exp ( − ∑T t=1 µ 2 t 2 ) .
Thus, the fraction of data that cannot be separated by fT with margin β is bounded by exp(β −∑T t=1 µ 2 t
8B2 ). If we choose
β :=
∑T t=1 µ 2 t
2 − log(n+ 1),
then this fraction is bounded by 1n+1 , meaning that all points are separated by margin β. Recall that f̂ is a scaled version of fT . As a consequence, all points are separated by f̂ with margin
Bβ
bT =
∑T t=1 µ 2 t − 2 log(n+ 1)
1 B ∑T t=1 log( 1−µt 1+µt ) .
Since µt ≥ −1/2, it is easy to verify that log(1−µt1+µt ) ≤ 4|µt|. Using this fact and Jensen’s inequality, we have
Bβ
bT ≥
( ∑T
t=1 |µt|)2/T − 2 log(n+ 1) 4 B ∑T t=1 |µt| .
The right-hand side is a monotonically increasing function of ∑T
t=1 |µt|. Plugging in the bound in (40), we find that
Bβ bT ≥ γ 2T/(4B2)− 2 log(n+ 1) 2γT/B2 .
Plugging in T = 16B 2 log(n+1) γ2
, some algebra shows that the right-hand side is equal to γ/16 which completes the proof.
F.2 Proof of part (b) Consider the empirical loss function `(f) := 1n ∑n
i=1 h(−yif(xi)), where h(t) := max{0, 1 + 16t/γ}. Part (a) implies that `(f̂) = 0 with probability at least 1− δ. Note that h is (16/γ)-Lipschitz continuous; the Rademacher complexity of Nm with respect to n i.i.d. samples is bounded by √ q/nBm (see Lemma 6). By the classical Rademacher generalization bound (Bartlett and Mendelson, 2003, Theorem 8 and Theorem 12), if (x, y) is randomly sampled form P, then we have
E[h(−yf̂(x))] ≤ `(f̂) + 32B m γ · √ q n + √ 8 log(2/δ) n with probabality at least 1− δ.
Thus, in order to bound the generalization loss by with probability 1 − 2δ, it suffices to choose n = poly(1/ , log(1/δ)). Since h(t) is an upper bound on the zero-one loss I[t ≥ 0], we obtain the claimed bound.
We observed peak F1-score performances at low percentiles for fatigue or loss of energy (5th percentile), disturbed sleep (10th percentile) as well as depressive symptoms and no evidence of depression (both 15th percentile) suggesting fewer features are needed to reach optimal performance. In contrast, peak F1-score performances occurred at moderate percentiles for evidence of depression (30th percentile) and depressed mood (55th percentile) suggesting that more features are needed to reach optimal performance. However, one notable difference between these two classes is the dramatic F1score improvements for depressed mood i.e., 20 point increase from the 1st percentile to the 20th percentile compared to the more gradual F1-score improvements for evidence of depression i.e., 11 point increase from the 1st percentile to the 20th percentile. This finding suggests that for identifying depressed mood a variety of features are needed before incremental gains are observed.
In this approach, gloss of keyword is only considered within specific sentence instead of selection of all words. Number of common words is being calculated between specific sentence and each dictionary based definitions of particular keyword.
• Consider, earlier mentioned sentence of “Example 1” as follows: “Ram and Sita everyday go to bank for withdrawal of money.”
• The instance sentence would be “Ram Sita everyday go bank withdrawal money” after discarding the “stop words” like “to”, “for”, and so on.
• If “Bank” is considered as keyword and its two senses are X and Y (refer Table 1). Then, number of common words should be calculated between the instance sentence and each
probable senses of “Bank” (refer Table 1).
• Number of common words found would be assigned to the counter of that sense of “Bank”. Consider, X-counter has the value I’ and Y-counter has the value I”.
• Finally, the higher counter value would be assigned as the sense of the keyword for the particular instance sentence.
• The dictionary definition (gloss) of the keyword would be taken from “WordNet”. • This approach also believes that entire sentence represents the particular sense of the
keyword.
We now show how a layer of neurons transforms an input vector into a sparse representation. From the above description, every neuron is producing an estimate x̂j of the input xFF, with length oj nFF reflecting how well the neuron represents or recognises the input. We form a sparse representation of the input by choosing a set YSDR of the top nSDR = sN neurons, whereN is the number of neurons in the layer, and s is the chosen sparsity we wish to impose (typically
s = 0.02 = 2%). The algorithm for choosing the top nSDR neurons may vary. In neocortex, this is achieved using a mechanism involving cascading inhibition: a cell firing quickly (because it depolarises quickly due to its input) activates nearby inhibitory cells, which shut down neighbouring excitatory cells, and also nearby inhibitory cells, which spread the inhibition outwards. This type of local inhibition can also be used in software simulations, but it is expensive and is only used where the design involves spatial topology (ie where the semantics of the data is to be reflected in the position of the neurons). A more efficient global inhibition algorithm - simply choosing the top nSDR neurons by their depolarisation values - is often used in practise.
If we form a bit vector ySDR ∈ {0, 1}N where yj = 1⇔ j ∈ YSDR, we have a function which maps an input xFF ∈ {0, 1}nFF to a sparse output ySDR ∈ {0, 1}N , where the length of each output vector is ‖ySDR‖`1 = sN N .
The reverse mapping or estimate x̂ of the input vector by the set YSDR of neurons in the SDR is given by the sum:
∑ j∈YSDR x̂j = ∑ YSDR π−1j (oj) = ∑ YSDR π−1j (cj xj) = ∑ YSDR π−1j (cj πj(xFF)) = ∑ j∈YSDR π−1j (cj) xFF
Keywords: Factor oracle, concurrent constraints programming, ccp, machine learning, machine improvisation, Ccfomi, Gecode, ntcc, pntcc, real-time.
The concept of pattern recognition is widely used in cognitive science related areas. It is generically defined in Math, Computer Science, Design, and few other areas as a method or technique of classification, regularity search, etc. But the formal overall concept of pattern recognition from physical to algorithmic basis is not clear, coherent or unambiguously defined in the scientific literature [Verhagen 1975], [Jie Liu, Jigui Sun, and Shengsheng Wang 2006].
Here is proposed some definitions of pattern recognition first as a general physical phenomenon. This is important because pattern recognition is frequently viewed as an artificial technique rather than a basic natural phenomena. Then is derived
the concept of cognitive pattern recognition, and its difference from the basic physical pattern recognition. Also is proposed the not commonly used concepts of pattern processing and pattern learning as playing key cognitive functions to explain the mind functioning.
RL is a learning method based on trail and error, where an agent does not necessarily have a prior knowledge about which is the correct action to take. The underlying model that RL learns is a Markov Decision Process (MDP): An agent interacts with the environment and selects an action. Applying the action at a single state, and the environment emits a new state and a reward signal. In order to maximize the expected rewards in long term, the agent learns the best policy to take actions [36].
Some works in computer vision literature and elsewhere e.g., [1, 3, 7, 31, 28, 27] have embraced vision as a sequential decision task. RL is capable of solving sequential decision making tasks especially with the help of good environment representations learned by DNN models. Some recent works included training an RL agent to play Atari games from raw input pixels [28], and a recurrent visual attention model [27] to reduce computation for computer vision tasks. Our work is similar to the attention model described in [27], but we designed our own network architecture especially for solving the visual tracking problem by combining CNN, RNN and RL algorithms.
Our work formulates visual tracking as a sequential decision making process and leverages RL to solve this problem. We describe an end-to-end optimization procedure that allows the model to be trained directly with respect to a visual tracking task and to maximize a long-term tracking performance measure which depends on the entire sequence of predictions made by the model. We will describe our framework in detail in next section.
It is a well-known phenomenon that for the same data set various clustering algorithms may produce different partitions. This is true both for objects described by continuous variables (like results of measurements) and for ones described by discrete features (like documents treated as points in term space). Consensus clustering and meta-clustering are two known techniques helping to select the best one among the competing partitions. It is also well known that by changing the geometry of the data space we may even obtain all possible partitions of the dataset.
In this paper we investigate which partition would be selected if we apply consensus clustering or meta-clustering to the set of all possible partitions. In particular we formulate (in Section 3) and prove (in Section 4) that, using the so-called Rand Index as a measure of partition similarity, we obtain via consensus clustering the partition putting each element in a separate set (which we will call subsequently total-separation partition) as a ”consensus” between these partitions. In Section 5 we discuss briefly practical lessons from this theorem. In Section 6 we demonstrate that a similar theorem can be formulated for the more realistic case where we consider only partitions containing not more
ar X
iv :1
70 2.
03 72
4v 1
[ cs
.A I]
clusters than a predefined threshold. In Section 7 we show experimentally that also the very same similarity measure applied in meta-clustering1 leads towards a similar choice of best partition.
We start with Section 2 explaining the concepts of concensus clustering and meta-clustering as well as pointing to the research on these topics. In Section 8 we summarise our findings and point to further research directions.
We performed two kinds of different experiments to compare our BDAE network with other models.
(1) Only single modality is available.
(2) When both modalities are available, the shared representations are obtained by linking the features directly.
SEED results Figure 3 shows the summary of multimodal facilitation experiment results. We can see from Figure 3 that our BDAE model has the best performance (91.01%). Besides, the standard deviation of our BDAE model is also the smallest. This indicates that the BDAE model has a good robustness. Table 5 shows the results when we linked the features extracted from EEG signals and eye movement data directly. The last column of Table 5 means that we linked both five frequency bands of EEG signals and eye movement data features directly.
Compared with Table 2, we can see that when linking different modalities together, the emotion recognition accuracy increased in almost all frequency bands, and the standard deviation becomes smaller.
The experimental results using the BDAE model are shown in Table 6. We examined the BDAE model three times and the recognition accuracies shown in Table 6 were average. We can see that the BDAE model achieved the best accuracy of 91.01%, which is higher than those of single modality and directly linking strategy. And the standard deviation of the BDAE model is 8.91, which is the smallest among three different approaches.
In [Lu et al., 2015], the authors employed fuzzy integral method to fuse different modalities. The classification accuracy is 87.59% and the deviation is 19.87%. Compared with [Lu et al., 2015], the BDAE model enhanced the performance of affective model significantly.
DEAP results In previous papers, Rozgic et al. treated the EEG signals as a sequence of overlapping segments and a novel non-parametric nearest neighbor model was employed to extract response-level feature from these segments [Rozgic et al., 2013]. Li et al. used Deep Belief Network (DBN) to automatically extract high-level features from raw EEG signals [Li et al., 2015].
The experimental results on the DEAP dataset are shown in Table 7. We compared the BDAE results with results in [Li et al., 2015] and [Rozgic et al., 2013]. As can be seen from Table 7, the BDAE model improved recognition accuracies in all classification tasks.
From the experimental results on the SEED and DEAP datasets, we have demonstrated that the BDAE network can be used to extract shared representations from different modalities and the extracted features have better performance than other features.
is that a non-multilingual LDA model applied to a comparable corpus estimates the predominant language of a document rather than its semantic content. Another improvement can be observed by combining the results of different models, a technique that is usually applied for pLSA Hofmann [2001]. In this case, the cosine scores of runs with different dimensional models were simply averaged (this corresponds to concatenating the L2-norm normalized sampling statistics vectors). This yielded a score of mrr = 0.68 for the cut-off model, showing performance in the same order of magnitude as ESA. Figure 11 and Table 4 give a survey of the results obtained with LDA. Scores significantly better than in the respective line above having p ≪ 0.005 in the paired t-test are marked with ∗∗. (Of course we could not test against scores reported elsewhere, for lack of the original numerical data.)
How different are the ESA and the LDA models, how much can they contribute to each other? In order to answer this question, we combined the cosine scores of both models by different interpolation factors 0 ≤ α ≤ 1. A stable improvement in performance with maximummrr = 0.89 was achieved for giving the cut-off LDA model a weight of 0.4 and the ESA model a weight of 0.6. See Figure 12.
45
Let Yk(j0) = ∑j0
j=1 Xk(j). Since we use the median to form the aggregate, we have σSt(t) = min{j : 1 m ∑ k∈[m] Yk(j) ≥ 0.5}. Define the event W = {rσk,St (t) ≤ r}. When W occurs, σk contributes 1 to Yk(r). Let Q = ∪ n−r j=1Qj, where Qj = {rσk,St (t) = j + r, lσk,St(u) ≤ r}. When Qj occurs, σk contributes a fractional vote Vj to Yk(r), where Vj = r−lσk,St (t) +1
rσk,St (t) −lσk,St (t)
+1 ≥ V ′ j = 1 j+1 . In fact, Vj = V ′ j when
lσk,St(t) = r. Therefore, based on the Lemma 4.7 of the main text, we have
E[Yk(r)] ≥ P[W ] +
t−r ∑
j=1
1
j + 1 P[Qj ] (23)
≥ 1− 1
2 φ1/2 −
1 2 φ. (24)
Let q′ = 1 − 12φ 1/2 − 12φ. When φ 1/2 + φ < 1, it follows that q′ > 0.5. By using Hoeffding’s inequality, we obtain
P


1
m
∑
k∈[m]
Yk(r) < 0.5

 ≤ exp(−2m(1/2− q′)2).
Let Zk(j0) = ∑t
j=j0 Xk(j). In an analogous manner, we can prove that
P


1
m
∑
k∈[m]
Zk(l) < 0.5

 ≤ exp(−2m(1/2− q′)2).
Therefore, the probability of success of iteration t may be bounded as
P[Dt] ≤P
[
1
m
m ∑
k=1
Yk(r) < 0.5
]
+ P
[
1
m
m ∑
k=1
Zk(l) < 0.5
]
≤ 2e−2m(1/2−q ′)2 .
As a result, when m ≥ c log 2nδ with c = 2 (1−2q′)2 , where q ′ = 1− 12φ 1/2 − 12φ, we have P[σ ∈ Σ0] > 1− δ.
The word embeddings output by the lookup-table are concatenated and fed through two successive 1-D convolution layers. The convolutions use a step size of one and extract context features for each word. The kernel sizes ke1 and k e 2 determine the size of the window dewin = k e 1 + k e 2 − 1 over which features will be extracted by nete. In order to obtain windows centered around each word, we add (ke1+k e 2)/2−1 padding words at the beginning and at the end of each sentence. The first layer cnne applies the linear transformationM e,1 exactly ke2 times to consecutive spans of size ke1 to the d e win words in a given window:
cnne(xei ) =M e,1  LTW e([e] ke1 i−a)
... LTW e([e] ke1 i+a)
 ,
where a = bk e 2 2 c, M e,1 ∈ Rd e hu×(d e emb k e 1) is a matrix of parameters, and dehu is the number of hidden units (hu). The outputs of the first layer cnne are concatenated to form a matrix of size ke2 d e hu which is fed to the second layer:
nete(xei ) =M e,2 tanh(cnne(xei )) (6)
where M e,2 ∈ Rdemb×(ke2 dehu) is a matrix of parameters, and the tanh(·) operation is applied element wise. The parameters W e, M e,1 and M e,2 are trained by stochastic gradient descent to minimize the loss (3) introduced in §2.1.
(MIN,499,1), (500,599,2), (600,699,3), (700,799,4), (800,MAX,5), 0 END;
 Association Rule Mining In this step, we want to mine association rules with the following form: itemsincome. Association rules
support and confidence are fixed respectively to 2 and 0.5. To achieve this task, we write this MSQL query: GETRULES(TRANSACTION_VIEW) INTO TRANSACTION_RB WHERE BODY HAS {(A=1) OR (B=1) OR (C=1) OR (E=1)} AND
CONSEQUENT IS{(INCOME=*)} AND SUPPORT>2 AND CONFIDENCE>=0.5

The process of breaking down the original features into meta-features and recombining them, allows similar features, i.e. features that are different only in some of their base components, to share weights, thus improving generalization.
Given an event the quick brown fox, the 4-gram feature for the prediction of the target fox would be broken down into the following elementary meta-features:
• feature identity, e.g. [the quick brown]
• feature type, e.g. 3-gram
• feature count Cf∗ • target identity, e.g. fox
• feature-target count Cfw
Elementary meta-features of different types are then joined with others to form more complex meta-features, as described best by the pseudo-code in Appendix C; note that the seemingly absent feature-target identity is represented by the conjunction of the feature identity and the target identity.
As count meta-features of the same order of magnitude carry similar information, we group them so they can share weights. We do this by bucketing the count meta-features according to their (floored) log2 value. Since this effectively puts the lowest count values, of which there are many, into a different bucket, we optionally introduce a second (ceiled) bucket to assure smoother transitions. Both buckets are then weighted according to the log2 fraction lost by the corresponding rounding operation.
To control memory usage, we employ a feature hashing technique [6],[13] where we store the meta-feature weights in a flat hash table of predefined size; strings are fingerprinted, counts are hashed and the resulting integer mapped to an index k in θ by taking its value modulo the pre-defined size(θ). We do not prevent collisions, which has the potentially undesirable effect of tying together the weights of different meta-features. However, when this happens the most frequent meta-feature will dominate the final value after training, which essentially boils down to a form of pruning. Because of this the model performance does not strongly depend on the size of the hash table.
In this chapter we identify the strict saddle property and show stochastic gradient descent converges to a local minimum under this assumption. This leads to new online algorithm for orthogonal tensor decomposition. We hope this is a first step towards understanding stochastic gradient for more classes of non-convex functions. We believe strict saddle property can be extended to handle more functions, especially those functions that have similar symmetry properties.
42
Chapter 3
Motivated by the practical problem of wireless sensor fusion, the
author introduced the concept of “graded set”. Using graded set of features, it is shown that an interesting set called “granular set” naturally arises in classification problem associated with an information system. Using graded set of target sets, it is reasoned that the lower and upper approximations ( of the target set ) constitute a graded set of rough sets. Applications of “graded” as well as granular sets are actively being explored.
Relation mentions consist of sentences marked with two entity mentions of interest. In this paper, we examine two different representations for the sentences in RE: (i) the standard representation, called SEQ that takes all the words in the sentences into account and (ii) the dependency representation, called DEP that only considers the words along the dependency paths between the two entity mention heads of the sentences. In the following, unless indicated specifically, all the statements about the sentences hold for both representations SEQ and DEP.
Throughout this paper, for convenience, we assume that the input sentences of the relation mentions have the same fixed length n. This can be
achieved by setting n to the length of the longest input sentences and padding the shorter sentences with a special token. Let W = w1w2 . . . wn be the input sentence of some relation mention, where wi is the i-th word in the sentence. Also, let wi1 and wi2 be the two heads of the two entity mentions of interest. In order to prepare the relation mention for neural networks, we first transform each word wi into a real-valued vector xi using the concatenation of the following seven vectors, motivated by the previous research on neural networks and feature analysis for RE (Zhou et al., 2005; Sun et al., 2011; Gormley et al., 2015).
- The real-valued word embedding vector ei of wi, obtained by looking up the word embedding table E.
- The real-valued distance embedding vectors di1 , di2 to encode the relative distances i − i1 and i − i2 of wi to the two entity heads of interest wi1 and wi2 : di1 = D[i − i1], di2 = D[i − i2] where D is the distance embedding table (initialized randomly). The objective is to inform the networks the positions of the two entity mentions for relation prediction.
- The real-valued embedding vectors for entity types ti and chunks qi to embed the entity type and chunking information for wi. These vectors are generated by looking up the entity type and chunk embedding tables (also initialized randomly) (i.e, T and Q respectively) for the entity type enti and chunking label chunki of wi: ti = T [enti], qi = Q[chunki].
- The binary vector pi with one dimension to indicate whether the word wi is on the dependency path between wi1 and wi2 or not.
- The binary vector gi whose dimensions correspond to the possible relations between words in the dependency trees. The value at a dimension of gi is only set to 1 if there exists one edge of the corresponding relation connected to wi in the dependency tree.
The transformation from the word wi to the vector xi = [ei, di1 , di2 , ti, qi, pi, gi] essentially converts the relation mention with the input sentence W into a real-valued matrix X = [x1, x2, . . . , xn], to be used by the neural networks presented below.
Gradually (in a process that spanned the 1980s and 1990s), some researchers on algorithmic composition with rule-based systems adopted formal techniques based on logic programming. For example, Boenn et al. (2008) used answer set programming to encode rules for melodic composition and harmonization. However, most of the work on logic programming has been under a different paradigm: the formulation of algorithmic composition tasks as constraint satisfaction problems (CSPs). Previously referenced work, as Steels’s (1979), Levitt’s (1981), Schottstaedt’s (1989) and Löthe’s (1999) can be seen as part of a gradual trend towards the formulation of musical problems as CSPs17, although constraint logic programming (CLP) came to be the tool of choice to solve CSPs. Good surveys on CLP for algorithmic composition have been written by Pachet and Roy (2001) and Anders and Miranda (2011).
Ebcioğlu worked for many years in this area, achieving notable results. In a first work implemented in Lisp (Ebcioğlu, 1980), he translated rules of fifth-species strict counterpoint to composable Boolean functions (he had to add rules of his own to bring the system into producing acceptable results, though), and used an algorithm that produced an exhaustive enumeration of the compositions satisfying a previously arranged set of rules: basically, he implemented a custom engine for logic programming in Lisp. Over the next decade, he the tackled the problem of writing four-part chorales in the style of J. S. Bach. Finally, he produced CHORAL, a monumental expert system (Ebcioğlu, 1988), distilling into it 350 rules to guide the harmonization process and the melody generation. To keep the problem
17. While Gill’s (1963) implementation was formulated as a CSP, it was somewhat primitive by later standards.
tractable, he designed a custom logic language (BSL) with optimizations over standard logic languages, as backjumping. His system received substantial publicity, and was supposed to reach the level of a talented music student, in his own words.
Following Ebcioğlu’s work, many constraint systems have been implemented for harmonization or counterpoint. Tsang and Aitken (1991) implemented a CLP system using Prolog to harmonize four-part chorales. However, their system was grossly inefficient.18 Ovans and Davison (1992) described an interactive CSP system for first-species counterpoint, where a human user drove the search process, and the system constrained the possible outputs (according to counterpoint rules) as the search progressed. They took care of efficiency by using arc-consistency in the resolution of the constraints. Ramírez and Peralta (1998) solved a different problem: given a monophonic melody, their CLP system generated a chord sequence to harmonize it. Phon-Amnuaisuk (2002) implemented a constraint system for harmonizing chorales in the style of J. S. Bach, but with an innovation over previous systems: to add knowledge to the system about how to apply the rules and control the harmonization process explicitly, thus modulating the search process in an explicit and flexible way. Anders and Miranda (2009) analyzed a Schoenberg’s textbook on the theory of harmony, programming a system in Strasheela (see below) to produce self-contained harmonic progressions, instead of harmonizing pre-existing melodies, as most other constraint systems do.
While many CLP systems have been implemented to solve classical problems in harmonization or counterpoint, some researchers have studied the application of CLP techniques to different problems. In a very simple application, Wiggins (1998) used a CLP system to generate short fragments of serial music. Zimmermann (2001) described a two-stage method, where both stages used CLP: the first stage (AARON) took as input a “storyboard” to specify the mood of a composition as a function of time, and generated a harmonic progression and a sequence of directives. The second (COMPOzE) generated a four-part harmonization according to the previously arranged progression and directives; the result was intended as background music. Laurson and Kuuskankare (2000) studied constraints for the instrumentation19 of guitars and trumpets (i.e., constraints for composing music easily playable in these instruments). Chemillier and Truchet (2001) analyzed two CSPs: a style of Central African harp music, and Ligeti textures. They used heuristic search in their analyzes instead of backtracking, heralding OMClouds’ approach to constraint programming (see below). Sandred (2004) proposed the application of constraint programming to rhythm.
Several general-purpose constraint programming systems for algorithmic composition have been proposed (i.e., languages and environments to program the constraints). One of the earliest examples was Courtot’s (1990) CARLA, a CLP system for generating polyphonies with a visual front-end and a rich, extendable type system designed to represent relationships between different musical concepts. Pachet and Roy (1995) implemented another general-purpose musical CLP (Backtalk) in an object-oriented framework (MusES), designing a generator of four-part harmonizations on top of it. Their key contribution was a hierarchical arrangement of constraints on notes and chords, dramatically decreasing the (both cognitive and computational) complexity of the resulting constraint system.
18. In spite of using just 20 rules, it required up to 70 megabytes of memory to harmonize a phrase of 11 notes. 19. That is to say, take into account the way an instrument is played when composing its part.
Rueda et al. (1998) reviewed two other early general-purpose systems, PWConstraints and Situation. PWConstraints was able to (relatively easily) handle problems in polyphonic composition through a subsystem (score-PMC), while Situation was more flexible and implemented more optimizations in its search procedures. PiCO (Rueda et al., 2001) was an experimental language for music composition that seamlessly integrated constraints, object-oriented programming and a calculus for concurrent processes. The idea was to use constraint programming to specify the voices in a composition, and to use the concurrent calculus to harmonize them. The authors also implemented a visual front-end to PiCO for ease of use, Cordial. In a similar way to PiCO, ntcc was another language for constraint programming that implemented primitives for defining concurrent systems, although it was not specifically designed for algorithmic composition. ntcc has been proposed to generate rhythm patterns and as a more expressive alternative to PiCO (Olarte et al., 2009), and has mainly been used for machine improvisation: Allombert et al. (2006) used it as the improvisation stage of their two-stage system (the first stage used a temporal logic system to compose abstract temporal relationships between musical objects, while the ntcc stage generated concrete music realizations), and Rueda et al. (2006) used ntcc to implement a real-time system that learned a Markovian model (using a Factor Oracle) from musicians and concurrently applied it to generate improvisations. Not related to ntcc, Pachet et al. (2011) has also proposed a framework to combine constraint satisfaction and Markov processes.
OMClouds (Truchet et al., 2003) was another general-purpose (but purely visual) constraint system for composition, but its implementation set it apart from most other formal systems: internally, the constraints are translated to cost functions. Instead of the optimized tree search with backtracking usual in CLP, an adaptive tabu search was performed, seeking to minimize a solution with minimal cost. This avoids some problems inherent to constraint programming, such as overconstraining, but it cannot be guaranteed to completely navigate the search space. Anders (2007) implemented Strasheela, a system that was expressly designed to be highly flexible and programmable, aiming to overcome a perceived limitation of previous general-purpose systems: the difficulty to implement complex with constraints related to multiple aspects of the compositions process. Finally, another purely visual constraint system, PWMC, was proposed by Sandred (2010) to overcome perceived limitations of score-PMC. It was able to handle constraints concerning not only pitch structure as score-PMC, but also rhythm and metric structure.
It should be stressed that, while CLP has become the tool of choice to solve CSPs, other approaches are also used. Previously cited OMClouds is just one of these. Carpentier and Bresson (2010) implemented a mixed system for orchestration that worked in a curious way: the user fed the system with a target sound and a set of symbolic constraints; a multi-objective evolutionary algorithm found a set of orchestration solutions matching the target sound, and a local search algorithm filtered out the solutions not complying with the constraints. Yilmaz and Telatar (2010) implemented a system for simple constraint harmonization with fuzzy logic, while Aguilera et al. (2010) used probabilistic logic to solve first-species counterpoint. More exotic solutions have been proposed, as the use of Ant Colony Optimization with a multi-objective approach to solve the constraints of Baroque harmonization (Geis & Middendorf, 2008), variable neighborhood with tabu search to solve soft constraints for first-species counterpoint (Herremans & Sorensena, 2012), or simulated
annealing to combine constraints with Markov processes (Davismoon & Eccles, 2010). Finally, Martin et al. (2012) presented an even more exotic approach: a real-time music performer that reacted to its environment. While some of the aspects of the music where controlled by Markov chains, others where expressed as a CSP. To solve this CSP in real time, a solution was calculated at random (but quickly) using binary decision diagrams.
Combining the modules. To summarize, we propose the following approach, given a performance attribution or debugging question Q:
1. Infer a GCM M , based on Section 4.1. Let W denote its root nodes. 2. Translate Q into a counterfactual probability query p(Ydo X=x′ = y
′|x, y, f), as described in Section 5.1. 3. Calculate the approximate answer pW (YdoX=x′ = y′|x, y, f) from the GCMM , based on Section 5.2, if H(E|W ) is small.
Advantages. Previous approaches [Ostrowski et al., 2011, Snee et al., 2015] which do not use a rigorous and explicit modeling and reasoning framework such as causal models may be more prone to making errors, such as not distinguishing between correlation and causation, in certain cases. Furthermore, formalization can be an important step towards more automated treatment of attribution and debugging. Note that generally, if there is a known, invariant distribution over the root nodes of a causal model, then one can optimize the performance on the population level, similarly as suggested in Section 4. However, if one is uncertain about the distribution of root nodes, or this distribution are time-varying in an unpredictable way, but one still believes in the invariance of the other mechanisms, then the proposed observation-level attribution and debugging can be seen as a suitable tool, since the distributions on the root nodes are either updated or completely determined, by the observation.
This section describes work that is currently underway to implement the Writers Workshop model, not only within one system but as a new paradigm for collaboration among disparate projects. In order to bring in other participants, we need a neutral environment that is not hard to develop for: the FloWr system mentioned in Section 2.1 offers one
7For a comparison case in computer Go, see http://cgos.computergo.org/.
such possibility. The basic primary objects in the FloWr system are flowcharts, which are comprised of interconnected process nodes (Charnley, Colton, & Llano, 2014; Colton & Charnley, 2014). Process nodes specify input and output types, and internal processing can be implemented in Java, or other languages that interoperate with the JVM, or by invoking external web services. One of the common applications to date is to generate computer poetry, and we will focus on that domain here.
A basic set of questions, relative to this system’s components, are as follow:
1. Population of nodes: What can they do? What do we learn when a new node is added?
2. Population of flowcharts: Pease et al. (2013) have described the potentially-serendipitous repair of “broken” flowcharts when new nodes become available; this suggests the need for test-driven development framework.
3. Population of output texts: How to assess and comment on a generated poetic artefact?
In a further evolution of the system, the sequence of steps in a Writers Workshop could itself be spelled out as a flowchart. The process of reading a poem could be conceptualised as generating a semantic graph (Harrington & Clark, 2007; Francisco & Gervás, 2006). Feedback could be modelled as annotations to a text, including suggested edits. These markup directives could themselves be expressed as flowcharts. A standardised set of markup structures may partially obviate the need for strong natural language understanding, at least in interagent communication. For example, we might agree that observations will consist of stand-off annotations that connect individual textual passages to public URIs using a limited comparison vocabulary, and that suggestions will consist of simple stand-off line-edits, which may themselves be marked up with rationale. In fact, we would not need to be so restrictive: it would not be much more complicated to add annotations to the annotations, so as to be able to form composite statements that express the relationships between different pieces of the annotated texts. Whatever restrictions we ultimately impose on annotation formats, as well as similar restrictions around constrained turn-taking in the workshop, could be progressively widened in future versions of the system. The way the poems that are generated, the models of poems that are created, and the way the feedback is generated, all depend on the contributing system’s body of code and
prior experience, which may vary widely between participating systems. In the list I.–IV. of functional steps below, all of the functions could have a subscripted “E”, which is omitted throughout. Exchanging path dependent points of view will tend to produce results that are different from what the individual participating systems would have come up with on their own.
I. Both the author and critic should be able to work with a model of the text. Some of the text’s features may be explicitly tagged as “interesting.” Outstanding questions may possibly be brought to the attention of critical listeners, e.g. with the request to compare two different versions of the poem (presentation, listening).
1. A model of the text. m : T → M . 2. Tagging elements of interest. µ : M → I .
II. Drawing on its experience, the critic will use its model of the poem to formulate feedback (feedback).
1. Generating feedback. f : (T,M, I) → F .
III. Given the constrained framework for feedback, statements about the text will be straightforward to understand, but rationale for making these statements may be more involved (questions, replies).
1. Asking for more information. q : (M,F, I) → Q.
2. Generating rationale. a : (M,F,Q) → ∆F .
IV. Finally, feedback may affect the author’s model of the world, and the way future poems are generated (reflection).
1. Updating point of view. ρ : (M,F ) → ∆E .
The final step is perhaps the most interesting one, since invites us to consider how individual elements of feedback can “snowball” and go beyond lineedits to a specific poem to much more fundamental changes in the way the presenting agent writes poetry. Here methods for pattern mining, discussed in Section 3.2, are particularly relevant. If systems can share code (as in our sample dialogue in Section 2.1) this will help with the rationale-generating step, and may also facilitate direct updates to the codebase. However, shared code may be more
suitably placed into the common pool of resources available to FloWr than copied over as new “intrinsic” features of an agent.
Although different systems with different approaches and histories are important for producing unexpected effects, “offline” programmatic access to a shared pool of nodes and existing flowcharts may be useful. Outside of the workshop itself, agents may work to recombine nodes based on their input and output properties to assemble new flowcharts. This can potentially help evaluate and evolve the population of nodes programmatically, if we can use this sort of feedback to define fitness functions. The role of temporality is interesting: if the workshop takes place in real time, this will require different approaches to composition that takes place offline (Perez, Samothrakis, Lucas, & Rohlfshagen, 2013). Complementing these “macro-level” considerations, it is also worth commenting on the potential role of “micro-level” feedback within flowcharts. Local evaluation of output from a predecessor node could feed backwards through the flowchart, similar to backpropagation in neural networks. This would rely on a reduced version of the functional schema described above.
adaptation
In comparative cognition, interfaces are usually associated with physical things: a cage with a small door, a peanut as reward, a set of cups, a touch screen, a light bulb, a set of ropes, etc. Thinking of a test that is able to adapt to all these possible physical configurations (and do this automatically) is far beyond reach. Instead of this, in this section, we will consider variations of the same physical (or virtual) ‘milieu’. For instance, given a screen we can think about many possible resolutions and colours, given an audio signal in a range of frequencies we can consider all the possible variations there. In fact, many human tests are still administered with a sheet of paper, and many different interfaces are still possible with this ‘rudimentary’ milieu.
If we focus on cognitive abilities as information-processing tasks we can fix the milieu and examine the possible variations around it. With this restriction, any interface is in the end a pair of input and output communication channels (in terms of information theory) with a given bandwidth. If we consider discrete interfaces, we can describe this in terms of the input/output resolution and a refresh rate6. For instance, considering time, if the task is the addition of two natural numbers lower than 10 represented in a unary system, and we agree on the representation of the numbers in the output channel, there is still the question of how much time the numbers are going to be displayed and how much time the agent is going to be allowed to give an answer. If we fix these values we make the evaluation possible for some agents but this also exclude others. For instance, plants are now claimed to do some kind of cognition [12], but their time-scale is much slower than those of animals.
The anytime test introduced in [41] arguably addresses part of the issue of time adaptively by starting with a very fast interaction rate and slowing it down as the results from the agent are not good. The direction of the time change (from very fast interaction to slower interaction) is reasonable as many agents would also react appropriately if the interaction is neither too fast nor too slow, and starting at fast interactions makes the adaptation feasible in finite time. While this is a first approach for making a universal test adaptive on time, there are more issues in terms of time-scale than those reflected by [41]. Also,
6Although the terms are exactly the same for screens, we consider any possible milieu here, either auditive, visual or other.
other kinds of resolutions are not considered. If we take a closer look at time rate, we see at least two time frames that could be (adaptively) increased/decreased:
• Working time, which can be the time between questions and answers in a questionnaire-like test or it can be the time other agents (e.g., predators) take to make actions or the time the environment makes rewards available. This is in fact what [41] adapts. This time typically includes the time the agent needs to act (or write an answer).
• Exposition time: the amount of time the agent gets to be given the information before it is removed7. This time frame is usually neglected unless this is the goal of a study on short-memory (or other kinds of memory abilities, such as photographic memory). However, if the exposition time frame is not appropriate, the agent may completely overlook some important data of a task.
On occasions we want a cognitive ability to consider time, such as measuring ‘reaction time’ [70]. But if time is not part of the ability (e.g., we may want to know the ability to sort a series of numbers, without considering speed) then we (or the test) need to find the optimal time windows for working and exposition time in order to make the test feasible. Let us use the term time configuration for the set of parameters for a given working and exposition time.
A related, but different thing, is resolution. Although we typically think in terms of spatial resolution, it is very useful for the discussion that follows to think of a case where we consider an audio signal, where resolutions appear on the same signal, using, e.g., frequency and amplitude. For instance, figure 4 [103, 65] shows how a sound signal can carry many different types of information at several resolutions.
Not only the resolution may be too coarse or too detailed for the agent to see any relevant pattern, but it can take infinitely many representations. Note that the detection of the appropriate resolution is different from any pattern that the signal may carry. This distinction is important, even though both things (resolution and pattern) are usually closely intertwined8. Nonetheless, while animals (including humans) usually have innate preprocessing systems that may be used to capture some resolutions (and ignore others) and see patterns in them, it is possible to disentangle one thing from the other. For instance, many
7As an example of humans requiring more time than some animals, Ayumu the chimpanzee [52] is able to note the locations of the numbers 1 to 9 (of which there are 9! = 362880 possibilities) after only 60 milliseconds observation time. A large proportion of humans struggle just to see (and recall) the location of even one of these numbers in this time-frame. This also happens with other milieux, e.g., auditive signals, as the case of a non-native speaker requiring slower speech or repetitions.
8The recognition of patterns depends on a correct resolution configuration. This is crucial to the notion of emergence, and can be translated to communication and interaction as well, since structures emerge from some low-level constituents that have no meaning by themselves. This recognition of patterns at a previously unknown resolution and time, and its relation with emergence, has also been explored (see, e.g., [29]), and can also be referred to the cognitive structures and constructs the agent is creating all along its life [36].
artificial pattern (image, speech, etc.) recognition systems have preprocessing devices that render the information ready (e.g., a bitmap or spectrogram) for the analysis of patterns. Working with several resolutions and representations in order to find the most appropriate one may take important time overloads to process. This of course makes resolution and time also closely intertwined in real systems9.
The complexity of resolution and the appropriateness of representation is one of the major issues in artificial intelligence, pattern recognition and machine learning. This issue, however, has been neglected by most machine intelligence tests that we reviewed in section 2. Being conscious of these limitations, let us move forward by considering a communication channel for which we can define any possible resolution, each of them denoted by the term resolution configuration.
From here, we just define a configuration θ as a pair of time configuration and resolution configuration. We want to evaluate a cognitive ability defined as a distribution of tasks M , i.e., a task class. Consider that we have a set (or distribution) of configurations Θ. Then we can define:
U(π,M,Θ) = max θ∈Θ lim τ→∞ Υ(π,M, θ, τ) (1)
where Υ(π,M, θ, τ) is any test on a family of tasks that is applicable to an agent π during time τ (for instance, time-bounded adaptations of [61] or some of the non-adaptive versions introduced in [41] or implemented in [54]). Eq. 1 above defines (or generalises) a universal test from a non-universal test. The expression
9It is relevant to mention here that there are some recognition tasks (recognising distorted letters, as in CAPTCHAs) that do not correlate with some other (higher-level) abilities using those letters. In fact, the ability of recognising distorted letters is used as a CAPTCHA because machines are not able to do this well with current technology, not because distorted character recognition is a sign of intelligence.
Υ(π,M, θ, τ) is an aggregate over the set of tasks M . This must be based on the result on single tasks, Υ(π, µ, θ, τ) and can be done in many different ways. One possibility is to weight by task probability: Υ(π,M, θ, τ) = Υ(π, µ, θ, τ)p(µ) (as in [61]), while another possibility is to define task difficulty and get the result in terms of this difficulty, as in [49, 34, 46, 45].
This maximisation can be translated into the goal of finding the configuration such that the test result is optimal for the subject. This leads to adaptive tests, which search for the appropriate configuration, or at least one that gives an approximation of Eq. 1 above. In the case we want the test to adapt, there is a need to have some interactive feedback from subject to testers, in terms of the score the subject is achieving. With this we are ready to introduce a first general procedure for an adaptive universal test. Figure 5 shows a general procedure for evaluating subject π in an adaptive way. The relevant part of the previous procedure is how we select tasks and configurations, especially after the first iteration, using the history of results and the tasks and configurations previously used. This can be seen as an extension/generalisation of [41] (and ultimately of [61] as well, with the appropriate modifications). For instance, if the agent’s score has been poor, we can either try to find a simpler task or change the configuration (which may imply a change in the time configuration, the resolution configuration or both). On the contrary, if the agent’s score has been good, we would be tempted to keep the configuration and change to a more difficult (or more informative) task.
An important question is how to obtain the final result of the test. Ideally, if we were able to evaluate all possible configurations and all possible tasks for an infinite amount of time τ each, the result would be calculated by taking the aggregated performance on the task class (using its distribution or the difficulty function) for the configuration that has given the best result. In practice, in finite time, the test should start with configurations not taking too much time (so τ can be small) and try as many resolutions as possible as the time configuration uses larger slots.
So, a universal test for an unknown agent would be a test that makes all the possible efforts to find a configuration with the evaluee at the best resolution and time configurations such that the subject can be evaluated in optimal conditions. Not coincidentally, this is what animal cognition usually does when designing
an experiment, and, in the most difficult cases, finding the correct configuration may take decades. Also, as we will discuss in section 6, we are never sure that the right configuration has even been found.
These difficulties appear even though we have already considered that the (physical) communication milieu is fixed (e.g., a sound channel or a screen) and we have also assumed that we recognise the agent and know its reward system. As mentioned above, we are not going to consider every possible physical milieu. Instead, in what follows we will consider an environment including both the evaluator and the evaluee (e.g., the real world, but most especially, virtual words, such as games, social networks, etc., because of the possible applications and the higher feasibility of a test of this kind, see, e.g., [40]).
Boosting (Freund and Schapire, 1995) is a popular method that leverages simple learning models (e.g., decision stumps) to generate powerful learners. Boosting has been used to great effect and trump other learning algorithms in a variety of applications. In computer vision, boosting was made popular by the seminal ViolaJones Cascade (Viola and Jones, 2001) and is still used
Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS) 2017, Fort Lauderdale, Florida, USA. JMLR: W&CP volume 54. Copyright 2017 by the author(s).
to generate state-of-the-art results in pedestrian detection (Nam et al., 2014; Yang et al., 2015; Zhu and Peng, 2016). Boosting has also found success in domains ranging from document relevance ranking (Chapelle et al., 2011) and transportation (Zhang and Haghani, 2015) to medical inference (Atkinson et al., 2012). Finally, boosting yields an anytime property at test time, which allows it to work with varying computation budgets (Grubb and Bagnell, 2012) for use in real-time applications such as controls and robotics.
The advent of large-scale data-sets has driven the need for adapting boosting from the traditional batch setting, where the optimization is done over the whole dataset, to the online setting where the weak learners (models) can be updated with streaming data. In fact, online boosting has received tremendous attention so far. For classification, (Chen et al., 2012; Oza and Russell, 2001; Beygelzimer et al., 2015b) proposed online boosting algorithms along with theoretical justifications. Recent work by Beygelzimer et al. (2015a), addressed the regression task through the introduction of Online Gradient Boosting (OGB). We build upon on the developments in (Beygelzimer et al., 2015a) to devise a new set of algorithms presented below.
In this work, we develop streaming boosting algorithms for regression with strong theoretical guarantees under stochastic setting, where at each round the data are i.i.d sampled from some unknown fixed distribution. In particular, our algorithms are streaming extension to the classic gradient boosting (Friedman, 2001), where weak predictors are trained in a stage-wise fashion to approximate the functional gradient of the loss with respect to the previous ensemble prediction, a procedure that is shown by Mason et al. (2000) to be functional gradient descent of the loss in the space of predictors. Since the weak learners cannot match the gradients of the loss exactly, we measure the error of approximation by redefining of edge of online weak learners (Beygelzimer et al., 2015b) for online regression setting.
Assuming a non-trivial edge can be achieved by each deployed weak online learner, we develop algorithms to handle smooth or non-smooth loss functions, and theo-
ar X
iv :1
70 3.
00 37
7v 1
[ cs
.L G
] 1
M ar
2 01
7
retically analyze the convergence rates of our streaming boosting algorithms. Our first algorithm targets strongly convex and smooth loss functions and achieves exponential decay on the average regret with respect to the number of weak learners. We show the ratio of the decay depends on the edge and also the condition number of the loss function. The second algorithm, designed for strongly convex but non-smooth loss functions, extends from the batch residual gradient boosting algorithm from (Grubb and Bagnell, 2011). We show that the algorithm achieves O(lnN/N) convergence rate with respect to the number of weak learners N , which matches the online gradient descent (OGD)’s no-regret rate for strongly convex loss (Hazan et al., 2007). Both of our algorithms promise that as T (the number of samples) and N go to infinity, the average regret converges to zero. Our analysis leverages Online-to-Batch reduction (Cesa-Bianchi et al., 2004; Hazan and Kale, 2014), hence our results naturally extends to adversarial online learning setting as long as the weak online learning edge holds in adversarial setting, a harsher setting than stochastic setting. We conclude with some proof-of-concept experiments to support our analysis. We demonstrate that our algorithm significantly boosts the performance of weak learners and converges to the performance of classic gradient boosting with less computation.
Rule: visarga followed by hard consonant –> visarga replaced with s.
General operator: ⊕2,15 (x, y) = z = z2y when x = 49,
34 <= y <= 46, w {44, 45, 46} ⊕2,15,1 (x, y) : z2 = 46
ścutva sandhi
Local landmark selection is a very effective heuristic for finding representative columns. Zhang and Kwok [36] proposed to set k = s and run k-means or k-centroids clustering algorithm to cluster the columns of A to s class, and use the s centroids as the sketch of A. This heuristic works very well in practice, though it has little theoretical guarantee.
There are several tricks to make the local landmark selection more efficient.
• One can simply solve k-centroids clustering approximately rather than accurately. For example, it is unnecessary to wait for k-centroids clustering to converge; running kcentroids for a few iterations suffices.
• When n is large, one can uniformly sample a subset of the data, e.g. max{0.2n, 20s} data points, and perform local landmark selection on this smaller dataset.
16
• In supervised learning problems, each datum ai is associated with a label yi. We can partition the data to g groups according to the labels and run k-centroids clustering independently on the data in each group. In this way, s = gk data points are selected as a sketch of A.
17
18
Chapter 4
Regression
Let A be an n× d (n ≥ d) matrix whose rows correspond to data and columns correspond to features, and let b ∈ Rn contain the response/label of each datum. The least squares regression (LSR)
min x ‖Ax− b‖22 (4.1)
is a ubiquitous problem in statistics, computer science, economics, etc. When n d, LSR can be efficiently solved using randomized algorithms.
In this section we give a simulation primer. The classic reference for this material is [1], chapter 26.
Here is an overall description of how we generate a parallel version of a corpus by simulating from an HMM. We leave the actual simulation details in (brief) abeyance. The inputs are the model, a dictionary, the transcript, and the real utterance. We use the model, the transcript, the dictionary, and forced alignment to pick the pronunciations (we discard the time information) and inter-word silence (sp or sil). We use an alignment to guarantee that the real and pseudo utterance share the underlying triphone sequence, but a perfectly reasonable alternative is to make random selections among the pronunciations and silence types.6 This results in a triphone sequence for the utterance, e.g., sil a+b a-b+c b-c sil. Next we generate the underlying state sequence for the utterance. For each triphone in the list we generate a state sequence by simulating from the transition models. This produces a list of state id’s : 1 1 2 2 3 4 . . . , and determines the number of frames. Finally we generate the actual frames: we walk down the list of state id’s, simulating one frame from the corresponding output distribution.
Now we turn to the details of simulation. How do we simulate data from a continuous distribution? The following result provides the key:
Theorem 2.1. Let F be a continuous, invertible, cumulative distribution function and let the continuous random variable U have uniform distribution on [0, 1]. If we define the continuous random variable X by X = F−1(U), then X has distribution F
To use Theorem 2.1 to generate data having distribution F , first we use a random number generator to choose u ∈ [0, 1], then we find the unique x satisfying F (x) = u. Figure 1 illustrates this procedure on N(0, 1), with u = 0.9 and the resulting value x = 1.25. The next two examples show how
6If we are creating pseudo utterances from scratch, i.e., if we do not have extant utterances, then by necessity we randomly select the pronunciations.
the simulation procedure for N(0, 1) is used to simulate multivariate normal distributions: first with diagonal covariance and second with full covariance.
Example 2.2. Simulating from a d-dimensional normal distribution with diagonal covariance. Let Φ be the cumulative distribution for N(0, 1), and let µ and σ2 be the d-dimensional mean and diagonal variance vectors.7 We use a random number generator d times to create the d-dimensional vector u with each component ui ∈ [0, 1] and use this to create a d-dimensional vector y by the rule yi = Φ
−1(ui) for i ≤ i ≤ d. Then the vectors x = µ + ytσ will have distribution N(µ, σ2).
Example 2.3. Simulating from a d-dimensional normal distribution with full covariance. Let µ d-dimensional mean and Σ be a positive definite d × d covariance matrix. Since Σ is positive definite there exists a non-singular d× d matrix M with Σ = MM t.8 If we follow the procedure in Example 2.2 to create the vectors y, then the vectors x = µ + My will have distribution N(µ,Σ).
How do we simulate data from a discrete distribution? We use a discrete analog of Theorem 2.1, that we describe in the following example:
Example 2.4. Simulating from a discrete probability distribution. Let the discrete probability distribution P have probabilities {pi}ni=1 with n > 1 and that satisfy 0 < pi < 1 for each i and ∑n i=1 pi = 1. Define intervals {Aj}nj=1 by
Aj =  [0, p1) if j = 1 [ ∑j−1 i=1 pi, ∑j i=1 pi) if 1 < j < n
[ ∑n−1
i=1 pi, 1] if j = n.
Then the {Aj}nj=1 form a partition of [0, 1], which means that ∪nj=1Aj = [0, 1] and ∩nj=1Aj = ∅. We use this partition to define a function h : [0, 1] → {1, 2, . . . , n} in the following way: given u ∈ [0, 1] there is a unique j with u ∈ Aj and we set h(u) = j. Finally, if the continuous random variable U has uniform distribution on [0, 1], then we define a discrete random variable X on a set of values {xi}ni=1 by X = h(U). By construction X has distribution P , i.e. P (X = xi) = pi.
Example 2.5. To help clarify Example 2.4, we work through the details in the special case n = 2. We can simplify the notation by setting p = p1 from
7When we write σ2 we mean the vector with components σ2i . 8The matrix M is not unique, but the Cholesky decomposition is particularly useful
for this application.
which it follows that 1 − p = p2. The two intervals A1 and A2 are given by A1 = [0, p) and A2 = [p, 1]. The function h is given in terms of u ∈ [0, 1] by
h(u) = { 1 if u ∈ [0, p) 2 if u ∈ [p, 1]
Since U has uniform distribution over [0, 1], it follows that the probability distribution of the random variable X = h(U) is given by
P (X = 1) = P (u ∈ [0, p)) = p
and P (X = 2) = P (u ∈ [p, 1]) = 1− p.
To simulate the random variable X we first use a random number generator to choose u ∈ [0, 1]. We next set the value of X to be 1 if 1 ≤ u < p and 2 otherwise. Figure 2 displays this simulation procedure in a manner that is analogous to Figure 1 when p = 0.4 and u = 0.9. Finally, we note that in this case, n = 2, we can think of X as a Bernoulli random variable that takes value 1 with success probability p and 2 with failure probability 1− p.
Example 2.6. Simulating from a geometric distribution. The geometric distribution with parameter p, 0 < p < 1, is a discrete probability distribution with probabilities {pi}∞i=1 given by9
pi = (1− p)i−1p.
Since the geometric distribution is discrete – albeit infinite – it is straightforward to directly adapt the machinery described in Example 2.4 to simulate data from it. However, we will describe an alternate procedure that is based on the intuition that a geometric random variable is the waiting time for a success in a series of independent Bernoulli trials. To do this, we first introduce a Bernoulli random variable, X, that takes value 1 with success probability p and value 0 with failure probability 1− p. We then conduct a series of independent Bernoulli trials, X1, X2, . . ., until we obtain a success. If we let Y be the random variable that gives the number of trials needed for the first success, then Y has a geometric distribution with parameter p, since
P (Y = n) = P ((X1, X2, . . . , Xn) = (0, 0, . . . , 1)) = (1− p)n−1p = pn. 9It is a standard exercise to show that ∑∞ i=1 pi = 1.
Thus to simulate one example from a geometric distribution, it suffices to simulate a series of Bernoulli trials until we obtain a success. Example 2.5 shows that this is equivalent to repeatedly running a random number generator, which takes values in [0, 1], to create a sequence u1, u2, . . . The first n satisfying un ∈ [0, p) gives the required value of Y .
Figure 3 displays the linear transition structure that we typically use in HMMs for speech recognition. However, the transition structure in the HMM displayed in Figure 3 also forces the model to spend at least one frame in each state, i.e., no state skipping is allowed. To simulate a state sequence from the model displayed in Figure 3 we apply the method described in Example 2.6 to each of the three states. For example, for the first state we need to simulate the waiting time for the states sequence to move on to the second state. In this case the Bernoulli random variable, X, describes if we stay in state 1 with failure probability a(1, 1) or we move on to state 2 with success probability a(1, 2).
We end this primer with a brief remark. The frames that we use in these experiments use a feature set that contains the first and second differences of the cepstral features. When we create a pseudo utterance by simulating from the HMM, the resulting frame sequence is somewhat peculiar in the sense that the difference features are not consistent with the underlying sequence of static cepstral features. This is because the HMM knows nothing about this structure, in fact, this structure violates the model assumptions. In section 5.2 we examine this more carefully.
A model checker can produce a trace of counterexample, for a erroneous program, which is often long and difficult to understand. In general, the part about the loops is the largest among the instructions in this trace. This makes the location of errors in loops critical, to analyze errors in the overall program. In this paper, we explore the scalability capabilities of LocFaults, our error localization approach exploiting paths of CFG(Control Flow Graph) from a counterexample to calculate the MCDs (Minimal Correction Deviations), and MCSs (Minimal Correction Subsets) from each MCD found. We present the times of our approach on programs with While-loops unfolded b times, and a number of diverted conditions ranging from 0 to n. Our preliminary results show that the times of our approach, constraintbased and flow-driven, are better compared to BugAssist which is based on SAT and transforms the entire
Les erreurs dans un programme sont inévitables, elles peuvent nuire à son bon fonctionnement et avoir des conséquences financières extrêmement graves et présenter une menace pour le bien-être humain [8]. Le lien suivant [7] cite des histoires récentes de bugs logiciels. Conséquemment, le processus de débogage (la détection, la localisation et la correction d’erreurs) est essentiel. La localisation d’erreurs est l’étape qui coûte le plus. Elle consiste à identifier l’emplacement exact des instructions suspectes [6] afin d’aider l’utilisateur à comprendre pourquoi le programme a échoué, ce qui lui facilite la tâche de la correction des erreurs. En effet, quand un programme P est non conforme vis-à-vis de sa spécification (P contient des erreurs), un vérificateur de modèle peut produire une trace d’un contre-exemple, qui est souvent longue et difficile à comprendre même pour les programmeurs expérimentés. Pour résoudre ce problème, nous avons proposé une approche [4] (nommée LocFaults) à base de contraintes qui explore les chemins du CFG(Control Flow Graph) du programme à partir du contre-exemple, pour calculer les sous-ensembles minimaux permettant de restaurer la conformité du programme vis-à-vis de sa postcondition. Assurer que notre méthode soit hautement scalable pour faire face à l’énorme complexité des systèmes logiciels est un critère important pour sa qualité [1].
Dans ce papier, nous explorons le passage à l’échelle de LocFaults sur des programmes avec boucles While dépliées b fois, et un nombre de conditions déviées allant de 0 à 3.
L’idée de notre approche est de réduire le problème
de la localisation d’erreurs vers celui qui consiste à calculer un ensemble minimal qui explique pourquoi un CSP (Constraint Satisfaction Problem) est infaisable. Le CSP représente l’union des contraintes du contre-exemple, du programme et de l’assertion violée. L’ensemble calculé peut être un MCS (Minimal Correction Subset) ou MUS (Minimal Unsatisfiable Subset). En général, tester la faisabilité d’un CSP sur un domaine fini est un problème NP-Complet (intraitable) 1, la classe des problèmes les plus difficiles de la classe NP. Cela veut dire, expliquer l’infaisabilité dans un CSP est aussi dur, voire plus (on peut classer le problème comme NP-Difficile). BugAssist [9] [10] est une méthode de localisation d’erreurs qui utilise un solveur Max-SAT pour calculer la fusion des MCSs de la formule Booléenne du programme en entier avec le contre-exemple. Elle devient inefficace pour les programmes de grande taille. LocFaults travaille aussi à partir d’un contre-exemple pour calculer les MCSs. La contribution de notre approche par rapport à BugAssist peut se résumer dans les points suivants : * Nous ne transformons pas la totalité du programme en un système de contraintes, mais nous utilisons le CFG du programme pour collecter les contraintes du chemin du contre-exemple et des chemins dérivés de ce dernier, en supposant qu’au plus k instructions conditionnelles sont susceptibles de contenir les erreurs. Nous calculons les MCSs uniquement sur le chemin du contre-exemple et les chemins qui corrigent le programme ;
* Nous ne traduisons pas les instructions du programme en une formule SAT, mais plutôt en contraintes numériques qui vont être manipulées par des solveurs de contraintes ; * Nous n’utilisons pas des solveurs MaxSAT comme bôıtes noires, mais plutôt un algorithme générique pour calculer les MCSs par l’usage d’un solveur de contraintes ; * Nous bornons la taille des MCSs générés et le nombre de conditions déviées ; * Nous pouvons faire collaborer plusieurs solveurs durant le processus de localisation et prendre celui le plus performant selon la catégorie du CSP construit. Exemple, si le CSP du chemin détecté est du type linéaire sur les entiers, nous faisons appel à un solveur MIP (Mixed Integer Programming) ; s’il est non linéaire, nous utilisons un solveur CP (Constraint Programming) ou aussi MINLP (Mixed Integer Nonlinear Programming).
Notre expérience pratique a montré que toutes ces restrictions et distinctions ont permis à LocFaults
1. Si ce problème pouvait être résolu en temps polynomial, alors tous les problèmes NP-Complet le seraient aussi.
d’être plus rapide et plus expressif.
Le papier est organisé comme suit. La section 2 introduit la définition d’un MUS et MCS. Dans la section 3, nous définirons le problème ≤ k-DCM. Nous expliquons une contribution du papier pour le traitement des boucles erronées, notamment le bug Off-by-one, dans la section 4. Une brève description de notre algorithme LocFaults est fournie dans la section 5. L’évaluation expérimentale est présentée dans la section 6. La section 7 parle de la conclusion et de nos travaux futurs.
2 Définitions
Dans cette section, nous introduirons la définition d’un IIS/MUS et MCS.
CSP Un CSP (Constraint Satisfaction Problem) P est un triplet < X,D,C > tel que : * X un ensemble de n variables x1, x2, ..., xn. * D le n-uplet < Dx1 , Dx2 , ..., Dxn >. L’ensemble Dxi contient les valeurs de la variable xi.
* C={c1, c2, ..., cn} est l’ensemble des contraintes. Une solution pour P est une instanciation des variables I ∈ D qui satisfait toutes les contraintes dans C. P est infaisable s’il ne dispose pas de solutions. Un sous-ensemble de contraintes C′ dans C est dit aussi infaisable pour la même raison sauf qu’ici on se limite à l’ensemble des contraintes dans C′. On note par : – Sol(< X,C′, D >) = ∅, pour spécifier que C′ n’a pas de solutions, et donc il est infaisable.
– Sol(< X,C′, D >) 6= ∅, pour spécifier que C′ dispose d’au moins une solution, et donc il est faisable.
On dit que P est en forme linéaire et on note LP(Linear Program) ssi toutes les contraintes dans C sont des équations/inégalités linéaires, il est continu si le domaine de toutes les variables est celui des réels. Si au moins une des variables dans X est du type entier ou binaire (cas spécial d’un entier), et les contraintes sont linéaires, P est dit un programme linéaire mixte MIP(Mixed-integer linear program). Si les contraintes sont non-linéaires, on dit que P est un programme non linéaire NLP(NonLinear Program).
Soit P =< X,D,C > un CSP infaisable, on définit pour P :
IS Un IS(Inconsistent Set) est un sous-ensemble de contraintes infaisable dans l’ensemble de contraintes infaisable C. C′ est un IS ssi : * C′ ⊆ C.
* Sol(< X,C′, D >) = ∅.
IIS ou MUS Un IIS(Irreducible Inconsistent Set) ou MUS (Minimal Unsatisfiable Subset) est un sousensemble de contraintes infaisable de C, et tous ses sous-ensembles stricts sont faisables. C′ est un IIS ssi : * C′ est un IS. * ∀ C′′ ⊂ C′.Sol(< X,C′′, D >) 6= ∅, (chacune de ses parties contribue à l’infaisabilité), C′ est dit irréductible.
MCS C′ est un MCS(Minimal Correction Set) ssi : * C′ ⊆ C. * Sol(< X,C\C′, D >) 6= ∅. * ∄ C′′ ⊂ C′ tel que Sol(< X,C\C′′, D >) 6= ∅.
3 Le problème ≤ k-DCM
Étant donné un programme erroné modélisé en un CFG 2 G = (C,A,E) : C est l’ensemble des nœuds conditionnels ; A est l’ensemble des blocs d’affectation ; E est l’ensemble des arcs, et un contre-exemple. Une DCM (Déviation de Correction Minimale) est un ensemble D ⊆ C telle que la propagation du contreexemple sur l’ensemble des instructions de G à partir de la racine, tout en ayant nié chaque condition 3 dans D, permet en sortie de satisfaire la postcondition. Elle est dite minimale (ou irréductible) dans le sens où aucun élément ne peut être retiré de D sans que celle-ci ne perde cette propriété. En d’autres termes,D est une correction minimale du programme dans l’ensemble des conditions. La taille d’une déviation minimale est son cardinal. Le problème ≤ k-DCM consiste à trouver toutes les DCMs de taille inférieure ou égale à k.
Exemple, le CFG du programme AbsMinus (voir fig. 2) possède une déviation minimale de taille 1 pour le contre-exemple {i = 0, j = 1}. Certes, la déviation {i0 ≤ j0,k1 = 1 ∧ i0 6= j0} permet de corriger le programme, mais elle n’est pas minimale ; la seule déviation minimale pour ce programme est {k1 = 1 ∧ i0 6= j0}.
Le tableau ci-dessous récapitule le déroulement de LocFaults pour le programme AbsMinus, avec au plus 2 conditions déviées à partir du contre-exemple suivant {i = 0, j = 1}.
2. Nous utilisons la transformation en forme DSA [5] qui assure que chaque variable est affectée une seule fois sur chaque chemin du CFG.
3. On nie la condition afin de prendre la branche opposée à celle où on devait aller.
1 c l a s s AbsMinus { 2 /∗@ ensure s 3 @ (( i<j )==>(\ r e s u l t==j− i ) )&& 4 @ (( i>=j )==>(\ r e s u l t==i −j ) ) ; ∗/ 5 i n t AbsMinus ( in t i , i n t j ){ 6 i n t r e s u l t ; 7 i n t k = 0 ; 8 i f ( i <= j ) { 9 k = k+2;// e r r o r :
should be k=k+1 10 } 11 i f (k == 1 && i != j ) { 12 r e su l t = j−i ; 13 } 14 e l s e { 15 r e su l t = i−j ; 16 } 17 } 18 }
Figure 1 – Le programme AbsMinus
k0 = 0
i0 ≤ j0
k1 = k0 + 2 Error k1 = k0
k1 = 1 ∧ i0! = j0
r1 = j0 − i0 r1 = i0 − j0
POST :{r1 == |i − j|}
If Else
If Else
Figure 2 – Le CFG DSA de AbsMinus
{(i0 == 0) ∧ (j0 == 1)}
k0 = 0
i0 ≤ j0
k1 = k0 + 2 k1 = k0
k1 = 1 ∧ i0! = j0
r1 = j0 − i0 r1 = i0 − j0
{r1 == |i − j|}
If Else
If Else
Figure 3 – Le chemin du contre-exemple
{(i0 == 0) ∧ (j0 == 1)}
k0 = 0
i0 ≤ j0
k1 = k0 + 2 k1 = k0
k1 = 1 ∧ i0! = j0
r1 = j0 − i0 r1 = i0 − j0
{r1 == |i − j|} is UNSAT
If Else
If Else
Figure 4 – Le chemin obtenu en déviant la condition i0 ≤ j0
Conditions déviées DCM MCS Figure
∅ / {r1 = i0 − j0 : 15} fig. 3 {i0 ≤ j0 : 8} Non / fig. 4 {k1 = 1 ∧ i0! = j0 : 11} Oui {k0 = 0 : 7}, fig. 5
{k1 = k0 + 2 : 9}
{i0 ≤ j0 : 8, Non / fig. 6 k1 = 1 ∧ i0! = j0 : 11}
Nous avons affiché les conditions déviées, si elles constituent une déviation minimale ou non, les MCSs calculés à partir du système construit : voir respectivement les colonnes 1, 2 et 3. La colonne 4 indique la figure qui illustre le chemin exploré pour chaque déviation. Sur la première et la troisième colonne, nous avons affiché en plus de l’instruction sa
ligne dans le programme. Exemple, la première ligne dans le tableau montre qu’il y a un seul MCS trouvé ({r1 = i0 − j0 : 15}) sur le chemin du contre-exemple.
4 Traitement des boucles
Dans le cadre du Bounded Model Checking (BMC) pour les programmes, le dépliage peut être appliqué au programme en entier comme il peut être appliqué aux boucles séparément [1]. Notre approche de localisation d’erreurs, LocFaults [3] [4], se place dans la deuxième démarche ; c’est-à-dire, nous utilisons une borne b pour déplier les boucles en les remplaçant par des imbrications de conditionnelles de profondeur b. Considérons le programme Minimum (voir fig. 7) contenant une seule boucle, qui calcule le minimum dans un tableau d’entiers. L’effet sur le graphe de flot de contrôle du programme Minimum avant et après le dépliage est illustré sur les figures respectivement 7 et 8 : la boucle While est dépliée 3 fois, tel que 3 est le nombre d’itérations nécessaires à la boucle pour calculer la valeur minimum dans un tableau de taille 4 dans le pire des cas.
LocFaults prend en entrée le CFG du programme erroné, CE un contre-exemple, bdcm : une borne sur le nombre de conditions déviées, bmcs : une borne sur la taille des MCSs calculés. Il permet d’explorer le CFG en profondeur en déviant au plus bdcm conditions par rapport au comportement du contre-exemple : * Il propage le contre-exemple jusqu’à la postcondition. Ensuite, il calcule les MCSs sur le CSP du chemin généré pour localiser les erreurs sur le chemin du contre-exemple.
* Il cherche à énumérer les ensembles ≤ bdcm-DCM. Pour chaque DCM trouvée, il calcule les MCSs dans le chemin qui arrive à la dernière condition déviée et qui permet de prendre le chemin de la déviation.
Parmi les erreurs les plus courantes associées aux boucles selon [2], le bug Off-by-one, c’est-à-dire, des boucles qui s’itèrent une fois de trop ou de moins. Cela peut être dû à une mauvaise initialisation des variables de contrôle de la boucle, ou à une condition incorrecte de la boucle. Le programme Minimum présente un cas de ce type d’erreur. Il est erroné à cause de sa boucle While, l’instruction falsifiée se situe sur la condition de la boucle (ligne 9) : la condition correcte doit être (i < tab.length) (tab.length est le nombre d’éléments
du tableau tab). À partir du contre-exemple suivant : {tab[0] = 3, tab[1] = 2, tab[2] = 1, tab[3] = 0}, nous avons illustré sur la figure 8 le chemin fautif initial (voir le chemin coloré en rouge), ainsi que la déviation pour laquelle la postcondition est satisfaisable (la dé-
viation ainsi que le chemin au-dessus de la condition déviée sont illustrés en vert).
Nous affichons dans le tableau ci-dessous les chemins erronés générés (la colonne PATH) ainsi que les MCSs calculés (la colonne MCSs) pour au plus 1 condition déviée par rapport au comportement du contreexemple. La première ligne correspond au chemin du contre-exemple ; la deuxième correspond au chemin obtenu en déviant la condition {i2 ≤ tab0.length− 1}.
PATH MCSs
{CE : [tab0[0] = 3 ∧ tab0[1] = 2 ∧ tab0[2] = 1
{min2 = tab0[i1]} ∧tab0[3] == 0], min0 = tab0[0], i0 = 1,
min1 = tab0[i0],i1 = i0 + 1,min2 = tab0[i1], i2 = i1 + 1,min3 = min2, i3 = i2,
POST : [(tab[0] ≥ min3) ∧ (tab[1] ≥ min3) ∧(tab[2] ≥ min3) ∧ (tab[3] ≥ min3)]}
{CE : [tab0[0] = 3 ∧ tab0[1] = 2 ∧ tab0[2] = 1 {i0 = 1},∧tab0[3] == 0], min0 = tab0[0], i0 = 1, {i1 = i0 + 1},min1 = tab0[i0],i1 = i0 + 1,min2 = tab0[i1], {i2 = i1 + 1}i2 = i1 + 1,[¬(i2 ≤ tab0.length − 1)]
LocFaults a permis d’identifier un seul MCS sur le chemin du contre-exemple qui contient la contrainte min2 = tab0[i1], l’instruction de la ligne 11 dans la deuxième itération de la boucle dépliée. Avec une condition déviée, l’algorithme suspecte la troisième condition de la boucle dépliée, i2 < tab0.length − 1 ; en d’autres termes, il faut une nouvelle itération pour satisfaire la postcondition.
Cet exemple montre un cas d’un programme avec une boucle erronée : l’erreur est sur le critère d’arrêt, elle ne permet pas en effet au programme d’itérer jusqu’au dernier élément du tableau en entrée. LocFaults avec son mécanisme de déviation arrive à supporter ce type d’erreur avec précision. Il fournit à l’utilisateur non seulement les instructions suspectes dans la boucle non dépliée du programme original, mais aussi des informations sur les itérations où elles se situent
concrètement en dépliant la boucle. Ces informations pourraient être très utiles pour le programmeur pour mieux comprendre les erreurs dans la boucle.
5 Algorithme amélioré
Notre but consiste à trouver les DCMs de taille inférieure à une borne k ; en d’autres termes, on cherche à donner une solution au problème posé ci-dessus (≤ k-DCM). Pour cela, notre algorithme (nommé LocFaults) parcourt en profondeur le CFG et génère les chemins où au plus k conditions sont déviées par rapport au comportement du contre-exemple.
Pour améliorer l’efficacité, notre solution heuristique procède de façon incrémentale. Elle dévie successivement de 0 à k conditions et elle recherche les MCSs pour les chemins correspondants. Toutefois, si à l’étape k LocFaults a dévié une condition ci et que cela a cor-
rigé le programme, elle n’explorera pas à l’étape k′ avec k′ > k les chemins qui impliquent une déviation de la condition ci. Pour cela, nous ajoutons la cardinalité de la déviation minimale trouvée (k) comme information sur le nœud de ci.
Nous allons sur un exemple illustrer le déroulement de notre approche, voir le graphe sur la figure 9. Chaque cercle dans le graphe représente un nœud conditionnel visité par l’algorithme. L’exemple ne montre pas les blocs d’affectations, car nous voulons illustrer uniquement comment nous trouverons les déviations de correction minimales d’une taille bornée de la manière citée ci-dessus. Un arc reliant une condition c1 à une autre c2 illustre que c2 est atteinte par l’algorithme. Il y a deux façons, par rapport au comportement du contre-exemple, par lesquelles LocFaults arrive à la condition c2 :
1. en suivant la branche normale induite par la condition c1 ;
2. en suivant la branche opposée.
La valeur de l’étiquette des arcs pour le cas (1) (resp. (2)) est ”next” (resp. ”devie”).
– À l’étape k = 5, notre algorithme a identifié deux déviations minimales de taille égale à 5 :
1. D1 = {1, 2, 3, 4, 7}, le nœud ”7” est marqué par la valeur 5 ;
2. D2 = {8, 9, 11, 12, 7}, elle a été autorisée, car la valeur de la marque du nœud ”7” est égale à la cardinalité de D2.
– À l’étape k = 6, l’algorithme a suspendu la déviation suivante D3 = {8, 13, 14, 15, 16, 7}, car la cardinalité de D3 est supérieure strictement à la valeur de l’étiquette du nœud ”7”.
6 Expérience pratique
Pour évaluer la scalabilité de notre méthode, nous avons comparé ses performances avec celles de BugAssist 4 sur deux ensembles de benchmarks 5. * Le premier benchmark est illustratif, il contient un ensemble de programmes sans boucles ;
* Le deuxième benchmark inclut 19, 49 et 91 variations pour respectivement les programmes BubbleSort, Sum et SquareRoot. Ces programmes contiennent des boucles pour étudier le passage à l’échelle de notre approche par rapport à BugAssist. Pour augmenter la complexité d’un programme, nous augmentons le nombre d’itérations dans les boucles à l’exécution de chaque outil ; nous utilisons la même borne de dépliage des boucles pour LocFaults et BugAssist.
Pour générer le CFG et le contre-exemple, nous utilisons l’outil CPBPV [11] (Constraint-Programming Framework for Bounded Program Verification). LocFaults et BugAssist travaillent respectivement sur des programmes Java et C. Pour que la comparaison soit juste, nous avons construit pour chaque programme deux versions équivalentes : * une version en Java annotée par une spécification JML ;
* une version en ANSI-C annotée par la même spécification mais en ACSL. Les deux versions ont les mêmes numéros de lignes d’instructions, notamment des erreurs. La précondition spécifie le contre-exemple employé pour le programme.
Pour calculer les MCSs, nous avons utilisé les solveurs IBM ILOG MIP 6 et CP 7 de CPLEX. Nous
4. L’outil BugAssist est disponible à l’adresse : http:// bugassist.mpi-sws.org/
5. Le code source de l’ensemble de programmes est disponible à l’adresse : http://www.i3s.unice.fr/~bekkouch/ Benchs_Mohammed.html
6. Disponible à l’adresse http ://www01.ibm.com/software/commerce/optimization/cplexoptimizer/
7. Disponible à l’adresse http ://www01.ibm.com/software/commerce/optimization/cplex-cp-
avons adapté et implémenté l’algorithme de Liffiton et Sakallah [12], voir alg. 1. Cette implémentation prend en entrée l’ensemble de contraintes infaisable qui correspond au chemin identifié (C), et bmcs : la borne sur la taille des MCSs calculés. Chaque contrainte ci dans le système construit C est augmentée par un indicateur yi pour donner yi → ci dans le nouveau système de contraintes C′. Affecter à yi la valeur V rai implique la contrainte ci ; en revanche, affecter à yi la valeur Faux implique la suppression de la contrainte ci. Un MCS est obtenu en cherchant une affectation qui satisfait le système de contraintes avec un ensemble minimal d’indicateurs de contraintes affectés avec Faux. Pour limiter le nombre de variables indicateurs de contraintes qui peuvent être assignées à Faux, on utilise la contrainte AtMost(¬y1,¬y2, ...,¬yn, k) (voir la ligne 5), le système créé est noté dans l’algorithme C′k (ligne 5). Chaque itération de la boucle While (lignes 6 − 19) permet de trouver tous les MCSs de taille k, k est incrémenté de 1 après chaque itération. Après chaque MCS trouvé (lignes 8− 13), une contrainte de blocage est ajoutée à C′k et C
′ pour empêcher de trouver ce nouveau MCS dans les prochaines itérations (lignes 15 − 16). La première boucle (lignes 4 − 19) s’itère jusqu’à ce que tous les MCSs de C soient générés (C′ devient infaisable) ; elle peut s’arrêter aussi si les MCSs de taille inférieure ou égale bmcs sont obtenus (k > bmcs).
1 Fonction MCS(C,bmcs ) Entrées: C : Ensemble de contraintes infaisable, bmcs : Entier Sorties: MCS : Liste de MCSs de C de cardinalité inférieure à bmcs 2 début 3 C′ ← AddYVars(C) ; MCS ← ∅ ; k ← 1 ; 4 tant que SAT(C′) ∧ k ≤ bmcs faire 5 C′ k ← C′ ∧ AtMost({¬y1,¬y2, ...,¬yn},k) 6 tant que SAT(C′ k ) faire 7 newMCS ← ∅ 8 pour chaque indicateur yi faire 9 % yi est l’indicateur de la contrainte ci ∈ C, et val(yi) la
valeur de yi dans la solution calculée de C ′ k .
10 si val(yi) = 0 alors 11 newMCS ← newMCS ∪ {ci}. 12 fin 13 fin 14 MCS.add(newMCS). 15 C′ k ← C′ k ∧ BlockingClause(newMCS) 16 C′ ← C′ ∧ BlockingClause(newMCS) 17 fin 18 k ← k + 1 19 fin 20 retourner MCS 21 fin
Algorithm 1: Algorithme de Liffiton et Sakallah
BugAssist utilise l’outil CBMC [13] pour générer la trace erronée et les données d’entrée. Pour le solveur Max-SAT, nous avons utilisé MSUnCore2 [14].
Les expérimentations ont été effectuées avec un processeur Intel Core i7-3720QM 2.60 GHz avec 8 GO de RAM.
optimizer/
6.1 Le benchmark sans boucles
Cette partie sert à illustrer l’amélioration apportée à LocFaults pour réduire le nombre d’ensembles suspects fournis à l’utilisateur : à une étape donnée de l’algorithme, le nœud dans le CFG du programme qui permet de détecter une DCM sera marqué par le cardinal de cette dernière ; ainsi aux prochaines étapes, l’algorithme n’autorisera pas le balayage d’une liste d’adjacence de ce nœud.
Nos résultats 8 montrent que LocFaults rate les erreurs uniquement pour TritypeKO6. Or, BugAssist rate l’erreur pour AbsMinusKO2, AbsMinusKO3, AbsMinusV2KO2, TritypeKO, TriPerimetreKO, TriMultPerimetreKO et une des deux erreurs dans TritypeKO5. Les temps 9 de notre outil sont meilleurs par rapport à BugAssist pour les programmes avec calcul numérique ; ils sont proches pour le reste des programmes.
Prenons trois exemples parmi ces programmes au hasard. Et considérons l’implémentation de deux versions de notre algorithme, sans et avec marquage des nœuds nommées respectivement LocFaultsV1 et LocFaultsV2.
– Les tables 1 et 2 montrent respectivement les ensembles suspects et les temps de LocFaultsV1 ; – Les tables 3 et 4 montrent respectivement les ensembles suspects et les temps de LocFaultsV2.
Dans les tables 1 et 3, nous avons affiché la liste des MCSs et DCMs calculés. Le numéro de la ligne correspondant à la condition est souligné. Les tables 2 et 4 donnent les temps de calcul : P est le temps de prétraitement qui inclut la traduction du programme Java en un arbre syntaxique abstrait avec l’outil JDT (Eclipse Java devlopment tools), ainsi que la construction du CFG ; L est le temps de l’exploration du CFG et de calcul des MCSs.
LocFaultsV2 a permis de réduire considérablement les déviations générées ainsi que les temps sommant l’exploration du CFG et le calcul des MCSs de LocFaultsV1, et cela sans perdre l’erreur ; les localisations fournies par LocFaultsV2 sont plus pertinentes. Les lignes éliminées de la table 3 sont colorées en bleu dans la table 1. Les temps améliorés sont affichés en gras dans la table 4. Par exemple, pour le programme TritypeKO2, à l’étape 1 de l’algorithme,
8. Le tableau qui donne les MCSs calculés par LocFaults pour les programmes sans boucles est disponible à l’adresse http://www.i3s.unice.fr/~bekkouch/Bench_ Mohammed.html#rsb
9. Les tableaux qui donnent les temps de LocFaults et BugAssist pour les programmes sans boucles sont disponibles à l’adresse http://www.i3s.unice.fr/~bekkouch/Bench_ Mohammed.html#rsba
LocFaultsV2marque le nœud de la condition 26, 35 et 53 (à partir du contre-exemple, le programme devient correct en déviant chacune de ces trois conditions). Cela permet, à l’étape 2, d’annuler les déviations suivantes : {26, 29}, {26, 35}, {29, 35}, {32, 35}. Toujours à l’étape 2, LocFaultsV2 détecte deux déviations minimales en plus : {29, 57}, {32, 44}, les nœuds 57 et 44 vont donc être marqués (la valeur de la marque est
2). À l’étape 3, aucune déviation n’est sélectionnée ; à titre d’exemple, {29, 32, 44} n’est pas considérée parce que son cardinal est supérieur strictement à la valeur de la marque du nœud 44.
6.2 Les benchmarks avec boucles
Ces benchmarks servent à mesurer l’extensibilité de LocFaults par rapport à BugAssist pour des programmes avec boucles, en fonction de l’augmentation du nombre de dépliage b. Nous avons pris trois programmes avec boucles : BubbleSort, Sum et SquareRoot. Nous avons provoqué le bug Off-by-one dans chacun. Le benchmark, pour chaque programme, est créé en faisant augmenter le nombre de dépliage b. b est égal au nombre d’itérations effectuées par la boucle dans le pire des cas. Nous faisons aussi varier le nombre de conditions déviées pour LocFaults de 0 à 3.
Nous avons utilisé le solveur MIP de CPLEX pour BubbleSort. Pour Sum et SquareRoot, nous avons fait collaborer les deux solveurs de CPLEX (CP et MIP) lors du processus de la localisation. En effet, lors de la collecte des contraintes, nous utilisons une variable pour garder l’information sur le type du CSP construit. Quand LocFaults détecte un chemin erroné 10 et avant de procéder au calcul des MCSs, il prend le bon solveur selon le type du CSP qui correspond à ce chemin : s’il est non linéaire, il utilise le
10. Un chemin erroné est celui sur lequel nous identifions les MCSs.
solveur CP OPTIMIZER; sinon, il utilise le solveur MIP.
Pour chaque benchmark, nous avons présenté un extrait de la table contenant les temps de calcul (les colonnes P et L affichent respectivement les temps de prétraitement et de calcul des MCSs), ainsi que le graphe qui correspond au temps de calcul des MCSs.
6.2.1 Le benchmark BubbleSort
BubbleSort est une implémentation de l’algorithme de tri à bulles. Ce programme contient deux boucles imbriquées ; sa complexité en moyenne est d’ordre n2, où n est la taille du tableau : le tri à bulles est considéré parmi les mauvais algorithmes de tri. L’instruction erronée dans ce programme entrâıne le programme à trier le tableau en entrée en considérant seulement ses n − 1 premiers éléments. Le mauvais fonctionnement du BubbleSort est dû au nombre d’itérations insuffisant effectué par la boucle. Cela est dû à l’initialisation fautive de la variable i : i = tab.length - 1 ; l’instruction devait être i = tab.length.
Les temps de LocFaults et BugAssist pour le benchmark BubbleSort sont présentés dans la table 5. Le graphe qui illustre l’augmentation des temps des différentes versions de LocFaults et de BugAssist en fonction du nombre de dépliages est donné dans la figure 10.
La durée d’exécution de LocFaults et de BugAssist crôıt exponentiellement avec le nombre de dépliages ; les temps de BugAssist sont toujours les plus grands. On peut considérer que BugAssist est inefficace pour ce benchmark. Les différentes versions de LocFaults (avec au plus 3, 2, 1 et 0 conditions déviées) restent utilisables jusqu’à un certain dépliage. Le nombre de dépliage au-delà de lequel la croissance des temps de BugAssist devient rédhibitoire est inférieur à celui de LocFaults, celui de LocFaults avec au plus 3 conditions déviées est inférieur à celui de LocFaults avec au plus 2 conditions déviées qui est inférieur lui aussi à
celui de LocFaults avec au plus 1 conditions déviées. Les temps de LocFaults avec au plus 1 et 0 condition déviée sont presque les mêmes.
6.2.2 Les benchmarks SquareRoot et Sum
Le programme SquareRoot (voir fig. 11) permet de trouver la partie entière de la racine carrée du nombre entier 50. Une erreur est injectée à la ligne 13, qui entrâıne de retourner la valeur 8 ; or le programme doit retourner 7. Ce programme a été utilisé dans le papier décrivant l’approche BugAssist, il contient un calcul numérique linéaire dans sa boucle et non linéaire dans sa postcondition.
1 c l a s s SquareRoot{ 2 /∗@ ensu res ( ( r e s ∗ res<=val ) && ( re s +1)∗( r e s+1)>val ) ; ∗/ 3 i n t SquareRoot ( ) 4 { 5 in t va l = 50 ; 6 in t i = 1; 7 in t v = 0; 8 in t r e s = 0 ; 9 whi l e (v < val ){
10 v = v + 2∗ i + 1 ; 11 i= i + 1 ; 12 } 13 re s = i ; /∗ e r ro r : the i n s t r u c t i o n should be r e s = i − 1∗/ 14 return re s ; 15 } 16 }
Figure 11 – Le programme SquareRoot
Avec un dépliage égal à 50, BugAssist calcule pour ce programme les instructions suspectes suivantes : {9, 10, 11, 13}. Le temps de la localisation est 36, 16s et le temps de prétraitement est 0, 12s.
LocFaults présente une instruction suspecte en indiquant à la fois son emplacement dans le programme (la ligne d’instruction), ainsi que la ligne de la condition et l’itération de chaque boucle menant à cette instruction. Par exemple, 9 : 2.11 correspond à l’instruction qui se trouve à la ligne 11 dans le programme, cette dernière est dans une boucle dont la ligne de la condition d’arrêt est 9 et le numéro d’itération est 2. Les ensembles suspectés par LocFaults sont fournis dans le tableau suivant. DCMs MCSs
∅ {5},{6},{9 : 1.11}, {9 : 2.11},{9 : 3.11}, {9 : 4.11},{9 : 5.11},{9 : 6.11},{9 : 7.11},{13}
{9 : 7} {5},{6},{7},{9 : 1.10},{9 : 2.10},{9 : 3.10}, {9 : 4.10},{9 : 5.10}, {9 : 6.10},{9 : 1.11},
{9 : 2.11},{9 : 3.11},{9 : 4.11},{9 : 5.11}, {9 : 6.11}
Le temps de prétraitement est 0, 769s. Le temps écoulé lors de l’exploration du CFG et le calcul des MCS est 1, 299s. Nous avons étudié le temps de LocFaults et BugAssist des valeurs de val allant de 10 à 100 (le nombre de dépliage b employé est égal à val), pour étudier le comportement combinatoire de chaque outil pour ce programme.
Le programme Sum prend un entier positif n de l’utilisateur, et il permet de calculer la valeur de∑n
i=1 i. La postcondition spécifie cette somme. L’erreur dans Sum est dans la condition de sa boucle. Elle cause de calculer la somme ∑n−1 i=1 i au lieu de ∑n i=1 i. Ce programme contient des instructions numériques linéaires dans le cœur de la boucle, et une postcondition non linéaire.
Les résultats en temps pour les benchmarks SquareRoot et Sum sont présentés dans les tables respectivement 6 et 7. Nous avons dessiné aussi le graphe qui correspond au résultat de chaque benchmark, voir respectivement le graphe de la figure 12 et 13. Le temps d’exécution de BugAssist crôıt rapidement ; les temps
de LocFaults sont presque constants. Les temps de LocFaults avec au plus 0, 1 et 2 conditions déviées sont proches de ceux de LocFaults avec au plus 3 conditions déviées.
7 Conclusion
La méthode LocFaults détecte les sous-ensembles suspects en analysant les chemins du CFG pour trouver les DCMs et les MCSs à partir de chaque DCM ; elle utilise des solveurs de contraintes. La méthode BugAssit calcule la fusion des MCSs du programme en transformant le programme complet en une formule booléenne ; elle utilise des solveurs Max-SAT. Les deux
méthodes travaillent en partant d’un contre-exemple. Dans ce papier, nous avons présenté une exploration de la scalabilité de LocFaults, particulièrement sur le traitement des boucles avec le bug Off-by-one. Les premiers résultats montrent que LocFaults est plus efficace que BugAssist sur des programmes avec boucles. Les temps de BugAssist croissent rapidement en fonction du nombre de dépliages.
Dans le cadre de nos travaux futurs, nous envisageons de confirmer nos résultats sur des programmes avec boucles plus complexes. Nous développons une version interactive de notre outil qui fournit les sousensembles suspects l’un après l’autre : nous voulons tirer profit des connaissances de l’utilisateur pour sélectionner les conditions qui doivent être déviées. Nous réfléchissons également sur comment étendre notre méthode pour supporter les instructions numériques avec calcul sur les flottants.
Remerciements. Nous remercions Bertrand Neveu pour sa lecture attentive et ses commentaires utiles sur ce papier. Merci également à Michel Rueher et Hélène Collavizza pour leurs remarques intéressantes.
Références
[1] D’silva, Vijay, Daniel Kroening, and Georg Weissenbacher. ”A survey of automated techniques for formal software verification.” Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on 27.7 (2008) : 1165-1178.
[2] Kok-Ming Leung. ”Debugging Loops.” In http://cis. poly.edu/~mleung/CS1114/s08/ch02/debug.htm
[3] Bekkouche, Mohammed, Hélène Collavizza, and Michel Rueher. ”Une approche CSP pour l’aide à la localisation d’erreurs.” arXiv preprint arXiv :1404.6567 (2014).
[4] Bekkouche, Mohammed, Hélène Collavizza, and Michel Rueher. ”LocFaults : A new flow-driven and constraint-based error localization approach*.” ACM. SAC’15, SVT track, Apr 2015, Salamanca, Spain. <10.1145/2695664.2695822>. <hal-01094227>
[5] Barnett, Mike, and K. Rustan M. Leino. ”Weakestprecondition of unstructured programs.” ACM SIGSOFT Software Engineering Notes. Vol. 31. No. 1. ACM, 2005.
[6] Wong, W. Eric, and Vidroha Debroy. ”A survey of software fault localization.” Department of Computer Science, University of Texas at Dallas, Tech. Rep. UTDCS-45-09 (2009).
[7] Bekkouche, Mohammed. ”Bug stories.” In http://www. i3s.unice.fr/~bekkouch/Bug_stories.html
[8] Wikipedia. ”List of software bugs — Wikipedia, The Free Encyclopedia.” In http: //en.wikipedia.org/w/index.php?title=List_
of_software_bugs&oldid=648559652
[9] Jose, Manu, and Rupak Majumdar. ”Cause clue clauses : error localization using maximum satisfiability.” ACM SIGPLAN Notices 46.6 (2011) : 437-446.
[10] Jose, Manu, and Rupak Majumdar. ”Bug-Assist : assisting fault localization in ANSI-C programs.” Computer Aided Verification. Springer Berlin Heidelberg, 2011.
[11] Collavizza, Hélène, Michel Rueher, and Pascal Van Hentenryck. ”CPBPV : a constraint-programming framework for bounded program verification.”Constraints 15.2 (2010) : 238-264.
[12] Liffiton, Mark H., and Karem A. Sakallah. ”Algorithms for computing minimal unsatisfiable subsets of constraints.” Journal of Automated Reasoning 40.1 (2008) : 1-33.
[13] Clarke, Edmund, Daniel Kroening, and Flavio Lerda. ”A tool for checking ANSI-C programs.” Tools and Algorithms for the Construction and Analysis of Systems. Springer Berlin Heidelberg, 2004. 168-176.
[14] Marques-Silva, Joao. ”The msuncore maxsat solver.” SAT 2009 competitive events booklet : preliminary version (2009) : 151.
Geo scientists are still classifying and zoning, but much more attention is turned to the meaning of the process, the interpretability of the result, and the ability to use it within a decision process. Moreover, geo-information also raised the question of what is in data, common agreement, external quality, data usability, etc., which are different aspects of a more general question often summarized into the word “ontologies” (plural!), and the subsequent problem of “ontology alignment” (Gruber 1994; Halevy 2005).
Here again, the terminology has been brought to public attention by the wide spread of the Internet and the mainstream studies following it, but, here again, geo-information was developing its research when questioning the problems of global data quality, of managing different quality levels for different types of information, and geomatics was among the first to provide a systematic and comprehensive approach, with the abovementioned ISO 191xx series of standards, in particular in 2002 with ISO 19150 (“ontologies”), and ISO 19157 (“data quality”). Deviller and Jeansoulin (2006) regroup a good part of this research on ontologies and quality, in particular the notion of external
Barry Wellar 198
quality (quality for the user) which is illustrated in Figure 5, where the quality defined by the producer of geo-data is internal (referring to its own specification) rather than external (quality as a fitness for use).
Let’s develop an example of ontologies alignment with agricultural data. Given 2 graphs representing two different surveys of the same region (Pham 2004), the observations at
the same locations (parcels) will differ according to their ontologies, and we can build a Galois lattice summing up the fusion of information.
In Figure 6, the conflated graph contains all the different nodes of both initial graphs, plus some additional mandatory nodes (yellow), minimal addition for a consistent fusion, and whose labels are a consensus between the two original ontologies, e.g. “(grass; herbaceous)”. This consensus between the two ontologies is minimal, in that it is much more efficient and meaningful than the mere cross-product of the two ontologies.
Therefore, ontologies are not made solely for Internet applications or e-business, but also for addressing real world queries: “What there is”, or “The nature of being”, as discussed by the philosopher W. O. Quine and others with an interest in ontology as a branch of philosophy which deals with the science of what is, what exists, and associated concerns. These fundamental questions have confronted geo-information scientists since the beginning of their work.
Barry Wellar 199
Figure 5 shows the learning curves for the dart throwing problem. We get a substantial improvement in the learning performance when using an algorithm that incorporates sensor data when compared to an algorithm that ignores the sensor data. At each hill-climbing step we drew a single sample from 12 different policies and we averaged over 48 hill-climbing runs. The policies for each hill-climbing step were drawn from a Gaussian distribution for exploration purposes.
Figure 6 shows the learning curves for the quadruped locomotion problem. We get an improvement in the learning performance when using of an algorithm that incorporates sensor data when compared to an algorithm that ignores the sensor data. At each hillclimbing step we drew a single sample from 30 different policies and we averaged over 48 hill-climbing runs. The policies for each hill-climbing step were drawn from a Gaussian distribution for exploration purposes.
These are used to restrict the set of resources that may have a given property (the property's domain) and the set of valid values for a property (its range). A property may have as many values for rdfs:domain as needed, but no more than one value for rdfs:range.
I. INTRODUCTION
People see the world differently, and objects are described from various point of views and modalities. Identifying an object can not only benefit from visual cues including color, texture and shape, but textual annotations from different observations and languages. Thanks to data enrichment from sensor technologies, the accuracy in image retrieval and recognition has been significantly improved by taking advantage of multiview and cross-domain learning [1], [2]. Since matching the data samples across various feature spaces directly is infeasible, subspace learning approaches, which learn a common feature space from multi-view spaces, becomes an effective approach in solving the problem.
Numerous methods have been proposed in subspace learning. They can be grouped into three major categories based on the characteristics of machine learning: two-view learning and multi-view learning; unsupervised learning and supervised learning; and linear learning and non-linear learning. While traditional techniques in multivariate analysis take two inputs [3], multi-view methods have been proposed to find an optimal representation from more than two views [4], [5]. Compared to learning the feature transformation in an unsupervised manner, discriminative methods, such as Linear Discriminant Analysis (LDA) have been extended to multi-view cases. Additionally, the transformation can also be kernel-based or learned by (deep) neural nets to exploit their non-linear properties.
Two-view learning and multi-view learning: One of the most popular methods in multivariate statistics is Canonical Correlation Analysis (CCA) [6]. It seeks to maximize the correlation between two sets of variables. Alternatively, its
multi-view counterpart aims to obtain a common space from V > 2 views [4], [5], [7]. This is achieved either by scaling the cross-covariance matrices to incorporate the covariances from more than two views, or by finding the best rank-1 approximation of the data covariance tensor. A similar approach to find the common subspace is Partial Least Square Regressions [8]. It maximizes the cross-covariance from two views by regressing the data samples to the common space. Besides transformation and regression, Multi-view Fisher Discriminant Analysis (MFDA) [9] learns the transformation minimizing the difference between data samples of predicted labels. The Dropout regularization was introduced for the multi-view linear discirminant analysis in [10].
Unsupervised learning and supervised learning: In contrast to unsupervised transformations, including CCA and PLS, LDA [11], [12] exploits the class labels effectively by maximizing the between-class scatter while minimizing the withinclass scatter simultaneously. CCA has been successfully combined with LDA to find a discriminative subspace in [13], [14], [15]. Coupled Spectral Regression (CSR) [16] projects two different inputs to the low-dimensional embedding of labels by PLS regressions. Consistent with the original LDA, a Multiview Discriminant Analysis (MvDA) [17] finds a discriminant representation over V views. The between-class scatter is maximized regardless of the difference between inter-view and intra-view covariances, while the within-class scatter is minimized in the mean time. Generalized Multi-view Analysis (GMA) [18] was proposed to maximize the intra-view discriminant information. Recently, a semi-supervised alternative [19] was also proposed for multi-view learning, which adopts a non-negative matrix factorization method for view mapping and a robust sparse regression model for clustering the labeled samples. Moreover, a multi-view information bottleneck method [20] was proposed to retain its discrimination and robustness for multi-view learning.
Linear and non-linear learning: Many problems are not linearly separable and thereby kernel-based methods and learning representation by (deep) neural nets are introduced. By mapping the features to the high dimensional feature space using the kernel trick [21], kernel CCA [22] adopts a predefined kernel and limits its application on small datasets. Many linear multi-view methods subsequently made their kernel extension [23], [15], [24]. Kernel approximation [5] was adopted later to work on big data. Deep CCA [25] was proposed using neural nets to learn adaptive non-linear representations from two views, and uses the weights in the last layers to find the maximum correlation. A similar idea has been exploited on LDA [26]. PCANet [27] was introduced to
ar X
iv :1
60 5.
09 69
6v 3
[ cs
.C V
] 3
1 A
ug 2
01 7
adopt a cascade of linear transformation, followed by binary hashing and block histograms.
We make several contributions in this paper: First, we propose a unified multi-view subspace learning method for CCA, PLS and LDA techniques using the graph embedding framework [11]. We design both intrinsic and penalty graphs to characterize the intra-view and inter-view information, respectively. The intra-view and inter-view covariance matrices are scaled up to incorporate more than two views for numerous techniques by exploiting their specific intrinsic and penalty graphs. In our proposed Multi-view Modular Discriminant Analysis (MvMDA), the two graphs also charaterize the within-class compactness and between-class separability. Based on the aforementioned characteristics of subspace learning algorithms, we propose a generalized objective function for multi-view subspace learning using Rayleigh quotient. This unified multi-view embedding approach can be solved as a generalized eigenvalue problem.
Second, we introduce a Multi-view Modular Discriminant Analysis (MvMDA) method by exploiting the distances between centers representing classes of different views. This is of particular interest since the resulting scatter encodes cross-view information, which empirically is shown to provide superior results. Third, we also extend the unified framework to the non-linear cases with kernels and (deep) neural networks. Kernel-based multi-view learning method is derived with an implicit kernel mapping. For larger datasets, we use the explicit kernel mapping [28] to approximate the kernel matrices. We also derive the formulation of stochastic gradient descent (SGD) for optimizing the objective function in the neural nets.
Last but not least, we demonstrate the effectiveness of the proposed embedding methods on visual object recognition and cross-modal image retrieval. Specifically, zero-shot recognition is evaluated by discovering novel object categories based on the underlying intermediate representation [29], [30], [31]. Its performance is heavily dependent on the representation in the latent space shared by visual and semantic cues. We integrate observations from attributes as a middle-level semantic property for the joint learning. Superior recognition results are achieved by exploiting the latent feature space with nonlinear solutions learned from the multi-view representations. We also employ the proposed multi-view subspace learning methods for cross-modal image retrieval [1], [32], [33], [34]. This type of methods differs from the co-training methods for image classification [35] and web image reranking [36], [37]. In the experiments, we show promising retrieval results performed by embedding more modalities into the common feature space, and find that even conventional content-based image retrieval can be improved.
The rest of the paper is organized as follows. Section II reviews the related work. In Section III, we show the unified formulation to generalize the subspace learning methods. It is followed by the extension to multi-view techniques and derivation in kernels and neural nets. Then, in Section IV, we present the comparative results in zero-shot object recognition and cross-modal image retrieval on three popular multimedia datasets. Finally, Section V concludes the paper.
The way we use temporal convlution neural network for event span and attribute classification is similar with the approach proposed by (Collobert et al., 2011). Generally speaking, we can consider a word as represented by K discrete features w ∈ D1 × · · · ×DK , where DK is the dictionary for the kth feature. In our scenario, we just use three features such as token mention, pos tag and word shape. Note that word shape features are used to represent the abstract letter pattern of the word by mapping lower-case letters to “x”, upper-case to “X”, numbers to “d”, and retaining punctuation. We associate to each feature a lookup table. Given a word, a feature vector is then obtained by concatenating all lookup table outputs. Then a clinical snippet is transformed into a word embedding matrix. The matrix can be fed to further 1-dimension convolutional neural network
and max pooling layers. Below we will briefly introduce core concepts of Convoluational Neural Network (CNN).
Temporal Convolution
Temporal Convolution applies one-dimensional convolution over the input sequence. The onedimensional convolution is an operation between a vector of weights m ∈ Rm and a vector of inputs viewed as a sequence x ∈ Rn. The vector m is the filter of the convolution. Concretely, we think of x as the input sentence and xi ∈ R as a single feature value associated with the i-th word in the sentence. The idea behind the one-dimensional convolution is to take the dot product of the vector m with each m-gram in the sentence x to obtain another sequence c:
cj = m T xj−m+1:j . (1)
Usually, xi is not a single value, but a ddimensional word vector so that x ∈ Rd×n. There exist two types of 1d convolution operations. One was introduced by (Waibel et al., 1989) and also known as Time Delay Neural Networks (TDNNs). The other one was introduced by (Collobert et al., 2011). In TDNN, weights m ∈ Rd×m form a matrix. Each row of m is convolved with the corresponding row of x. In (Collobert et al., 2011) architecture, a sequence of length n is represented as:
x1:n = x1 ⊕ x2 · · · ⊕ xn , (2)
where ⊕ is the concatenation operation. In general, let xi:i+j refer to the concatenation of words xi,xi+1, . . . ,xi+j . A convolution operation involves a filter w ∈ Rhk, which is applied to a window of h words to produce the new feature. For example, a feature ci is generated from a window of words xi:i+h−1 by:
ci = f(w · xi:i+h−1 + b) , (3)
where b ∈ R is a bias term and f is a non-linear function such as the hyperbolic tangent. This filter is applied to each possible window of words in the sequence {x1:h,x2:h+1, . . . ,xn−h+1:n} to produce the feature map:
c = [c1, c2, . . . , cn−h+1] , (4)
where c ∈ Rn−h+1.
We also employ dropout on the penultimate layer with a constraint on ℓ2-norms of the weight vector. Dropout prevents co-adaptation of hidden units by randomly dropping out a proportion p of the hidden units during forwardbackpropagation. That is, given the penultimate layer z = [ĉ1, . . . , ĉm], instead of using:
y = w · z+ b (5)
for output unit y in forward propagation, dropout uses:
y = w · (z ◦ r) + b , (6)
where ◦ is the element-wise multiplication operator and r ∈ Rm is a masking vector of Bernoulli random variables with probability p of being 1. Gradients are backpropagated only through the unmasked units. At test step, the learned weight vectors are scaled by p such that ŵ = pw, and ŵ is used to score unseen sentences. We additionally constrain l2-norms of the weight vectors by rescaling w to have ||w||2 = s whenever ||w||2 > s after a gradient descent step.
We choose to use the trace norm, which is defined as the sum of matrix’s singular values.
||X ||∗ = ∑
i=1
σi (2)
1Matriciation is also known as tensor unfolding or flattening. 2For the case that each task has a different number of outputs, the parameters of topmost layers from
neural networks should be of different size, thus they are opted out for sharing.
The trace norm is named by the fact that when X is a positive semidefinite matrix, it is the trace of X . It is sometimes referred to as nuclear norm. Trace norm is the tightest convex relation of matrix rank [Recht et al., 2010]. The extension of trace norm from matrix to tensor is not unique, just like the rank of tensor has different definitions. How to define the rank of tensor largely depends on how the tensor is factorised, e.g., Tucker decomposition [Tucker, 1966] and Tensor-Train decomposition Oseledets [2011]. We propose three tensor trace norm designs here, which are corresponding to three variants of the proposed method. For an N -way tensor W of size D1 ×D2 × · · · ×DN . We define Tensor Trace Norm Tucker
||W||∗ =
N∑
i=1
γi||W(i)||∗ (3)
Here W(i) is the mode-i tensor flattening/unfolding, which is obtained by,
W(i) = reshape(transpose(W , [Di, D1, . . . , Di−1, Di+1 . . . , DN ]), [Di, ∏
j¬i
Dj ]) (4)
Tensor Trace Norm TT
||W||∗ =
N−1∑
i=1
γi||W[i]||∗ (5)
Here W[i] is yet another way to unfold the tensor, which is obtained by,
W[i] = reshape(W , [D1 ×D2 . . .Di, Di+1 ×Di+2 . . . DN ]) (6)
Tensor Trace Norm Matrix
||W||∗ = γ||W(N)||∗ (7)
This is the simplest definition. Given that in our framework, the last axis of tensor indexes the tasks, i.e., DN = T , it is the most straightforward way to adapt the technique in matrix-based MTL – reshape the D1 ×D2 × · · · × T tensor to D1D2 · · · × T matrix.
In this paper we have shown that RNN LMs can be trained on large amounts of data, and outperform competing models including carefully tuned N-grams. The reduction in perplexity from 51.3 to 30.0 is due to several key components which we studied in this paper. Thus, a large, regularized LSTM LM, with projection layers and trained with an approximation to the true Softmax with importance sampling performs much better than N-grams. Unlike previous work, we do not require to interpolate both the RNN LM and the N-gram, and the gains of doing so are rather marginal.
By exploring recent advances in model architectures (e.g. LSTMs), exploiting small character CNNs, and by sharing our findings in this paper and accompanying code and models (to be released upon publication), we hope to inspire research on large scale Language Modeling, a problem we consider crucial towards language understanding. We hope for future research to focus on reasonably sized datasets taking inspiration from recent advances seen in the computer vision community thanks to efforts such as Imagenet (Deng et al., 2009).
Control theory deals with influencing the behaviour of dynamical systems by monitoring output and comparing it with reference values. By using the feedback of the input system (difference between actual and desired output level), the controller tries to align actual output to the reference. For auto-scaling, the reference parameter, i.e., an object to be
1http://aws.amazon.com/ec2 2http://azure.microsoft.com 3https://www.openstack.org
controlled, is the targeted SLA value [15]. The system is the target platform and system output are parameters to evaluate system performance (response time or CPU load). Zhu and Agrawal [26] present a framework using Proportional-Integral (PI) control, combined with a reinforcement learning component in order to minimize application cost. Ali-Eldin et al. [2], [1] propose two adaptive hybrid reactive/proactive controllers in order to support service elasticity by using the queueing theory to estimate the future load. Padala et al. [21] propose a feedback resource control system that automatically adapts to dynamic workload changes to satisfy service level objectives. They use an online model estimator to dynamically maintain the relationship between applications and resources, and a two-layer multi-input multi-output (MIMO) controller that allocates resources to applications dynamically. Kalyvianaki et al. [18] integrate a Kalman filter into feedback controllers that continuously detects CPU utilization and dynamically adjusts resource allocation in order to meet QoS objectives.
To address the aforementioned challenges in distributed MTL, we propose to perform asynchronous multi-task learning (AMTL), where the central server begins to update model matrix W after it receives a gradient computation from one task node, without waiting for the other task nodes to finish their computations. While the server and all task agents maintain their own copies of W in the memory, the copy at one task node may be different from the copies at other nodes. The convergence analysis of the proposed AMTL framework
is backed up by a recent approach for asynchronous parallel coordinate update problems by using Krasnosel’skii-Mann (KM) iteration [6], [7]. A task node is said to be activated when it performs computation and network communication with the central server for updates. The framework is based on the following assumption on the activation rate:
Assumption 1. All the task nodes follow independent Poisson processes and have the same activation rate.
Remark 1. We note that when the activation rates are different for different task nodes, we can modify the theoretical result by changing the step size: if a task node’s activation rate is large, then the probability that this task node is activated is large and thus the corresponding step size should be small. In Section III-D we propose a dynamic step size strategy for the proposed AMTL.
The proposed AMTL uses a backward-forward operator splitting method [23], [8] to solve problem (III.1). Solving problem (III.1) is equivalent to finding the optimal solution W ∗ such that 0 ∈ ∇f(W ∗) + λ∂g(W ∗), where ∂g(W ∗) denotes the set of subgradients of non-smooth function g(·) at W ∗ and we have the following:
0 ∈ ∇f(W ∗) + λ∂g(W ∗) ⇐⇒ −∇f(W ∗) ∈ λ∂g(W ∗) ⇐⇒W ∗ − η∇f(W ∗) ∈W ∗ + ηλ∂g(W ∗).
Therefore the forward-backward iteration is given by:
W+ = (I + ηλ∂g)−1(I − η∇f)(W ),
which converges to the solution if η ∈ (0, 2/L). Since ∇f(W ) is separable, i.e., ∇f(W ) = [∇`1(w1),∇`2(w2), · · · ,∇`T (wT )], the forward operator, i.e., I − η∇f , is also separable. However, the backward operator, i.e., (I+ηλ∂g)−1, is not separable. Thus, we can not apply the coordinate update directly on the forward-backward iteration. If we switch the order of forward and backward steps, we obtain the following backward-forward iteration:
V + = (I − η∇f)(I + ηλ∂g)−1(V ),
where we use an auxiliary matrix V ∈ Rd×T instead of W during the update. This is because the update variables in the forward-backward and backward-forward are different variables. Moreover, one additional backward step is needed to obtain W ∗ from V ∗. We can thus follow [6] to perform task block coordinate update at the backward-forward iteration, where each task block is defined by the variables associated to a task. The update procedure is given as follows:
v+t = (I − η∇`t) ( (I + ηλ∂g)−1(V ) ) t ,
where vt ∈ Rd is the corresponding auxiliary variable of wt for task t. Note that updating one task block vt will need one full backward step and a forward step only on the task block. The overall AMTL algorithm is given in 1. The formulation in Eq. III.4 is the update rule of KM iteration discussed in [6]. KM iteration provides a generic framework to solve fixed
point problems where the goal is to find the fixed point of a nonexpansive operator. In the problem setting of this study, backward-forward operator is our fixed-point operator. We refer to Section 2.4 of [6] to see how Eq. III.4 is derived.
In general, the choice between forward-backward or backward-forward is largely dependent on the difficulty of the sub problem. If the backward step is easier to compute compared to the forward step, e.g., data (xt, yt) is large, then we can apply coordinate update on the backward-forward iteration. Specifically in the MTL settings, the backward step is given by a proximal mapping in Eq. III.3 and usually admits an analytical solution (e.g., soft-thresholding on singular values for trace norm). On the other hand, the gradient computation in Eq. III.2 is typically the most time consuming step for large datasets. Therefore backward-forward provides a more computational efficient optimization framework for distributed MTL. In addition, we note that the backward-forward iteration is a non-expansive operator if η ∈ (0, 2/L) because both the forward and backward steps are non-expansive.
When applying the coordinate update scheme in [6], all task nodes have access to the shared memory, and they do not communicate with each other. Further the communicate between each task node and the central server is only the vector vt , which is typically much smaller than the data Dt stored locally at each task node. In the proposed AMTL scheme, the task nodes do not share memory but are exclusively connected and communicate with the central node. The computation of the backward step is located in the central node, which performs the proximal mapping after one gradient update is received from a task node (the proximal mapping can be also applied after several gradient updates depending on the speed of gradient update). In this case, we further save the communication cost between each task node and the central node, because each task node only need the task block corresponding to the task node.
To illustrate the asynchronous update mechanism in AMTL, we show an example in Fig. 2. The figure shows order of the backward and the forward steps performed by the central node and the task nodes. At time t1, the task node 2 receives the model corresponding to the task 2 which was previously updated by the proximal mapping step in the central node. As soon as task node 2 receives its model, the forward (gradient) step is launched. After the task gradient descent update is done, the model of the task 2 is sent back to the central node. When the central node receives the updated model from the task node 2, it starts applying the proximal step on the whole multi-task model matrix. As we can see from the figure, while task node 2 was performing its gradient step, task node 4 had already sent its updated model to the central node and triggered a proximal mapping step during time steps t2 and t3. Therefore, the model matrix was updated upon the request of the task node 4 during the gradient computation of task node 2. Thus we know that the model received by the task node 2 at time t1 is not the same copy as in the central server any more. When the updated model from the task node 2 is received by the central node, proximal mapping computations
Algorithm 1 The proposed Asynchronous Multi-Task Learning framework Require: Multiple related learning tasks reside at task nodes, including the training data and the loss function for each task {x1, y1, `t}, ..., {xT , yT , `T }, maximum delay τ , step size η, multi-task regularization parameter λ. Ensure: Predictive models of each task v1, ..., vT . Initialize task nodes and the central server. Choose ηk ∈ [ηmin, c2τ/√T+1 ] for any constant ηmin > 0 and 0 < c < 1 while every task node asynchronously and continuously do
Task node t requests the server for the forward step computation Proxηλg ( v̂k ) , and
Retrieves ( Proxηλg ( v̂k ))
t
from the central server and
Computes the coordinate update on vt
vk+1t = v k t + ηk (( Proxηλg ( v̂k ))
t
− η∇`t (( Proxηλg(v̂k) ) t ) − vkt ) (III.4)
Sends updated vt to the central node. end while
are done by using the model received from task node 2 and the updated models at the end of the proximal step triggered by the task node 4. Similarly, if we think of the model received by task node 4 at time t3, we can say that it will not be the same model as in the central server when task node 4 is ready to send its model to the central node because of the proximal step triggered by the task node 2 during time steps t4 and t5. This is because in AMTL, there is no memory lock during reads. As we can see, the asynchronous update scheme has inconsistency when it comes to read model vectors from the central server. We note that such inconsistency caused by the backward step is already taken into account in the convergence analysis.
We summarize the proposed AMTL algorithm in Algorithm 1. The AMTL framework enjoys the following convergence property:
Theorem 1. Let (V k)k≥0 be the sequence generated by the proposed AMTL with ηk ∈ [ηmin, c2τ/√T+1 ] for any ηmin > 0 and 0 < c < 1, where τ is the maximum delay. Then (V k)k≥0 converges to an V ∗-valued random variable almost surely. If the MTL problem in Eq. III.1 has a unique solution, then the sequence converges to the unique solution.
According to our assumptions, all the task nodes are independent Poisson processes and each task node has the same activation rate. The probability that each task node is activated before other task nodes is 1/T [24], and therefore we can assume that each coordinate is selected with the same probability. The results in [6] can be applied to directly obtain the convergence. We note that some MTL algorithms based on sparsity-inducing norms may not have a unique solution, as commonly seen in many sparse learning formulations, but in this case we typically can add an `2 term to ensure the strict convexity and obtain linear convergence as shown in [6]. An example of such technique is the elastic net variant from the original Lasso problem [25].
Enterprises are nowadays operating in a complex economical environment where markets are more open and globalised. By this way, their competitiveness shall be intensively improved and changes in business models are necessary. As a concrete consequence of these new requirements, modern types of industrial networks have been set up. This fact is widely recognized as a major innovation in business management. It is obviously one of the most important trends in industrial engineering practices for the two last decades and is naturally becoming a consideration subject for numerous research studies. The analysis of business practices in cooperative organizations delivers new corpus of knowledge. For example, a large characterization and classification study of collaborative networked organizations is proposed in [1]. Core competencies and extensive cooperation could be identified as main sources for business efficiency expected by such new forms of organization whatever their nature. Consequently, the capacity of one enterprise to join a target collaborative networked organization, to adapt and react rapidly to market dynamics, as well as the synergy developed by such networked organizations, are subjects of prime interest that address the problem of enterprise architectures with a new point of view. Enterprise integration is a major issue for information system design (IS) when business performance improvement is expected. Particularly, the quality of communications between actors and systems is deeply enhanced when databases and software components are well integrated. Information technologies have made drastic progresses on that way, and we could say that they have become the major key for the most mature organizations at the integration level. Even though this integration problem is still an open subject, the support of new business practices based on collaboration of partners modifies the approach of enterprise information system design. Since activities have always to be performed under the pressure of time, it is not possible to imagine an open organization without having an open information system. The ability to easily communicate between partners that are beyond the perimeter of the organization is intrinsically different from an integration view of the information system design. The ability to capture and share information seamlessly amongst information systems of different enterprises is often limited by the heterogeneity of business processes, organization units, data structures and technologies, as well as the difficulty to share knowledge about these different artefacts between partners. Collaborative information system is the term we use to call the information system that is attempted to work at the level of the network. Designing and running such a collaborative information system, keeping in mind the goal of a minimal effort for each actor to participate, is an overall issue to which we try to contribute.
This particular question receives a lot of attention that considers it as an enterprise interoperability concern. Among many definitions, interoperability is defined as the ability of two or more systems or components to exchange information and to use the information that has been exchanged [2]. New architectures and technologies have been emerging in order to deal efficiently with the interoperability problem. When the systems that have to interact are information systems of independent organizations, the definition of interoperability has to be refined with the objective to explicitly reduce the complexity of problem formulation. In the first step, we will address it at two complementary levels giving a set of hypothesis:
- at the business level, collaboration is driven by communications and messages that are supposed to be
defined and controlled by specific business processes, so-called collaborative business processes (CBP). The CBP executions contribute to the achievement of common objectives in the collaboration space. They are a mean to control the coordination between the tasks performed by partners. - at the technological level, information resources like data and information technologies like software
components are assumed to be defined at the interface of each individual information system. This level is in charge of CBP executions, that is to say that the information flows are managed between software components that make the expected operations in compliance with the required structure of the CBP.
In the second step, we will add a brand new level, the organizational level, to the two previous ones. At the organizational level, in each independent organization, the actors who are involved in the collaboration are known. These actors are in charge of the activities defined in the CBP of the business level. They are also able to supervise the relevant resources and technologies at the technological level. These actors are aware of the processes in which they are involved. Based on this framework, we propose to define a collaborative information system structure which is composed of two types of components:
- native information systems of partners who are involved in collaboration are the business added value
components. Their contribution is to ensure collaboration performances. The components could behave independently from each other, and they are configured to fulfil the needs of the collaboration. - mediation components are mainly in charge of coordinating between the business added value
components that should be conformed to the specification imposed by the CBP. The parallelism or the synchronisation of activities defined in the business added value components is done by the mediation components. Moreover, such components are able to guarantee the exchange of information between
the business added value components. Indeed, all types of heterogeneity between the native information systems of partners are tackled by the mediation components.
The concept of Mediation Information System (MIS) has been previously introduced by the authors in order to deal with this enterprise interoperability challenge [3]. A MIS is related to a collection of mediation components and is therefore an important part of a collaborative information system. The MIS is relevant at all the levels that we have defined before: business, technology and organization. This simple perspective about the structure of a collaborative information system satisfy the requirement of a less binding between components, i.e relative autonomy and low adaptation needs for partners, before and during collaboration. The overall system architecture that results from this choice is indeed very close to the concept of system of systems [4]. Mediation is a word that is often used in scientific works related to interoperability, either when one is explaining a federated approach while dealing with architectural aspects, or when one is focusing on data or service semantics while dealing with reducing heterogeneity barriers. Our proposal on MIS includes these two views in an emancipated approach of a solution, and in some manner, could be introduced as a business oriented extension of the middleware concept. As the native information systems of partners are considered as predefined components that are not supposed to be modified when they participate in any collaborations according to an interoperability philosophy, most of the collaborative information system engineering effort is linked to the MIS design and operation. The MIS engineering process that we used as a main guideline is depicted in Figure 1. It follows Model Driven Engineering (MDE) principles [5].
Insert Figure 1 here
Classically, the model driven architecture consists of three abstraction levels: Computer Independent Model (CIM), Platform Independent Model (PIM), and Platform Specific Model (PSM). The work presented in this paper address the transition from the knowledge available before the preliminary MIS design and expressed by the partners to the result expected at the CIM level. Many different works have been performed, in complement to the one presented here, to put all the steps depicted in Figure 1 into practice. We will discuss during our presentation on some of the constraints that are imposed by the compatibility with the other steps, from CIM to PIM, and from PIM to PSM. Let us simply consider here that PIM has been defined using a Service Oriented Architecture (SOA), and PSM has been conceived with an Enterprise service Bus (ESB) as an ultimate target platform [3].
According to [5], the CIM is a model of a system that shows the system in the environment where it will operate. It helps in understanding a problem and defining a shared vocabulary for use in the models of the other levels. In our case, this CIM level concerns the organization, objectives, business, processes, and responsibilities. Thus, collaborative process model has been selected as a good candidate for representing interactions between enterprises, including the data exchanges, resources exposed to others. The CIM will be represented using the BPMN modelling formalism. So, our objective is to be able to catch, adapt and transform different kinds of knowledge about the collaboration with the aim to produce a CBP model, compliant with the requirements of the CIM level of our MIS engineering approach. A knowledge-based system (KbS) has been developed in order to support the design of this CBP model. The system consists of three parts (Figure 2): 1) knowledge gathering to build the collaborative network model, 2) knowledge representation and reasoning to define fragments of the future CBP model, and 3) collaborative process modelling that will assemble the result obtained from the second part. The core of the KbS is the knowledge representation and reasoning part which delivers candidate fragments of the final CBP model from the collaborative network model using an ontology-based approach.
Insert Figure 2 here
The last part of the KbS, the collaborative process modelling, has been divided into two phases: specific knowledge extraction and BPMN construction. This leads us to obtain finally the four main functionalities as shown at the bottom of Figure 2. A prototype has been conceived, developed and implemented in order to support these four functionalities. The technical architecture of the prototype is shown below:
SVM in disease prediction (detection of cancer). Pattern classification problems can be effectively solved by Support vector machines. Here we propose a classifier which can differentiate patients having benign and malignant cancer cells. To improve the accuracy of classification, we propose to determine the optimal size of the training set and perform feature selection. To find the optimal size of the training set, different sizes of training sets are experimented and the one with highest classification rate is selected. The optimal features are selected through their F-Scores. KEYWORDS: Pattern classification, Gist SVM, F-Score
Introduction
Pattern classification problems can be effectively solved by Support Vector Machines (SVMs) [CV95; Vap98]. SVMs find applications in data mining, bioinformatics, computer vision, and pattern recognition. There is a need to accelerate SVM training as the size of the training data sets is becoming larger.
In this paper, we study the application of SVM in the prediction of
cancer using the GIST SVM. SVM is a class of learning methods that can be used for classification. The problem of the optimal feature selection plays a vital role in SVM which ensures high performance classification. Nonoptimal features ensure robustness since SVMs are trained for maximized margins.
This paper is structured as follows. Section 1 gives a brief
introduction to SVM classifier and the f-score method used for feature selection, In Section 2, we discuss the results obtained and concluding remarks are given in Section 3 to address further research issues.

We now present our detailed empirical study using realworld datasets on 10 classifiers, including 7 high-capacity classifiers (CART decision tree with gini, information gain, and gain ratio; SVM with RBF and quadratic kernels; multilayer perceptron ANN; the “braindead” 1-nearest neighbor), and 3 linear classifiers (Naive Bayes with backward selection, logistic regression with L1 regularization, and linear SVM). We also conducted experiments with a few other feature selection techniques for the linear classifiers: Naive Bayes with forward selection and filter methods, as well as logistic regression L2 regularization Since these additional linear classifiers did not provide any new insights, we omit them due to space constraints.
In this section we discuss our online algorithm for the adversarial model, which is given in Algorithm 2. The algorithm is a version of Online Mirror Descent with adaptive regularization. It maintains a positive-definite matrix H, which is being updated whenever the newly observed loss vector ℓt is not in the span of previously appeared losses. In all other time steps—i.e., when ℓt remains in the previous span—the algorithm preforms an Online Mirror Descent type update (see Algorithm 1), with the function }x}2H “ xTHx as a regularizer.
The algorithm updates the regularization matrix H so as to adapt to the low-dimensional geometry of the set of feasible loss vectors. Indeed, as our analysis below reveals, H is an ellipsoidal approximation of a certain low-dimensional convex set in RN to which the loss vectors ℓt can be localized. This low-dimensional set is the intersection of the unit cube in N dimensions— in which the loss vectors ℓt reside by definition—and the low dimensional subspace spanned by previously observed loss vectors, given by spanpUq. Whenever the latter subspace changes, namely, once a newly observed loss vector leaves the span of previous vectors, the ellipsoidal approximation is recomputed and the matrix H is updated accordingly.
Algorithm 2 Online Low Rank Experts
1: Initialize: x1 “ 1N 1N , τ “ 0, k “ 0, U “ tu 2: for t “ 1 to T do 3: Observe ℓt, suffer cost xt ¨ ℓt. 4: if ℓt R spanpUq then 5: Add ℓt as a new column of U , reset τ “ 0, and set k Ð k ` 1. 6: Compute M “ MVEEpUTq and H “ In ` UTMU . 7: end if 8: let τ Ð τ ` 1 and ηt “ 4 a k{τ , and set:
xt`1 “ arg min xP∆N ℓt ¨ x` η´1t }x´ xt}2H .
9: end for
To derive Theorem 1, we begin with analyzing a simpler case where the learner is aware of the subspace from which losses are derived. Specifically, assume that at the beginning of the rounds, the learner is equipped with a rank d matrix U such that for all losses ℓ1, ℓ2, . . . P spanpUq where we denote by spanpUq the span of the columns of the matrix U .
In this simplified setting, we can obtain a regret bound of Op ? dT q via John’s theorem (Theorem 3).4 As discussed above, the loss vectors ℓ1, . . . , ℓT can be localized to the intersection of the unit cube in N dimensions with the d-dimensional subspace spanned by the columns of U . Then, John’s theorem asserts that the minimal-volume enclosing ellipsoid of the intersection is a ? d-approximation to the set of feasible loss vectors.
4We remark that for the simplified setting, the Op ? dT q regret bound is in fact tight, as our Ωp ? dT q lower bound (given in Section 5) applies in a setting where the subspace of the loss vectors is known a-priori to the learner.
Theorem 8. Run Algorithm 1 with Input H, tηtu and x1 defined as follows: (i) H “ In ` UTMU , where M “ MVEEpUTq, (ii) ηt “ 4 a
d{t, where d “ rankpUq, and (iii) x1 P ∆ is arbitrary. If ℓ1, . . . , ℓT P spanpUq , then the expected T -round regret of the algorithm is at most 8 ? dT .
Proof. Consider the d-dimensional polytope
P “ tv P Rd : }UTv}8 ď 1u.
Then by John’s Theorem (Theorem 3), we have,
Ep 1 2d Mq Ď P Ď EpMq . (2)
In order to apply Lemma 5, we need to bound both }ℓt}˚H and }x1 ´ x˚}2H . We first bound the norms }ℓt}˚H . Notice that for each loss vector ℓt there exists vt P P such that ℓt “ UTvt (as ℓt P spanpUq and }ℓt}8 ď 1). Thus, we can write,
p}ℓt}˚Hq2 “ ℓTt H´1ℓt “ vTt UpIn ` UTMUq´1UTvt ď vTt UpUTMUq:UTvt “ vTt M´1vt ,
where we have used Lemma 11 (see Appendix A). Now, since vt P P and EpMq is enclosing P , we obtain vTt M ´1vt ď 1. This proves that
p}ℓt}˚Hq2 ď 1.
Next we bound }x1 ´ x˚}H ď 2 Since }x1 ´ x˚}H ď 2maxxP∆n }x}H , it suffices to bound maxxP∆n }x}H . Hence, our goal is to show that }x}H ď 2 ? d for all x P ∆n. Since }x}2H “ 1` 2d }x}2H 1 with H 1 “ 12dUTMU , it is enough to bound the norm }x}2H 1 . Given a convex set in R d, recall that the dual set is given by
P ˚ “ tx : sup pPP |x ¨ p| ď 1u.
The dual of an ellipsoid EpMq is given by pEpMqq˚ “ EpM´1q and it is standard to show that Eq. (2) implies in the dual:
pEpMqq˚ Ď P ˚ Ď pEp 1 2d Mqq˚.
Taken together we obtain that P ˚ Ď Ep2dM´1q. Note that by definition the columns of U are in P ˚, hence, for every ui, }ui}2M ď 2d. Since x P ∆N ,
}x}2H 1 “ 12d}Ux} 2 M ď 12d maxi }ui} 2 M ď 1 .
Equipped with the bounds }x}H ď ? 1` 2d ď 2 ? d for all x P ∆n and }ℓt}˚H ď 1 for all t, we are now ready to analyze the regret of the algorithm, which via Lemma 5 can be bounded as follows:
RegretT “ T ÿ
t“1 ℓt ¨ xt ´
T ÿ t“1 ℓt ¨ x˚
ď 1 ηT }x1 ´ x˚}2H ` 1 2
T ÿ t“1 ηtp}ℓt}˚Hq2 7 Lemma 5
ď 4 ηT max xP∆n }x}2H ` 1 2
T ÿ t“1 ηtp}ℓt}˚Hq2 7 }x1 ´ x˚}H ď 4 max xP∆n }x}H
ď 16d ηT ` 1 2
T ÿ t“1 ηt 7 max xP∆n }x}2H ď 4d, }ℓt}˚H ď 1.
A choice of ηt “ 4 a d{t, together with the inequality řTt“1 1{ ? t ď 2 ? T , gives the theorem.
The d-low rank setting does not assume that the learner has access to the subspace U , and potentially an adversary may adapt her choice of subspace to the learner’s strategy. However, the learner can still obtain regret bounds that are independent of the number of experts. We are now ready to prove Theorem 1.
Proof of Theorem 1. Let t0 “ 1, td`1 “ T and for all 1 ď k ď d let tk be the round where the k’th column is added to U . Also, let Tk “ tk`1 ´ tk the length of the k’th epoch. Notice that between rounds tk and tk`1 the algorithm’s execution is identical to Algorithm 1 with input depicted in Theorem 8. Therefore its regret in this time period is at most 8 ? kTk. The total regret is then bounded by
8 d ÿ
k“0
a
kTk ď 8
g f f e d ÿ
k“0 k ¨
g f f e d ÿ
k“0 Tk ď 8d
? T ,
and the theorem follows.
We have presented a principled method to learn the structure of a synchronous GDS based on collective transfer entropy and independence tests. We derived this method analytically by reformulating the KL divergence of factorised from joint distributions of a network, which Theorem 4 shows can be computed in terms of stochastic interaction and transfer entropy. We arrived at this result by first reconsidering the GDS as a DBN, and then employed generalised versions of Takens’ embedding theorem to compute densities comprising hidden and observed variables.
The decomposition of KL divergence in Theorem 4 captures an interesting parallel between fully observable systems and partially observable systems. De Campos [50] showed previously that the KL
divergence in a fully observable system is given by the difference between multi-information [64] and mutual information.4 Specifically, a condition for generalised Takens’ theorems to hold is that the observation functions {ψi} are injective [57, 33]. We conjecture that if the functions are also surjective (i.e., there is a one-to-one mapping between state and observation), the embedding dimension would reduce to unity and we would arrive at the MIT scoring function.
In Corollary 4.1, we have shown that, under certain circumstances, maximising collective transfer entropy minimises the KL divergence of a model from the true distribution. KL divergence is related to model encoding, which is a fundamental measure used in complex systems analysis. Our result, therefore, has potential implications to other areas of complex systems research. For example, the notion of equivalence classes in BN structure learning should lend some insight into the area of effective network analysis [34, 35]. We believe the concepts of effective networks referred to in complex systems literature can be unified with essential graphs and RPDAGs. This would allow for a more rigorous definition of effective networks and a benchmark for analysing the efficacy of an algorithm to reconstruct these networks.
We have presented the TEA (32) and TEE (33) scores above based on the MIT scoring function [50]. These scoring functions, however, could be considered to be a generalisation of MIT. There are numerous approaches to recover the time delay τ and embedding dimension κ for use in transfer entropy [65, 66]. Given a system of fully observed variables, these criteria should optimally select no embedding dimension or time delay, and thus as a special case of our scores we obtain the MIT algorithm with time-lagged mutual information.

We have selected six open UCI datasets, which are often used in clustering tasks.
Banknote Authentication. The data were extracted from images that were taken from genuine and forged banknote-like specimens and Wavelet Transform tool were used to extract features from images. there are 1,372 items with 5 attributes for binary classes.
Glass. The study of classification of types of glass was motivated by criminological investigation. At the scene of the crime, the glass left can be used as evidence. We use this dataset to testify our clustering method. There are 214 instances with 10 attributes for 6 classes.
Red Wine Quality And White Wine Quality. The two datasets are related to red and white variants of the Portuguese ”Vinho Verde” wine. There are 1,599 instances with 11 attributes in the first dataset for 4 classes. And there are 4,899 instances with 11 attributes in the second dataset for 5 classes.
Image Segmentation. The instances were drawn randomly from a database of 7 outdoor images. The images were hand-segmented to create a classification for every pixel. There are 2,310 instances with 18 attributes for 7 classes.
MAGIC Gamma Telescope. It is generated to simulate registration of high energy gamma particles in a ground-based atmospheric Cherenkov gamma telescope using the imaging technique. There are 19,020 items with 11 attributes for binary classes.
We have shown that weight pruning with retraining is a highly effective method of compression and regularization on a state-of-the-art NMT system, compressing the model to 20% of its size with no loss of performance. Though we are the first to apply compression techniques to NMT, we obtain a similar degree of compression to other current work on compressing state-of-the-art deep neural networks, with an approach that is simpler than most. We have found that the absolute size of parameters is of primary importance when choosing which to prune, leading to an approach that is extremely simple to implement, and can be applied to any neural network. Lastly, we have gained insight into the distribution of redundancy in the NMT architecture.
The algorithm (44) differs from related earlier algorithms such as TD(λ) in a few subtle but important ways. The most notable differences are the updates to the traces and the averaging due to βt. Known results on convergence therefore do not automatically transfer to this new algorithm and it is appropriate to take a moment to analyze it.
The convergence of the trusted weights θ depends on the convergence of the online weights θ̃ and so we must investigate these jointly. The online weights, in turn, depend on the sequences of parameters and residual predictions that are supplied. We want our analysis to be general, which means we want to be able to handle general sequences of discounts {γt}∞t=1, persistency parameters, {λt}∞t=1, and residual predictions {Pt}∞t=1. Naturally, if any of these can change completely arbitrarily, we can have no hope of converging to any predeterminable solution. Therefore, as in Sutton et al. (2014), we allow the features, discounts and persistency parameters to be stationary functions of an underlying unobserved state, such that φt . = φ(St), γt . = γ(St) and λt . = λ(St) for some fixed functions φ : S → Rn, γ : S → [0, 1] and λ : S → [0, 1], where S is a state space and St ∈ S is the, unobserved, state of the world at time t. We assume there is a steady-state distribution over these states such that all expectations used below are well-defined with respect to a distribution over states defined by limt→∞ Pr(St = s). This setting generalizes the more standard approach where γt = γ and λt = λ are constants, because now these parameters can still change over time, but it avoids the possibility of arbitrary non-stationarity that would ruin convergence.
We first consider convergence when the residual predictions are also due to a fixed function of state, for instance because they are due to otherwise stationary experts or oracles. Theorem 1. Let Xt . = X(St), φt . = φ(St), γt . = γ(St), λt . = λ(St) and Pt . = P (St) all be fixed functions of (unobserved) states St ∈ S, with a stable steady-state distribution d. Then, if ∑∞ t=0 αt = ∑∞ t=0 βt = ∞, and ∑∞ t=0 α 2 t < ∞, algorithm (44) converges almost surely to the fixed-point solution
θ∗ . = E[φtφt]−1 E[Z∞t φt] .
Proof. We start by analyzing the online weights θ̃t. Because of the equivalence of the forward and backward views, we can investigate the forward view, which is easier to
analyze. In other words, instead of investigating limt→∞ θ̃t as updated through (44), we investigate limt→∞ θ̃ t t as updated through (37). By construction, the end result is exactly the same. The asymptotic forward view as the horizon goes to infinity is
θ̃∞t+1 = θ̃ ∞ t + αtφt(Z ∞ t − φ>t θ̃∞t ) , t = 0, . . . ,
where Z∞t . = lim h→∞ Zht , t = 0, . . . .
Because Zht does not depend on the weights, this is a standard stochastic gradient-descent update θ̃t+1 = θ̃t − αt∇θ̃l(θ̃)|θ̃t on the quadratic loss function
l(θ̃) . = E [ (Z∞t − φ>t θ̃)2 ] .
If the step sizes are suitably chosen, for instance such that ∑∞ t=0 αt =∞ and ∑∞ t=0 α 2 t <∞ (Robbins and Monro 1951), and if the means and variances of Z∞t and φt are well-defined and bounded for all t, this update converges to the fixed-point solution θ∗ that minimizes the quadratic loss (cf. Kushner and Yin 2003), such that
lim t→∞
θ̃t = θ∗ . = E [ φtφ > t ]−1 E[φtZ∞t ] . It is straightforward to see that θt will have the same limit; it suffices to have ∑∞ t=0 βt = ∞.
Although convergence is already guaranteed when βt = 1 for all t, recent work has shown that for similar stochastic gradient algorithms the optimal rate of convergence is attained if βt decreases much faster than αt, specifically when βt = O(t
−1) while αt = α for some constant α (Bach and Moulines 2013). More generally, it seems likely that convergence also holds if ∑∞ t=0 β 2 t <∞ and ∑∞ t=0 α 2 t =∞. The observation that βt should perhaps decrease over time for faster learning may seem at odds with our introduction of this parameter as a degree of trust. However, these two views are quite compatible if we consider βt to be the degree of trust we place in the online updates relative to the trust we place in our current predictions due to the trusted weights. When the trust in the predictions increases over time, the relative trust in the inherently noisy online targets should then decrease.
Although Theorem 1 is already fairly general, it does not cover the important case when the residual predictions additionally depend on the weights we are updating. It makes sense to use the predictions we trust most and therefore we now consider what happens when Pt . = φ>t θt−1. Notice that we have to use θt−1 rather than θt, because Pt is used in the computation of θt and so the latter is not yet available when we compute Pt. The analysis of this case is more complex than the previous one, because Pt is no longer a constant function of state. This means the update is no longer a standard gradient-descent update on a quadratic loss, because the target Zht for the online weights itself depends on the trusted weights that we are simultaneously updating.
The results by Bach and Moulines (2013) on stochastic gradient descent indicate that perhaps the most interesting case is where βt decreases faster than αt, such that limt→∞ βt/αt = 0. This suggests an analysis on two time scales is appropriate.
Theorem 2. Let Xt . = X(St), φt . = φ(St), γt . = γ(St) and λt . = λ(St) all be fixed bounded functions of (unobserved) states St ∈ S, with a stable steady-state distribution d. Define Pt . = φ>t θt−1. Then, if ∑∞ t=0 αt = ∑∞ t=0 βt = ∞, ∑∞ t=0 α 2 t < ∞, and limt→∞ βt αt
= 0, algorithm (44) converges almost surely to the TD fixed-point solution θ∗ that minimizes the mean-squared projected Bellman error (Sutton, Szepesvári, and Maei 2008; Sutton et al. 2009), such that
E [ (Zt(θ∗)− φ>t θ∗)φ>t ] E[φtφt]−1 E [ φt(Zt(θ∗)− φ>t θ∗) ] = 0 , (45)
where Zt(θ) . = Xt+1 + γt+1(1− λt+1)φ>t θ + Zt(θ) , ∀θ .
Proof. In two-time-scale analyses, we are allowed to analyze the faster updates as if the slower updates have stopped. This means that in analyzing the updates to the online weights θ̃t, we can assume the trusted weights θt are constant to analyze where θ̃t converges towards as a function of the stationary θt. On the other hand, when we analyze the slower updates to the trusted weights θt we are allowed to assume the faster updates to the online weights converge completely between each two steps. For more detail on analyzing stochastic approximations on two times scales, we refer to Borkar (1997), Borkar (2008), Kushner and Yin (2003), and Konda and Tsitsiklis (2004).
We first analyze the convergence of the faster updates to the online weights, where we can assume that the trusted weights are stationary at some value θ. Then, using Pt . = φ>t θ, the targets for the updates of the forward view are
Ztt (θ) = φ > t θ , Zht (θ) = Xt+1 + γt+1(1− λt+1)φ>t θ + γt+1λt+1Zht+1(θ) ,
where we have extended the notation slightly to make the dependence of Zht (θ) on θ explicit. Notice that the residual predictions on each time step depend on the same stationary trusted weights θ. Because of the assumed stationarity of θ, the updates to the weights θ̃ht of the forward view can again be considered standard stochastic gradient updates and therefore these weights converge towards the fixed point θ̃∗(θ), where again we make the dependence on θ explicit, defined by
θ̃∗(θ) = E [ φtφ > t ]−1 E[φtZ∞t (θ)] , where Z∞t (θ) = limh→∞ Z h t (θ) denotes the limit of the target of the update as the horizon grows to infinity. In an episodic setting, Z∞t = Z T t for all t < T , where T denotes the first
termination after time k. More in general, Z∞t is always well-defined because we require∏∞ t=0 γt = 0. For the analysis of the slower updates to θt, we can now assume the faster time scale has already converged to its fixed point θ̃∗(θt) for the current weights. Therefore, we analyze the update
θt+1 = θt + βt+1(θ̃∗(θt)− θt) = θt + βt+1(E [ φkφ > k ]−1 E[φkZ∞k (θt)]− θt) .
This is a stochastic-approximation update that, under the conditions that ∑∞ t=0 βt = ∞
and ∑∞ t=0 β 2 t <∞, converges almost surely to the fixed point θ∗ that satisfies
θ∗ = E [ φtφ > t ]−1 E[φtZ∞t (θ∗)] . If we multiply both sides with E [ φtφ > t ] , this implies that E [ φtφ > t θ∗ ] = E[φtZ∞t (θ∗)] and
therefore, by moving both terms to the same side and then multiplying with E [ φtφ > t ]−1 ,
E [ φtφ > t ]−1 E[φt(Z∞t (θ∗)− φ>t θ∗)] = 0 . It follows immediately that θ∗ minimizes the mean-squared projected Bellman error completely to zero, as desired.
While we realize there are different ways to classify existing security problems based on purpose, mechanism, targeted assets, and point of flow of the attack, our SoK’s section structure is based on the “Security and Privacy” category of 2012 ACM Computing Classification System[8], which is a
ar X
iv :1
61 1.
03 18
6v 1
[ cs
.C R
] 1
0 N
ov 2
01 6
combination of specific use cases(e.g. malware, phishing), technique (e.g. information flow), and targeted assets(e.g. web application, proxies). We present the state-of-art ML applications in security as the following: Section 3 and Table 2 & 3 discusses Network Security1, Section 4 and Table 4 surveys Security Services, Section 5 and Table 5 specifies advances in Software & Applications Security, Section 6 and Table 6 & 7 lays out taxonomy for System Security, and Section 7 and Table 8, 9 & 10 summarizes progress since 2008 in Malware Detection, IDS, and Social Engineering. Throughout the survey, we share our frameworks for ML system designs, assumptions, and algorithm deployments in security.
We focus our survey on security applications and securityrelated ML and AI problems on the defense side, hence our scope excludes theories related to security such as differential privacy and privacy-preservation in ML algorithms[9, 10, 11], and excludes ML applications in side channel attacks such as [12, 13, 14]. Partly because there is already a 2013 SoK on evolution of Sybil defense[15] in online social networks(OSN), and partly because we would like to leave it as a small exercise to our readers, we excluded Sybil defense schemes in OSN as well[16, 17, 18, 19, 20]. Still with a broad base, we propose an alternative position to frame security issues, and we also recommend a taxonomy for ML applications in security use cases. Yet, we do not conclude with a terminal list of “right” or “correct” approaches or methods. We believe that the range of the applications is too wide to fit into one singular use case or analysis framework. Instead, we intend this paper as a systematic design and method overview of thinking about researching and developing ML algorithms and applications, that will guide researchers in their problem domains on an individual basis. We target our work to security researchers and practitioners, so we assume that our readers have general knowledge for key security domains and awareness of common ML algorithms, and we also define terms when needed.
We agree with assessment of top conferences in [21]2. We
1All papers are listed in chronological order with the first author’s last name followed by venue acronym and year. 2We use the same conference-ranking websites to first decide our list of top-level conferences:(1)Microsoft Academic Search - Top Conferences in Security & Privacyhttp://academic.research.microsoft.com/ RankList?entitytype=3&topDomainID=2&subDomainID=2, (2)Guofei Gu - Computer Security Conference Ranking and Statistichttp://faculty.cs.tamu.edu/guofei/sec_
systematically went through all proceedings between 2008 and early 2016 of the top 6 network- and computer-security conferences to collect relevant papers. Because of KDD’s early and consistent publication record on ML applications in Security, and its status as a top-level venue for ML and DM applications, we also include KDD’s 2008-2015 proceedings. To demonstrate the wide-ranging research attention drawn to ML applications in security, we also added chosen selections from the workshop AISec, International Conference on Machine Learning(ICML), Neural Information Processing Systems(NIPS), and Internet Measurement Conference(IMC) papers between 2008-2015, mostly in the “Future Development” section.
The objective is to take account into the influence of pedagogical context on features of the text. For this we introduce the notion of facet-prism.
to be viable in many scenarios robots need to perform complex manipulation tasks. These complex manipulations need high degree-of-freedom arms and manipulators. For example, the PR2 robot has two 7 DoF arms. When learning position, velocity and acceleration control, this leads to a
21 dimensional state space per arm. Learning in these high-dimensional spaces is computationally intractable without optimization techniques.
To learn in high-dimensional state spaces, our algorithm first computes a transform between the high-dimensional state space and a lower-dimensional space. To perform this computation, we need trajectories across a representative set of the agent’s state space. We can then use any dimensionality reduction technique to learn the transform. In this work, we use Principal Component Analysis (PCA) (Shlens, 2005).
PCA identifies patterns in data and reduces the dimensions of the dataset with minimal loss of information. It does this by computing a transform to convert correlated data to linearly uncorrelated data. This transformation ensures that the first principal component has the largest possible variance. Each additional component has the largest possible variance uncorrelated with all previous components. Essentially, PCA represents as much as the demonstrated state space as possible in a lower dimension. This transform is given by:
T = XW (1)
where X is the demonstrated data, W is a p by p matrix whose columns are eigenvectors of XTX and p is the number of principle components (in this case, the number of dimensions).
To transform to any arbitrary dimension, k, we can choose k eigenvectors from W with the largest eigenvalues to form a p by k dimensional matrix Wk:
Tk = XWk (2)
We then use reinforcement learning to learn trajectories in the new manifold. All learning is in the lower-dimensional space. For each learning iteration, we project state x down to the lowerdimensional space k:
xk =W T k x (3)
We can now compute the action using the chosen learning algorithm, and execute that action in the simulation. The simulation calculates the new state given the executed action, and we can project that state down to the same lower-dimensional space. We can then perform a learning update (Figure 1).
By learning in a smaller space, reinforcement learning algorithms will converge must faster. However, PCA cannot represent all the variance in all the demonstrations. Therefore, given infinite time, the converged learning performance will always be worse than learning in the full space. This leads to a critical trade-off. By projecting onto a low-dimensional manifold, we are throwing out low variance, yet possibly important data. Yet, learning can still converge to a good, yet suboptimal, policy much faster.
Based on the memory model developed in the previous section (as summarized in Eqn. 2), we now present a stochastic model for a spaced repetition system, and outline how we can use it to design good review scheduling policies. We note again that all existing schemes for assigning review frequencies to decks in the Leitner system, and in fact, in all other spaced repetition systems, are based on heuristics with no formal optimality guarantees. One of our main contributions is to provide a principled method for determining appropriate schedules for spaced repetition systems.
We focus on a regime where the learner wants to memorize a very large set of items – in particular, the number of available items is much larger than the potential amount of time spent by the learner in memorizing them. A canonical example of such a setting is learning words in a foreign language. From a technical point of view, this translates to assuming that new items are always available for introduction into the system, similar to an open queueing system (i.e., one with a potentially infinite stream of arrivals). Moreover, this allows us to use the steady-state performance of the queue as a measure of performance of the scheduler as an appropriate metric in this setting. We refer to this as the long-term learning regime.
As mentioned before, our model is based on the Leitner system [13], one of the simplest and oldest spaced repetition systems. It comprises of a series of n decks of flashcards, indexed as {1, 2, . . . , n}, where new items enter the system at deck 1, and items upon being reviewed either move up a deck if recalled correctly or down if forgotten. In principle, the deck numbers can extend in both directions; in practice however, they are bounded both below and above – we follow this convention and assume that items in deck 1 are reflected (i.e., they remain in deck 1 if they are incorrectly reviewed), and all items which are recalled at deck n (which in experiments we take as n = 5), are declared to be ‘mastered’ and removed from the system. For simplicity of exposition, we also assume that the difficulty parameter θ is the same for all items (i.e., model 8 in Table 2), but we will discuss later how to allow for item-specific difficulties (i.e., model 13 in Table 2).
The backend interface that we defined while vectorizing DiffSharp allows us to plug in other computation backends that the user can select to run their AD code. Our current work on DiffSharp includes the implementation of a CUDA-based backend using cuBLAS for BLAS operations, custom CUDA kernels for non-BLAS operations such as element-wise function application, and cuDNN for convolution operations.
We presented the Transductive and Inductive Conformal Predictors based on the k-Nearest Neighbours Regression algorithm. In addition to the typical regression nonconformity measure, we developed six novel definitions which take into account the expected accuracy of the k-NNR algorithm on the example in question. Our definitions assess the expected accuracy of k-NNR on the example based on its distances from its k nearest examples (24) and (25), on the standard deviation of their labels (29) and (30), or on a combination of the two (31) and (32).
The experimental results obtained by applying our methods to various data sets show that in all cases they produce reliable predictive intervals that are tight enough to be useful in practice. Additionally, they illustrate the great extent to which our new nonconformity measure definitions improve the performance of both the transductive and inductive method in terms of predictive region tightness. In the case of the ICP, with which all new measures were evaluated, definitions (31) and (32) appear to be superior to all other measures, giving the overall tightest predictive regions. Moreover, a comparison between the TCP and ICP methods suggests that, when dealing with relatively large data sets the use of nonconformity measures (31) and (32) makes ICP perform equally well with TCP in terms of predictive region tightness, whereas it has a vast advantage when it comes to computational efficiency. Finally, a comparison with Gaussian Process Regression (GPR) demonstrated that our
methods produce almost as tight predictive regions as those of GPR when the correct prior is known, while GPR may produce misleading regions on real world data on which the required prior knowledge is not available.
The main future direction of this work is the development of normalized nonconformity measures like the ones presented in this paper based on other popular regression techniques, such as Ridge Regression and Support Vector Regression. Although in the case of Ridge Regression one such measure was already defined for ICP (Papadopoulos et al., 2002a), it unfortunately cannot be used with the TCP approach; thus there is potentially a considerable performance gain to be achieved from a definition of this kind for TCP. Moreover, an equally important future aim is the application of our methods to medical or other problems where the provision of predictive regions is desirable, and the evaluation of their results by experts in the corresponding field.
I am grateful to the anonymous reviewers for suggestions to improve this article. This work was supported by the Austrian Science Fund (FWF) under grants P18019 and P20841, and by the Vienna Science and Technology Fund (WWTF) under project ICT08-020.
In unimodal enhancement task, we used EEG signals to reconstruct information of two modalities. Once the DAE network was trained, we could use it as a feature selector to generate shared representations, even if only one modality information is available. For eye movement data, the process was the same as when only EEG signals were available.
Figure 2 is the summary of all unimodal enhancement results. We can see from Figure 2 that the DAE model performed best on both EEG features and eye movement features.
For EEG-based unimodal enhancement experiments, we constructed an affective model using EEG features of different frequency bands. The experimental results are shown in Table 2. After that an EEG-based DAE network was built to reconstruct both EEG and eye movement features and the shared representations were used as new features to classify emotions. The EEG-only DAE results are shown in Table 3.
For eye-based unimodal enhancement experiments, the processes were the same. We carried out experiments using eye movement features. We linked all eye movement features listed in Table 1 together to classify different emotions and the recognition accuracy of 79.64% is achieved. Then, eye movement features were used to train the DAE network to reconstruct both EEG and eye movement features. Emotion recognition accuracies, as shown in Table 4, were got with shared representations.
When only EEG features were used, we can see from Tables 2 and 3, the DAE network increased the recognition rate from 77.64% to 81.19% and the standard deviation for the best accuracy is 13.82, which is smaller than 17.19. We also compared our results with [Lu et al., 2015].
In [Lu et al., 2015], the best result achieved when only EEG signal used was 78.51% and the standard deviation for its best accuracy was 14.32. It is clear that the DAE network is supe-
rior to the state-of-the-art approach. When only eye movement data were available, the DAE network achieved the highest accuracy of 82.11% (in Table 4) in comparison with the state-of-the-art approach [Lu et al., 2015] (77.80%) and directly using eye movement features (79.46%).
Many applications of RNNs involve text processing. Some applications, e.g. image captioning, involve generating strings of text. Others, such as machine translation and dialogue systems, require both inputting and outputting text.
In this section, we provide the background necessary to understand how text is represented in most recurrent net models.
When words are output at each time step, generally the output consists of a softmax vector y(t) ∈ RK where K is the size of the vocabulary. A softmax layer is like an element-wise logistic function that is normalized so that all of its components sum to 1. Intuitively these outputs correspond to probabilities that each word is the correct output at that time step.
For application where inputs consist of sequences of words, typically the words are fed to the network one at a time as inputs in consecutive time steps. In these cases, the simplest way to represent words is a one-hot encoding, using binary vectors with a length equal to the size of the vocabulary, e.g. “1000” and “0100” would represent the first and second words in the vocabulary respectively. Such an encoding is discussed in [18] among others. However, this encoding is inefficient, requiring as many bits as the vocabulary is large. Further, it offers no direct way to capture different aspects of similarity between words in the encoding itself. Thus it is more common to model words with a distributed representation using a meaning vector. In some cases, these meanings for words are learned given a large corpus of supervised data, but it is common to initialize the meaning vectors using an embedding based on word co-occurence statistics. Freely available code to produce word vectors from co-occurrence stats include Glove from Pennington Socher and Manning [51], and word2vec [23], which implements a word embedding algorithm from Mikolov et al. [46].
Such distributed representations for symbolic data were described by Hinton in 1986 [29], used extensively for natural language by Bengio et al. in 2003 [8], and more recently brought to wider attention in the deep learning community by Socher, Manning and Ng in a number of papers describing Recursive Autoencoder (RAE) networks ([59], [56], [60], [58]). For clarity we point out that these recursive networks, which are sometimes referred to as “RNNs” are not recurrent neural nets. In contrast to RNNs which model arbitrary dependencies between inputs at different points in the sequence, recursive networks assume a tree structure where each word in a sentence is a leaf and each sentence can be represented as a binary tree. In this model, each internal node has a meaning vector which can be calculated by concatenating the meanings of its children and multiplying the composite vector by an encoding matrix. Each meaning vector mj ∈ Rd and the encoding matrix Ae ∈ R2d×d. The composite meaning mp ∈ Rd given to the parent p of two child nodes l and r in the syntax tree is
mp = σ(Ae[ml;mr] T )
This method seems restricted to sentences, and incompatible with the online setting where the full sequence may not be fixed in advance. Additionally, while it has been successfully used for classification tasks, it offers no generative model for composing text.
In many experiments with recurrent neural networks ([18], [63], [71]), input is fed in one character at a time (and output generated one character at a time). While the output is nearly always a softmax layer, many papers omit details
of how they represent single-character inputs. It seems reasonable to infer that characters are encoded with a one-hot encoding. We know of no cases of paper using a distributed representation at the single-character level.
Given a function κ : X × X → F and x1, . . . , xN ∈ X, the matrix1 K = (Ki,j)N with elements Ki,j = κ(xi, xj), for i, j = 1, . . . , N , is called the Gram matrix (or kernel matrix) of κ with respect to x1, . . . , xN . A Hermitian matrix K = (Ki,j)N satisfying
cH ·K · c = N,N ∑
i=1,j=1
c∗i cjKi,j ≥ 0,
for all ci ∈ F, i = 1, . . . , N , where the notation ∗ denotes the conjugate element, is called Positive Definite. In matrix analysis literature, this is the definition of a positive semidefinite matrix. However, since this is a rather cumbersome term and the distinction between positive definite and positive semidefinite matrices is not important in this paper, we employ the term positive definite in the way presented here. Furthermore, the term positive definite was introduced for the first time by Mercer in the kernel context (see [26]). Let X be a nonempty set. Then a function κ : X ×X → F, which for all N ∈ N and all x1, . . . , xN ∈ X gives rise to a positive definite Gram matrix K, is called a Positive Definite Kernel. In the following, we will frequently refer to a positive definite kernel simply as kernel.
Next, consider a linear class H of complex valued functions f defined on a set X. Suppose further, that in H we can define an inner product 〈·, ·〉H with corresponding norm ‖ · ‖H and that H is complete with respect to that norm, i.e., H is a Hilbert space. We call H a Reproducing Kernel Hilbert Space (RKHS), if for all y ∈ X the evaluation functional Ty : H → F : Ty(f) = f(y) is a linear continuous (or, equivalently, bounded) operator. If this is true, then by the Riesz’s representation theorem, for all y ∈ X there is a function gy ∈ H such that Ty(f) = f(y) = 〈f, gy〉H. The function κ : X ×X → F :
1The term (Ki,j)N denotes a square N ×N matrix.
κ(x, y) = gy(x) is called a reproducing kernel of H. It can be easily proved that the function κ is a positive definite kernel.
Alternatively, we can define a RKHS as a Hilbert space H for which there exists a function κ : X ×X → F with the following two important properties:
1) For every x ∈ X, κ(·, x) belongs to H. 2) κ has the so called reproducing property, i.e.,
f(x) = 〈f, κ(·, x)〉H, for all f ∈ H, (1)
in particular κ(x, y) = 〈κ(·, y), κ(·, x)〉H .
It has been proved (see [27]) that to every positive definite kernel κ there corresponds one and only one class of functions H with a uniquely determined inner product in it, forming a Hilbert space and admitting κ as a reproducing kernel. In fact, the kernel κ produces the entire space H, i.e., H = span{κ(x, ·)|x ∈ X}. The map Φ : X → H : Φ(x) = κ(·, x) is called the feature map of H. Recall, that in the case of complex Hilbert spaces (i.e., F = C) the inner product is sesqui-linear (i.e. linear in one argument and antilinear in the other) and Hermitian:
〈ax+ by, z〉H = a〈x, z〉H + b〈y, z〉H,
〈x, ay + bz〉H = a∗〈x, y〉H + b∗〈x, z〉H,
〈x, y〉∗H = 〈y, x〉H,
for all x, y, z ∈ H, and a, b ∈ C. In the real case, the condition κ(x, y) = 〈κ(·, y), κ(·, x)〉H may be replaced by the well known equation κ(x, y) = 〈κ(·, x), κ(·, y)〉H . However, since in the complex case the inner product is Hermitian, the aforementioned condition is equivalent to κ(x, y) = (〈κ(·, x), κ(·, y)〉H)∗. One of the most important properties of RKHSs is that norm convergence implies pointwise convergence. More precisely, let {fn}n∈N ⊂ H be a sequence such that limn ‖fn − f‖ = 0, for some f ∈ H. Then, the continuity of Tx gives:
lim n fn(x) = lim n Tx(fn) = Tx(f) = f(x),
for all x ∈ X. Although, the underlying theory has been developed by the mathematicians for general complex reproducing kernels with associated RKHSs, only the real kernels have been considered by the machine learning community. One of the most widely used is the Gaussian RBF kernel, i.e.,
κσ,Rd(x,y) := exp
(
− ∑d i=1(xi − yi)2 σ2
)
, (2)
defined for x,y ∈ Rd, where σ is a free positive parameter. Another popular kernel is the polynomial kernel:
κd(x,y) := ( 1 + xTy )d , (3)
for d ∈ N. Many more can be found in the relative literature [7]–[9]. Complex reproducing kernels, that have been extensively studied by the mathematicians, are, among others, the Szego kernels, i.e, κ(z, w) = 11−w∗z , for Hardy spaces on the unit disk, and the Bergman kernels, i.e., κ(z, w) = 1(1−w∗z)2 , for Bergman spaces on the unit disk, where |z|, |w| < 1 [25]. In the following, we discuss another complex kernel that has remained relatively unknown in the Machine Learning and Signal Processing societies.
Following the methodology adopted in [19], we examine the correlation between word-level predictors (frequency and in-context information content) and metric of structural form (word length or PIC) for the 25,000 most frequent types in each language. Unlike [19], we limit our analysis to indictionary types, thereby excluding person names, place names, and acronyms from the analysis. We obtain a systematically stronger negative correlation between frequency and distinctiveness, as measured by type-weighted model PIC, than frequency and word length (Fig. 3). Even holding word length constant, distinctiveness explains substantial additional variance in word frequency (Fig. 2). Building the model from phonemic transcriptions, this pattern holds in 11 of 11 languages in the Google 1T datasets, 6 of 7 languages from Google Books 2012, and all 13 languages from the 2013 OPUS corpus. Building the model from raw characters—as an approximation of the phonological form—this pattern holds in all cases. In many cases (10 of 11 languages in Google 1T, 3 of 7 in Google Books 2012, and 1 of 13 in OPUS) the partial correlation of PIC and frequency—with word length partialed out—is higher than the simple correlation of frequency and word length.
The obtained correlations are even stronger than those recently obtained between word length and average in-context information content [19].
PIC computed under the token-weighted model demonstrates an even stronger correlation with frequency across the languages in the sample (Fig. 4). We caution against an overly strong interpretation of this result, however, in that this correlation is not significantly higher than that obtained when phonemic content is shuffled among word forms (maintaining length for each word) and PIC subsequently recomputed. In contrast, the correlation between frequency and PIC computed using a type-weighted model under the permuted dataset is no stronger than the correlation between frequency and length.
A similar pattern of results emerges regardless of whether the type-weighted model is computed over characters or phonemes; the sole exception is the Russian corpus from Google Books 2012 where word length is a stronger predictor when the model is computed over phonemes. However, this dataset is an outlier in two notable ways. Russian shows the lowest correlation between frequencies obtained from Google Books and OPUS (Pearson’s r = .48), as well as the lowest correlation between PIC estimates derived from Google Books and those derived from OPUS (Pearson’s r = .63). Across languages, models built over phonemes and character transitions provide proportional estimates of PIC (Pearson’s r between .789 and .919 across languages, median = .874). While more research is required to extend these findings beyond Germanic, Romance, and Slavic languages, Hebrew provides an important test of whether this relationship holds in languages with extensive nonconcatenative in addition to affixal morphology.
The evaluation is performed on first split of UCF-101 dataset. UCF-101 contains 13.2K videos (25 frames/second) annotated into 101 classes, where each split contains 9.5K training videos.
Another architecture models temporal data using a sequence of GAEs (Michalski et al., 2014b)3. Beyond a sequence, it even uses a hierarchy of GAEs to learn transformations of transformations. The model of (Michalski et al., 2014b), called Predictive Gating Pyramides (PGP), cascades two level of GAEs to predict sequences. As the authors state, the reconstruction error is inadequate in their context, thus the model is trained explictly to predict rather than to reconstruct. Actually, it is trained to predict over multiple steps. A strong assumption in PGP is that the highest-order relational structure in the sequence is constant. It uses Back-Propagation Through Time (BPTT) to perform gradient descent on the weights over time. However, the model is used to learn temporal features, it does not predict long sequences of images. And a major drawback is that the architecture requires as many GAEs as time steps.
Understanding how new words are formed is a fundamental task in linguistics and language modelling, with significant implications for tasks with a generation component, such as abstractive summarisation and machine translation. In this paper we focus on modelling derivational morphology, to learn, e.g., that the appropriate derivational form of the verb succeed is succession given the context As third in the line of . . . , but is success in The play was a great .
English is broadly considered to be a morphologically impoverished language, and there are certainly many regularities in morphological patterns, e.g., the common usage of -able to transform a verb into an adjective, or -ly to form an adverb from an adjective. However there is considerable subtlety in English derivational morphology, in the form of: (a) idiosyncratic derivations; e.g. picturesque vs. beautiful vs. splendid as adjectival forms of the nouns picture, beauty and splendour, respectively; (b) derivational generation in context, which requires the automatic determination of the part-
of-speech (POS) of the stem and the likely POS of the word in context, and POS-specific derivational rules; and (c) multiple derivational forms often exist for a given stem, and these must be selected between based on the context (e.g. success and succession as nominal forms of success, as seen above). As such, there are many aspects that affect the choice of derivational transformation, including morphotactics, phonology, semantics or even etymological characteristics. Earlier works (Thorndike, 1941) analysed ambiguity of derivational suffixes themselves when the same suffix might present different semantics depending on the base form it is attached to (cf. beautiful vs. cupful). Furthermore, as Richardson (1977) previously noted, even words with quite similar semantics and orthography such as horror and terror might have non-overlapping patterns: although we observe regularity in some common forms, for example, horrify and terrify, and horrible and terrible, nothing tells us why we observe terrorize and no instances of horrorize, or horrid, but not terrid.
In this paper, we propose the new task of predicting a derived form from its context and a base form. Our motivation in this research is primarily linguistic, i.e. we measure the degree to which it is possible to predict particular derivation forms from context. A similar task has been proposed in the context of studying how children master derivations (Singson et al., 2000). In their work, children were asked to complete a sentence by choosing one of four possible derivations. Each derivation corresponded either to a noun, verb, adjective, or adverbial form. Singson et al. (2000) showed that childrens’ ability to recognize the correct form correlates with their reading ability. This observation confirms an earlier idea that orthographical regularities provide a clearer clues to morphological transformations comparing to phonological rules (Templeton, 1980; Moskowitz, 1973), especially in lanar X iv :1
70 2.
06 67
5v 1
[ cs
.C L
] 2
2 Fe
b 20
17
guages such as English where grapheme-phoneme correspondences are opaque. For this reason we consider orthographic rather than phonological representations.
In our approach, we test how well models incorporating distributional semantics can capture derivational transformations. Deep learning models capable of learning real-valued word embeddings have been shown to perform well on a range of tasks, from language modelling (Mikolov et al., 2013a) to parsing (Dyer et al., 2015) and machine translation (Bahdanau et al., 2015). Recently, these models have also been successfully applied to morphological reinflection tasks (Kann and Schütze, 2016; Cotterell et al., 2016a).
The results on the binary class data sets are shown in Table 4. Both conventional classifiers have high accuracies and low error rates of the negative class, but the error rates of the positive class are high. SMOTE is an effective method with low error rate of the positive class. However, it does not have the ability to reject instances. Cost-sensitive learning performs well under the current cost settings, but its accuracy is the lowest when the class distribution differs greatly. On Nursery and Letter, the error rate of the positive class is zero with Cost-sensitive learning at the price of high error rate of the negative class. Besides, Gmean no rej and NI no rej perform well on balancing the classification of two classes. When a
8
9 (Continued from previous page) Data set Method EN (%) EP (%) E(%) A(%) RejN (%) RejP (%) Rej(%) G(%) F (%) NI
Optdigits
k N N kNN classifier 0.10 4.80 0.56 99.44 — — — 97.52 97.08 0.8938 SMOTE 0.59 1.65 0.69 99.31 — — — 98.88 96.57 0.9019 Cost-sensitive 1.09 1.19 1.10 98.90 — — — 98.86 94.65 0.8709 Gmean no rej 0.84 1.34 0.89 99.11 — — — 98.91 95.64 0.8868 NI no rej 0.28 2.26 0.48 99.52 — — — 98.72 97.59 0.9160 Chow’s reject 0.06 2.35 0.28 99.72 0.21 7.29 0.90 98.70 98.45 0.8898 Gmean rej 0.07 0.76 0.14 99.86 1.85 10.06 2.66 99.54 99.25 0.9221 NI rej 0.15 1.41 0.27 99.72 0.74 2.04 0.87 99.21 98.58 0.9307
B a y e s
Bayes classifier 0.00 100.00 9.86 90.14 — — — 0.00 0.00 0.0000 SMOTE 1.29 60.59 7.14 92.86 — — — 46.68 40.41 0.2741 Cost-sensitive 52.14 0.00 47.00 53.00 — — — 69.18 29.56 0.1855 Gmean no rej 6.19 3.90 5.96 94.04 — — — 94.94 76.13 0.6109 NI no rej 4.39 8.26 4.77 95.23 — — — 93.62 79.18 0.6134 Chow’s reject 0.00 100.00 9.86 90.14 0.00 0.00 0.00 0.00 0.00 0.0000 Gmean rej 0.08 0.18 0.09 99.86 39.84 59.48 41.78 99.70 99.02 0.4653 NI rej 2.50 1.53 2.40 97.31 10.44 15.21 10.91 97.69 87.48 0.6626
Vehicle
k N N kNN classifier 5.81 68.30 21.47 78.53 — — — 54.46 42.35 0.0915 SMOTE 30.69 24.93 29.25 70.75 — — — 70.47 55.69 0.1542 Cost-sensitive 21.33 34.90 24.73 75.27 — — — 71.45 56.95 0.1442 Gmean no rej 29.47 24.50 28.23 71.77 — — — 72.79 57.28 0.1523 NI no rej 39.57 14.94 33.40 66.60 — — — 71.14 56.21 0.1579 Chow’s reject 0.79 36.43 9.72 86.61 19.09 52.44 27.44 47.72 36.05 0.1039 Gmean rej 3.97 3.49 3.85 90.71 55.62 74.26 60.29 88.49 74.85 0.1764 NI rej 14.67 6.86 12.71 80.69 34.63 42.54 36.61 82.46 68.11 0.1982
B a y e s
Bayes classifier 0.62 93.13 23.80 76.20 — — — 23.22 11.94 0.0288 SMOTE 30.04 41.96 33.03 66.97 — — — 57.28 42.86 0.0755 Cost-sensitive 26.97 40.42 30.33 69.67 — — — 65.86 49.54 0.0790 Gmean no rej 36.91 25.71 34.09 65.91 — — — 68.24 52.08 0.1034 NI no rej 45.48 17.48 38.46 61.54 — — — 66.52 51.85 0.1072 Chow’s reject 0.00 48.42 12.13 82.85 21.86 51.58 29.31 0.00 0.00 0.0236 Gmean rej 2.40 1.60 2.20 91.40 73.51 82.10 75.66 84.71 73.28 0.1223 NI rej 9.64 8.30 9.30 81.23 50.04 58.16 52.07 79.80 65.67 0.1322
Yeast
k N N kNN classifier 9.44 58.16 23.52 76.48 — — — 61.50 50.64 0.1086 SMOTE 36.11 25.29 32.98 67.02 — — — 67.48 56.39 0.1180 Cost-sensitive 25.08 32.82 27.32 72.68 — — — 70.92 58.70 0.1289 Gmean no rej 30.99 26.62 29.73 70.27 — — — 71.02 58.84 0.1289 NI no rej 31.52 26.25 30.00 70.00 — — — 70.35 58.72 0.1340 Chow’s reject 3.07 34.17 12.06 83.31 20.68 45.41 27.83 59.93 49.45 0.1108 Gmean rej 3.45 3.45 3.45 89.39 65.17 74.78 67.95 87.59 77.92 0.1421 NI rej 13.20 9.86 12.23 79.35 41.55 41.96 41.67 80.30 69.91 0.1607
B a y e s
Bayes classifier 1.60 89.32 26.96 73.04 — — — 32.23 18.55 0.0313 SMOTE 50.67 26.09 43.57 56.43 — — — 50.16 45.92 0.0719 Cost-sensitive 51.30 10.35 39.46 60.54 — — — 65.99 56.81 0.1234 Gmean no rej 30.48 30.12 30.38 69.62 — — — 69.38 57.19 0.1138 NI no rej 35.56 26.50 32.93 67.07 — — — 66.49 55.84 0.1202 Chow’s reject 0.19 20.33 6.01 88.02 38.44 78.60 50.05 19.78 8.77 0.0742 Gmean rej 2.67 3.12 2.80 86.14 77.69 81.72 78.86 83.67 73.48 0.0822 NI rej 12.15 9.46 11.37 80.98 40.91 41.45 41.06 81.53 71.72 0.1786
Phoneme
k N N kNN classifier 6.44 23.03 11.31 88.69 — — — 84.86 79.98 0.4261 SMOTE 17.40 10.46 15.36 84.64 — — — 85.81 77.51 0.4106 Cost-sensitive 13.69 11.55 13.06 86.94 — — — 87.37 79.90 0.4372 Gmean no rej 14.09 11.00 13.18 86.82 — — — 87.43 79.86 0.4388 NI no rej 12.53 12.77 12.60 87.40 — — — 87.32 80.28 0.4406 Chow’s reject 2.27 11.80 5.07 93.94 11.08 29.17 16.39 90.10 87.21 0.4683 Gmean rej 1.20 1.94 1.42 97.62 36.01 52.78 40.93 96.93 94.98 0.4509 NI rej 5.35 4.62 5.14 93.54 20.22 21.79 20.68 93.69 89.41 0.5086
B a y e s
Bayes classifier 10.44 31.25 16.55 83.45 — — — 78.46 70.92 0.2816 SMOTE 24.61 12.53 21.06 78.94 — — — 80.70 71.02 0.3122 Cost-sensitive 21.89 12.41 19.10 80.90 — — — 82.71 72.91 0.3248 Gmean no rej 23.81 9.96 19.74 80.26 — — — 82.81 72.81 0.3331 NI no rej 26.03 7.94 20.72 79.28 — — — 82.49 72.29 0.3359 Chow’s reject 1.17 13.14 4.69 93.05 20.36 62.33 32.68 80.01 75.37 0.2679 Gmean rej 0.17 0.14 0.16 99.25 78.47 91.71 82.36 96.97 95.24 0.1292 NI rej 10.05 4.35 8.37 88.92 23.50 28.33 24.92 90.32 82.69 0.3845
German
k N N kNN classifier 9.67 69.03 27.48 72.52 — — — 52.75 40.24 0.0554 SMOTE 49.80 21.39 41.28 58.72 — — — 60.30 53.23 0.0746 Cost-sensitive 29.57 35.53 31.36 68.64 — — — 67.32 55.19 0.0888 Gmean no rej 32.92 29.47 31.88 68.12 — — — 68.67 57.03 0.1017 NI no rej 34.40 28.00 32.48 67.52 — — — 68.38 57.12 0.1037 Chow’s reject 2.17 35.53 12.18 81.03 27.40 55.60 35.86 43.97 30.40 0.0582 Gmean rej 6.20 6.07 6.16 80.66 68.63 72.53 69.80 78.48 67.65 0.0797 NI rej 17.30 16.83 17.16 73.89 34.85 33.50 34.45 73.63 63.57 0.1139
B a y e s
Bayes classifier 0.00 100.00 30.03 69.97 — — — 0.00 0.00 0.0000 SMOTE 61.84 32.33 52.99 47.01 — — — 13.06 37.16 0.0183 Cost-sensitive 21.43 39.27 26.78 73.22 — — — 69.06 57.64 0.1179 Gmean no rej 32.23 26.73 30.58 69.42 — — — 70.23 58.96 0.1144 NI no rej 30.80 29.50 30.41 69.59 — — — 69.12 57.87 0.1189 Chow’s reject 0.00 30.60 9.18 84.62 28.51 69.40 40.78 0.00 0.00 0.0567 Gmean rej 1.77 2.07 1.86 89.16 82.11 88.33 83.98 79.02 68.39 0.0689 NI rej 15.40 12.23 14.45 76.33 40.20 39.50 39.99 75.88 66.92 0.1334
(Continued on next page)
10
(a) kNN Classifier Based
Data set α∗N ( λ̄FP ) T∗rN T ∗ rP λ̄RN λ̄RP Ism 0.2312(0.0408) 0.0743(0.0085) 0.7643(0.0296) 0.0272 0.6616 Nursery 0.3482(0.0328) 0.1215(0.0565) 0.7125(0.0403) 0.0288 0.7914 Letter 0.3802(0.0321) 0.1284(0.0343) 0.6140(0.0517) 0.0760 0.4838 Rooftop 0.1372(0.0302) 0.0705(0.0610) 0.7733(0.0404) 0.0544 0.2823 Pendigits 0.4714(0.0771) 0.1745(0.0851) 0.3890(0.0555) 0.1710 0.1913 Optdigits 0.4061(0.0667) 0.1487(0.0372) 0.5913(0.0486) 0.0964 0.4481 Vehicle 0.1972(0.0769) 0.1101(0.0297) 0.6266(0.0719) 0.1045 0.1556 Yeast 0.3610(0.1333) 0.1245(0.0335) 0.5608(0.0536) 0.0937 0.3414 Phoneme 0.4651(0.0835) 0.1319(0.0180) 0.4543(0.0409) 0.1066 0.2985 German 0.3848(0.0785) 0.1915(0.0336) 0.6021(0.0368) 0.1542 0.3489 Diabetes 0.3725(0.0796) 0.1725(0.0455) 0.5284(0.0672) 0.1585 0.2398 Gamma 0.5682(0.0207) 0.1663(0.0225) 0.4188(0.0319) 0.1376 0.3103
(b) Bayes Classifier Based
Data set α∗N ( λ̄FP ) T∗rN T ∗ rP λ̄RN λ̄RP Ism 0.1420(0.0230) 0.0222(0.0168) 0.8560(0.0212) 0.0041 0.8198 Nursery 0.1052(0.0117) 0.0593(0.0076) 0.8845(0.0090) 0.0237 0.6242 Letter 0.1155(0.0066) 0.0687(0.0163) 0.8772(0.0128) 0.0273 0.6302 Rooftop 0.0786(0.0299) 0.0242(0.0080) 0.8363(0.0242) 0.0170 0.3147 Pendigits 0.4283(0.0472) 0.1521(0.0488) 0.6170(0.0394) 0.0782 0.5640 Optdigits 0.1883(0.0087) 0.1339(0.0047) 0.8274(0.0078) 0.0581 0.6240 Vehicle 0.2490(0.0262) 0.1301(0.0149) 0.5369(0.0384) 0.1287 0.1395 Yeast 0.4469(0.0734) 0.2668(0.0147) 0.6413(0.0137) 0.2093 0.4247 Phoneme 0.3364(0.0374) 0.1723(0.0266) 0.5511(0.0363) 0.1641 0.2115 German 0.4265(0.0137) 0.2802(0.0081) 0.6866(0.0085) 0.1736 0.5541 Diabetes 0.3808(0.0460) 0.2461(0.0174) 0.6638(0.0327) 0.2279 0.3020 Gamma 0.5859(0.0483) 0.1723(0.0212) 0.4660(0.0272) 0.1243 0.4028
Optimal values are listed as mean(standard deviation). (a) Derived based on kNN classifier. (b) Derived based on Bayes classifier.
11
reject option is added, the error rate may be reduced and the accuracy may be increased. But it is difficult to decide the rejection costs and the rejection thresholds for lack of information about the rejections. Regarding to Chow’s reject, it is usually wasteful to reject lots of instances from the positive class with arbitrary settings on the rejection thresholds. On most data sets, Gmean rej achieves the highest accuracy and the lowest error rate of the positive class, at the price of considerably high reject rate. However, the accuracy of Gmean rej is lower than the conventional classifications on Ism and Rooftop. One explanation is that the goal of the G-mean based methods is to maximize the geometric mean of the accuracy within each class rather than the total accuracy. Compared with Gmean rej and Chow’s reject, our NI rej performs best on the whole with low error rate of the positive class, high accuracy, a certain amount of reject rate, high G-mean, high F-measure and the highest NI.
Table 5 lists the values of the optimal weight α∗N and rejection thresholds T ∗r . The last two columns represent the “equivalent” rejection costs computed with the mean values of α∗N and T ∗
r . Moreover, these values are purely determined by the data sets besides the conventional classification algorithms. They can be adopted as “objective” references while the cost information is unknown. In addition, the “equivalent” costs of these data sets are consistent with human assumption, which also reflects the effectiveness of our NI based strategy.
Fig. 3 shows the ROC convex hull (ROCCH) of kNN for Diabetes generated from 90 validation sets by threshold averaging [44]. We just list some of the vertices in Table 6. And we use them to approximatively locate the parameters [15]. In Fig. 3a, we use equal and “equivalent” misclassification costs to compute the slopes, respectively. The slopes computed with costs are the same as those computed with rejection thresholds on ROCCH, so we only plot the latter in Fig. 3b. Point B in Fig. 3a lies between D and F. Due to approximation, points D and F that the slopes find are not cohere with the optimal
threshold points C and E. In addition, the parameters can be adjusted with the graphical interpretations in Fig. 2 under the property P3 by the users.
The set of algorithm selection scenarios in release version 1.0.1 of our library, shown in Table 2, has been assembled to represent a diverse set of selection problem settings that covers a wide range of problem domains, types of algorithms, features and problem instances. Our scenarios include both problems that have been broadly studied in the context of algorithm selection techniques (such as SAT and CSP), as well as more recent ones (such as the container premarshalling problem). All of our scenarios were taken from publications that report performance improvements through algorithm selection and consist of
algorithms where the virtual best solver (VBS)6 is significantly better than the single best algorithm.7 Therefore, these are problems on which it makes sense to seek performance improvements via algorithm selection. All scenarios are available on our online platform (http://www.aslib.net/).
The scenarios we provide here are focused on constraint satisfaction problems, but we encourage readers to submit new scenarios. In the following, we briefly describe the scenarios we included and what makes them interesting.
In this section we discuss three variants of collaboration models for the SoD.
1) Centralised: In this collaboration model, there is a powerful, master drone in the SoD that collets all the feeds from individual drones and assists the swarm in computing and agreeing on decisions. 2) Decentralised: In this collaboration model, there is no single master drone, but instead a small subset of powerful drones that collect the feeds from their neighbouring drones and then these powerful drones perform the collaborative learning, evaluation and decision. 3) Distributed (Peer-Oriented): In this model, each and every participant of the SoD have more or less equal role in all collaborative learning, evaluation and decision making process. It can be noted that the activity load is distributed among the population based on their individual capabilities, current performance and power resources and the criticality of their unique features to overall mission.
In a RBPNN both the input and the first hidden layer exactly match the PNN architecture: the input neurones are used as distribution units that supply the same input values to all the neurones in the first hidden layer that, for historical reasons, are called pattern units. In a PNN, each pattern unit performs the dot product of the input pattern vector v by a weight vector W(0), and then performs a nonlinear operation on the result. This nonlinear operation gives output x(1) that is then provided to the following summation layer. While a common sigmoid function is used for a standard FFNN with BPTA, in a PNN the activation function is an exponential, such that, for the j-esime neurone the output is
x (1) j ∝ exp
( ||W(0) · v||
2σ2
) (1)
where σ represents the statistical distribution spread.
The given activation function can be modified or substituted while the condition of Parzen (window function) is still satisfied for the estimator N̂ . In order to satisfy such a condition some rules must be verified for the chosen window function in order to obtain the expected estimate, which can be expressed as a Parzen window estimate p(x) by means of the kernel K of f in the d-dimensional space Sd
pn(x) = 1 n n∑ i=1 1 hdn K ( x−xi hn ) ∫ Sd K(x)dx = 1
(2)
where hn ∈ N is called window width or bandwidth parameter and corresponds to the width of the kernel. In general hn ∈ N depends on the number of available sample data n for the estimator pn(x). Since the estimator pn(x) converges in mean square to the expected value p(x) if
lim n→∞
〈pn(x)〉 = p(x)
lim n→∞
var (pn(x)) = 0 (3)
where 〈pn(x)〉 represents the mean estimator values and var (pn(x)) the variance of the estimated output with respect to the expected values, the Parzen condition states that such convergence holds within the following conditions:
sup x K(x) < ∞
lim |x|→∞ xK(x) = 0
lim n→∞
hnd = 0
lim n→∞
nhnd = ∞
(4)
In this case, while preserving the PNN topology, to obtain the RBPNN capabilities, the activation function is substituted with a radial basis function (RBF); an RBF still verifies all the conditions stated before. It then follows the equivalence between the W(0) vector of weights and the centroids vector of a radial basis neural network, which, in this case, are computed as the statistical centroids of all the input sets given to the network.
We name f the chosen radial basis function, then the new output of the first hidden layer for the j-esime neurone is
x (1) j , f
( ||v −W(0)||
β
) (5)
where β is a parameter that is intended to control the distribution shape, quite similar to the σ used in (1).
The second hidden layer in a RBPNN is identical to a PNN, it just computes weighted sums of the received values from the preceding neurones. This second hidden layer is called indeed summation layer: the output of the k-esime summation unit is
x (2) k = ∑ j Wjkx (1) j (6)
where Wjk represents the weight matrix. Such weight matrix consists of a weight value for each connection from the j-esime pattern units to the k-esime summation unit. These summation
units work as in the neurones of a linear perceptron network. The training for the output layer is performed as in a RBNN, however since the number of summation units is very small and in general remarkably less than in a RBNN, the training is simplified and the speed greatly increased [27].
The output of the RBPNN (as shown in Figure 4) is given to the maximum probability selector module, which effectively acts as a one-neuron output layer. This selector receives as input the probability score generated by the RBPNN and attributes to one author only the analysed text, by selecting the most probable author, i.e. the one having the maximum input probability score. Note that the links to this selector are weighted (with weights adjusted during the training), hence the actual input is the product between the weight and the output of the summation layer of the RBPNN.
Several strategies for computing cautious consequences of a given program are reported in this section. Some of these strategies aim at solving the problem producing overestimates of the solution, which are improved and eventually result in the set of cautious consequences of the input program. Among them are the algorithms implemented by the ASP solvers DLV (Alviano et al. 2011) and clasp (Gebser et al. 2012a), respectively called enumeration of models and overestimate reduction in the following. Other strategies can in addition produce sound answers during the computation of the complete solution, thus providing underestimates also when termination is not affordable in reasonable time. One of these strategies is iterative coherence testing, an adaptation of an algorithm computing backbones of propositional formulas (Marques-Silva et al. 2010). To the best of our knowledge, no previous attempt to bring such an algorithm in ASP is reported in the literature. A variant of this algorithm, namely iterative partial coherence testing, is also introduced here. Finally, a strategy for obtaining underestimates from enumeration of models and overestimate reduction is presented, which can also be used to improve the other algorithms. More in detail, the algorithms considered here have a common skeleton, reported as Algorithm 1. They receive as input a program P and a set of atoms Q representing answer candidates of a query, and produce as output either the largest subset of Q that only contains cautious consequences of P, in case P is coherent, or ⊥ when P is incoherent. Initially, the underestimate U and the overestimate O are set to /0 and Q, respectively (line 1). A coherence test of P is then performed (lines 2–4) by calling function ComputeStableModel, which actually implements stable model search as described in Section 2. (To simplify the presentation, branching atoms are assumed to be assigned the false truth value.) The first argument of the function is a program P. The second argument is a set of learned constraints, which is initially empty. The third argument is a set C of atoms used to restrict branching atoms of level 1. The function returns either I in case a stable model I of P is found, or ⊥ otherwise. Note that ⊥ is returned not only when P is incoherent, but in general when each stable model M of P is such that C ⊆ M. Similarly, when I is returned, stable model I satisfies C 6⊆ I. When C = A , the
Algorithm 1: CautiousReasoning Input : a program P and a set of atoms Q Output: atoms in Q that are cautious consequences of P, or ⊥
1 U := /0; O := Q; L := /0; 2 I := ComputeStableModel(P, L, A ); 3 if I =⊥ then 4 return ⊥;
5 O := O∩ I; 6 while U 6= O do
// EnumerationOfModels or other procedure
7 return U ;
Procedure EnumerationOfModels (A1) 1 P := P∪ Constraint(I); 2 I := ComputeStableModel(P, L, A ); 3 if I =⊥ then 4 U := O; 5 else
6 O := O∩ I;
Procedure OverestimateReduction (A2) 1 P := P∪ Constraint(O); 2 I := ComputeStableModel(P, L, A ); 3 if I =⊥ then 4 U := O; 5 else 6 P := P\ Constraint(O); 7 O := O∩ I;
Procedure IterativeCoherenceTesting (A3)
1 a := OneOf(O\U); 2 I := ComputeStableModel(P, L, {a}); 3 if I =⊥ then 4 U :=U ∪{a}; 5 else 6 O := O∩ I;
Procedure IterativePartialCoherTest (A4)
1 a := OneOf(O\U); 2 I := ComputeUpToNextRestart(P, L, {a}); 3 if I =⊥ then 4 U :=U ∪{a}; 5 else if I 6= RESTART then 6 O := O∩ I;
Function ComputeStableModel∗(P: program, L: learned constraints, C: set of atoms) Global variables: the underestimate U
1 repeat 2 U :=U ∪{a ∈ Q | L contains ⊥← ∼a}; 3 I := ComputeUpToNextRestart(P, L, OneOf(C)); 4 until I 6= RESTART ; 5 return I;
condition C 6⊆ I is trivially satisfied because I ⊆ A \ {⊥} by definition of interpretation. When C = {a} for some atom a, instead, this function results in an incremental stable model search in which a is forced to be false (Eén and Sörensson 2003b).
The first stable model found improves the overestimate (line 5). At this point, estimates are improved according to different strategies until they are equal (line 6). EnumerationOfModels adds to P a constraint that eliminates the last stable model found (line 1). In fact, function Constraint({a1, . . . ,an}) returns a singleton of the form {⊥ ← a1, . . . ,an}. The algorithm then searches for a new stable model (line 2) to improve the overestimate (line 6). If no new stable model exists, the underestimate is set equal to the overestimate (lines 3–4), thus terminating the computation. OverestimateReduction is similar, but the constraint added is obtained from the
current overestimate (line 1). In this way, when a new stable model is found, an improvement of the overestimate is guaranteed, and the constraint can be reduced accordingly (lines 6 and 1).
The strategy implemented by IterativeCoherenceTesting can also improve the underestimate many times during its computation. In fact, one cautious consequence candidate is selected by calling function OneOf (line 1). This candidate is then constrained to be false and a stable model is searched (line 2). If none is found then the underestimate can be increased (lines 3–4). Otherwise, the overestimate can be improved (lines 5–6). IterativePartialCoherenceTesting is similar, but forces falsity of a candidate only up to the next restart (lines 1–2). In fact, ComputeStableModel is replaced by ComputeUpToNextRestart, a function that searches for a stable model but also terminates when a restart occurs, in which case it returns the value RESTART . In this way, the algorithm can select the most promising candidate after each restart.
Variants of these four algorithms can be obtained by replacing function ComputeStableModel with function ComputeStableModel∗, which actually implements stable model search, but also improves the current underestimate after each restart (line 2).
Theorem 1 Let P be a program and Q ⊆ A a set of atoms. CautiousReasoning(P,Q) terminates after finitely many steps and returns Q ∩CC(P) if P is coherent; otherwise, it returns ⊥. Moreover, U ⊆ Q∩CC(P)⊆O holds at each step of computation. The claim holds for all variants of Algorithm 1.
There are different ways to tune the parameters λ1, λ2 for the GS-based and λ1, λ2, λ3 for MGS-based localization schemes. One approach is the well-known Cross Validation (CV) [64]. The CV finds the Mean Square Error (MSE) of the residuals for a range of λi, i = 1, 2, 3, and chooses the one with the least MSE. Another approach is to use training data
and find the best values that minimize the localization error. The latter method, which has been used in this paper, is an easy and effective way for selecting these parameters. Only 10 training samples have been used for tuning. Fig. 10 shows the localization error for an increasing λ1/λ2 when 12 APs have been used. The best parameter values must be calibrated for each building (e.g., at the fingerprinting stage). In the above results, we chose λ1/λ2 = 0.5 for GS-based localization and λ1 = λ2 = 0.1, λ3 = 0.01 for the MGS-based localization approach.
Since the GS-based localization needs to divide the area into groups and assign a weight to each, the number of groups also plays an important role in localization accuracy. To this end, an experiment is conducted to evaluate the localization error for different number of groups and the results are shown in Fig. 11 for AET and BSE building environments when 4 and 10 APs are used for localization. The results illustrate that 15 groups are optimal to achieve the highest localization accuracy compared to 25 groups for BSE environment. The
BSE environment is structurally more complex than the AET environment and hence, it introduces a diverse set of RPs. However, it is clear that small localization errors for both environments are achieved which indicates the consistency of the method in different environments.
Mental health is an increasingly important healthrelated challenge in society; mental health conditions are associated with impaired health-related quality of life and social functioning (Saarni et al., 2007; Strine et al., 2015). Self-harm and suicide, as serious mental health conditions, are leading reasons of death world-wide (Nock et al., 2008; American Foundation for Suicide Prevention, 2016). Each year an estimated number of 43,000 Americans die by suicide, on average there are 117 suicides per day, and about 500,000 people visit hospital for injuries due to self-harm (Karch et al., 2009; Centers for Disease Control and Prevention, 2015; American Foundation for Suicide Prevention, 2016).
Despite its pervasiveness, our understanding of suicide and self-harm related issues is limited. A notable reason for this is lack of large scale data on suicide. Most existing research on suicide is based on sparse curated data from a limited number of health-care centers. Furthermore, detecting and preventing potential self-harm acts remain a significant challenge due to reasons including lack of real-time data, privacy and confidentiality issues, and existence of bias in studies (Coppersmith et al., 2016a).
As social media usage has increased dramatically, individuals have tried to resolve their health problems by sharing them online, asking other users’ opinions and seeking support. Therefore, social media have provided a valuable platform for large-scale analysis of mental health data and this analyses have offered great insights into mental health. Generally, it has been shown that social media can have broad applicability for public health research as the data from social media can reflect a variety of characteristics about individuals (Paul and Dredze, 2011; Eichstaedt et al., 2015).
ar X
iv :1
70 2.
06 87
5v 1
[ cs
.C L
] 2
2 Fe
b 20
17
Online forums are a type of social media which are essentially communities in which users engage in discussion about topics of common interest. Mental health forums are centered around users who have directly or indirectly been involved in mental health conditions.
General social media platforms such as Twitter and Facebook are less topic-centric and more general purpose, in the sense that millions of users use them to discuss mundane events in their lives. While the signals coming from general social systems such as Twitter and Facebook are subtle and not directly about mental health, they are relevant and they have been previously utilized to support certain important tasks (e.g. (Coppersmith et al., 2015; Tsugawa et al., 2015; Schwartz et al., 2014)). On the other hand, online forums are specifically designed for discussion around specific topics and they attract users with similar interests and goals (De Choudhury and De, 2014). Some users in general social media such as Twitter can choose to be pseudonymous or anonymous, however, the identity of majority of the users are known. On the other hand, to protect their users, many online mental health forums such as ReachOut specifically ask their users to have anonymous profiles. The moderators in many of these forums further actively redact any post that could reveal the identity of the user. Such support for anonymity further encourages users to engage in sensitive mental health discussions and express their real thoughts and feelings. In this paper, we are focusing on online mental health forums as anonymous support platforms centered around people with similar experiences and problems.
There are three stages that lead to suicidal action among individuals who are in some sort of mental distress (Silverman and Maris, 1995; De Choudhury et al., 2016): 1- thinking, 2- ambivalence and 3- decision making. In the first two stages the individual is experiencing thoughts of distress, hopelessness, and low self-esteem. In the decision making stage, the individual might show explicit plans of taking their life. Individuals might seek support in any of these stages and online health forums are a ready platform enabling these individuals to ask for support. In many online mental health forums, there are moderators or more senior members who help the users with mental distress. Troubled users who are at risk of self-harm need to be attended to as quickly as pos-
sible to prevent a potential self-harm act. However, the volume of newly posted content each day makes it difficult for the moderators to locate and respond to more critical posts. Effective online manual triaging of all the forum contents is highly costly and not scalable.
We propose an approach for automated triaging of the severity of user content in online forums based on indication of self-harm thoughts. Triaging the content severity makes it possible for moderators to identify critical posts and help a troubled user in a timely manner to hopefully reduce the risk of self-harm to the user. We propose a featurerich supervised classification framework that takes advantage of various types of features in the forums. The features include lexical, psycholinguistic, contextual, topic modeling, and dense representation features. We evaluate our approach on data provided by ReachOut1, a large mental health forum. We show that our approach can effectively identify the critical content which will assist the moderators in attending to the in-need users in a timely manner. We show that without an automatic way for identifying critical posts, the moderator’s response time does not correlate with the severity of the posts, which further confirms that manually identifying these posts is a challenge for moderators. Finally, analysis of the user content on this forum shows that on average, the content severity of users tends to decline as they interact with the forum which is evidenced by the transition from more critical to less critical content.
The contributions of this work are as follows: (i) an effective approach for triaging the content severity in online mental health forums based on indication of self-harm ideation; (ii) providing insight into the effect of online mental health forums on users through analysis of their content; (iii) analyzing the interaction of moderators with users; and (iv) extensive evaluation of the proposed approach on a real-world dataset.
Although industrial robots have reached a good level of accuracy, it is difficult to set peg and hole to few tens of µm of precision by using a position controller. Visual servoing is
ar X
iv :1
70 8.
04 03
3v 1
[ cs
.R O
] 1
4 A
ug 2
01 7
also impractical due to the limited resolution of cameras or internal parts that are occluded during assembly, for example, in case of meshing gears and splines in transmission. In this paper, we use a common 6-axis force-torque sensor to learn the hole location with respect to the peg position.
Newman et al. [5] calculate the moments from sensors and interprets the current position of the peg by mapping the moments onto positions. Sharma et al. [4] utilize depth profile in addition to roll and pitch data to interpret the current position of the peg. Although, these approaches are demonstrated to work in simulation, it is difficult to generalize them for the real world scenario. In the real case, it is very difficult to obtain a precise model of the physical interaction between two objects and calculate the moments caused by the contact forces and friction [6].
B. Insertion Phase
The insertion phase has been extensively researched. Gullapalli et al. [7] use associative reinforcement learning methods for learning the robot control. Majors and Richards [8] use a neural network based approach. Kim et al. [9] propose the insertion algorithm which can recover from tilted mode without resetting the task to the initial state. Tang et al. [10] propose an autonomous alignment method by force and moment measurement before insertion phase based on a three-point contact model.
Compared to these previous works, we insert a peg into a hole with a very small clearance of 10 µm. This high precision insertion is extremely difficult even for humans. This is due to the fact that humans cannot be so precise and the peg usually gets stuck in the very initial stage of insertion. It is also very difficult for the robot to perform an insertion with clearance tighter than its position accuracy. Therefore, robots need to learn in order to perform this precise insertion task using the force-torque sensor information.
The aim of signal sequence labeling is to assign a label to each sample of a multichannel signal while taking into account the sequentiality of the samples. This problem typically arises in speech signal segmentation or in Brain Computer Interfaces (BCI). Indeed, in real-time BCI applications, each sample of an electro-encephalography signal has to be interpreted as a specific command for a virtual keyboard or a robot hence the need for sample labeling [1, 2].
Many methods and algorithms have already been proposed for signal sequence labeling. For instance, Hidden Markov Models (HMM) [3] are statistical models that are able to learn a joint probability distribution of samples in a sequence and their labels. In some cases, Conditional Random Fields (CRF) [4] have been shown to outperform the HMM approach as they do not suppose the observation are independent. Structural Support Vector Machines (StructSVM), which are SVMs that learn a mapping from structured input to structured output, have also been considered for signal segmentation [5]. Signal sequence labeling can also be viewed from a very different perspective by considering a change detection method coupled with a supervised classifier. For instance, a Kernel Change Detection algorithm [6] can be used for detecting abrupt changes in a signal and afterwards a classifier applied for labeling the segmented regions.
This work is funded in part by the FP7-ICT Programme of the European Community, under the PASCAL2 Network of Excellence, ICT- 216886 and by the French ANR Project ANR-09-EMER-001.
In order to preprocess the signal, a filtering is often applied and the resulting filtered samples are used as training examples for learning. Such an approach poses the issue of the filter choice, which is oftenly based on prior knowledge on the information brought by the signals. Moreover, measured signals and extracted features may not be in phase with the labels and a time-lag due to the acquisition process appears in the signals. For example, in the problem of decoding arm movements from brain signals, there exists a natural time shift between these two entries, hence in their works, Pistohl et al. [7] had to select by a validation method a delay in their signal processing method.
In this work, we address the problem of automated tuning of the filtering stage including its time-lag. Indeed, our objective is to adapt the preprocessing filter and all its properties by including its setting into the learning process. Our hypothesis is that by fitting properly the filter to the classification problem at hand, without relying on ad-hoc prior-knowledge, we should be able to considerably improve the sequence labeling performance. So we propose to take into account the temporal neighborhood of the current sample directly into the decision function and the learning process, leading to an automatic setting of the signal filtering.
For this purpose, we first propose a naive approach based on SVMs which consists in considering, instead of a given time sample, a time-window around the sample. This method named as Window-SVM, allows us to learn a spatio-temporal classifier that will adapt itself to the signal time-lag. Then, we introduce another approach denoted as Filter-SVM which dissociates the filter and the classifier. This novel method jointly learns a SVM classifier and FIR filters coefficients. By doing so, we can interpret our filter as a large-margin filter for the problem at hand. These two methods are tested on a toy dataset and on a real life BCI signal sequence labeling problem from BCI Competition III [1].
In this section we will briefly describe topic modeling process used by ROST [9], which we use to give high level labels to the low level features observed by the robot, and also to compute the perplexity score of the observations.
3.1 Generative Model
An observation word is a discrete observation made by a robot. Given the observation words and their location, we would like to compute the posterior distribution of topics at this location. Let w be the observed word at location x. We assume the following generative process for the observation words:
1. word distribution for each topic k:
φk ∼ Dirichlet(β),
2. topic distribution for words at location x :
θx ∼ Dirichlet(α+H(x)),
3. topic label for w: z ∼ Discrete(θx),
4. word label: w ∼ Discrete(φz),
where y ∼ Y implies that random variable y is sampled from distribution Y , z is the topic label for the word observation w, and H(x) is the distribution of topics in the neighborhood of location x. Each topic is modeled by distribution φk over V possible word in the observation vocabulary.
φk(v) = P(w = v|z = k) ∝ nvk + β, (1)
where nvk is the number of times we have observed word v taking topic label k, and β is the Dirichlet prior hyperparameter. Topic model Φ = {φk} is a K×V matrix that encodes the global topic description information shared by all locations.
The main difference between this generative process and the generative process of words in a text document as proposed by LDA [4, 11] is in step 2. The context of words in LDA is modeled by the topic distribution of the document, which is independent of other documents in the corpora. We relax this assumption and instead propose the context of an observation word to be defined by the topic distribution of its spatiotemporal neighborhood. This is achieved via the use of a kernel. The posterior topic distribution at location x is thus defined as:
θx(k) = P(z = k|x) ∝ (∑ y K(x− y)nky ) + α, (2)
where K(·) is the kernel, α is the Dirichlet prior hyperameter and, nky is the number of times we observed topic k at location y.
3.2 Approximating Neighborhoods using Cells
The generative process defined above models the clustering behavior of observations from a natural scene well, but is difficult to implement because it requires keeping track of the topic distribution at every location in the world. This is computationally infeasible for any large dataset. For the special case when the kernel is a uniform distribution over a finite region, we can assume a cell decomposition of the world, and approximate the topic distribution around a location by summing over topic distribution of cells in and around the location.
Let the world be decomposed into C cells, in which each cell c ∈ C is connected to its neighboring cells G(c) ⊆ C. Let c(x) be the cell that contains points x. In this paper we only experiment with a grid decomposition of the world in which each cell is connected to its six nearest neighbors, 4 spatial and 2 temporal. However, the general ideas presented here are applicable to any other topological decomposition of the spacetime. Six neighbors is the smallest number which we need to consider while working with streaming 2D image data.
The topic distribution around x can then be approximated using cells as:
θx(k) ∝  ∑ c′∈G(c(x)) nkc′ + α (3) Due to this approximation, the following properties emerge:
Initialize ∀i, zi ∼ Uniform({1, . . . ,K}) while true do
foreach cell c ∈ C do foreach word wi ∈ c do
zi ∼ P(zi = k|wi = v, xi) Update Θ,Φ given the new zi by updating n v k and n k G
end
end
end
Algorithm 1: Batch Gibbs sampling
1. θx = θy if c(x) = c(y), i.e., all the points in a cell share the same neighborhood topic distribution. 2. The topic distribution of the neighborhood is computed by summing over the topic distribution of the neighboring cells rather than individual points.
We take advantage of these properties while doing inference in realtime.
3.3 Realtime Inference using Gibbs Sampling
Given a word observation wi, its location xi, and its neighborhood Gi = G(c(xi)), we use a Gibbs sampler to assign a new topic label to the word, by sampling from the posterior topic distribution:
P(zi = k|wi = v, xi) ∝ nvk,−i + β∑V
v=1(n v k,−i + β)
·
nkGi,−i + α∑K k=1(n k Gi,−i + α) ,
(4)
where nwk,−i counts the number of words of type w in topic k, excluding the current word wi, n k Gi,−i is the number of words with topic label k in neighborhood Gi, excluding the current word wi, and α, β are the Dirichlet hyperparameters. Note that for a neighborhood size of 0, the above Gibbs sampler is equivalent to the LDA Gibbs sampler proposed by Griffiths et al.[11], where each cell corresponds to a document. Algorithm 1 shows a simple iterative technique to compute the topic labels for the observed words in batch mode.
In the context of robotics we are interested in the online refinement of observation data. After each new observation, we only have a constant amount of time to do topic label refinement. Hence, any online refinement algorithm that has computational complexity which increases with new data, is not useful. Moreover, if we are to use the topic labels of an incoming observation for making realtime decisions, then it is essential that the topic labels for the last observation converge before the next observation arrives.
Since the total amount of data collected grows linearly with time, we must use a refinement strategy that efficiently handles global (previously observed) data and local (recently observed) data.
while true do Add new observed words to their corresponding cells. Initialize ∀i ∈MT , zi ∼ Uniform({1, . . . ,K}) while no new observation do
t ∼ P(t|T ) foreach cell c ∈Mt do
foreach word wi ∈ c do zi ∼ P(zi = k|wi = v, xi) Update Θ,Φ given the new zi by updating n v k and n k G end
end
end T ← T + 1
end
Algorithm 2: Realtime Gibbs sampler
Our general strategy is described by Algorithm 2. At each time step we add the new observations to the model, and then randomly pick observation times t ∼ P(t|T ), where T is the current time, for which we resample the topic labels and update the topic model.
We choose P(t|T ) such that with probability η we refine the last observation, and with probability (1 − η) we refine a randomly picked previous observation. We call η the refinement bias of the Gibbs sampler.
P(t|T ) =
{ η, if t = T
(1− η)/(T − 1), otherwise (5)
Choco allows to change the default backtrack strategy of trailing to copying. The n-Queens, the Social Golfers, and the Balanced Incomplete Block Design problems were rerun with copying instead of trailing for backtrack memory.
The results were compared with the Wilcoxon test. The differences are statistically significant at the 0.01 level. The CPU times are shown in relation to the number of backtracks because the correlation between the CPU time and the number of backtracks is stronger than the correlation between the CPU time and the number of variables.
Figure 9 shows the results. For all instances of all problems, trailing performs better than copying. For the n-Queens problem the differences are only up to about 20% because of the small number of variables (cf. table 2). The results for the Social Golfers and the Balanced Incomplete Block Design problems show that the relative difference between trailing and copying backtrack memory increases as the number of backtracks increases.
The results suggest that trailing backtrack memory performs better than copying backtrack memory; especially with increasing number of backtracks and variables. This is most likely limited to Choco though; Minion for example
uses a different implementation of copying backtrack memory which scales much better with increasing number of variables and has less overhead – instead of copying each variable domain individually, it only copies one contiguous memory region.
In general the results show that for problems with many backtracks trailing backtrack memory performs better than copying backtrack memory. The following section investigates this further.
In this paper we presented an experimental survey of the use of backpropagation through time on a physical delay-coupled electro-optical dynamical system, in order to use it as a machine learning model. We have shown that such a physical setup can be approached as a special case of recurrent neural network, and consequently can be trained with gradient descent using backpropagation. Specifically, we have shown that both the input and output encodings (input and output masks) for such a system can be fully optimized in this way, and that the encodings can be successfully applied to the real physical setup. Previous research in the usage of electro-optical dynamical systems for signal processing used random input encodings, which are quite inefficient in scenarios where the input dimensionality is high. We focused on two tasks with a relatively high input dimensionality: the MNIST written digit recognition dataset and the TIMIT phoneme recognition dataset. We showed that in both cases, optimizing the input encoding provides a significant performance boost over random masks. We also showed that the input encoding for the MNIST dataset seems to directly utilize the inherent dynamics of the system, and hence does more than simply provide a useful feature set. Note that the comparison with Reservoir Computing is based on the constraints by a given physical setup and a given set of resources. We note that the Reservoir Computing setup could give good results on the proposed tasks too, if we were greatly scaling up its effective dimensionality. This has been evidenced in, for example, [31], where good results on the TIMIT dataset were achieved by using Echo State Networks (a particular kind of Reservoir Computing) of up to 20,000 nodes. In our setup this would be achieved by increasing the number of masking steps Nm within one masking period. In reality, however, we will face two practical problems. First of all, there are bandwidth limitations in signal generation and measurement. Parts of the signal that fluctuate rapidly would be lost when reducing the duration of a single masking step. If one would scale up by keeping the length of the
mask steps fixed but use a longer physical delay, for instance a fiber of tens or hundreds of kilometers, the potential gain in performance comes at the cost of one of the systems important advantages: its speed. Also it is hard to foresee how other optical effects in such long fibers such as dispersion and attenuation, would affect performance. This would be an interesting research topic for future investigations. At the current stage we did not quantify how much the results in this paper hinge on the ability to model the system mathematically. This particular system can be modeled rather precisely, but it is unclear how fast the usefulness of the presented approach would degrade when the model becomes less acute. Several directions for future improvements are apparent. The most obvious one is that we could greatly simplify the training process by putting the DCMZ measurements directly in the training loop: instead of optimizing input masks in simulations, we could just as well directly use real, measured data. The Jacobians required for the backpropagation phase can be computed from the measured data. A training iteration would then consist of the following steps: sample data, produce the corresponding input to the DCMZ with the current input mask, measure the output, perform backpropagation in simulation, and update the parameters. The benefit would be that we directly end up with functional input and output masks, without the need for retraining. On top of that, data collection would be much faster. The only additional requirement for this setup would be the need for a single computer controlling both signal generation and signal measurement. The next direction for improvement would be to rethink the design of the system from a machine learning perspective. The current physical setup on which we applied backpropagation finds its origins in reservoir computing research. As we argue in Section 2, the system can be considered as a special case of recurrent network with a fixed, specific connection matrix between the hidden states at different time steps. In the reservoir computing paradigm, one always uses fixed dynamical systems that remain largely unoptimized, such that in the past this fact was not particularly restrictive. However, given the possibility of fully optimizing the system that was demonstrated in this paper, the question on how to redesign this system such that we can assert more control over the recurrent connection matrix, and hence the dynamics of the system itself, becomes far more relevant. Currently we have a fixed dynamical system of which we optimize the input signal to accommodate a certain signal processing task. As explained at the end of Section 3.2, it appears that backpropagation can currently only leverage the recurrence of the system to a limited degree, when using
a single delay loop. Therefore it would be more desirable to optimize both the input signal and the internal dynamics of the system to accommodate a certain task. Alternatively, the configuration can be easily extended to multiple delay loops, allowing for a richer recurrent connectivity. The most significant result of this paper is that we have shown experimentally that the backpropagation algorithm, a highly abstract machine learning algorithm, can be used as a tool in designing analog hardware to perform signal processing. This means that we may be able to vastly broaden the scope of research into physical and analog realizations of neural architectures. In the end this may result in systems that combine the best of both worlds: powerful processing capabilities at a tremendous speed and with a very low power consumption.
For each type of feature, we conduct one trial and tune the parameters for the logistic classifier using
5-fold cross validation. Then we adopt held-out testing taking advantage of the 20% sentences left.
Figure 4 and Figure 5 show the precision-recall curves for the twelve lexical features, and Table 3 displays the average F1-measure comparison among different features. We find out that the WS-class features generally outperform the BOWclass features, and the short-distance contextual features (K = 1) are more effective than the longdistance ones (K = 2, 3).
SentiPairs are built on three kinds of words: adjectives, verbs and nouns. In order to build ANP/VNP, we should first build the collection of words to choose from. We concluded some criterions for a good word collection.
1. Coverage, a good word collection should cover as much domains as possible in order to convey the information
2. Discrepancy, words of similar meanings should ap-
We introduced Synset Forest to resolve these three criterions. The Synset Forest is a forest consists of three trees, namely adjective tree, verb tree and the noun tree. An overlook of all three trees can be found at Figure 5.
In the Wordnet[6], Synsets are interlinked by means of conceptual-semantic and lexical relations.By proposing the Synset Forest, we modeled a unified semantic and concept architecture. The Synset Forest acts as a collection of candidate words for Adjective Noun Pairs and Verb Noun Pairs. Since each node comes with a sentiment score, the weight for each ANP/VNP is decided at the first place.
We now turn to describe the set of programs that implement the principal higher-level cognitive functions of the architecture, learning, planning, and attentional control.
As already described, the main components of AERA are its executive and memory, the latter containing inputs (both external and internal), predictions, goals, programs (models, monitors, etc.) and a collection of jobs. The executive uses these elements to (a) maintain and improve the system by adding/removing models to/from the memory, and by (b) controlling the priorities of all the jobs while (c) achieving the goals the system has set itself to fulfill the drives (top-level goals) given by the programmer.
Besides low-level technical components (scheduler, threads, etc.) the executive contains (fixed) algorithms, that are parameterized by jobs, to create programs such as chaining, monitoring, and pattern extraction programs (defined in section 5.2). Models and composite states are also programs, even though they don’t result from any parameterization: They are instead either given in the bootstrap code (shown here in grey), or learned. The inputs of the system consist of (a) sensory inputs, (b) goals and predictions produced by jobs and, (c) internal inputs produced by the executive (including instantiated models, instantiated composite states,
RUTR-SCS13006 24/56
success/failure of goals and predictions, performance assessments, etc., as explained in the preceding sections). Drives also constitute inputs, and are given in the bootstrap code (in grey).
A running AERA system faces three main challenges: (a) To update and revise its knowledge based on its experience, (b) to cope with its resource limitation while making decisions to satisfy its drives and, (c) to focus its attention on the most important inputs, discarding the rest or saving them for later processing. These three challenges are commonly addressed by, respectively, learning, planning, and controlling the attention of the system. Notice that all of these activities have an associated cost and have to be carried out concurrently. All these activities fit to some extent into the resource- and knowledge budget the system has at its disposal. That is the reason why they have been designed to result from the fine-grained interoperation of a multitude of lower-level jobs, the ordering of which is enforced by a scheduling strategy. This strategy has been designed to get the maximal global value for the system from the available inputs, knowledge, and resources, given (potentially conflicting) necessities. The list of jobs, their purpose, and scheduling priorities, is given in section 5.2.
Learning 5.1.1
Learning involves several phases: Acquiring new models, evaluating the performance of existing ones, and controlling the learning activity itself. Acquiring new models is referred to as pattern extraction, and consists of the identification of causal relationships between input pairs: Inputs which exhibit correlation are turned into patterns and used as the LT and RT of a new model. Model acquisition is triggered by either the unpredicted success of a goal or the failure of a prediction. In both cases AERA will consider the unpredicted outcome as the RT of new models and explore buffers of historical inputs to find suitable LTs. Once models have been produced, the system has to monitor their performance (a) to identify and delete unreliable models and, (b) to update the reliability as this control value is essential for scheduling (as described in section 4.3 above). Both these activities – model acquisition and revision – have an associated cost, and the system must allocate its limited resources to the jobs from which it expects the most value. Last but not least, the system is enticed to learn, based on its experience, about its progress in modeling inputs. The system computes and maintains the history of the success rate for classes of goals and predictions, and the priority of jobs dedicated to acquire new models is proportional to the first derivative of this success rate (this is detailed in section 5.2.5).
Planning 5.1.2
Planning concerns observing desired inputs (the states specified by goals) by acting on the environment (i.e. issuing commands) to achieve goals in due time in adversarial conditions, like for example the lack of appropriate models, underperforming models, conflicting or redundant goals, and lack of relevant inputs. Planning is initiated and sustained by the regular injection of drives (as defined by the programmer), thus putting the system under constant pressure from both its drives and its inputs. In our approach, sub-goals derived from goals are simulated, meaning that as long as time allows, the system will run “what if” scenarios to predict the outcome of the hypothetical success of these simulated goals, checking for conflicts and redundancies, eventually committing to the best goals found so far and discarding other contenders. Here again, goals are rated with respect to their expected value. Simulation and commitment operate concurrently with (and also make direct use of) forward and backward chaining.
RUTR-SCS13006 25/56
In this section, we assume that the function f is a linear mapping f(x) := 〈w, x〉, so that our cost function can be written as
`(w) := n∑ i=1 αih(−yi〈w, xi〉). (8)
In this section, we present two polynomial-time algorithms to approximately minimize this cost function over certain types of `p-balls. Both algorithms run multiple rounds of random initialization followed by arbitrary optimization steps. The first algorithm initializes the weight vector by uniformly drawing from a sphere. We next present and analyze a second algorithm in which the initialization follows from the solution of a least-square problem. It attains the stronger theoretical guarantees promised in the introduction.
The task of extractive summarization is to select a subset of sentences from a source document to present as a summary. Supervised approaches to this problem make use of training data in the form of source documents paired with existing summaries (Marcu, 1999; Osborne, 2002; Jing and McKeown, 1999; Ceylan and Mihalcea, 2009). These methods learn what features of a source sentence are likely to result in that sentence appearing in the summary; for news articles, for example, strong predictive features include the position of a sentence in a document (earlier is better), the sentence length (shorter is better), and the number of words in a sentence that are among the most frequent in the document.
Supervised discriminative summarization relies on an alignment between a source document and
its summary. For short texts and training pairs where a one-to-one alignment between source and abstract sentences can be expected, standard techniques from machine translation can be applied, including word-level alignment (Brown et al., 1990; Vogel et al., 1996; Och and Ney, 2003) and longer phrasal alignment (Daumé and Marcu, 2005), especially as adapted to the monolingual setting (Quirk et al., 2004). For longer texts where inference over all possible word alignments becomes intractable, effective approximations can be made, such as restricting the space of the available target alignments to only those that match the identity of the source word (Jing and McKeown, 1999).
The use of alignment techniques for book summarization, however, challenges some of these assumptions. The first is the disparity between the length of the source document and that of a summary. While the ratio between abstracts and source documents in the benchmark Ziff-Davis corpus of newswire (Marcu, 1999) is approximately 12% (133 words vs. 1,066 words), the length of a full-text book greatly overshadows the length of a simple summary. Figure 1 illustrates this with a dataset comprised of books from Project Gutenberg paired with plot summaries extracted from Wikipedia for a set of 439 books (described more fully in §4.1 below). The average ratio between a summary and its corresponding book is 1.2%.
This disparity in size leads to a potential violation of a second assumption: that we expect words and phrases in the source document to align with words and phrases in the target. When the disparity is so great, we might rather expect that an entire paragraph, page, or even chapter in a book aligns to a single summary sentence.
ar X
iv :1
30 5.
13 19
v1 [
cs .C
L ]
6 M
ay 2
01 3
To help adapt existing methods of supervised document summarization to books, we present two alignment techniques that are specifically adapted to the problem of book alignment, one that aligns passages of varying size in the source document to sentences in the summary, guided by the unigram language model probability of the sentence under that passage; and one that generalizes the HMM alignment model of Och and Ney (2003) to the case of long but sparsely aligned documents.
Since the recommendation accuracies of each recommended video for different users have unknown stochastic distributions, the natural way to learn a video’s performance is to record and update its sample mean reward (recommendation accuracy) for the same context arrivals. Using such an empirical value to evaluate the expected reward is the basic approach to help the service vendors to learn. Unfortunately, to handle the problem of social big data, recording and updating the sample mean reward of different videos for the same context are scarcely possible. The context space X can be extremely large and the dimension is prohibitively high. The memory capacity of the sever can not meet the need of keeping a sample mean reward for all contexts. To overcome this difficulty, we dynamically partition the entire context space into multiple smaller context subspaces (according to the number of arriving users). Then, we maintain and update the sample mean reward estimates for each subspace. This is due to the fact that the expected result of a video is likely to be similar for similar contexts. Next we propose our online learning model. In section V, we will refine our algorithm to geometric differential privacy to extensively reduce the performance loss.
The static analyzer takes the domain and problem instance as an input, grounds its propositional state information and infers different forms of planner independent static information.
Parsing Our simple Lisp parser generates a tree of Lisp entities. It reads the input files and recognizes the domain and problem name. To cope with variable typing, we temporarily assert constant typed predicates to be removed together with other constant predicates in a further pre-compilation step. Thereby, we infer a type hierarchy and an associated mapping of objects to types.
Indexing Based on the number of counted objects, indices for the grounded predicates, functions and actions are devised. Since in our example problem we have eight objects and the predicates at and in have two parameters, we reserve 2 · 8 · 8 = 128 index positions. Similarly, the function distance consumes 64 indices, while fuel,
1. [...] denotes that source fragments were omitted for the sake of brevity. In the given example these are the action definitions for passenger debarking and flying the airplane.
slow-speed, fast-speed, slow-burn, fast-burn, capacity, and refuel-rate each reserve eight index positions. For the quantities total-fuel-used, boarding-time, debarking-time only a single fact identifier is needed. Last but not least we model duration as an additional quantity total-time. This special variable is the only one that is overwritten in the least commitment planning approach when scheduling plans as described in Section 4.
Flattening Temporal Identifiers We interpret each action as an integral entity, so that all timed propositional and numerical preconditions can be merged. Similarly, all effects are merged, independent of time at which they happen. Invariant conditions like (over all (at ?a ?c)) in the action board are added into the precondition set. We discuss the rationale for this step in Section 4.1.
Grounding Propositions Fact-space exploration is a relaxed enumeration of the planning problem to determine a superset of all reachable facts. Algorithmically, a FIFO fact queue is compiled. Successively extracted facts at the front of the queue are matched to the operators. Each time all preconditions of an operator are fulfilled, the resulting atoms according to the positive effect (add) list are determined and enqueued. This allows us to separate off constant facts from fluents, since only the latter are reached by exploration.
Clustering Atoms For a concise encoding of the propositional part we separate fluents into groups, so that each state in the planning space can be expressed as a conjunction of (possibly trivial) facts drawn from each fact group (Edelkamp & Helmert, 1999). More precisely, let #pi(o1, . . . , oi−1, oi+1, . . . , on) be the number of objects oi for which the fact (p o1 . . . on) is true. We establish a single-valued invariant at i if #pi(o1, . . . , oi−1, oi+1, . . . , on) = 1. To allow for a better encoding, some predicates like at and in are merged. In the example, three groups determine the unique position of the persons (one of five) and one group determines the position of the plane (one of four). Therefore, 3 · ⌈log 5⌉ + 1 · ⌈log 4⌉ = 11 bits suffice to encode the total of 19 fluents.
Grounding Actions Fact-space exploration also determines all grounded operators. Once all preconditions are met and grounded, the symbolic effect lists are instantiated. In our case we determine 98 instantiated operators, which, by some further simplifications that eliminate duplicates and trivial operators (no-ops), are reduced to 43.
Grounding Functions Simultanous to fact space exploration of the propositional part of the problem, all heads of the numerical formulae in the effect lists are grounded. In the example case only three instantiated formulae are fluent (vary with time): (fuel plane) with initial value 750 as well as total-fuel-used and total-time both initialized with zero. All other numerical predicates are in fact constants that can be substituted in the formula-bodies. In the example, the effect in (board dan city-a) reduces to (increase (total-time) 30), while (zoom plane city-a city-b) has the numerical effects (increase (total-time) 150),(increase (total-fuel-used) 300)), and (decrease (fuel plane) 300). Refuelling, however, does not reduce to a single rational number, for example the effects in (refuel plane city-a) only simplify to (increase (total-time) (/ (- (750 (fuel plane)) / 12.5))) and (assign (fuel plane) 750). To evaluate the former assignment especially for a forward chaining planner, the variable (total-time) has to be instantiated on-the-fly. This is due to the fact that the value of the quantity (fuel plane) is not constant and itself changes over time.
Symmetry Detection Regularities of the planning problem with respect to the transposition of domain objects is partially determined in the static analyzer and is addressed in detail in Section 5.
The intermediate textual format of the static analyzer in annotated grounded PDDLlike representation serves as an interface for other planners or model checkers, and as an additional resource for plan visualization. Figures 5 and 6 show parts of the intermediate representation as inferred in the Zeno-Travel example.
In this section, we describe our graph-community detectionbased approach to the ts2ts relationship identification task. Given that we want to group similar document topic segments, we can formalize this problem in a clustering setting:
Input: A set of items S1, ..., Sm. Each item corresponds to the textual content of a topic segment.
Output: A mapping from each item to a particular cluster k ∈ 1, ...,K.
To perform the clustering task we propose the use of graph-community detection techniques. The idea is to analyze a weighted co-occurrence graph representation of document segments and find word communities that are representative of the topics in the documents. This is similar to the Metro Maps approach in [4]. We hypothesize that cooccurrence graphs can better model how words relate to one another in different documents. This contrasts with cluster approaches where features relate to individual words and the presence or absence of the features is based on whether the corresponding word occurred in the document or not.
The graph-community detection problem can be formalized as follows:
Input: a weighted co-occurrence graphGco = (W,E), where W is the set of nodes and E the set of edges. W corresponds to the set of words from a given set S of document segments. An edge (wi, wj) exists if words wi and wj occur in some segment Si ∈ S.
Output: a mapping from each word wi ∈W to a particular community c ∈ 1, ..., C.
Appropriately setting the weights w(i, j) of the edge is a topic of research in this work. Depending on how these weights are set, different word communities can be obtained. Therefore, it is necessary to develop an appropriate weighting scheme to the cross-document topic relationship identification task. In a related problem described in [4], a weighting scheme that counts the number of co-occurrences between words is used to discover a Metro Map structure of news articles. We hypothesize that having all word cooccurrence contribute in the same way to a weight score might not work in the document segment relationship task. Therefore, we designed the following tf-idf -based weighting schemes:
• Count: the number of times the words co-occurred in different segments (equivalent to [4]).
• Best tf-idf : the sum of the highest tf-idf values of the words.
• Count + Best tf-idf : the sum of the previous weights.
• Count + Avg tf-idf : the sum of the count weight and the sum of the average tf-idf values of the words.
After obtaining the word-communities, it is necessary to perform a mapping between them and the topics segments. This task needs to be performed since the previous step only provides groups of words. Therefore, it is necessary to determine how to group the topic segments. Doing this using word communities is, to the best of our knowledge, novel. It should be noted that although the work in [4] used graphcommunity detection, the problem being solved is different. The relationships between the documents were given a priori by grouping documents if they occurred in the same time span. The graph-community detection task was then carried out in those groups of documents to determine the metro stops. Our work is different since we cannot rely on this criterion to discover the document relationships. In this work we propose the use of a function to assign scores between a document segment and word communities. Segments are assigned to the highest scoring community. If
two document segments are assigned to the same community they are considered as equivalent. The mathematical formula that expresses this idea is defined as follows:
argmax c∈C score(seg, c), (1)
where C is the set of communities discovered in Gco, and seg is the set of words in a segment. It should be noted that different formulations of the score function can be designed. In the experiments to identify similar document segments we considered the following scoring functions:
scorec(seg, c) = |seg ∩ c| |c| , (2)
scoreseg(seg, c) = |seg ∩ c| |seg| , (3)
scoretfif (seg, c) =
|seg∩c|∑ wi tfidf(wi)
seg∑ wi tfidf(wi)
(4)
The first two functions count the common words between the segment and the community. The score is then normalized either by the total number of words in c or seg, respectively. The previous functions treat all words in the same way. Therefore, we also defined a function that makes words contribute according to their relevance, scoretfidf . The difference is that common words have a score corresponding to their normalized tf-idf value.
In this study, P characterizes the fraction of tweets which were actually generated in source bin (s, t) end up in the three detector bins: precise location st(1), potentially noisy location st(2), and missing location t(3). We define P as follows:
P (s,t)(1),(s,t) = 0.03, and P (r,t)(1),(s,t) = 0 for ∀r 6= s to reflect the fact that we know precisely 3% of the target posts’ location.
P (r,t)(2),(s,t) = 0.47Mr,s for all r, s.M is a 49×49 “mis-self-declare” matrix.Mr,s is the probability that a user self-declares in her profile that she is in state r, but her post is in fact generated in state s. We estimated M from a separate large set of tweets with both coordinates and self-declared profile locations. The M matrix is asymmetric and interesting in its own right: many posts self-declared in California or New York were actually produced all over the country; many self-declared in
3There were actually only a fraction of all tweets without location which came from all over the world. We estimated this US/World fraction using z.
Washington DC were actually produced in Maryland or Virgina; more posts self-declare Wisconsin but were actually in Illinois than the other way around.
Pt(3),(s,t) = 0.50. This aggregates tweets with missing information into the third kind of detector bins.
In order to arrive at the desired contradiction we will distinguish two cases: (a) p = r ; and (b) p ∈ S .
Case (a): p = r . In this case, p = r not accepted means that v∗r ≤ η , and all α ∈ C \ {p} rejected means all s ∈ S accepted, i. e. v∗s > η . By plugging these inequalities in (45) we arrive at the false conclusion that η > η .
Case (b): p ∈ S . In this case, r is rejected, so that v∗r < η , and the s ∈ S are either accepted or not rejected, so that v∗s ≥ η . Therefore, we also arrive at the false conclusion that η > η .
Compared with the bilateral decision criterion (4–5), the unilateral one (42–44) leaves much less room for undecidedness. For the bilateral criterion, increasing η has the effect of thickening the region of undecidedness. In contrast, for the unilateral criterion it has only the effect of moving the boundary. This happens at the expense of disregarding the evidence in favour of p for p ∈ Π+ . So, the unilateral criterion may accept p in spite of a stronger evidence in favour of p , or it may reject p in spite of p having a weaker evidence than p . On the other hand, the extent of such discrepancies is limited as stated in the following result:
Proposition 4.13. The unilateral decisions associated with a definite Horn doctrine are related to the bilateral ones in the following ways: (a) For any p ∈ Π+ satisfying vp + vp ≥ 1, if p is accepted [resp. not rejected ] by the bilateral decision of margin η , then p is accepted [resp. not rejected ] by the unilateral decision of margin (1+η)/2. (b) For any p ∈ Π+ , if p is accepted [resp. not rejected ] by the unilateral decision of margin η ≥ η0 , then p is also accepted [resp. not rejected ] by the bilateral decision of margin η − η0 , where η0 := maxq∈Π+ vq .
Proof. In order to obtain part (a) it suffices to notice that vp + vp ≥ 1 implies v∗p + v ∗ p ≥ 1 and therefore v∗p ≥ [ (v∗p−v∗p) + 1 ] / 2. Part (b) is a consequence of the inequality v∗p − v∗p ≥ v∗p − η0 , which follows immediately from (41).
4.5 Autarkic sets. A set Σ ⊂ Π will be said to be autarkic for a conjunctive normal form Φ(D) when it has the following properties: (a) Σ does not contain at the same time a literal p and its negation p ; (b) for any clause C ∈ D , if C contains p for some p ∈ Σ , then C contains also some q ∈ Σ . Such a situation arises also in satisfiability theory, where one calls then autarkic the partial truth assignment u that sets up = 1 and up = 0 for p ∈ Σ and up = 12 (undecided) for p ∈ Π \ (Σ ∪ Σ) [20 ].
Deciding about logically constrained issues, § 4 29
Again, the property of autarky is preserved by the operations of absorption and resolution (checking it is a little exercise). As a consequence, it passes on to the corresponding Blake canonical form. So, Σ ⊂ Π is autarkic for the latter if and only if it is autarkic for some logically equivalent conjunctive normal form. In such a situation, we can say simply that Σ is autarkic for the given doctrine.
For a definite Horn doctrine, Π+ is easily seen to be an autarkic set. This is a particular case of the following more general fact: if u is a truth assignment consistent with D , then Σ = {α ∈ Π | up = 1 } is an autarkic set. Generally speaking, however, autarkic sets need not decide on every issue. Even so, their definition allows for the following generalization of Proposition 3.8:
Proposition 4.14. An autarkic set Σ has the property that
max p∈Σ v∗p ≤ max q∈Σ vq. (46)
Proof. Since v∗ is obtained by iterating the transformation v 7→ v′ , it suffices to show that this transformation has the following property analogous to (46): v′p ≤ maxq∈Σ vq for any p ∈ Σ . This follows immediately from (7) because, by definition, Σ being autarkic requires every clause C that contains p with p ∈ Σ to contain also some q ∈ Σ , which ensures that minα∈C,α6=p vα ≤ maxq∈Σ vq .
Theorem 4.15. Assume that the original valuation satisfies
min p∈Σ vp − max p∈Σ vp > η, (47)
for some autarkic set Σ and some η ∈ [0, 1). In this case, the decision of margin η associated with the upper revised valuation v∗ accepts every proposition in Σ .
Proof. By combining (46), (47) and part (c) of Theorem 3.2, it follows that
min p∈Σ v∗p − max p∈Σ v∗p > η, (48)
which implies v∗p − v∗p > η for any p ∈ Σ .
As can be seen in the results, the final variance of the scores achieved is consistently lower when using macros (for Qbert in particular, the variance is dramatically smaller, while achieving higher final scores). We believe that this results from a combination of better exploration and better propagation of rewards. Using atomic actions only, the agent may not consistently explore areas with higher Q-values, which may lead the learner for finite training time to learn different policies. Higher exploration, and better propagation of the rewards, will make this less likely, since the learner will explore more distant states, and will have states with Q-values differing to choose from. Conclusively, macros lead to a policy where the agent has high confidence for the best action, leading to more stable policies.
Most of the computational cost of CDCL solvers is spent on BCP. Moskewicz et al. [3] state that in most cases it is greater than 90% of the total cost. This observation has consequences for rapid restart strategies: If a solver would restart very frequently, say after every couple of conflicts, then it often has to go down the search-tree all the way from the root. As a result, much more time will be spent on BCP slowing down the solver.
An important breakthrough in speeding up BCP is the introduction of the watch literal data structure in zChaff [3]. This data structure is now used in all state-of-the-art CDCL solvers. It has been implemented very efficiently in MiniSAT [15]. Recent improvements of this data structure were used in picoSAT [7]. Additionally, the relative burden of BCP can be reduced by spending more time on reasoning techniques. For example by making conflict analysis stronger. Two recent improvements in this direction are conflict clause minimization [19] and conflict clause (self-) subsumption [20].
Both developments influence the optimal restart strategy. The cheaper the relative cost of going down the search-tree, the cheaper it is to perform a restart. Therefore, it is expected that future improvements in BCP speed and additional reasoning will ensure that the optimal restart strategy will be more rapid.
Blocksworld, Depots, and Grid domains. The description of the planners is given in Section 6; here the fork-decomposition heuristics are computed fully online. Column task denotes problem instance, column h∗ denotes optimal solution length. Other columns capture the run time and number of expanded nodes.
Logistics-ipc2, and Mprime domains.
Pipesworld-Tankage, TPP, and Trucks domains.
Blocksworld, Depots, and Driverlog domains. The description of the planners is given in Section 6; here the fork-decomposition heuristics are via structural-pattern databases. Column task denotes problem instance, column h∗ denotes optimal solution length. Other columns capture the run time and number of expanded nodes.
Logistics-ipc2, and Mprime domains.
NoTankage, Pipesworld-Tankage, Rovers, and Satellite domains.
Openstacks-strips-08, Parcprinter, and Scanalyzer domains. The description of the planners is given in Section 6; here the fork-decomposition heuristics are via structural-pattern databases. Column task denotes problem instance, column h∗ denotes optimal solution length. Other columns capture the run time and number of expanded nodes.
ing domains.
The problem of learning discrete Bayesian networks from data is encoded as a weighted MAX-SAT problem and the MaxWalkSat local search algorithm is used to address it. For each dataset, the per-variable summands of the (BDeu) marginal likelihood for different choices of parents (‘family scores’) are computed prior to applying MaxWalkSat. Each permissible choice of parents for each variable is encoded as a distinct propositional atom and the associated family score encoded as a ‘soft’ weighted single-literal clause. Two approaches to enforcing acyclicity are considered: either by encoding the ancestor relation or by attaching a total order to each graph and encoding that. The latter approach gives better results. Learning experiments have been conducted on 21 synthetic datasets sampled from 7 BNs. The largest dataset has 10,000 datapoints and 60 variables producing (for the ‘ancestor’ encoding) a weighted CNF input file with 19,932 atoms and 269,367 clauses. For most datasets, MaxWalkSat quickly finds BNs with higher BDeu score than the ‘true’ BN. The effect of adding prior information is assessed. It is further shown that Bayesian model averaging can be effected by collecting BNs generated during the search.
We implemented our model into a numerical procedure in order to compare the statistical measures to the analytical calculations in practice, adhering to Algorithm 1, Cstat, statistically sampled as described therein. Numerical validation is provided here for two aspects of our theoretical work: Theorem 1 and the analytic approximation for the covariance matrix.
5.1 Cstat versus H We generated a large set of random positive-definite matrices at various dimensions {n} with a spectrum of condition numbers. For each trial , the numerical procedure generated a random symmetric matrix A, diagonalized it into a set of orthonormal eigenvectors U, drew n random positive numbers in a diagonal matrix D, and set H = UAU−1 . We then applied Algorithm 1 by considering {H} as landscape Hessians.
The resultant covariance matrices, { Cstat }
, were diagonalized and compared to the Hessian matrices and their eigendecomposition – which always matched. In practice, it was evident that the two matrices always commute (applying the commutator operator yields a zero matrix to a practical precision considering the max norm),
∀ ‖HCstat − Cstat H‖max < 10−1, (33)
as claimed. However, the covariance matrices were not the inverse forms of the Hessian matrices.
5.2 Analytic Approximation for Cstat
Here, we corroborated the analytic approximation for the covariance matrix. To this end, we considered four quadratic basins of attraction at various search-space dimensions: (H-1) n = 3, H1 = [√ 2/2 0.25 0.1; 0.25 1 0; 0.1 0 √ 2 ] (H-2) n = 10, H2 = diag [1.0, 1.5, . . . , 5.5] (H-3) n = 30, H3 = diag [ ~I10, 2 · ~I10, 3 · ~I10 ] (H-4) n = 100, H4 = 2.0 · I100×100
Sentiment Analysis is the computational study of people’s opinions, attitudes, and emotions towards topics covered by reviews or news [1]. SA is considered also a classification process which is the task of classifying text to represent a positive or negative sentiment [2-4]. The classification process is usually formulated as a two-class classification problem; positive and negative. Since it is a text classification problem, any existing supervised learning method can be applied, e.g., Naïve Bayes (NB) classifier.
The web has become a very important source of information recently as it becomes a readwrite platform. The dramatic increase of OSN, video sharing sites, online news, online reviews sites, online forums and blogs has made the user-generated content, in the form of unstructured free text gains a considerable attention due to its importance for many businesses. The web is used by many languages’ speakers. It is no longer used by English speakers only. The need of SA systems that can analyze OSN in other languages than English is compulsory.
Arabic is spoken by more than 300 million people, and is the fastest-growing language on the web (with an annual growth rate of 2,501.2% in the number of Internet users as of 2010, compared to 1,825.8% for Russian, 1,478.7% for Chinese and 301.4% for English) (http://www.internetworldstats.com/stats7.htm). Arabic is a Semitic language [5] and consists of many different regional dialects. However, these dialects are true native language forms which are used in informal daily communication and are not standardized or taught in schools [6]. Despite this fact but in reality the internet users especially on OSN sites and some of the blogs and reviews site as well, use their own dialect to express their feelings. The only formal written standard for Arabic is the MSA. It is commonly used in written media and education. There is a large degree of difference between MSA and most Arabic dialects as MSA is not actually the native language of any Arabic country [7].
There is lack of language resources of Arabic language and most of them are under development. In order to use Arabic language in SA, there are some text processing are needed like removing stopwords or Part-of-Speech (POS) tagging. There are some sources of stopword lists and POS taggers are publicly available but they work on MSA not Arabic dialect. This paper tackles the first problem of removing stopwords. Stopwords are more
typical words used in many sentences and have no significant semantic relation to the context in which they exist.
In the literature, there are some research works have generated stopword lists but as far as our knowledge no one has generated a stopword list for Arabic dialects. In [8] they have proposed an algorithm for removing stopwords based on a finite state machine. They have used a previously generated stopword list on MSA. In [9] they have created a corpus-based list from newswire and query sets and a general list using the same corpus and then compare the effectiveness of these lists on the information retrieval systems. The lists are on MSA too. In [10] they have generated a stopword list of MSA from the highest frequent meaningless words appear in their corpus.
The aim of this paper is to investigate the effect of removing stopwords on SA for OSN Arabic data. Since the OSN sites and the reviews sites use the simple Egyptian dialect. The creation of a stopword list of Egyptian dialect is mandatory. The data are collected from OSN sites Facebook and Twitter [11-20] on Egyptian movies. We used an Arabic review site as well that allow users to write critics about the movies (https://www.elcinema.com). The used language by the users in the review is syntactically simple with many words of Egyptian dialects included. The data from OSN is characterized by being noisy and unstructured. Abbreviations and smiley faces are frequently used in OSN and sometimes in review site too. There is a need for many preprocessing and cleaning steps for this data to be prepared for SA. The Arabic users either write with Arabic or with franco-arab (writing Arabic words in English letters) e.g. the word “maloosh” which stands for “شولام” which means “doesn’t have”. This is an Egyptian dialect word which is written in MSA as “هل سيل”. Sometimes they use English word in the middle of an Arabic sentence which must be translated.
We are tackling the problem of classifying reviews and OSN data about movies into two classes, positive and negative as was first presented in [2] but on Arabic language. In their work they used unigram and bigram as Feature Selection (FS) techniques. It was shown that using unigrams as features in classification gives the highest accuracy with NB. We have used the same feature selection techniques, unigrams and bigram along with NB and Decision Tree (DT) as classifiers.
We have proposed a methodology for preparing corpora from OSN which consists of many steps of cleaning, converting Franc-arab to Arabic words and translation of English words that appear in the middle of Arabic sentences to Arabic. We have also proposed a methodology of generating stopword lists from the corpora. The methodology consists of three phases which are: calculating the words’ frequency of occurrence, check the validity of a word to be a stopword, and adding possible prefixes and suffixes to the words generated.
The contribution of this paper is as follows. First, we propose a methodology for preparing corpora from OSN sites in Arabic language. Second, we propose a methodology for creating a stopword list for Egyptian dialect to be suitable for OSN corpora. Third, we prepare corpus from Facebook which was not tackled in the literature for Arabic language. Fourth, tackling OSN data in Arabic language is new as it wasn’t investigated much. Fourth, tackling DT classifier with these kinds of corpora is new as it wasn’t investigated much in the literature. Finally, the measure of classifiers’ training time and considering it in the evaluation is new in this field.
The paper is organized as follows; section 2 presents the methodology. The stopword list generation is tackled in section 3. The Experimental setup and results are presented in section 4. A discussion of the results and analysis of corpora is presented in section 5. Section 6 presents the conclusion and future work.
We use Gringo version 4.5 (Gebser et al., 2011) and we use WASP version 2 (Git hash a44a95) (Alviano et al., 2015a) configured to use unsat-core optimisation with disjunctive core partitioning, core trimming, a budget of 30 seconds for computing the first answer set and for shrinking unsatisfiable cores with progressive shrinking strategy. These parameters were found most effective in preliminary experiments. We configure our modified XHAIL solver to allocate a budget of 1800 seconds for the Induction part which optimises the hypothesis (see Section 4.2). Memory usage never exceeded 5 GB.
Tables 4–6 contains the experimental results for each Dataset, where columns Size, Pr, and So respectively, show the number of sentences used to learn the hypothesis, the pruning parameter for generalising the learned hypothesis (see Section 4.1), and the rate of how close the learned hypothesis is to the optimal result, respectively. So is computed according to the following formula: So= Upperbound−Lowerbound
Lowerbound ,
which is based on upper and lower bounds on the cost of the answer set. An So value of zero means optimality, and values above zero mean suboptimality; so the higher the value, the further away from optimality. Our results comprise of the mean and standard deviation of the F1-scores obtained from our 11-fold cross-validation test set of S1 and S2 individually (column CV). Due to lack of space, we opted to leave out the scores of precision and recall, but these values show similar trends as in the Test set. For the Test sets of both S1 and S2, we include the mean and standard deviation of the Precision, Recall and F1-scores (column group T).
12
When testing machine-learning based systems, comparing results obtained on a single test set is often not sufficient, therefore we performed cross-validation to obtain mean and standard deviation about our benchmark metrics. To obtain even more solid evidence about the significance of the measured results, we additionally performed a one-tailed paired t-test to check if a measured F1 score is significantly higher in one setting than in another one. We consider a result significant if p < 0.05, i.e., if there is a probability of less than 5 % that the result is due to chance. Our test is one-tailed because we check whether one result is higher than another one, and it is a paired test because we test different parameters on the same set of 11 training/test splits in cross-validation. There are even more powerful methods for proving significance of results such as bootstrap sampling (Efron and Tibshirani, 1986), however these methods require markedly higher computational effort in experiments and our experiments already show significance with the t-test.
Rows of Tables 4–6 contain results for learning from 100 resp. 500 example sentences, and for different pruning parameters. For both learning set sizes, we increased pruning stepwise starting from value 0 until we found an optimal hypothesis (So=0) or until we saw a clear peak in classification score in cross-validation (in that case, increasing the pruning is pointless because it would increase optimality of the hypothesis but decrease the prediction scores).
Note that datasets have been tokenised very differently, and that also state-of-the-art systems in SemEval used separate preprocessing methods for each dataset. We follow this strategy to allow a fair comparison. One example for such a difference is the Images dataset, where the ‘.’ is considered as a separate token and is later defined as a separate chunk, however in Answers-Students dataset it is integrated onto neighboring tokens.
Obtaining spatial awareness from raw
input text has been a primary research area in the field of situation awareness. The main issues faced by researches in this area are modelling/ representation, even extraction and disambiguation, querying, reasoning and visualization [20]. Spatial descriptions found in free text are mostly based on human perceptions and perceptions are intrinsically imprecise.
The qualitative approach to the
representation of spatial knowledge is heavily inspired by the way spatial information is expressed verbally. The spatial propositions might correspond to several different spatial relations depending on the context in which they are used. This leads to a degree of spatial uncertainty. One of the sources of uncertainty is the need to express a certain relationship independently of the context in which it occurs [21].
The proof is by contradiction. Let j be the smallest index with 0 ≤ j ≤ n such that ISj 6= ISn |DPSj or I − Sj 6= I−Sn |DPSj . Note that 0 < j as IS0 = I − S0 = DPS0 = ∅. As Sj is a successor of Sj−1, we have ISj = ISj−1 ∪∆ and I−Sj = I−Sj−1 ∪∆−, where ∆,∆− ⊆ Drnew(Sj−1,Sj ), DSj−1 ∩ (∆ ∪∆−) = ∅, and Drnew(Sj−1,Sj ) ⊆ ISj ∪ I−Sj . As we have ISj−1 = ISn |DPSj−1 and I − Sj−1 = I − Sn |DPSj−1 , it holds that
ISj−1 ∪ ISn |Dδ = ISn |DPSj−1 ∪ ISn |Dδ = ISn |DPSj and I−Sj−1 ∪ I−Sn |Dδ = I−Sn |DPSj−1 ∪ I − Sn |Dδ = I−Sn |DPSj ,
where Dδ = DPSj \ DPSj−1 . For establishing the contradiction, it suffices to show that ISn |Dδ = ∆ and I−Sn |Dδ = ∆−. Consider some a ∈ ∆. Then, a ∈ Dδ because a ∈ Drnew(Sj−1,Sj ), DSj−1 ∩ (∆ ∪∆−) = ∅, and DPSj−1 ⊆ DSj−1 . Moreover, a ∈ ISj implies a ∈ ISn and therefore ∆ ⊆ ISn |Dδ . Now, consider some b ∈ ISn |Dδ . As Drnew(Sj−1,Sj ) ⊆ ISj ∪ I−Sj , we have b ∈ ISj ∪ I−Sj . Consider the case that b ∈ I−Sj . Then, also b ∈ I−Sn which is a contradiction to b ∈ ISn as Sn is a state structure. Hence, b ∈ ISj = ISj−1 ∪∆. First, assume b ∈ ISj−1 . This leads to a contradiction as then b ∈ DPSj−1 since ISj−1 = ISn |DPSj−1 . It follows that b ∈ ∆ and therefore ∆ = ISn |Dδ . One can show that ∆− = I−Sn |Dδ analogously.
Semantic segmentation is a problem that requires the integration of information from various spatial scales. It also implies balancing local and global information. On the one hand, fine-grained or local information is crucial to achieve good pixel-level accuracy. On the other hand, it is also important to integrate information from the global context of the image to be able to resolve local ambiguities.
Fig. 10: Comparison of SegNet (left) and FCN (right) decoders. While SegNet uses max-pooling indices from the corresponding encoder stage to upsample, FCN learns deconvolution filters to upsample (adding the corresponding feature map from the encoder stage). Figure reproduced from [66].
Vanilla CNNs struggle with this balance. Pooling layers, which allow the networks to achieve some degree of spatial invariance and keep computational cost at bay, dispose of the global context information. Even purely CNNs – without pooling layers – are limited since the receptive field of their units can only grow linearly with the number of layers.
Many approaches can be taken to make CNNs aware of that global information: refinement as a post-processing step with Conditional Random Fields (CRFs), dilated convolutions, multi-scale aggregation, or even defer the context modeling to another kind of deep networks such as RNNs.
Out-of-domain data set. For out-of-domain data sets we use the Chinese TreeBank (CTB) 7.0. The Chinese TreeBank is a word segmented, POS-tagged and syntactically bracketed corpus and it is widely used in the NLP community to train word segmentation, POS-tagging, and syntactic parsing systems. This version of the Chinese TreeBank consists of 2,448 text files, 51,447 sentences, 1,196,329 words and 1,931,381 hanzi (Chinese characters). This data set has a variety of different sources, including Xinhua news wire, news magazine articles, transcribed broadcast news and broadcast conversations, as well as newsgroup and weblog articles. However, none of these data sources are technical in nature. We use the word segmentation and POS tags annotation in this data set and make no use of its syntactic structures in our experiments.
In-domain data set. Since we are not aware of a publicly available manually annotated Chinese patent data sets that we can use for training and benchmarking purposes, we annotated 142 Chinese patents following the CTB word segmentation guidelines (Xia 2000). Since the original guidelines are mainly designed to cover non-technical everyday language, particularly newswire, many scientific and technical terms found in patents are not covered in the guidelines. We had to extend the CTB word segmentation guidelines to handle these new cases. Deciding on how to segment these scientific and technical terms is a big challenge since these patents cover many different technical fields and without proper technical background, even a native speaker has difficulty in segmenting them properly. For example, “大肠杆菌” is a biomedical terminology, and it means "colibacillus", but since “大肠” (meaning "colon") and “杆菌” (meaning "bacillus") are also words in Chinese, there are two possible ways of segmenting the string “大 肠杆菌”: as two words or as one single word. This is a familiar dilemma in word segmentation of Chinese text, even for everyday language. The difference is that in this case, one has to have some background knowledge in bio-medicine in order to realize that “大肠杆菌” is a technical term and should be treated as one word. For difficult scientific and technical terms, we consult BaiduBaike ("Baidu Encyclopedia", http://baike.baidu.com/), which we use as a scientific and technical terminology dictionary during our annotation. There are still many words that do not appear in BaiduBaiKe, and these include chemical names and formulas. These chemical names and formulas (e.g., “１－溴－３－氯丙烷/1-bromo-3chloropropane”) are usually very long, and unlike everyday words, they often have numbers and punctuation marks in them. We decided not to try segmenting the internal structures of such chemical terms and treat them as single words, because without a technical background in chemistry, it is very hard to segment their internal structures consistently.
The annotated patent dataset covers many topics and they include chemistry, mechanics, medicine,
etc. If we consider the words in our annotated dataset but not in CTB 7.0 data as new words (or outof-vocabulary, OOV), the new words account for 18.3% of the patent corpus by token and 68.1% by type. This shows that there is a large number of words in the patent corpus that are not in the everyday language vocabulary. Table 1 presents the data split used in our experiments.
The ask predicate [Merritt1989] will have to determine from the user whether or not a given attribute-value pair is true for a specific person. A new predicate, known is used to remember the user's answers to questions. It is not specified directly in the program, but rather is dynamically asserted whenever ask gets
new information from the user. Every time ask is called it first checks to see if the answer is already known to be yes or no. If it is not already known, then ask will assert it after it gets a response from the user. The arguments to known are: yes/no, attribute, person, and value. Our new version of ask looks like: ask(A, P, V) known(yes, A, P, V), !. % succeed if true and stop looking ask(A, P, V) known(_,A, P, V), !, fail. % fail if false ask(A, P, V) write(A, " of person ", P, " is ", V, " ? "), readln(Y), % get the answer asserta(known(Y, A, P, V)), % remember it Y = yes. % succeed or fail
1 // Fast2food with Gaussian RBF ke rne l 2 3 random device rd ; 4 5 // seed random d i s t r i b u t i o n s 6 const unsigned long seed = (unsigned long ) rd ( ) ; 7 8 Fast2food ∗ f a s t 2 f o od = 9 Fast2food ∗ f a s t 2 f o od = Fast2foodFactory : : c r ea t eFas t2 food ( Fast2foodFactory : :RBF
, datain , nvec , dim , D, seed , sigma ) ; 10 11 f loat ∗ da t a f e a t u r e s ; 12 fa s t2 f ood−> f f f a s t f o o d ( ) ; // f a s t f o od computation 13 da t a f e a t u r e s = fa s t2 f ood−> f f f e a t u r e s ( ) ; // complex map computation
1 // Fast2food with Matern RBF ke rne l 2 3 random device rd ; 4 5 // seed random d i s t r i b u t i o n s 6 const unsigned long seed = (unsigned long ) rd ( ) ; 7 8 Fast2food ∗ f a s t 2 f o od = 9 Fast2food ∗ f a s t 2 f o od = Fast2foodFactory : : c r ea t eFas t2 food ( Fast2foodFactory : :
Matern , datain , nvec , dim , D, seed , sigma , t ) ; 10 11 f loat ∗ da t a f e a t u r e s ; 12 fa s t2 f ood−> f f f a s t f o o d ( ) ; // f a s t f o od computation 13 da t a f e a t u r e s = fa s t2 f ood−> f f f e a t u r e s ( ) ; // complex map computation
The second reason for the incorrectness of the assertion in the original proof (Mailler & Lesser, 2006) is the potential existence of obsolete information of external constraints. This reason involves a scenario in which two neighboring mediation sessions are performed concurrently. Both the mediation sessions in the scenario end with finding a solution that presumably has no external conflicts, but the combination of both solutions causes new conflicts. This was the case with the mediation sessions of agents A1 and A5 in the example of section 4. Such a scenario seemingly fits the first case in the assertion, in which no external conflicts are found by each of the mediation sessions. Consequently, no externalconflict-free partial solution is found – contradicting the assertion. Furthermore, none of the mediators increase their good list. This enables the occurrence of an infinite loop, as displayed in section 4.
Our experiments show it is effective to use SAFs to improve the robustness of CNNs. Without substantial changes to the accuracies on clean samples, we obtain remarkably better robustness against adversarial, nonsense and noisy samples. That supports our proposal that SAFs can improve the robustness of CNNs by suppressing unusual signals.
Since we change only the activation functions of CNNs, while leaving their structure, training strategies and optimization methods untouched, it implies there are no severe weaknesses in existing CNN frameworks. Nevertheless, in contrast with popular activation functions including sigmoid and ReLU, the SAFs have less documented support from neuroscience research. Furthermore, it remains a question how to accommodate SAFs better by fine-tuning the CNN structure and training strategies to get the better
Table 3. Structures of the plain and robust CNNs for CIFAR-10. Parameters of convolutional layers: cv1-(5, 5, 32), cv2-(5, 5, 32), cv3(5, 5, 64) (in the height-width-channel order). The number of hidden units in fully-connected layers are 64 (fc1) and 10 (fc2). max-max pooling. avg-average pooling. sloss-softmaxloss. hloss-hrbridloss.
Models Layers #Layers Plain cv1 - max cv2 - avg cv3 - avg fc1 ReLU fc2 - sloss 10 RBF cv1 1-D RBF max cv2 1-D RBF avg cv3 1-D RBF avg fc1 ReLU fc2 1-D RBF hloss 14 mReLU cv1 mReLU max cv2 mReLU avg cv3 mReLU avg fc1 ReLU fc2 1-D RBF hloss 14
0 0.05 0.1 0.15 0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
cl as
si fic
at io
n ac
cu ra
cy
β 0 0.05 0.1 0.15
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
cl as
si fic
at io
n ac
cu ra
cy
β 0 0.05 0.1 0.15
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
cl as
si fic
at io
n ac
cu ra
cy
β
plain plain−r−m RBF RBF−r−m mReLU mReLU−r−m
(a) (b) (c)
Figure 11. Accuracies of the CNNs on the CIFAR-10 test set. (a), (b) and (c) are the accuracies on adversarial, nonsense and noisy samples respectively. The horizontal axes are the perturbation strength β’s.
performance. In that sense, we believe the attempt in this paper is just a start and far from the end.
We constructed a senone-based utterance VSM (section 2.1) based on 20 hours of speech; 10 hours English (which we got from [?]) and 10 hours Arabic (randomly sampled from our training data, section 3). Binary classification (English vs Arabic) using an SVM classifier, was then performed and it yielded 100% accuracy on the 1.5 hour test set. The reason to choose the senone-based feature space and not the i-vector-based feature space for classification is to avoid channel mismatch, as the English data came from a different source domain. We did a similar experiment to classify MSA versus all dialectal Arabic and again obtained 100% classification accuracy.
In order to build a reference corpus, 500 messages from the initial dataset have been randomly selected. Then, 3 human judges determined the main discursive function of each message among the 3 functions identified in the previous step (Emotive, Phatic or Referential). We used the Fleiss’ kappa [42] to measure the reliability of agreement between judges. For this task, the Fleiss’ Kappa is 0.70 which is a good agreement according to the scale proposed by Landis et al. [43]. In other words, even if in a few cases this task can be complex for humans (when messages have a really poor syntax or contain only an emoticon for example), most of the time humans are able to understand with what intention a message was transmitted.
Finally, for each message of the reference corpus, the category is the one chosen by at least two judges. There is no cases where all judges disagreed with the two others. The reference corpus contains 240 messages with a Phatic function, 137 messages with an Emotive function and 123 messages with a Referential function.
The critic step of pAC estimates Z-value and the average cost by minimizing the least-square error between the true Z-value and estimated one denoted by Ẑ.
min ν,Ẑavg
1
2 ∫ x ( ẐavgẐ(x;ν)−ZavgZ(x) )2 dx, (7)
s.t. ∫ x Ẑ(x;ν)dx = C, ∀x 0 < Ẑ(x;ν) ≤ 1 Ẑavg ,
where ν is a parameter vector of the approximation and C is a constant value used to avoid convergence to the trivial solution Ẑ(x;ν) = 0 for all x. The second constraint comes from ∀x, Z(x) := e−V (x) > 0 and ∀x, q(x) ≥ 0. The latter implies that V + Vavg > 0, and note that ZavgZ(x) := e −(V+Vavg), which is less than 1.
We minimize the least-square error in Eq. 7, ẐavgẐk − ZavgZk, with TD-learning. The latter minimizes TD error instead of the least-square error that requires the true Z(x) and Zavg , which are not available. The TD error denoted as eik for linearized Bellman equation is defined using a sample (xk,xk+1) of passive dynamics as, eik := Ẑ i avgẐ i k − e−qk Ẑik+1, where the superscript i denotes the
iteration. Ẑavg here is updated using the gradient as follows:
Ẑi+1avg = Ẑ i avg − αi1
∂ ( eik )2
∂Ẑavg = Ẑiavg − 2αi1eikẐik, (8)
where αi1 is the learning rate, which may adapt with iterations.
In this work, we approximate the Z-value function in two ways: (i) using a linear combination of weighted RBFs, and (ii) using a neural network (NN). When a NN with an exponentiated activation function of output layer is used, the parameters ν are updated with the following gradient based on backpropagation. 1
∂
∂νi
( ẐavgẐ i k − ZavgZk )2 ≈ 2eikẐiavg
∂Ẑik ∂νi , (9)
where eik is the TD error as defined previously.
On the other hand, when weighted RBFs are used, Ẑ(x;ν) := ν>f(x), and a Lagrangian relaxation of the objective function is useful as it includes the three constraints weighted using Lagrangian parameters λ1, λ2 and λ3. For convenience, denote ν̃i as,
ν̃i := νi − 2αi2eikẐiavg ∂Ẑik ∂νi ,
1e−tanh(x) or e−softplus(x) is used as an activation function of the output layer to satisfy the constraint Ẑ ≥ 0. The constraint ∫ x Ẑ(x;ν)dx = C is ignored in practice because convergence to ∀x, Ẑ(x;ν) = 0 is rare. min ([1, e−qk Ẑik+1]) is used instead of e −qk Ẑik+1 to satisfy Ẑ ≤ 1/Zavg in Eq. 7.
where αi2 is the learning rate. The update of ν cognizant of the constraints is then,
νi+1 = ν̃i + ∂
∂ν̃i
( λi1 (∫ x ν̃i>f(x)dx− C ) + λi>2 ( ν̃i − 0 ) + λi3 ( Ẑk − 1/Ẑavg )) ,
= ν̃i + λi11 + λ i 2 + λ i 3fk, (10)
where we utilize ∫ x
f(x)dx = 1 and replace the constraint ∀x Ẑ(x;ν) > 0 by ν > 0 because the constraint on ν always satisfies the former. 1 is a vector of all ones.
Lagrangian parameters λ1, λ2 and λ3 in each iteration are obtained by ensuring that the updated parameter vector νi+1 satisfies the three constraints in Eq. 7 for xk. Formally,
Ẑ(xk;ν i+1) = (ν̃i + λi11 + λ i 2 + λ i 3fk) >fk ≤ 1/Ẑavg, νi+1 = ν̃i + λi11 + λ i 2 + λ
i 3fk > 0 and,∫
x
Ẑ(x;νi+1)dx = ( ν̃i + λi11 + λ i 2 + λ i 3fk )> 1 = C.
Proof. Let M = U1Γ1V T 1 and N = U2Γ2V T 2 be the condensed SVD of M and N. Thus, we have range(M) = range(U1) and range(N) = range(U2). Moreover, we have N † = V2Γ −1 2 U T 2 and NN
† = U2UT2 . It follows from range(M) ⊆ range(N) that range(U1) ⊆ range(U2). This implies that U1 can be expressed as U1 = U2Q where Q is some matrix of appropriate order. As a result, we have
NN†M = U2U T 2 U2QΓ1V T 1 = M.
It is worth noting that the condition NN†M = M is not only necessary but also sufficient for range(M) ⊆ range(N).
If (Λ,X) are the eigenpairs of N†M, then it is easily seen that (Λ,X) are also the eigenpairs of (M,N) due to NN†M = M.
Conversely, suppose (Λ,X) are the eigenpairs of (M,N). Then we have NN†MX = NXΛ. This implies that (Λ,N†NX) are the eigenpairs of N†M due to NN†M = M and N†NN† = N†.
Fisher discriminant analysis (FDA) is a classical method for classification and dimension reduction simultaneously [Mardia et al., 1979]. It is essentially a generalized eigenvalue problem in which the matrices N and M correspond to a pooled scatter matrix and a between-class scatter matrix [Ye and Xiong, 2006, Zhang et al., 2010]. Moreover, the condition range(M) ⊆ range(N) meets. Thus, Theorem 4.2 provides a solution when the pooled scatter matrix is singular or nearly singular. We will present more details about FDA in Section 4.3.
We first substitute (7) into (8), in order to derive an update form explicitly dependent on only z and p:
zt+1 = Bλ((D TD + βI)−1(DT y + βzt + pt)− ptβ ),
(9) where Bλ is defined as a box-constrained element-wise operator (u denotes a vector and ui is its i-th element):
[Bλ(u)]i = min(max(ui,−λ), λ). (10)
Eqn. (9) could be alternatively rewritten as:
zt+1 = Bλ(Wy + Szt + bt), where: W = (DTD + βI)−1DT ,S = β(DTD + βI)−1, bt = [(D TD + βI)−1 − 1β I]pt, (11)
and expressed as the block diagram in Fig. 1, which outlines a recurrent structure of solving (1). Note that in (11), while
W and S are pre-computed hyperparamters shared across iterations, bt remains to be a variable dependent on pt, and has to be updated throughout iterations too (bt’s update block is omitted in Fig. 1).
By time-unfolding and truncating Fig. 1 to a fixed number of K iterations (K = 2 by default)1, we obtain a feed-forward network structure in Fig. 2, named Deep `∞ Encoder. Since the threshold λ is less straightforward to update, we repeat the same trick in [Wang et al., 2016c] to rewrite (10) as: [Bλ(u)]i = λiB1(ui/λi). The original operator is thus decomposed into two linear diagonal scaling layers, plus a unitthreshold neuron, the latter of which is called a Bounded Linear Unit (BLU) by us. All the hyperparameters W, Sk and bk (k = 1, 2), as well as λ, are all to be learnt from data by back-propogation. Although the equations in (11) do not directly apply to solving the deep `∞ encoder, they can serve as high-quality initializations.
It is crucial to notice the modeling of the Lagrange multipliers pt as the biases, and to incorporate its updates into network learning. That provides important clues on how to relate deep networks to a larger class of optimization models, whose solutions rely on dual domain methods. Comparing BLU with existing neurons As shown in Fig. 3 (e), BLU tends to suppress large entries while not penalizing small ones, resulting in dense, nearly antipodal representations. A first look at the plot of BLU easily reminds the tanh neuron (Fig. 3 (a)). In fact, with the its output range [−1, 1] and a slope of 1 at the origin, tanh could be viewed as a smoothened differentiable approximation of BLU.
We further compare BLU with other popular and recently
1We test larger K values (3 or 4). In several cases they do bring performance improvements, but add complexity too.
proposed neurons: Rectifier Linear Unit (ReLU) [Krizhevsky et al., 2012], Soft-tHresholding Linear Unit (SHeLU) [Wang et al., 2016b], and Hard thrEsholding Linear Unit (HELU) [Wang et al., 2016c], as depicted in Fig. 3 (b)-(d), respectively. Contrary to BLU and tanh, they all introduce sparsity in the outputs, and thus prove successful and outperform tanh in classification and recognition tasks. Interestingly, HELU seems exactly the rival against BLU, as it does not penalize large entries but suppresses small ones down to zero.
The encoder takes as input a sentence S of length n, represented by a sequence of vector X = (x1, x2, ..., xn). In an LSTM recurrent neural network, the input X is processed through time and produces a series of memory states (c1, c2, ..., cn) and hidden states (h1, h2, ..., hn). In order to counterbalance the impact of time on hidden states we process the input X two times, forward and backward, to fully encode the information classifier needs.
The forward LSTM updates its memory state −→ci and hidden state −→ hi at each time step t:
[ −→ ht ; −→ct ] = −−−−→ LSTM([ −−→ ht−1; −−→ct−1]). (4)
Similarly, memory state←−ci and hidden state ←− hi is updated by backward LSTM at time step t:
[ ←− ht ; ←−ct ] = ←−−−− LSTM([ ←−− ht+1; ←−−ct+1]). (5)
The encoder outputs a hidden tape h̃ = (h̃1, h̃2, ..., h̃n), where
h̃t = [−→ ht←− ht ] . (6)
In this paper, we presented the CTU Prague Relational Learning Repository (PRLR for short), an easily accessible collection of datasets for relational learning. The PRLR was designed with supervised learning in mind. To this end, the PRLR contains 49 ready to download datasets. One of the important features of the PRLR is that it provides meta-data about the datasets. The PRLR meta-data can be accessed at https://relational.fit.cvut.cz/.
We now present some observations on and properties of slither. Some of the observations made in this section have independently been pointed out earlier in the abstract game community, especially on BoardGameGeek.8 However, we prove for the first time that the standard slither variant cannot end in a draw, thereby settling an open-problem often raised in this community.
The concept of zugzwang appears in chess and denotes a position in which the current player has no desirable move and would rather pass and have the opponent act. A mutual zugzwang is a position in which both players would rather have the opponent play. Although zugzwangs are virtually unheard of in typical connection games, where additional moves can never hurt you, things are different in slither.
Proposition 2. Zugzwangs and mutual zugzwangs can occur in slither.
Proof. In Figure 18a, if it is White (resp. Black) to play, only one move is available, moving stone on C2 (resp. C4) to a and placing a stone on c or equivalently moving to c and placing on a. Then Black (resp. White) wins by placing a stone on C2 (resp. moving stone B5 to C4 and placing a stone on b).
As we have seen, the strategy-stealing argument can be applied to many other games including twixt, havannah, and games of the connect(m,n, k) family [48]. Unfortunately, the argument cannot be applied to slither.
Proposition 3. Nash’s strategy-stealing argument does not apply in slither.
Proof. Consider the White zugzwang position in Figure 18a. Had the A4 square not been occupied with a White stone, White would have a winning move: move B4 to b and place a stone on c. Since there are positions where having one too many stones on the board can make a player lose the game, Nash’s strategy-stealing argument does not apply.
Therefore, there is no theoretical indication yet that slither is not a secondplayer win on an empty board. However, in practice it is a huge advantage to play first, so much that if the swap rule is used, it is recommended to swap no matter
8http://boardgamegeek.com/thread/692652/what-if-there-no-legal-move
where the first move is played, including corner locations. The slither-specific intuition behind this practical advice is that the game is dynamic and a player can bring back a stone from a corner towards the center, moving it closer every turn.
Draws would occur in slither if there were positions where no player has a legal move and yet no player connects their own sides. The slither community had indeed identified non-terminal positions in which one of the players had no legal moves, but the opponent always had at least one move possible. In that case, the player with no legal moves would simply skip their turn.
Proposition 4. There exist positions in which a player has no legal moves.
Proof. For instance, Black has no legal move in the position Figure 18b.
The designer of slither has long claimed that the game did not admit any draw. Since there were no formal proofs, many members of the community were left unconvinced and attempted to find counter-examples. They would submit positions on forums dedicated to slither, and Corey Clark or some slither players would point out that the counter-example was not valid, usually because the diagonal rule was not respected or because a legal move actually existed for one of the players. Before settling this question formally, let us point out that draws can actually occur when the board topology is not restricted to be rectangular.
Proposition 5. Draws are possible when slither is played on a cylinder or on a torus.
Proof. In Figure 19a (resp. Figure 19b), if black (resp. black and white) sides are connected, then both players have no legal moves.
Draws are possible on some exotic boards. So, there is probably no fundamental reasons why the following result is true, and the proof, which we defer to the appendix, consists of a large case analysis on the consequences of forbidding diagonal configurations and the possibility of moving stones.
Theorem 7. Draws are not possible in slither on rectangular boards.
Proof. This theorem relies on two properties of slither. First, if a slither board is filled, then at least one player has a winning group. Second, if a board is not filled, then at least one player has a legal move. As as a result, the game always continues, possibly with a player passing, until one player has a winning group Since each move adds a stone to an empty intersection, these properties ensure that the game is bound to end after a finite number of turns with a winner. The Appendix proves the two essential properties mentioned above.
Different corpora were made available for the unshared task. From those, we chose to focus on the reports, and excluded bills and auditions as the former seemed to be informationally richer and easier to process. Therefore, we worked on three different files: the Congressional Reports “Wall Street and the Financial Crisis: Anatomy of a Financial Collapse” (referred to as AoC in the rest of the document), “The Stock Market Plunge: What Happened and What is Next?” (referred to as SMP), and the “Financial Crisis Inquiry Report” (referred to as FCIC). Each of these files was accessible as a PDF, or alternatively as a list of HTML pages, each page corresponding to a page of the PDF file.
The first task we performed was to transform the HTML files into a single text document (thanks to a Python script). In the case of AoC, an option was added to remove page numbers, which were present in the content part of the HTML file. Moreover, we normalized the files so that every space break (line breaks, multiple space breaks or tabular breaks) was changed into a single space break. After this procedure, we obtained three text
ar X
iv :1
40 6.
42 11
v1 [
cs .C
L ]
1 7
Ju n
20 14
files: AoC.txt, SMP.txt and FCIC.txt.
MusECI is able to parse both MIDI and MusicXML files into a normal form with the representations discussed so
5 In MusECI, measure and beat numbers both index from zero. Therefore, “measure 3” produces a value of 2.
far. We use the following algorithm to convert MIDI and MusicXML information into our system’s representations:
1. Greedily group Notes with the same onset and duration with Par. 2. Greedily group temporally adjacent structures (potentially including chords from step 1) with Seq. 3. Group any remaining temporally separated, but still sequential structures with Seq, using Rests to fill temporal gaps. 4. Group any leftover items under a global Par.
Importantly, this normal form for reading MusicXML is not the only way to represent musical features. Other normal forms are possible, and we hope to improve our normal forms through learning in later work, such that nested structures within melodies and phrases are identified in genre-specific ways.
We address the following questions in our experiments: (1) How does TVI compare with VI and heuristic search algorithms on MDPs that contain multiple SCCs? (2) What are the most favorable problem features for TVI?
We compared TVI with several other optimal algorithms, including VI (Bellman, 1957), ILAO* (Hansen & Zilberstein, 2001), LRTDP (Bonet & Geffner, 2003b), BRTDP (McMahan et al., 2005), Bayesian RTDP (Sanner et al., 2009) (BaRTDP), and HDP (Bonet & Geffner, 2003a)1. We used the fully optimized C code of ILAO* provided by Eric A. Hansen and additionally implemented the rest of the algorithms over the same framework. We performed all experiments on a 2.5GHz Dual-Core AMD Opteron(tm) Processor with 2GB memory. Recall that BRTDP and BaRTDP use upper bounds. We used upper bounds as described in Section 4.2. We used α = 2 × 10−6 and τ = 10 for BRTDP and BaRTDP.2 For BaRTDP, we used the probabilistic termination condition in Algorithm 3 of Sanner et al. (2009). 3
We compared all algorithms on running time, time between an algorithm starts solving a problem until generating a policy with a Bellman error of at most δ(= 10−6). We terminated an algorithm if it did not find such a policy within five minutes. Note that there are other performance measures such as anytime performance (the original motivation of BaRTDP) and space consumption, but the main motivation of TVI is to decrease convergence time. We expect TVI to have a very steep anytime performance curve, because it postpones backing up the initial state till it starts working on the SCC where the initial state belongs to. Space, on the other hand, is less interesting because in-memory MDPs algorithms requires that the MDP model stored in the main memory before dynamic programming can apply. Therefore, they all share the same space limit. For work on overcoming space limitation, see, for example the work of Dai et al. (2008, 2009a).
We tested all algorithms on a set of artificially-generated “layered” MDPs. For each such MDP of state size |S|, we partition the state space evenly into a number nl of layers, labeled by integers 1, . . . , nl. We allow states in higher numbered layers to be the successors of states in lower numbered layers, but not vice versa, so each state s only has a limited set of allowable successor states, named succ(s). A layered MDP is parameterized by two other variables: the number of actions per state, na, and the maximum number of successor states per action, ns. When generating the transition function of a state-action pair (s, a), we draw an integer k uniformly from [1, ns]. Then k distinct successors are uniformly sampled from succ(s) with random transition probabilities. We pick one state from layer nl as the only goal state. One property of a layered MDP is that it contains at least nl connected components.
1. Notice that this comparison is somewhat unfair to TVI, since heuristic search algorithms may not expand portions of the state space, if their sub-optimality can be proved. Still, we make this comparison to understand the practical benefits of TVI v.s. all other known optimal MDP algorithms 2. α is the termination threshold of BRTDP (it terminates when vu(s0) − Vl(s0) < α). τ indicates the stopping condition of each heuristic search trial. For more detailed discussions on the two parameters, please refer to the work of McMahanet al. (2005). We carefully tuned these parameters. 3. This termination condition may result in sub-optimal policies, so the reported times of BaRTDP in this paper are lower bounds. Note that BaRTDP mainly aims at improving the anytime performance of RTDP, which is orthogonal to convergence time. We report its convergence speed for thorough investigation purposes.
There are several planning domains that lead to multi-layered MDPs. An example is the game Bejeweled, or any game with difficulty levels: each level is at least one layer. Or consider a chess variant without pawn promotions, played against a stochastic opponent. Each set of pieces that could appear on the board together leads to at least one strongly connected component. But we know of no multi-layered standard MDP benchmarks. Therefore, we compare, in this section, on artificial problems to study TVI’s performance across controlled parameters, such as nl and |S|. Next section contains more comprehensive experiments on benchmark problems.
We generated problems with different parameter configurations and ran all algorithms on the same set of problems. The running times, if the process converged within the cut-off, are reported in Figures 2 and 3. Each element of the table represents the median convergence time of running 10 MDPs with the same configuration.4 Note that varying |S|, nl, na, and ns yields many MDP configurations. We tried more combinations than the representative ones reported. We found HDP much slower than the other algorithms, so did not include its performance.
For the first experiment, we fixed |S| to be 50,000 and varied nl from 1 to 1,000. Observing Figure 2 we first find that, when there is only one layer, the performance of TVI is slightly worse than VI, as such an MDP probably contains an SCC that contains the majority of the state space, which defeats the benefit of TVI. But TVI consistently outperforms VI if nl > 1. When nl ≤ 10, TVI equals or beats ILAO*, the fastest heuristic search algorithm for this set of problems. When nl > 10, TVI outperforms all the other algorithms in all cases by a visible margin. Also note that, as the number of layers increases the running times of all algorithms decrease. This is because
4. We picked median instead of mean just to avoid an unexpected hard problem, which takes a long time to solve, thereby dominating the performance.
the MDPs become more structured, therefore simpler to solve. The running time of TVI decreases second fastest to that of LRTDP. LRTDP is very slow when nl = 1 and its running time drops dramatically when nl increases from 1 to 20. As TVI spends nearly constant time in generating the topological order of the SCCs, its fast convergence is mainly due to the fact that VI is much more efficient in solving many small (and roughly equal-sized) problems than a large problem whose size is the same as the sum of the small ones. This experiment shows TVI is good at solving MDPs with many SCCs.
For the second experiment, we fixed nl to be 100 and varied |S| from 10,000 to 100,000. We find that, when the state space is 10,000 TVI outperforms VI, BRTDP and BaRTDP, but slightly underperforms ILAO* and LRTDP. However, as the problem size grows TVI soon takes the lead. It outperforms all the other algorithms when the state space is 20,000 or larger. When the state space grows to 100,000, TVI solves a problem 6 times as fast as VI, 4 times as fast as ILAO*, 2 times as fast as LRTDP, 21 times as fast as BRTDP, and 3 times as fast as BaRTDP. This experiment shows that TVI is even more efficient when the problem space is larger.
In this section, we present an experimental study of the proposed method on Lung cancer database. We set the population size (Np), to 30 and the number of generations to 200. Crossover rate (Pc) and mutation rate (Pm) is set to 0.7 and 0.1, respectively. Table 1 shows the best and average accuracy of 10 runs of our method on Lung cancer database. Because the proposed method is a random search algorithm, different results may be obtained at different runs. Therefore, we run this algorithm 10 times and report their average. In addition to the accuracy, the number of selected features is also reported. As shown in this table, this number decreases considerably and for low number of features we can see higher accuracy. Our method can achieve these excellent effects in low generations (200). In fact, heuristic algorithms should be run for high generation, e.g. at least 10000, to find global optimum but our method find their results in 200 generations.
The theoretical properties of PS and RSD, even in restricted domains such as lexicographic preferences, only provide limited insight into their practical applications. In particular, when deciding which mechanism to use in different settings, the incomparability of PS and RSD leaves us with an ambiguous choice. Thus, we examine the properties of RSD and PS in the space of all possible preference profiles.
The number of all possible preference profiles is super exponential (m!)n. Thus, for each combination of n agents and m objects we considered 1,000 randomly generated instances by sampling from a uniform preference profile distribution. Thus, each data point is the average of 1,000 replications. For each preference profile, we ran both PS and RSD mechanisms and compared their outcomes (in terms of random assignments).
A preliminary look at our empirical results illustrates the followings: when m ≤ 2, n ≤ 3, PS coincides exactly with RSD, which results in the best of the two mechanisms, i.e., both mechanisms are sd -efficient, sd -strategyproof, and sd -envyfree.3 Moreover, when m = 2, for all n PS is sd -strategyproof (although the PS assignments are not necessarily equivalent to assignments induced by RSD), RSD is sd -envyfree, and for most instances PS stochastically dominates RSD, particularly when n ≥ 4.
We now extend our model to stacked multiple layers. Figure 2 shows a three layer version of our model. The layers are stacked in the following way:
Functionality of Each Layer. For TD, the input to the (k+1)th layer is the sum of the output otk and the input uk from the k-th layer, followed by a sigmoid nonlinearity: uk+1 = σ(Htuk+otk), where σ(x) = 1/(1 + exp(x)) is the sigmoid function and Ht is a learnable linear mapping matrix shared across layers. For the PC task, the input to the first layer is the transformed sum from the last layer of the TD module, z1 = σ(HtuKt + otKt), where Kt is the number of stacked layers in the TD task. Thus the prediction of polarity would depend on the output of the TD task and reversely the TD task would benefit from indirect supervision from the PC task by backward propagation of errors. Similarly for PC, the input to the (k+1)-th layer is the sum of the output opk and the input zk
from the k-th layer, followed by a sigmoid nonlinearity: zk+1 = σ(Hpzk + o p k).
Attention for PC. In the single layer case, the attention for PC is based on that of the TD module. When layers are stacked, all layers of the first module collectively identify important attention words to detect the target. Therefore we compute the averaged attention vector across all layers in the TD module āt = 1
Kt
∑Kt k=1 a t k.
Accordingly for k-th layer of the PC module, the final attention vector is bpk = (1 − λ)a p k + λf(ā
t), and the output vector is opk = ∑ i b p k,ic p k,i.
Tying Embedding and Projection Matrices. The embedding matrices and projection matrices are constrained to ease training and reduce the number of parameters [35]. The embedding matrices and the projection matrices are shared for different layers. Specifically, using subscription (k) denote the parameters in the k-th layer, for any layer k, we have At(1) ≡ At(k), Ct(1) ≡ Ct(k), Ap(1) ≡ Ap(k), Cp(1) ≡ Cp(k), Vt(1)q ≡ Vt(k)q and Vp(1)q ≡ Vp(k)q .
Predictions for TD and PC. The prediction stage is similar to the single-layer case, with the prediction based on the output of the last layer Kt (for TD) and Kp (for PC). For the TD task, yt = SoftMax(Wtσ(HtuKt+o t Kt)), while for PC, y p = SoftMax(Wp σ(HpzKp + o p Kp )).
The proposed deep attribute method consists of the following four steps.
(1) Region proposals extraction: We use advanced techniques like selective search [31] or edge-boxes [35] to extract semantical regions, as both of which show satisfactory performance in benchmarks [16].
(2) Computing of neural codes for region proposals: We use CNN models trained from 1000 categories of ILSVRC 2012 to produce 1000 dimensional semantic output for each extracted region proposals. Such computed neural codes will serve as the semantic input for the next pooling stage.
(3) Cross-region-pooling: We make pooling per dimension across extracted regions to get a 1000-dimensional holistic representations. Different pooling layout schemes are applicable for further possible performance improvement.
(4) Context-aware region refining and classifier build: A linear classifier can be trained over deep attribute representations for visual recognition tasks. We observed that only a small portion of regions are correlated to classification targets. We thus impose classifier feedback to pick out a few contextual regions and re-pooling to get a context-aware classifier.
The first two steps are easy to understand. We will describe the latter two items in details below.
This paper is an extension of the paper on the theory of flexibly-bounded rationality which was proposed by Marwala (2013). Making decisions has been a complex task since time immemorial. In primitive society making decisions was muddled with superstitions and therefore quite irrational (Vyse, 2000; Foster and Kokko, 2009). Such irrationality in making decisions has even been speculated to occur in animals such as pigeons (Skinner, 1948). In essence, superstition which is irrational can be defined as supernatural causality where something is caused by another without them linked to one another. The idea of one event causing another without any connection between them whatsoever will be characterized as irrational thinking because it defies logic.
This paper proposes that the three fundamentals of decision making are information, correlation machine which is used to estimate missing data and causality, which relates inputs to outputs. As societies make attempt to move away from superstition and irrationality the concept of rational decision making comes to the fore. The concept of rationality has attracted many philosophers from different fields. Max Weber studied the role of rationality in social action (Weber, 1922) whereas Grayling studied the role of other social forces such as emotions on the quality or degree of rationality. Mosterin (2008) defined reason as a psychological facility and rationality as the optimizing strategy. In artificial intelligence a rational agent is that which aims to maximize its utility in making its decision and this is the optimization view of rationality.
Fundamentally, rational decision making is a process of reaching decisions through logic, information and in an optimized fashion (Nozick, 1993; Spohn, 2002). Decision making can be defined as a process through which a decision is reached. It usually involves many possible decisions outcomes and is said to be rational and optimal if it maximizes the utility that is derived from its consequences. The philosophical concept that states that the best
course of action is that which maximizes utility is known as utilitarianism and was advocated by philosophers such as Jeremy Bentham and John Stuart Mills (Adams, 1976; Anscombe, 1958; Bentham, 2009; Mill, 2011).
There is a theory that has been proposed to achieve this desired outcome of a decision making of maximizing utility and this is the theory of rational choice and it states that one chooses a decision based on the product of the impact of the decision and its probability of occurrence (Allingham, 2002; Bicchieri, 2003). However, this theory was found to be inadequate because it does not take into account where the person will be relative to where they initially were before making a decision. Kahneman and Tversky (1979) extended the theory of rational choice by introducing the prospect theory which includes the reference position on evaluating the optimal decision (Kahneman, 2011).
Marwala (2013) proposed a generalized decision making process with two sets of input information which form the basis for decision making and options that are available for making such decisions (model) and the output being the decision being made. He further proposed that within this generalized decision making framework lie the statistical analysis device, correlation machine and a causal machine. Thus Mawala’s model for decision making can be summarized as follows:
1. Optimization Device: e.g. maximizing utility and minimizing loss. 2. Decision making Device: In this paper we propose that this element contains statistical
analysis device, correlation machine and causal machine.
The next section will describe in detail the concept of rational decision making, and the following section will describe the concept of bounded rationality which will then be followed by the concept of flexibly-bounded rationality. Thereafter, a generalized model for decision making is proposed followed by the theory of marginalization of irrationality for decision making process.
Figure 1 shows a unified framework for sequential feature selection. The first step is generating several attribute subsets. The second step is to employ an attribute significance
measure to evaluate all generated candidates, and outputs the current optimal feature subset. The third step is checking whether the stopping criterion (e.g., number of attributes) is satisfied, a) if yes, output the current optimal feature subset as final optimal subset; b) otherwise, it goes back the first step, generating candidates then evaluating until the selected feature subset meets the stopping criterion.
In this framework, the key component is the evaluating and almost the computational work comes from this step. Hence, the efficiency of feature selection depends on evaluating. The main contribution of this paper is to extend this framework and propose a parallel framework to accelerate the whole process of evaluating. For the evaluation of attributes, there are mainly two general methods: a) wrapper which employs a learning algorithm (e.g. support vector machine) to evaluate; b) filter which measures the attributes’ significance with a metric, including information gain [29], distance [30], dependency [31] and consistency [4], [32]. The representative significance measures of attributes used in this paper are all based on rough set theory. It would also be interesting to study how to use our proposed framework to scale up on other attribute reduction algorithms.
The semantic roles are employed by some researchers as linguistic features in the MT evaluation. To utilize the semantic roles, the sentences are usually first shallow parsed and entity tagged. Then the semantic roles used to specify the arguments and adjuncts that occur in both the candidate translation and reference translation. For instance, the semantic roles introduced by Giménez and Márquez (2007, 2008) include causative agent, adverbial adjunct, directional adjunct, negation marker, and predication adjunct, etc. In the further development, Lo and Wu (2011a and 2011b) design the metric MEANT to capture the predicate-argument relations as the structural relations in semantic frames, which is not reflected by the flat semantic role label features in the work of (Giménez and Márquez, 2007). Furthermore, instead of using uniform weights, Lo, Tumuluru and Wu (2012) weight the different types of semantic roles according to their relative importance to the adequate preservation of meaning, which is empirically determined. Generally, the semantic roles account for the semantic structure of a segment and have proved effective to assess adequacy in the above papers.
Consider the situation where some evidence e has been entered to a Bayesian network. When performing conflict analysis, sensitiv ity analysis, or when answering questions like "What if the finding on X had been y instead of x?", you need probabilities P(e' I h) where e' is a subset of e, and his a configuration of a (possibly empty) set of variables.
Cautious propagation is a modification of HUGIN propagation into a Shafer-Shenoy like architecture. It is less efficient than HUG IN propagation; however, it provides easy access to P(e' I h) for a great deal of relevant subsets e'.
Keywords: Bayesian networks, propagation, fast retraction, sensitivity analysis.
1 Introduction
As an example for motivating the introduction of yet another propagation method, consider the junction tree in Figure 1, with evidence e = { s, t, u, v, w, x, y, z} entered as indicated.
Suppose you want to perform a conflict analysis (Jensen, Chamberlain, Nordahl & Jensen 1991). Then you first calculate
conf(e) =
l P(r )P(s )P(t)P( u)P( v)P( w )P(x)P(y )P(z)
og P(e)
If conf is well above 0, it is an indicaiton that the ev idence is conflicting. The next thing you would like to do is to trace the conflict further down. You may for example want to "retract" some of the evidence and calculate the conflict measure for the remaining evidence. You may also want to investigate whether
W X y Z
Figure 1: A junction tree with evidence e = {r, s, t, u, v, w, x, y, z} entered. The circles are nodes consisting of sets of variables, and the boxes are sepa rators consisting of the intersection of the neighbour ing nodes. The top node is the chosen root.
it is the group of findings {x,y,z} which is in con flict with the rest. To do so you need probabilities of the type P(e'), where e' is a subset of e. There is a straightforward way to calculate these probabilities, namely to enter e' as evidence, and then most prop agation algorithms will provide P(e') as a side effect. However, this is clumsy and time-consuming. There fore you may be willing to pay an overhead in time and space for a propagation method which gives you a more direct access to probabilities of subsets.
In this paper we shall present a propagation method which we call cautious propagation. It provides very easy access to P(e') for more subsets than any other propagation method known to the author, and ex tended with cautious evidence entering it provides a very fast way of calculating the impact on a hypothe sis if a finding as retracted or changed from one value to another.
Cautious propagation is a modification of HUGIN propagation (Jensen, Lauritzen & Olesen 1990) to a message passing design like Shafer-Shenoy propagation (Shafer & Shenoy 1990). We call it cautious propa gation because it works in a way not destroying the original tables in the junction tree. Cautious evidence entering was proposed by Dawid (1992) as fast retrac-
324 Jensen
tion, however, in his setting it only provided fast re traction in special cases.
We start with a closer analysis of the HUG IN propaga tion algorithm to pin-point its drawback with respect to calculation of P(e')s, and cautious propagation is defined together with its proof of correctness. Then cautious evidence entering is defined, and we compare the time and space complexity of HUGIN and cau tious propagation. Finally, it is illustrated how cau tious propagation can be used for sensitivity analysis.
2 P(e')s provided by HUGIN propagation
HUGIN propagation is performed in two phases, a CollectEvidence phase, where messages are sent from the leaves to a chosen root, and a DistributeEvidence phase, where messages are reversed. The content of the messages is explained below.
Consider the situation in Figure 2. Assume that the junction tree is consistent before the evidence is en tered (for each clique V its table is P(V), and for each separator S its table is P(S)), and let the evidence e be divided by S into e1 and er.
: er
Figure 2: The transmitted tables in a network where CollectEvidence is called at the left of W.
If CollectEvidence is called in a node at the left of W, the table to be transmitted from V to S is P(en S), and PJ:tsfl is transmitted further to W to be multi plied to P(W). When later DistributeEvidence is acti vated, the table transmitted from W to S is P(er, e1, S) and P�(·��e,'sfl is transmitted to V and multiplied to V 's table (§ is set to 0).
This gives us a way to calculate probabilities for sub sets of the evidence at each separator. Since S initially holds P(S), we have at our disposal at S the three ta bles P(S),P(enS) and P(er,e�,S). Then
P(er) = I: P(er, S) s
Since er is independent of e1 given S we have
( I) I) ( I P(er,et,S)
Per S P(e1 S = Per,el S) = P(S)
This yields a formula for the calculation of P(e1):
Unfortunately it can only be used if P(er, s) =f. 0 for all states s of S. If P(er, s) = 0 the fraction is put at 0, and the sum is only a lower bound for P(e1) (If we for these states put the fraction to P(s) we also get an upper bound for P(el)). The problem is that the clique tables P(V) are affected by the propagation, and if O 's are introduced you cannot restore the table.
3 Cautious propagation
Consider Figure 2 again and observe that in the DistributeEvidence phase, P(V, er) is multiplied by P�(·�:�.'sfl. If instead we multiply by P(e1 I S) the only difference will be multiplications for configurations s, where P(s , er) = 0. In that case the corresponding en tries in P(V, er) are also 0. So multiplying by P( e1 I S) will give exactly the same result as the HUGIN prop agation will give.
Therefore, the idea behind cautious propagation is that in the CollectEvidence phase let S keep P(S), and store P(W) and P(er I S) separately near W. Now, assume that W has received and stored the mes sages P(e1 I S1), . . . , P(en I Sn) (see Figure 3) where ew = el u ... u en.
If W is the root, Figure 3 describes the situation af ter CollectEvidence(W), and recursively it will in the DistributeEvidence phase be the situation for all non leaf nodes.
A more precise description of cautious propagation is the following:
Before evidence is entered, each separator S holds P(S) and each clique V holds P(V). Whenever S re ceives a table Tv(S) from a neighbour W, it is divided by P(S), stored, and a message is sent to the other neighbour clique V (see Figure 4).
Figure 4: A link in a junction tree and the tables stored when a message has been passed in both directions.
A clique W can send a table to a neighbour separa tor S if it has received a message from all its other neighbouring separators 81, ... , Sn. The table sent is
T (S) = ""P(W) Tw(Sl) Tw(Sn) v L..t P(S ) .. . P(S ) W\S 1 n
For the correctness of the method we see from the considerations above that it is enough to show that Tv(S) = P(S, ew ) , where ew is the evidence entered at the subtree containing Sand W, but not V:
If W is a leaf then
Tv(S) = L P(W,ew) = P(S, ew) W\S
If Tw(Si) = P(Si, ei) then Tl;k�)) = P(ei I Si), and because e1, ... , en are independent given W we have (see Figure 3)
P(ew I W) = P(e1 I W) ... P(en I W)
Therefore
Tv(S)
= P(el I 81) ... P(en I Sn)
"" P(W) Tw(St) Tw(Sn)
L..t P(S ) . . . P(S ) W\S 1 n
= L P(W)P(ei I 81) ... P(en I Sn) W\S
= L P(W)P(ew I W) W\S L P(W,ew) W\S
= P(S, ew)
4 Cautious entering of evidence
Consider the situation in Figure 5. We can access the complement of s. However, with the present tech niques we cannot access the complements of {t}, {u}, and { v}. Fortunately there is an easy way out: Add dummy variables such that findings are always inserted to a leaf in the junction tree, and such that at most one finding is inserted in any node. The construction is illustrated in Figure 6.
s u v
Figure 6: Now the probability of all complements of single findings can be accessed by cautious propaga tion.
This structural way of accessing complements of single findings through propagation can be achieved simpler by the following modification of the way that findings are inserted: Instead of changing the table in the clique when inserting the finding f, consider the finding as a message Ft, store it, and treat it in the same way as ta bles from neighbour separators. This way of handling findings corresponds to the one suggested by Dawid (1992).
Dawid used it in connection to the HUGIN propaga tion method to get P(x I e \ {x}). However, O's can mess it up and in order for this to work always you have to use cautious propagation.
In the following we include cautious entering of evi dence in the term cautious propagation.
5 P(e')s provided by cautious propagation
LetS be any separator. S divides the evidence e into two sets er and e1, and cautious propagation provides both P(er) and P(e1). Hence we for the situation in Figure 1 have access to the probability of the following
326 Jensen
sets:
{ s }, e \ { s }, { t}, e \ { t}, { u }, e \ { u }, { x }, {y }, {r }, { v }, e \ {v}, {w},e \ {w}, {x,y},e \ {s,y}, {z},e \ {z}, {s, t}, e \ { s, t}, { u, v }, e \ { u, v }, {r, stuv, w }, {x, y, z} Let V be a clique with findings It, . . . , f m entered and with adjacent separators 81, ... , Bn. Let ei denote the evidence entered to the subtree containing Si but not V. Take for example the set e1 U e2 U {fi}. Then
P(V, e1, e2, It) = P(V )P(e1 I S1)P(e2 I S2)Fft
and all the factors in the product are available local to V. Therefore P( e1 U e2 U {It}) is easy to calculate.
In general, cautious propagation gives access to the probability of any union of the sets {!I}, . . . , {fm} , el, 0 0 0 'en.
In Figure 1 we therefore also get access to the proba bility of
e \ { x }, e \ {y }, e \ {r }, { s, t, r }, { u, v, w, x, y, z }, {s, t, u, v}, {r, w,x,y, z}, {s, t, w}, {r, u,v, x, y,z}, { s, t, x, y, z }, {r, u, v, w }, {r, s, t, u, v }, { w, x, y, z }, {r, s, t, w }, {u, v, x, y, z }, {r, s, t,x, y, z}, {u, v, w }, {s, t, u, v, w}, {r,x,y, z}, {s, t, u, v,x, y, z}, {r, w}, {s, t, w, x, y, z, }, {r, u, v}
6 Complexity of cautious propagation
We shall compare cautious propagation (with cautious entering of evidence) and HUGIN propagation, which is the fastest known updating method (Shachter, An dersen & Szolovits 1991).
Let the junction tree have n cliques (and n - 1 separa tors), and assume that all cliques in the junction tree shall receive evidence.
The space requirements for cautious propagation is two extra tables for each separator and a way to store ev idence. This will never require more than two times the space as for HUGIN propagation (most often con siderably less).
As described in Section 3 cautious propagation re quires the junction tree to be consistent before evi dence is entered. This can be achieved by a HUGIN propagation; however, it may also be done by cautious propagation with tables of 1 's in the separators. In this case, cautious propagation corresponds exactly to the propagation method suggested by Shafer & Shenoy (1990). When the propagation has terminated, the ta bles P(S) for separators and P(V) for cliques can be calculated from the tables stored (we shall revert to this later) .
The methods are composed of the table operations marginalization, multiplication and division. To get
insight into the time complexity of the methods we ig nore that the time for the operations varies with table sizes. However, multiplication of n tables is counted as n - 1 multiplications.
There are the following phases in the calculation of new probabilities:
a) Entering of evidence.
b) Propagation.
c) Calculation of marginals.
d) Reinitialization: Prepare the junction tree for a new set e' of evidence. That is, all tables shall be conditioned on the evidence e just entered. This phase is redundant when the case is closed, and also when there is no need for distinguishing be tween the two sets e and e' of evidence.
Re a) HUGIN: n multiplications. Cautious: No operations.
Re b) For both methods there are two marginalizations and two divisions for each link. For the multipli cations we have
HUGIN (including entering of evidence): Two multiplications for each clique except the root, where only one multiplication is necessary, 2n -1.
Cautious: Let V be a clique with k neighbours. When a message has to be sent along a link, the table for V shall be multiplied with k -1 tables. In order to take care of all messages sent from V we need k(k - 1) multiplications. For the entire propaga tion we therefore need
Multc = :L neighb(V)(neighb(V)- 1) v
propagations.
To analyse what that means, assume that m cliques in the junction tree have k neighbours and the remaining n - m cliques are leaves. Since the graph is a tree we have2(n-1) = km+(n-m) and we get m = k=�. This gives Multc = k(n -2).
So, in the worst case k = n - 1 and Multc = (n- 1)(n-2); in the best case we have Multc = 2(n -2).
For the rest of this analysis we assume that k = 6 (few junction trees are more heavily branched).
The operations above are performed on clique ta bles where findings have been entered. To es tablish this, the findings are multiplied on auxil iary tables. This expands the space requirement,
so that it altogether is twice the requirement for HUGIN propagation.
Re c) The number of propagations are the same. We assume the number of variables to be 6n, which gives 6n marginalizations.
Furthermore cautious propagation requires two multiplications for each link.
Re d) There is really nothing to do here in HUGIN prop agation. What should be done is the normaliza tion of all tables by dividing by P(e) , however, this can be postponed till later when table op erations are performed. In cautious propagation the clique tables are calculated under c). For the separator tables we have (see Figure 4)
P(e,S) = P(ev,ew,S) = P(ev I S)P(ew I S)P(S)
So the three tables stored are multiplied. This re quires 2(n- 1) multiplications. Normalization is treated in the same way as for HUGIN propaga tion.
Counting koefficients of n the complexity of HUGIN propagation amounts to 13n, and under the assump tions made the complexity of cautious propagation is 2ln. So, altogether cautious propagation will very sel dom take more than twice the time of HUGIN propa gation.
7 An application: Sensitivity analysis
Sensitivity analysis is part of explanation, which has to do with explaining to a user how the system has ar rived at its conclusions. Explanation for Bayesian net works has been studied systematically by Suermondt (1992) , and Madigan & Mosurski (1993) have imple mented various explanation facilities. Some of the questions to answer in connection to explanation con cern the sensitivity of the conclusions to the particular evidence.
Let h be a hypothesis (in the form of a particular con figuration of states for some hypothesis variables), and let e be the evidence entered. That is, P(h I e) has been calculated, and now you would like to analyse the relation between the evidence and resulting probabil ity.
Definitions: Let e be evidence, h a hypothesis, and let (h, fh, (}3 be predefined thresholds. Evidence e' � e is
. 'f P(hie\e1) 1 () amportant 1 P(hle) < - 1
jJi . t "f P(hi() 1 - su czen 1 P(h e) >
Cautious Propagation in Bayesian Networks 327
- minimal sufficient if e1 is sufficient , but no proper subset of e1 is so
- crucial if e' is a subset of any sufficient set
- decisive if P(h I e') > 1 03
As can be seen from the definitions above, the heart of sensitivity analysis is to calculate P(h ! e') for subsets e1 of e. If e is not a very small set we cannot hope for an easy way of calculating P(h I e1) for all subsets. However, with cautious propagation (and cautious en tering of evidence) the probability for a crucial number of subsets can be achieved.
Cautious propagation yields P(e') for subsets as de scribed in Sections 3 and 4. Now, enter has evidence to a clean junction tree, and HUGIN propagate such that all tables are conditioned on h (this also yields P(h)). Next e is entered cautiously and cautious prop agation is performed. This gives access to P(e' I h) for the same subsets as before, and Bayes' formula gives P(h I e'). (Jensen, Aldenryd & Jensen 1995) gives some examples of how cautious propagation can be used in sensitivity analysis.
Another natural set of questions in sensitivity analysis are What-if-questions: If the finding on the variable X had been y instead of x, what would P(h I eU{y}\{x}) be?
This type of question can also easily be answered through cautious propagation and cautious evidence entering. Let namely X be a member of V, and let a finding on X be x. After cautious propagation the situation is, that local to V you have P(V), P(ei I Si) for all adjacent separators Si, and tables Ft for the findings f to multiply on P(V). It is then easy to sub stitute Fx with Fy and calculate P(V, e U {y} \ { x}) as the product of all tables local to V, and finally marginalize V out to get P(e U {y} \ {x}). The same is done with the junction tree conditioned on h to get P(e U {y} \ {x} I h), and Bayes' formula yields P(h I e U {y} \ {x}).
Note that the analysis above did not require any extra propagations.
Acknowledgements
Thanks to the ODIN group at Aalborg University (http:www.iesd.auc.dkfodin), in particular to S0ren Dittmer for valuable discussions. Thanks also to S0ren Aldenryd and Klaus B. Jensen for their contributions to the part on sensitivity analysis.
328 Jensen
References
Dawid, A. (1992) . Applications of a general propa gation algorithm for probabilistic expert system, Statistics and Computing 2: 25-36.
Jensen, F. V., Aldenryd, S. & Jensen, K. B. (1995) . Sensitivity analysis in bayesian networks, Pro ceedings of ECSQARU'95, Fribourg, Switzerland. To appear.
Jensen, F. V., Chamberlain, B., Nordahl, T. & Jensen, F. (1991) . Analysis in HUGIN of data conflict, in N.-H. Bonnisone et al. (ed.) , Uncertainty in Artificial Intelligence 6, pp. 519-528.
Jensen, F. V., Lauritzen, S. L. & Olesen, K. G. (1990) . Bayesian updating in causal probabilistic networks by local computations, Computational Statistics Quarterly 4: 269-282.
Madigan, D. & Mosurski, K. (1993) . Explanation in belief networks, Technical report, University of Washington, US and Trinity College, Dublin, Ire land.
Shachter, R. D., Andersen, S. K. & Szolovits, P. (1991) . The equivalence of exact methods for probabilistic inference on belief networks, Techni cal report, Department of Engineering-Economic Systems, Stanford University.
Shafer, G. & Shenoy, P. (1990) . Probability propaga tion, Annals of Mathematics and Artificial Intel ligence 2: 327-352.
Suermondt, H. J. (1992) . Explanation in Bayesian Belief Networks, PhD thesis, Knowledge Systems Laboratory, Medical Computer Science, Stanford University, California, Stanford, California 94305. Report No. STAN-CS-92-1417.
Our feature-set into four categories viz. (1) Sentiment features (2) Sarcasm, Irony and Thwarting related Features (3) Cognitive features from eyemovement (4) Textual features related to reading difficulty. We describe our feature-set below.
To verify the effectiveness of our Greedy control mechanism, we ran additional experiments combining our inference mechanism with different control baselines. We experimented with Random-RelCrowd and Max-degree-RelCrowd by replacing only the greedy step of Algorithm 1 with respective selection heuristic. We observed that random selection lead to 1.1x increase in budget while max-degree converged within 1.05x. Although the final converged values were within acceptable range (±0.05%), we still want to emphasize that theoretical guarantees are made only over Greedy-RelCrowd and not other baselines. One might always construct adversarial ECG which perform poor with Max-degree or Random selection.
In this section we describe the structure of a dataset which applies the above improvements. First, we need to define the preferred-relation (to apply improvement (2)). In what follows we use the hyponym-hypernym relation. The dataset is
based on target words. For each target word we create a group of candidate words, which we refer to as the target-group. Each candidate word belongs to one of three categories: positives (related to the target, and the type of the relation is the preferred one), distractors (related to the target, but the type of the relation is not the preferred one), and randoms (not related to the target at all). For example, for the target word singer, the target group may include musician, performer, person and artist as positives, dancer and song as distractors, and laptop as random. For each target word, the human annotators will be asked to rank the positive candidates by their similarity to the target word (improvements (1) & (3)). For example, a possible ranking may be: musician > performer > artist > person. The annotators responses allow us to create the actual dataset, which consists of a collection of binary comparisons. A binary comparison is a value R>(w1, w2;wt) indicating how likely it is to rank the pair (wt, w1) higher than (wt, w2), where wt is a target word and w1, w2 are two candidate words. By definition, R>(w1, w2;wt) = 1 - R>(w2, w1;wt). For each target-group, the dataset will contain a binary comparison for any possible combination of two positive candidates, as well as for all the combinations in which the first candidate is positive and the second is negative (either distractor or random). When comparing two positive candidates wp1 ,wp2 – the value of R>(wp1 , wp2 ;wt) is the portion of annotators who ranked (wt, wp1) over (wt, wp2). When comparing a positive candidate wp to a negative one wn – the value of R>(wp, wn;wt) is 1. This reflects the intuition that a good model should always rank preferred-relation pairs above other pairs. Notice that R>(w1, w2;wt) is the reliability indicator for each of the dataset key answers, which will be used to apply improvement (4). For some example comparisons, see Table 1.
As a case study, we have experimented with benchmark binary CSPs taken from Christophe Lecoutre’s XCSP repository and used in CSP solvers competitions. Specifically, we experimented with 300 instances belonging to the following classes: radio links frequency assignment (rlfap), graph coloring (gc), driver (dr), forced random (frb), quasigroup completion (qcp), quasigroups with holes (qwh).
All experiments were performed in a multiprocessor shared memory system consisting of 8 cores (the total number with hyperthreading is 16) of Intel(R) Xeon(R) CPU E5520 at 2.27GHz and with 78GB of memory, under the operating system Ubuntu Linux 12.04. The framework that we used for the implementation was the OpenMP 3.0 OpenMP (2008) API extensions on the GCC 4.6.3 compiler , without any compiler optimization parameters. To assess the performance we used the practical execution time (or wall clock) as a measure. The practical execution time (or wall clock) is the total time that a process requires in order to complete its computation. The execution time is obtained by calling the C POSIX.1-2001 function clock gettime() and it is measured in nano seconds, with the highest available timer accuracy.
We used a standard MAC algorithm as the baseline solver, and in addition we implemented and applied maxRPC within the context of the proposed framework. Specifically, the two algorithms presented in Section 3 (denoted Sync and Async hereafter) apply maxRPC in slave processes parallel to the master process which applies AC. We compare Sync and Async to: 1) the baseline solver, i.e. MAC (denoted AC), and 2) a simple portfolio of two search algorithms that are run in parallel independently from one another. These algorithms respectively apply AC and maxRPC throughout search. For any given problem, the portfolio (denoted pfAC+maxRPC) terminates once one of the algorithms finds a solution (or proves that none exists). Hence, for any given instance the portfolio gives the same result as either AC or maxRPC, depending on which of the two is better for the specific instance. Finally, we integrated Sync and Async within the portfolio resulting in a new portfolio that consists of four algorithms (denoted pfall).
All algorithms used the dom/wdeg heuristic for variable ordering and lexicographical value ordering. Our algorithms were run 50 times on each instance and the median cpu times and node visits are reported2. A time limit of 1 hour was set.
Table 1 compares the performance of all the tested methods on various problem instances. Figures 2 and 3 give pairwise comparisons between our
2The mean values are quite close in general but are heavily influenced by a few outliers in some cases of soluble instances.
algorithms and AC (resp. pfAC+maxRPC) by showing cpu times in log scale. We exclude very easy instances that are solvable by all methods in less than a second and very hard ones where the time limit was reached by all methods. Any point above (resp. below) the diagonal corresponds to an instance where AC/pfAC+maxRPC was better (resp. worse) than Sync/Async.
The results demonstrate the validity of the motivation behind this work. As Figures 2 and 3 demonstrate, our algorithms are able to almost always outperform AC by taking advantage of the extra filtering offered by maxRPC without slowing down search, since this strong LC is applied in parallel to the main solver. Considering each problem class separately, we can note the following:
• The rlfap is a class where AC dominates maxRPC as can been by the
results of pfAC+maxRPC in Table 1, which usually follow those of AC. In this class both Async and Sync outperform AC, and therefore also pfAC+maxRPC , with the latter algorithm being more efficient. Specifically, Sync can be up to twice as fast as AC and pfAC+maxRPC .
• In gc problems AC is again better on average than maxRPC, but there are quite a few cases where the opposite occurs. In these problems Async and Sync are in most cases more efficient than AC and pfAC+maxRPC , and there is no clear winner between them.
• AC is clearly better than maxRPC on driver and frb problems. The performance of Sync and Async is usually close to that of AC, but there are instances where they clearly outperform it.
• In the qcp and qwh classes maxRPC is typically by far faster than AC, very often by exponential margins. This is because of the considerable extra pruning it achieves. In most cases Sync and Async cannot match the performance of maxRPC and are therefore less efficient than pfAC+maxRPC . However, the pruning achieved by the application of maxRPC inside Sync and Async is enough to make them clearly more efficient than AC. Also, there are some instances where our algorithms, and especially Async, are able to outperform maxRPC.
Overall, we can say that the performance of Sync is perhaps surprising. Despite the limited time given to maxRPC while AC runs on the master process, it is able to achieve considerable extra pruning. This is reflected on cpu times where Sync is often twice as fast as AC, while it is rarely outperformed. Regarding the portfolio pfAC+maxRPC , Sync is usually better on problems where the winner among the portfolio’s solvers is AC (rlfap, gc, dr, frb). On problems where the winner is maxRPC (qcp, qwh) pfAC+maxRPC is in most cases better than Sync.
Regarding Async, which has the greater potential for further development, results demonstrate that its performance follows that of maxRPC in terms of search space reduction, but without penalizing cpu times on problems where maxRPC is not successful. That is, while Async is faster than AC in most cases, it performs much better in problem classes where maxRPC excells. In qcp it can be up to 7 times faster than AC while being always better than Sync. On the other hand, the differences between Async and AC are not very significant on rlfap, while on graph coloring AC is slightly better on some instances.
These results are explained by looking at the backjumps that take place in each problem when solved by Async. The number of backjumps, as well as the mean and maximum numbers of search tree levels that are jumped over, are considerably higher in quasigroup instances compared to some other classes. For example, on qcp15-5 there were 7547 backjumps on average, and their mean and maximum lengths (i.e. #levels jumped over) were 2.38 and 17.35 respectively. On driver-9, where Async is also very successful, there were only 52 backjumps on average but their mean and maximum lengths were 4.8 and 42.1. In contrast, on s11-f09 there were 2113 backjumps on average, and their mean and maximum lengths were 1.26 and 2.75.
Finally, it is clear from Table 1 that the portfolio pfall which includes all four methods outperforms each individual method, as well as the portfolio pfAC+maxRPC . For each given instance, pfall matches the performance of the algorithm that performs best on this instance among AC, maxRPC, Sync, and Async. Therefore, it is the clear winner overall. It is well known that when building portfolios using different solvers or a single solver under different parameter settings, a very important desired attribute is variability. That is, in order for the portfolio to be successful the solvers included in the portfolio should display dissimilar behavior. This is achieved by pfall because, as demonstrated in Table 1, any method among pfAC+maxRPC , Sync, and Async can be the winner on different instances and problem classes. Hence, they display variability in their performance, and this is why their integration into pfall is very successful.
the computation of the solutions of a peer is a global problem that does include recursion. In this case, we fix all the instances to be the initial ones for each peer.
Notice from Definition 3.2, that when D̄ = D(P) ∪⋃Q∈N◦(P) Core(Q,D) in Definition 5.6, it holds localCore(P, D̄) = Core(P,D). In particular, Proposition 5.1 remains true if JQ = Core(Q,D), for Q ∈ N ◦(P), and JP = D(P).
Definition 5.7 Consider a PDES schema P = 〈P,S,Σ, T rust〉 and a peer P ∈ P, the InLocalCore decision problem is about membership of the set:
InLocalCore(P, P) = {(R(t̄), D̄) |R(t̄) ∈ localCore(P, D̄) and D̄ is an instance for S(N (P))}. 2
Proposition 5.2 For a PDES schema P = 〈P,S,Σ, T rust〉 and a peer P ∈ P, InLocalCore(P, P) is ΠP2 -complete in the size of D̄ , the input neighborhood instance. 2
Despite the assumption that the peer graphs G(P) are acyclic (cf. Remark 2.1), sets Σ of DECs may contain cycles through inclusion dependencies, in particular with existential quantifiers, i.e. RDECs.
More precisely, assume Σ is (or contains) a set of sentences ϕ of the form ∀x̄ (ψ(x̄ ) → ∃ȳχ(x̄ , ȳ)), of the form (12). When ȳ is not empty, ϕ is called existential. A directed graph can be associated to Σ. The nodes are the database predicates, and there is an edge from predicate P to predicate Q when P appears in the antecedent, and Q in the consequent of a ϕ ∈ Σ. The edge is marked if ϕ is existential. Σ is ref-acyclic if there no cycles with marked edges in the graph.20
For example, IC1 = {∀x (S (x ) → Q(x )), ∀x (Q(x ) → ∃y T (x , y))} and IC2 = {∀x (S (x ) → Q(x )), ∀x (Q(x ) → S (x ))} are ref-acyclic sets, whereas IC3 = IC1 ∪ {∀xy (T (x , y) → Q(y))} is not. This notion can be applied without changes to the sets Σ(P) in PDESs.
Example 5.6 Given a PDES P = 〈P,S,Σ, T rust〉 with P = {P1, P2}, S = {S(P1),S(P1)}, S(P1) = {R1(·, ·)}, S(P2) = {R2(·, ·)}, Σ(P1, P2) = {∀x∀z (R1(x , z )→ ∃yR2(x , y)), ∀x∀z (R2(x , z )→ ∃yR1(x , y)}, Σ(P2, P1) = ∅; and Trust = {(P1, less, P2)}. In this case, the peer graph G(P) is acyclic. However, schema P is not ref-acyclic, because Σ(P1) = Σ(P1, P2) has a cycle through RDECs. 2
In some cases we will make the assumption that the sets of DECs at hand are ref-acyclic. For this reason, we make notice that the proof of Proposition 5.2 uses a ref-acyclic set of DECs. So, the proposition still holds for this class of DECs, which will become relevant in Section 6. (Cf. electronic Appendix A.1 for an additional discussion.)
20 The condition of ref-acyclicity was already used in (Bravo and Bertossi 2006), as RIC-acyclicity, for sets of ICs on a single database schema.
Consistency and Trust in Peer Data Exchange Systems 37
Corollary 5.1 For a PDES schema P = 〈P,S,Σ, T rust〉 and a peer P ∈ P, with ref-acyclic sets of DECs, the InLocalCore(P, P) decision problem is ΠP2 -complete in the size of D̄ , the input neighborhood instance. 2
We can also apply the null-based solution semantics to Definition 3.3, obtaining a specific definition of peer consistent answer. More precisely, given a PDES schema P, an instance D for it, a peer P and a query Q(x̄ ) ∈ L(P), the set of peer consistent answers to Q from P is
PCADP,P(Q) = {t | D |=N Q[t ], for all D ∈ Sol(P,D) }, (31)
with Sol(P,D) as defined in this section, on the basis of the ≤Σ(P)D relations. Proposition 5.2 and its Corollary 5.1 still hold when, for each Q ∈ N ◦(P), JQ = Core(Q,D), and JP = D(P). Therefore, the local computation of peer consistent answers to a conjunctive query -for which the cores of the neighboring peers are used- is also ΠP2 -complete.
Corollary 5.2 For a PDES schema P = 〈P,S,Σ, T rust〉, an instance D for P, and a peer P ∈ P, deciding answers to a conjunctive query posed to P that are true in ⋂ NS(P, D̄), where D̄ = D(P) ∪⋃Q∈N◦(P) Core(Q,D), is ΠP2 -complete in the size of D̄ . This is also true for P with ref-acyclic sets of DECs. 2
The complexity of the decision and computational problems at the neighborhood level we have obtained so far are the most interesting, due to the modular and local manner solutions and peer-consistent answers are computed. These results will be useful in Section 6, where specification and computation of neighborhood solutions and PCAs are addressed.
We make only some final remarks in relation to global solutions and PCAs on their basis. Corollary 5.2 already tells us that deciding peer-consistent answers (which involves global solutions as opposed to neighborhood solutions) will be at least ΠP2 - hard. Actually, using Proposition 5.2 it is possible to provide a non-deterministic polynomial time algorithm (in data) with a ΠP2 -oracle to decide if an instance for a peer is a (global) solution for the peer. On this basis, one can obtain that PCA, as a decision problem, belongs to ΠP3 , in data (that includes those of all peers in the system).
We calculated the participant acceptance rate of meal recommendations as: # Meals in Yummy bucket# Recommended meals .
The cumulative distribution of the acceptance rate is shown in Fig.6 and the average values per approach are presented in Table.5. The results clearly demonstrate that Yum-me significantly improves the quality of the presented food items. The per-user difference between the two approaches was normally distributed5, and a paired Student’s t-test indicated a significant difference between the two methods (p < 0.0001).6
To further quantify the improvement provided by the Yumme system, we calculated the difference between the acceptance rates of the two systems, i.e. difference = Yum-me acceptance rate−baseline acceptance rate. The distribution and average values of the differences are presented in Fig.7 and Table.5 respectively. It is noteworthy that Yum-me 5A Shapiro Wilk W test was not significant (p= 0.12), which justifies that the difference is normally distributed. 6We also performed a non-parametric Wilcoxon signed-rank test and found a comparable result.
outperformed the baseline by 42.63% in terms of the number of preferred recommendations, which demonstrates its utility over the traditional meal recommendation approach.
The MNIST data-set contains 60,000 training images and 10,000 testing images of hand-written digits of size 28x28. The baseline model is composed of two convolutional layers and two fully-connected layers, as shown in Table 2, with ReLU and pooling following each convolutional layer. This baseline model achieves 0.82% Error rate with this simple network. The DCCKs training algorithm begins by splitting the first convolutional layer from 100 to 200 kernels; after the subsequent fine-tuning the model achieved 0.59% error rate, which is almost 30% relative improvement from the original model. This compared favorably to a 200 kernel models trained from scratch, which achieves 0.78%, and even a 300 kernels model trained from scratch, which achieves 0.75%. This verifies that splitting filters has the potential to help the following SGD based fine-tuning to achieve an optimal point which generalists better. Also, more importantly after following merging step, back to 100 kernels, the performance dropped only 0.01% to an error rate of 0.59%.
Let x = col{x1, x2, . . . , xN} denote an N × 1 block column vector whose individual entries are of size M × 1 each. Following [52, 54, 55], the block maximum norm of x is denoted by ‖x‖b,∞ and is defined as
‖x‖b,∞ ∆ = max
1≤k≤N ‖xk‖ (582)
where ‖·‖ denotes the Euclidean norm of its vector argument. Correspondingly, the induced block maximum norm of an arbitrary N×N block matrix A, whose individual block entries are of size M×M each, is defined as
‖A‖b,∞ ∆ = max
x 6=0 ‖Ax‖b,∞ ‖x‖b,∞
(583)
The block maximum norm inherits the unitary invariance property of the Euclidean norm, as the following result indicates [54].
Lemma D.1. (Unitary Invariance) Let U = diag{U1, U2, . . . , UN} be an N × N block diagonal matrix with M ×M unitary blocks {Uk}. Then, the following properties hold:
(a) ‖Ux‖b,∞ = ‖x‖b,∞
(b) ‖UAU∗‖b,∞ = ‖A‖b,∞
for all block vectors x and block matrices A of appropriate dimensions.
The next result provides useful bounds for the block maximum norm of a block matrix.
Lemma D.2. (Useful Bounds) Let A be an arbitrary N ×N block matrix with blocks Aℓk of size M ×M each. Then, the following results hold:
(a) The norms of A and its complex conjugate are related as follows:
‖A∗‖b,∞ ≤ N · ‖A‖b,∞ (584)
(b) The norm of A is bounded as follows:
max 1≤ℓ,k≤N ‖Aℓk‖ ≤ ‖A‖b,∞ ≤ N ·
( max
1≤ℓ,k≤N ‖Aℓk‖
) (585)
where ‖ · ‖ denotes the 2−induced norm (or maximum singular value) of its matrix argument.
(c) If A is Hermitian and nonnegative-definite (A ≥ 0), then there exist finite positive constants c1 and c2 such that
c1 · Tr(A) ≤ ‖A‖b,∞ ≤ c2 · Tr(A) (586)
Proof. Part (a) follows directly from part (b) by noting that
‖A∗‖b,∞ ≤ N · ( max
1≤ℓ,k≤N ‖A∗ℓk‖
)
= N · ( max
1≤ℓ,k≤N ‖Aℓk‖
)
≤ N · ‖A‖b,∞
where the equality in the second step is because ‖A∗ℓk‖ = ‖Aℓk‖; i.e., complex conjugation does not alter the 2−induced norm of a matrix.
To establish part (b), we consider arbitrary N × 1 block vectors x with entries x = col{x1, x2, . . . , xN} and where each xk is M × 1. Then, note that
‖Ax‖b,∞ = max 1≤ℓ≤N ∥∥∥∥∥ N∑
k=1
Aℓkxk ∥∥∥∥∥
≤ max 1≤ℓ≤N
( N∑
k=1
‖Aℓk‖ · ‖xk‖
)
≤ ( max
1≤ℓ≤N
N∑
k=1
‖Aℓk‖ ) · max 1≤k≤N ‖xk‖
≤ ( max
1≤ℓ≤N
N∑
k=1
max 1≤k≤N ‖Aℓk‖
) · ‖x‖b,∞
≤ N · ( max
1≤ℓ,k≤N ‖Aℓk‖
) · ‖x‖b,∞
so that
‖A‖b,∞ ∆ = max
x 6=0 ‖Ax‖b,∞ ‖x‖b,∞ ≤ N ·
( max
1≤ℓ,k≤N ‖Aℓk‖
)
which establishes the upper bound in (585). To establish the lower bound, we assume without loss of generality that max1≤ℓ,k≤N ‖Aℓk‖ is attained at ℓ = 1 and k = 1. Let σ1 denote the largest singular value of A11 and let {v1, u1} denote the corresponding M × 1 right and left singular vectors. That is,
‖A11‖ = σ1, A11v1 = σ1u1 (587)
where v1 and u1 have unit norms. We now construct an N × 1 block vector x o as follows:
xo ∆ = col{v1, 0M , 0M , . . . , 0M} (588)
Then, obviously, ‖xo‖b,∞ = 1 (589)
and Axo = col{A11v1, A21v1, . . . , AN1v1} (590)
It follows that
‖Axo‖b,∞ = max { ‖A11v1‖, ‖A21v1‖, . . . , ‖AN1v1‖ }
≥ ‖A11v1‖
= ‖σ1u1‖
= σ1
= ‖A11‖
= max 1≤ℓ,k≤N ‖Aℓk‖ (591)
Therefore, by the definition of the block maximum norm,
‖A‖b,∞ ∆ = max
x 6=0 ( ‖Ax‖b,∞ ‖x‖b,∞ )
≥ ‖Axo‖b,∞ ‖xo‖b,∞ = ‖Axo‖b,∞
≥ max 1≤ℓ,k≤N ‖Aℓk‖ (592)
which establishes the lower bound in (585). To establish part (c), we start by recalling that all norms on finite-dimensional vector spaces are equivalent [20,21]. This means that if ‖ · ‖a and ‖ · ‖d denote two different matrix norms, then there exist positive constants c1 and c2 such that for any matrix X,
c1 · ‖X‖a ≤ ‖X‖d ≤ c2 · ‖X‖a (593)
Now, let ‖A‖∗ denote the nuclear norm of the square matrix A. It is defined as the sum of its singular values:
‖A‖∗ ∆ =
∑
m
σm(A) (594)
Since A is Hermitian and nonnegative-definite, its eigenvalues coincide with its singular values and, therefore,
‖A‖∗ = ∑
m
λm(A) = Tr(A)
Now applying result (593) to the two norms ‖A‖b,∞ and ‖A‖∗ we conclude that
c1 · Tr(A) ≤ ‖A‖b,∞ ≤ c2 · Tr(A) (595)
as desired.
The next result relates the block maximum norm of an extended matrix to the ∞−norm (i.e., maximum absolute row sum) of the originating matrix. Specifically, let A be an N ×N matrix with bounded entries and introduce the block matrix
A ∆ = A⊗ IM (596)
The extended matrix A has blocks of size M ×M each.
Lemma D.3. (Relation to Maximum Absolute Row Sum) Let A and A be related as in (596). Then, the following properties hold:
(a) ‖A‖b,∞ = ‖A‖∞, where the notation ‖ · ‖∞ denotes the maximum absolute row sum of its argument.
(b) ‖A∗‖b,∞ ≤ N · ‖A‖b,∞.
Proof. The results are obvious for a zero matrix A. So assume A is nonzero. Let x = col{x1, x2, . . . , xN} denote an arbitrary N × 1 block vector whose individual entries {xk} are vectors of size M × 1 each. Then,
‖Ax‖b,∞ = max 1≤k≤N ∥∥∥∥∥ N∑
ℓ=1
akℓxℓ ∥∥∥∥∥
≤ max 1≤k≤N
( N∑
ℓ=1
|akℓ| · ‖xℓ‖
)
≤ ( max
1≤k≤N
N∑
ℓ=1
|akℓ| ) · max 1≤ℓ≤N ‖xℓ‖
= ‖A‖∞ · ‖x‖b,∞ (597)
so that
‖A‖b,∞ ∆ = max
x 6=0 ‖Ax‖b,∞ ‖x‖b,∞ ≤ ‖A‖∞ (598)
The argument so far establishes that ‖A‖b,∞ ≤ ‖A‖∞. Now, let ko denote the row index that corresponds to the maximum absolute row sum of A, i.e.,
‖A‖∞ = N∑
ℓ=1
|akoℓ|
We construct an N × 1 block vector z = col{z1, z2, . . . , zN}, whose M × 1 entries {zℓ} are chosen as follows:
zℓ = sign(akoℓ) · e1
where e1 is the M × 1 basis vector: e1 = col{1, 0, 0, . . . , 0}
and the sign function is defined as
sign(a) =
{ 1, if a ≥ 0
−1, otherwise
Then, note that z 6= 0 for any nonzero matrix A, and
‖z‖b,∞ = max 1≤ℓ≤N ‖zℓ‖ = 1
Moreover,
‖A‖b,∞ ∆ = max
x 6=0 ‖Ax‖b,∞ ‖x‖b,∞
≥ ‖Az‖b,∞ ‖z‖b,∞ = ‖Az‖b,∞
= max 1≤k≤N ∥∥∥∥∥ N∑
ℓ=1
akℓzℓ ∥∥∥∥∥
≥ ∥∥∥∥∥ N∑
ℓ=1
akoℓzℓ ∥∥∥∥∥
= ∥∥∥∥∥ N∑
ℓ=1
akoℓ · sign(akoℓ)e1 ∥∥∥∥∥
= N∑
ℓ=1
|akoℓ| · ‖e1‖
=
N∑
ℓ=1
|akoℓ|
= ‖A‖∞ (599)
Combining this result with (598) we conclude that ‖A‖b,∞ = ‖A‖∞, which establishes part (a). Part (b) follows from the statement of part (a) in Lemma D.2.
The next result establishes a useful property for the block maximum norm of right or left stochastic matrices; such matrices arise as combination matrices for distributed processing over networks as in (166) and (185).
Lemma D.4. (Right and Left Stochastic Matrices) Let C be an N ×N right stochastic matrix, i.e., its entries are nonnegative and it satisfies C1 = 1. Let A be an N ×N left stochastic matrix, i.e., its entries are nonnegative and it satisfies AT1 = 1. Introduce the block matrices
AT ∆ = AT ⊗ IM , C ∆ = C ⊗ IM (600)
The matrices A and C have blocks of size M ×M each. It holds that
‖AT ‖b,∞ = 1, ‖C‖b,∞ = 1 (601)
Proof. Since AT and C are right stochastic matrices, it holds that ‖AT ‖∞ = 1 and ‖C‖∞ = 1. The desired result then follows from part (a) of Lemma D.3.
The next two results establish useful properties for the block maximum norm of a block diagonal matrix transformed by stochastic matrices; such transformations arise as coefficient matrices that control the evolution of weight error vectors over networks, as in (189).
Lemma D.5. (Block Diagonal Hermitian Matrices) Consider an N × N block diagonal Hermitian matrix D = diag{D1, D2, . . . , DN}, where each Dk is M ×M Hermitian. It holds that
ρ(D) = max 1≤k≤N ρ(Dk) = ‖D‖b,∞ (602)
where ρ(·) denotes the spectral radius (largest eigenvalue magnitude) of its argument. That is, the spectral radius of D agrees with the block maximum norm of D, which in turn agrees with the largest spectral radius of its block components.
Proof. We already know that the spectral radius of any matrix X satisfies ρ(X ) ≤ ‖X‖, for any induced matrix norm [19,20]. Applying this result to D we readily get that ρ(D) ≤ ‖D‖b,∞. We now establish the reverse inequality, namely, ‖D‖b,∞ ≤ ρ(D). Thus, pick an arbitrary N × 1 block vector x with entries {x1, x2, . . . , xN}, where each xk is M × 1. From definition (583) we have
‖D‖b,∞ ∆ = max
x 6=0 ‖Dx‖b,∞ ‖x‖b,∞
= max x 6=0
( 1
‖x‖b,∞ · max 1≤k≤N ‖Dkxk‖
)
≤ max x 6=0
( 1
‖x‖b,∞ · max 1≤k≤N (‖Dk‖ · ‖xk‖)
)
= max x 6=0 max 1≤k≤N
( ‖Dk‖ · ‖xk‖
‖x‖b,∞
)
≤ max 1≤k≤N ‖Dk‖
= max 1≤k≤N ρ(Dk) (603)
where the notation ‖Dk‖ denotes the 2−induced norm of Dk (i.e., its largest singular value). But since Dk is assumed to be Hermitian, its 2−induced norm agrees with its spectral radius, which explains the last equality.
Lemma D.6. (Block Diagonal Matrix Transformed by Left Stochastic Matrices) Consider an N ×N block diagonal Hermitian matrix D = diag{D1, D2, . . . , DN}, where each Dk is M ×M Hermitian. Let A1 and A2 be N×N left stochastic matrices, i.e., their entries are nonnegative and they satisfy AT1 1 = 1 and AT2 1 = 1. Introduce the block matrices
AT1 = A T 1 ⊗ IM , A T 2 ∆ = AT2 ⊗ IM (604)
The matrices A1 and A2 have blocks of size M ×M each. Then it holds that
ρ ( AT2 · D · A T 1 ) ≤ ρ(D) (605)
Proof. Since the spectral radius of any matrix never exceeds any induced norm of the same matrix, we have that
ρ ( AT2 · D · A T 1 ) ≤ ∥∥∥ AT2 · D · AT1 ∥∥∥ b,∞
≤ ∥∥∥AT2 ∥∥∥ b,∞ · ‖D‖ b,∞ · ∥∥∥AT1 ∥∥∥ b,∞
(601) = ‖D‖
b,∞
(602) = ρ(D) (606)
In view of the result of Lemma D.5, we also conclude from (605) that
ρ ( AT2 · D · A T 1 ) ≤ max
1≤k≤N ρ(Dk) (607)
It is worth noting that there are choices for the matrices {A1,A2,D} that would result in strict inequality in (605). Indeed, consider the special case:
D = [ 2 0 0 1 ] , AT1 = [ 1 3 2 3
2 3 1 3
] , AT2 = [ 1 3 2 3
2 3 1 3
]
This case corresponds to N = 2 and M = 1 (scalar blocks). Then,
AT2 DA T 1 =
[ 2 3 2 3
2 3 1
]
and it is easy to verify that ρ(D) = 2, ρ(AT2 DA T 1 ) ≈ 1.52
The following conclusions follow as corollaries to the statement of Lemma D.6, where by a stable matrix X we mean one whose eigenvalues lie strictly inside the unit circle.
Corollary D.1. (Stability Properties) Under the same setting of Lemma D.6, the following conclusions hold:
(a) The matrix AT2 DA T 1 is stable whenever D is stable.
(b) The matrix AT2 DA T 1 is stable for all possible choices of left stochastic matrices A1 and A2 if, and only
if, D is stable.
Proof. Since D is block diagonal, part (a) follows immediately from (605) by noting that ρ(D) < 1 whenever D is stable. [This statement fixes the argument that appeared in App. I of [18] and Lemma 2 of [33]. Since the matrix X in App. I of [18] and the matrix M in Lemma 2 of [33] are block diagonal, the ‖ · ‖b,∞ norm should replace the ‖ · ‖ρ norm used there, as in the proof that led to (606) and as already done in [54].] For part (b), assume first that D is stable, then AT2 DA T 1 will also be stable by part (a) for any left-stochastic matrices A1 and A2. To prove the converse, assume that AT2 DA T 1 is stable for any choice of left stochastic matrices A1 and A2. Then, A T 2 DA T 1 is stable for the particular choice A1 = I = A2 and it follows that D must be stable.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. PCSC 2016 March 16-18, 2016, Puerto Princesa City, Palawan. Copyright 2016 ACM x-xxxxx-xx ...$15.00.
In broad terms, a formal grammar may be defined as a set of rules to expand high-level symbols into more detailed sequences of symbols (words) representing elements of formal languages. Words are generated by repeatedly applying rewriting rules, in a sequence of so-called derivation steps. In this way, grammars are suited to represent systems with hierarchical structure, which is reflected in the recursive application of the rules. As hierarchical structures can be recognized in most styles of music, it is hardly surprising that formal grammar theory has been applied to analyze and compose music for a long time12, despite recurring concerns that grammars fail to capture the internal coherency and subtleties required for music composition (Moorer, 1972).
To compose music using formal grammars, an important step is to define the set of rules of the grammar, which will drive the generative process. The rules are traditionally multi-layered, defining several subsets (maybe even separated in distinct grammars) of rules for different phases of the composition process: from the general themes of the composition, down to the arrangement of individual notes. While early authors derived the rules by hand from principles grounded in music theory, other methods are possible, like examining a corpus of pre-existing musical compositions to distill a grammar able to generate compositions in the general style of the corpus, or using evolutionary algorithms. Another important aspect is the mapping between the formal grammar and the musical objects that it generates, which usually relates the symbols of the derived sequences with elements of the music composition, as notes, chords or melodic lines. However, other mappings are possible, as using the derivation tree to define the different aspects of the musical composition. Another important aspect of the automatic composition process is the election of the grammatical
10. For example, Markov chains can be formulated as stochastic grammars; some self-similar systems can be characterized as L-system grammars; rule learning and case-based reasoning are also machine learning methods; etc. 11. For example, Kohonen’s method (Kohonen et al., 1991), which is neither grammatical nor neural nor Markovian, but can be framed in either way, according to its creator. 12. See, e.g., the survey by Roads (1979).
rules to be applied. While many approaches are possible, the use of activation probabilities for the rules (stochastic grammars) is common. In the process of compiling information for this survey, it has been noted that almost all research has been done on regular and context-free grammars, as context-sensitive and more general grammars seem to be very difficult to implement effectively, except for very simple toy systems.
Lidov and Gabura (1973) implemented an early example of a formal grammar to compose simple rhythms. Another early example was implemented by Rader (1974): he defined a grammar by hand from rather simple music concepts, enriching the rules of the grammar with activation probabilities. Other early examples used grammars driven by rules from music theories, either as a small part of a synthesis engine, as Ulrich’s (1977) grammar
for enumerating jazz chords, or by inferring the rules from classical works, as Baroni and Jacoboni’s (1978) grammar to generate melodies. A Generative Theory of Tonal Music (Lerdahl et al., 1983), a book presenting a grammatical analysis of tonal music, is a relatively early theoretical work that can be said to have influenced the use of grammars for algorithmic composition, though it is not directly concerned with algorithmic composition, but with a grammatical approach to the analysis of music. This book has been widely popular, and has had a lasting impact on the field and high citation rates. Examples of later work inspired by this book include Pope’s (1991) “T-R Trees”, Leach and Fitch’s (1995) “event trees”, and Hamanaka et al.’s (2008) “melody morphing”.
In the 1980s, some proposed approaches more in line with computer science, abstracting the process to generate the grammars instead of codifying them by hand, though at the cost of producing less interesting compositions. Roads (1977) proposed a framework to define, process and use grammars to compose music, while Holtzman (1981) described a language to define music grammars and automatically compose music from them. Meanwhile, Jones (1980) proposed the concept of space grammars, in conjunction with a novel mapping technique: instead of using the terminal symbols as the building blocks of the composition, he used the derivation tree of the terminal sequence to define the characteristics of the composition. This approach was unfortunately not developed far enough to yield significant results. In spite of these early efforts, most research on grammatical representations of music was focused on analysis rather than synthesis. Some instances, such as Steedman’s (1984) influential grammar for the analysis of jazz chord progressions, were later adapted for synthesis (see below).
The problem with a grammatical approach to algorithmic composition is the difficulty to manually define a set of grammatical rules to produce good compositions. This problem can be solved by generating the rules of the grammar (and the way they are applied) automatically. For example, although Bel (1992) implemented the BOL processor to facilitate the creation by hand of more or less sophisticated music grammars13, he also explored the automated inference of regular grammars (Kippen & Bel, 1989). Later, Cruz-Alcázar and Vidal-Ruiz (1998) implemented several methods of grammatical inference: analyze a corpus of pre-existing classical music compositions, represented with a suitable set of symbols, then inducing stochastic regular grammars (Markov chains) able to parse the compositions in the corpus, and finally applying these grammars to generate new compositions that are in a similar style to the compositions in the corpus. Gillick et al. (2009) used a similar approach (also Markovian) to synthesize jazz solos, but with a more elaborated synthesis phase. Kitani and Koike (2010) provide another example of grammatical inference, in this case used for real-time improvised accompaniment.
However, others still designed their grammars by hand, carefully choosing the mapping between terminal symbols and musical objects, as Keller and Morrison (2007) did for jazz improvisations. Another approach is to take a pre-existing music theory with a strong hierarchical methodology, as designing a grammar inspired in Schenkerian analysis (Quick, 2010), or using Lerdhal’s grammatical analysis to derive new compositions from two previously existing ones by altering the derivation tree (Hamanaka et al., 2008), or even de-
13. Initially to represent and analyze informal knowledge about Indian tabla drumming, but later also to represent other music styles.
veloping a jazz on-the-fly improviser (Chemillier, 2004) by adapting Steedman’s grammar, previously implemented for analysis purposes.
Let X denote the source input, which is the input dialogue history for conversational response generation or a source sentence for machine translation. The input X is mapped to a vector representation, which is used as the initial input to the decoder. Each X is paired with a target sequence Y , which corresponds to a dialogue utterance in response generation or a target sentence in machine translation. Y consists a sequence of words, where Y = {y1, y2, ..., yny} and ny denotes the length of Y . A neural generation model defines a distribution over outputs and sequentially predicts tokens using a softmax function:
p(Y |X) = ny∏ t=1 p(yt|X, y1, y2, ..., yt−1)
At test time, the goal is to find the sequence Y ∗ that maximizes the probability given input X:
Y ∗ = arg max X p(Y ∗|X) (1)
The above approach effectively defines a kernel over interpretations
K(z, z′) = K(Gz, Gz′)
where Gz is the result of graphicalization applied to interpretation z. For learning jobs such as classification or regression on interpretations (see Table 2), this kernel is directly usable in conjunction with plain kernel machines like SVM. When moving to more complex jobs involving, e.g., classification of entities or tuples of entities, the kernel induces a feature vector φ(x, y) suitable for the application of a structured output technique where f(x) = argmaxy w
>φ(x, y). Alternatively, we may convert the structured output problem into a set of independent subproblems as follows. For simplicity, assume the learning job consists of a single relation r of relational arity n. We call each ground atom of r a case. Intuitively, cases correspond to training targets or prediction-time queries in supervised learning. Usually an interpretation contains several cases corresponding to specific entities such as individual Web pages (as in Section 5.3) or movies (as in Section 5.4), or tuples of entities for link prediction problems (as in Section 2). Given a case c ∈ y, the viewpoint of c, Wc, is the set of vertices that are adjacent to c in the graph. In order to define the kernel, we first consider the mutilated graph Gc where all vertices in y, except c, are removed (see Figure 5 for an illustration). We then define a kernel κ̂ on mutilated graphs, following the same approach of the NSPDK, but with the additional constraint that the first endpoint must be in Wc. The decomposition is thus defined as
R̂r,d = {(A,B,Gc) : N vr , Nur , v ∈ Wc, d?(u, v) = d}.
We obtain in this way a kernel “centered” around case c:
K̂(Gc, G ′ c′) = ∑ r,d ∑ A,B ∈ R̂−1r,d(Gc) A′, B′ ∈ R̂−1r,d(G ′ c′ ) δ(A,A′)δ(B,B′)
and finally we let K(G,G′) = ∑ c∈y,c′∈y′ K̂(Gc, G ′ c′).
This kernel corresponds to the potential
F (x, y) = w> ∑ c φ̂(x, c)
which is clearly maximized by maximizing, independently, all sub-potentials w>φ̂(x, c) with respect to c.
By following this approach, we do not obtain a collective prediction (individual ground atoms are predicted independently). Still, even in this reduced setting, the kLog framework can be exploited in conjunction with metalearning approaches that surrogate collective prediction. For example, Prolog predicates in intensional signatures can effectively be used as expressive relational templates for stacked graphical models [48] where input features for one instance are computed from predictions on other related instances. Results in Section 7.2 for the “partial information” setting are obtained using a special form of stacking.
e use of the OMD model for quality data speci cation and extraction generalizes a previous approach to- and work on context-based data quality assessment and extraction [16, 19], which was brie y described in Section 1. e important new element in comparison to previous work is the presence in an ontological context Oc as in Figure 5 of the core multi-dimensional (MD) ontology OM represented by an OMD model as introduced in Section 3.
In the rest of this section we show in detail the components and use of an MD context in quality data speci cation and extraction, for which we refer to Figure 5. For motivation and illustration we use a running example that extends those in Sections 1 and 3.
On the LHS of Figure 5, we nd a database instance, D, for a relational schema R = {R1, ...,Rn}. e goal is to specify and extract quality data from D. For this we use the contextual ontology Oc shown in the middle, which contains the following elements and components: (a) Nickname predicates R′ in a nickname schema R ′ for predicates R in R. ese are copies of the
predicates for D and are populated exactly as in D, by means of the simple mappings (rules) forming a set Σ′ of tgds, of the form:
R(x̄) → R′(x̄). (25) whose enforcement producing a material or virtual instance D ′ within Oc . (b) e core MD ontology, OM , as in Section 3, with an instance IM = DH ∪ I c , a set ΣM of dimensional tgds, and a set κM of dimensional constraints, among them egds and ncs. (c) ere can be, for data quality use, extra contextual data forming an instance E, with schema RE , that is not necessarily part of (or related to) the OMD ontology OM . It is shown in Figure 5 on the RHS of the middle box. (d) A set of quality predicates, P, with their de nitions as Datalog rules forming a set ΣP of tgds. ey may be de ned in terms of predicates in RE , built-ins, and dimensional predicates in RM .
, Vol. 1, No. 1, Article 1. Publication date: March 2017.
We will assume that quality predicates in P do not appear in the core dimensional ontology OM that de nes the dimensional predicates in RM . As a consequence, the program de ning quality predicates can be seen as a “top layer”, or top sub-program, that can be computed a er the core (or base) ontological program has been computed.16 For a quality predicate P ∈ P, its de nition of the form:
φEP (x̄),φMP (x̄) → P(x̄). (26)
Here, φEP (x̄) is a conjunction of atoms with predicates in RE or plus built-ins, and φ M P (x̄) is a conjunction of atoms with predicates in RM .17 Due to their de nitions, quality predicates in the context can be syntactically told apart from dimensional predicates. ality predicate re ect application dependent, speci c quality concerns.
Example 5.1. (ex. 1.1 and 3.1 cont.) Predicate for Temperatures ∈ R, the initial schema, has Temperatures′ ∈ R ′ as a nickname, and de ned by Temperatures(x̄) → Temperatures′(x̄). e former has Table 1 as extension in instance D, which is under quality assessment, and with this rule, the data are copied into the context. e core MD ontology OM has WorkSchedules and Shi s as categorical relations, linked to the Hospital and Temporal dimensions (cf. Figure 4). OM has a set of dimensional tgds, ΣM , that includes σ1 and σ2, and also a dimensional rule de ning a categorical relation WorkTimes, as a view in terms of WorkSchedules the TimeDay child-parent dimensional relation, to create data from the day level down to the time (of the day) level:
WorkSchedules(u,d ;n, s), TimeDay(t ,d) → WorkTimes(u, t ;n, s). (27)
OM also has a set κM of dimensional constraints, including the dimensional nc and egd, (21) and (24), resp.
Now, in order to address data quality concerns, e.g. about certi ed nurses or thermometers, we introduce quality predicates, e.g. TakenWith erm, about times at which nurses use certain thermometers, with a de nition of the form (26):
WorkTimes(intensive, t ;n,y) → TakenWith erm(t ,n, b1), (28) which captures the guideline about thermometers used in intensive care units; and becomes a member of ΣP (cf. Figure 5).
In this case, we are not using any contextual database E outside the MD ontology, but we could have an extension for a predicate Supply(Ins, ) ∈ RE , showing thermometer brands ( ) supplied to hospital institutions (Ins), in the Hospital dimension.18 It could be used to de ne (or supplement the previous de nition of) TakenWith erm(t,n,th):
Supply(ins, th),UnitInstitution(u, ins),WorkTimes(u, t ;n,y) → TakenWith erm(t, n, th). (29)
16 is assumption does not guarantee that the resulting, combined ontology has the same syntactic properties of the core MD ontology, e.g. being WS (cf. Example 5.3), but the analysis of the combined ontology becomes easier, and in some cases it allows us to establish that the combination inherits the good computational properties from the MD ontology. We could allow de nitions of quality predicates in Datalog with strati ed negation (not) or even in Datalog+. In the former case, the complexity of CQA would not increase, but in the la er we cannot say anything general about the complexity of CQA. 17 We could also have predicates from P in the body if we allow mutual or even recursive dependencies between quality predicates. 18 E could represent data brought from external sources, possible at query answering time [16, 19]. In this example, it governmental data about hospital supplies.
, Vol. 1, No. 1, Article 1. Publication date: March 2017.
Now the main idea consists in using the data brought into the context via the nickname predicates and all the contextual elements to specify quality data for the original schema R, as a quality alternative to instance D.
(e) We introduce a “quality schema”, Rq , a copy of schema R, with a predicate Rq for each predicate R ∈ R. ese are quality versions of the original predicates. ey are de ned, and populated if needed, through quality data extraction rules that form a set, Σq (cf. Figure 5), of Datalog rules of the form:
R′(x̄),ψ PR′(x̄) → R q(x̄). (30)
Here,ψ PR′(x̄) is an ad hoc for predicate R conjunction of quality predicates (in P) and builtins. e connection with the data in the corresponding original predicate is captured with the join with its nickname predicate R′.19
De nitions of the initial predicates’ quality versions impose conditions corresponding to user’s data quality pro les, and their extensions form the quality data (instance).
Example 5.2. (ex. 5.1 cont.) e quality version of the original predicate Temperatures is Temperaturesq ∈ Rq , de ned by:
Temperatures′(t ,p,v,n), TakenWith erm(t ,n, b1) → Temperaturesq(t ,p,v,n), (31) imposing extra quality conditions on the former. is is a de nition of the form (30) in Σq (cf. also Example 1.2).
All the solvers were compiled by g++ 4.6.3 with the ’-O3’ option. For FastVC4, we adopt the parameter setting reported in (Cai 2015). The experiments were conducted on a cluster equipped with a number of Intel(R) Xeon(R) CPUs X5650 @2.67GHz with 8GB RAM, running Red Hat Santiago OS.
All the algorithms are executed on each instance with a time limit of 1000 seconds, with seeds from 1 to 100. For each algorithm on each instance, we report the minimum size (”Cmin”) and averaged size (”Cavg”) of vertex covers found by the algorithm. To make the comparisons clearer, we also report the difference (”∆”) between the minimum size of vertex cover found by FastVC and that found by LinCom. A positive ∆ means that LinCom finds a smaller vertex cover, while a negative ∆ means that FastVC finds a smaller vertex cover. The numbers of vertices of these graphs lie between 1 × 103 to 4 × 106. We omit them and readers may refer to (Cai 2015) or the download website.
In this paper, we propose a new approach, medical Generative Adversarial Network (medGAN), to generate realistic synthetic EHRs. Based on an input EHR dataset, medGAN can generate high-dimensional discrete variables (e.g., binary and count features) via a combination of an autoencoder and generative adversarial networks. We also propose minibatch averaging to efficiently avoid mode collapse, and increase the learning efficiency with batch normalization and shortcut connections. To demonstrate feasibility, we showed that medGAN generates synthetic EHR datasets that achieve comparable performance to real data on many experiments including distribution statistics, predictive modeling tasks and medical expert review.
Given the low correlation of individual frames with user identity, we found it strongly beneficial to make the input layer convolutional regardless of model type, thereby forcing earlier fusion of temporal information. To simplify the presentation, we have not made convolution explicit in the description of the methods above, however, it can be absorbed into the inputto-hidden matrix W.
5 input, x(t) x(t+1)x(t 1) x(t+2)x(t 2) V V V V V k=0 k=1 h(t 1) h(t) h(t+1) WWW W W UUU UU h(t+2)h (t 2) k=2 U õ(t 2) õ(t 1) output, õ(t) õ(t+1) õ(t+2)
Fig. 5. Proposed dense clockwork RNN with the same parameters as the original clockwork RNN shown in Fig. 3a.

Let V = {X1, . . . , Xm} be a set of m attributes in a combinatorial domain, For each X ∈ V, D (X) is the domain of X. A variable X is binary if D (X) = {x, x̄}. An alternative is uniquely identified by the combination of its attribute values. Hence, there are D(X1)×· · ·×D(Xm) possible alternatives (outcomes), denoted by O. Elements of O are denoted by o, o′, o′′ etc. and represented by concatenating the values of the variables. For example, if V = {A,B,C}, D(A) = {a, ā}, D(B) = {b, b̄} andD(C) = {c, c̄}, then the assignment ab̄c assigns a to variable A, b̄ to B and c to C. If X = {Xσ1 , . . . , Xσ`} ⊆ V, with σ1 < · · · < σ` then D (X) denotes D (Xσ1) × · · · × D (Xσ`) and x (x ∈ D (X)) denotes an assignment of variable values to X. If X = V, x is a complete assignment (corresponds to a possible outcome); otherwise x is called a partial assignment. If x and y are assignments to disjoint sets X and Y, respectively (X ∩Y = ∅), we denote the combination of x and y by xy. If X∪Y = V, we call xy a completion of assignment x. We denote by Comp(x) the set of completions of x. For any assignment x ∈ D (X), we denote by x [X] the value x ∈ D (X) (X ∈ X) assigned to variable X by that assignment; and x [W] denotes the assignment of variable values w ∈ D (W) assigned to the set of variables W ⊆ X by that assignment. We allow concatenations of partial assignments. For instance, let V = {A,B,C,D,E, F}, X = {A,B,C}, Y = {D,E}, x = ab̄c̄, y = d̄e, then xyf̄ denotes the alternative ab̄c̄d̄ef̄ .
The extensions to Resolver attempt to address the confusion between similar and identical pairs. Experiments with the extensions, using the same datasets and metrics as in Section 6 demonstrate that the Function Filter (FF) and the Coordination-Phrase Filter (CPF) boost Resolver’s precision. Unfortunately, the W-ESP model yielded essentially no improvement of Resolver.
Table 6 contains the results of our experiments. With coordination-phrase filtering, Resolver’s F1 is 28% higher than SSM’s on objects, and 6% higher than Resolver’s F1 without filtering. While function filtering is a promising idea, FF provides a smaller benefit than CPF on this dataset, and the merges that it prevents are, with a few exceptions, a subset of the merges prevented by CPF. This is in part due to the limited number of functions available in the data.
Both the Function Filter and the Coordination-Phrase Filter consistently blocked merges between highly similar countries, continents, planets, and people in our data, as well as some other smaller classes. The biggest difference is that CPF more consistently has hitcounts for the similar pairs that tend to be confused with identical pairs. Perhaps as the amount of extracted data grows, more functions and extractions with functions will be extracted, allowing the Function Filter to improve.
Part of the appeal of the W-ESP model is that it requires none of the additional inputs that the other two models require, and it applies to each property, rather than to a subset of the relations like the Function Filter. Like TFIDF weighting for the Cosine Similarity Metric, the W-ESP model uses information about the distribution of the properties in the data to weight each property. For the data extracted by TextRunner, neither W-ESP nor TFIDF weighting seems to have a positive effect. More experiments are required to test
whether W-ESP might prove more beneficial on other data sets where TFIDF does have a positive effect.
8. Resolver and Cross-Document Entity Resolution
Up to this point, we have made the single-sense assumption, or the assumption that every token has exactly one meaning. While this assumption is defensible in small domains, where named entities and relations rarely have multiple meanings, even there it can cause problems: for example, the names Clinton and Bush each refer to two major players in American politics, as well as a host of other people. When extractions are taken from multiple domains, this assumption becomes more and more problematic.
We now describe a refinement of the Resolver system that handles the task of CrossDocument Entity Resolution (Bagga & Baldwin, 1998), in which tokens or names may have multiple referents, depending on context. An experiment below compares Resolver with an existing entity resolution system (Li et al., 2004a), and demonstrates that Resolver can handle polysemous named entities with high accuracy. This extension could theoretically be applied to highly polysemous tokens such as common nouns, but this has not yet been empirically demonstrated.
8.1 Clustering Polysemous Names with Resolver
Recall that the synonym resolution task is defined as finding clusters in the set of distinct strings S found in a set of extractions D (Section 3). Cross-Document Entity Resolution differs from synonym resolution in that it requires a clustering of the set of all string occurrences, rather than the set of distinct strings. For example, suppose a document contains two occurrences of the token DP, one where it means “Design Pattern” and one where it means “Dynamic Programming.” Synonym resolution systems treat DP as a single item, and will implicitly cluster both occurrences of DP together. A Cross-Document Entity Resolution system treats each occurrence of DP separately, and therefore has the potential to put each occurrence in a separate cluster when they mean different things. In this way, a Cross-Document Entity Resolution system has the potential to handle polysemous names correctly.
Because of the change in task definition, the sources of evidence for similarity are sparser. For each occurrence of a named entity in its input, Resolver has just a single TextRunner extraction describing the occurrence. To achieve reasonable performance, it needs more information about the context in which a named entity appears. We change Resolver’s representation of entity occurrences to include the nearest E named entities in the text surrounding the occurrence. That is, each entity occurrence x is represented as a set of named entities y, where y appears among the nearest E entities in the text surrounding x. Suppose, for instance, that e1 is an occurrence of DP with Bellman and Viterbi in its context, and e2 is another occurrence with OOPSLA and Factory in its context. e1 would be represented by the set {Bellman,Viterbi}, and e2 would be represented by the set {Factory,OOPSLA}.
Table 7 summarizes the major ways in which we extended Resolver to handle polysemous names. With these extensions in place, Resolver can proceed to cluster occurrences of entities more or less the same way that it clusters entity names for synonym resolution.
The SSM model works as above, and the ESP model calculates probabilities of coreference based on sets of named entities in context rather than extracted properties. The clustering algorithm eliminates comparisons between occurrences that share no part of their contexts, or only very common contextual elements. In the end, Resolver produces sets of coreferential entity occurrences, which can be used to annotate extractions containing these entity occurrences for coreference relationships.
ar X
iv :1
60 4.
03 34
8v 1
[ cs
.L G
] 1
2 A
Support vector machine (SVM) has been one of the most popular learning algorithms, with the central idea of maximizing the minimum margin, i.e., the smallest distance from the instances to the classification boundary. Recent theoretical results, however, disclosed that maximizing the minimum margin does not necessarily lead to better generalization performances, and instead, the margin distribution has been proven to be more crucial. Based on this idea, we propose a new method, named Optimal margin Distribution Machine (ODM), which tries to achieve a better generalization performance by optimizing the margin distribution. We characterize the margin distribution by the first- and second-order statistics, i.e., the margin mean and variance. The proposed method is a general learning approach which can be used in any place where SVM can be applied, and their superiority is verified both theoretically and empirically in this paper.
Keywords: margin, margin distribution, minimum margin, classification
The dialogue manager is based on Markov Decision Process (MDP). MDP [31] is defined as a tuple {S,A, T ,R, γ}, where S is the set of states, A the set of actions, T (s′|s, a) is the transition probability of ending up in state s′ when executing action a in state s,R is the reward function, and γ the discount factor. A mapping from a state s ∈ S to an action a ∈ A, or action selection at each state, is a policy π. Given a policy π, the
value of the Q-function (Qπ : S ×A→ R) is the estimation of the expected discounted sum of all rewards that can be received over an infinite state transition path starting from state s taking action π(s): Q(s, a) = E[ ∑∞ k=0 γ
krk|s0 = s, a0 = a], where rk is the reward received from the action ak taken at state sk, and k is the sequence index for states and actions. The optimal policy maximizes the value of each state-action pair: π∗(s) = argmaxa∈AQ
∗(s, a), so finding an optimal policy is equivalent to finding the optimal Q-function.
The reward of ak,the action taken at state sk, is defined as
rk = −Ck + τ [E(sk)− E(sk−1)]. (2) E(s) is some retrieval quantity metric at the state s and Ck is the estimated effort by the user to perform the action ak. τ is a trade-off parameter between user effort and the retrieval quality improvement.
2.4.1. Previous Approach: Estimating the hand-crafted state
The previous approach [14, 15] for the dialogue management is path (A) in Figure 1. The underlying assumption behind this approach is that the proper choice of the action can be made by considering some evaluation metric for the retrieved results, which is the average precision (AP) here. This assumption leads to a two-stage process, shown as blocks A-1 and A-2, in the dialogue manager of Fig.1. Block A-1 is the state estimation. It takes the feature set extracted from the retrieved results in Section 2.3 as the input, and estimates the AP for them, taken as the state s in the Q-function for action selection. Block A-2 is for action decision. It uses fitted value iteration (FVI) [32] to train a Gaussian mixture model (GMM) to approximate the Qvalue function Q(s, a) for each action a, and the system takes the action a with the maximum Q(s, a).
This approach have several weaknesses. First, the evaluation metric AP is not necessary a good representation for states. AP simply indicates how well the retrieved results are. Empirical results have shown that even with the same AP, the optimal actions can be different. However, it’s not easy to come up with better state definition with human knowledge. Next, it’s not able to fix the error margin for relatively weak state estimation, because the state estimation (block A-1) and action selection (block A-2) are separately trained rather than considered jointly.
2.4.2. Proposed Approach: Deep Reinforcement Learning
The proposed approach used Deep-Q-Network (DQN) to do deep reinforcement learning of Q-function. DQN is able to overcome the problems of the previous approach mentioned above at least to some degree. As can be seem in path (B) of Fig 1, the DQN directly generates the proper action from the input features through the hidden layers. In this way the error propagation for the two cascaded stages (block A-1 and block A-2 in path (A)) is eliminated, and the machine automatically learns from the features extracted from the retrieve module including the human knowledge features and the raw relevance scores of the retrieved results.
The DQN is a deep neural network (DNN) [33, 34] with parameters θ to estimate the state-action value function Q(s, a; θ)1. The input of the DQN is the features extracted in Section 2.3, while its output dimension is the same as the number of possible actions a, and the output is the state-action value Q(s, a; θ) for each action a in the action set A. The DQN is
1Because the state-action value function Q(s, a) here depends on the deep neural network parameters θ, the function should be written as Q(s, a; θ).
trained by iteratively updating the parameters θ. With parameters obtained at the i-th iteration, denoted as θi, θ can be learned by minimizing the following the loss function Li(θi) in (3) by gradient descent.
Li(θ i) = E s,a,r,s′∼U(D) [(ŷi −Q(s, a; θi))2]. (3)
D = {e1, e2, ..., et, ...eL} includes experiences et = (st, at, rt, st+1) (taking action at at state st obtaining reward rt and reaching state st+1 at the next time step) is a dataset collected from many retrieval episodes to be used for training. The expression s, a, r, s′ ∼ U(D) in (3) means, instead of using the current experience as prescribed by the standard temporal-difference learning, the network is trained by sampling mini-batches of experiences from D uniformly at random. This method is referred to as experience replay, which is a key ingredient behind the success of DQN. In this way the efficiency in using the training data can be improve through reuse of the experience samples in multiple updates, and the correlation among the samples used in the update can be reduced through the uniform sampling from the replay buffer [35, 17].
ŷi in (3) is defined as below:
ŷi = r + γ ·max a′∈A
Q(s′, a′; θ−) (4)
where θ− represents the parameters of a fixed and separate target network, which is taken here as the parameters obtained several iterations before. Freezing the parameters of the target networkQ(s′, a′; θ−) for a fixed number of iterations while updating the online network Q(s, a; θi) is another key innovation for the success of DQN [35, 17], which improved the stability of the algorithm.
Commonly used topic models implicitly take advantage of rich word co-occurrence patterns in documents. However, short texts naturally lack of enough contextual information. Furthermore, the goal of traditional topic models is to maximize the probability of generating observed documents, and rare topics that reflected by fewer documents are tended to be ignored. As a result, directly applying conventional topic models on short or imbalanced texts can not perform as well as that on normal balanced texts. In order to solve the problem mentioned above through a simple but general method, we propose a new framework, which applies the same Gibbs sampling [14] with LDA to discover latent word groups in a word co-occurrence network. Here latent word groups of the network are taken as topic components of a corpus. In addition, the distribution over latent groups for each word is also learned by our model. The details of the new framework is presented in the following subsections.
3.1 Word co-occurrence network
In a word co-occurrence network (which may also be denoted as word network in the following text if there is no conflict), nodes are words occurring in the corpus and an edge between two words indicates that the connected two words have co-occurred in the same context at least once. Here the context can refer to a document or a sliding window with fixed size. To
limit the size of word network and reserve only the local context for each word, we take a sliding window of fixed size as the context in the present work. We empirically set the size of siding window to 10 (a typical value employed in the previous study) in this paper, since a word is only semantically related with adjacent words, especially in the short text. Degree of a node is defined as the sum of weights over its adjacent links. While activity of a node is defined as the averaged weight of its adjacent links.
In order to convert the given document collection into a word network, we first filter out stopping words and low frequency words, and then a sliding window is moved to scan each document. As the window scanning word by word through the document, any two distinct words appear in the same window would be regarded as co-occurred with each other. Times that two words co-occurred are accumulated and defined as the weight of the corresponding edge between them.
Note that in topic models, a topic can be viewed as a bag of words co-occurred frequently in the same document, which is very similar to latent word groups (or communities) in the word network, since words co-occurred frequently in the same sliding windows are closely connected in the semantic space and they could appear in the same document with high probabilities. Therefore, we could take latent word groups in our word network based model as the topics in LDA. At the same time, learning topics from word co-occurrence network, a special form of word-word space, has a theoretical guarantee for topic coherence according to the work of Arora et al. [2]. What’s more, rare topics may form compact latent word groups in word network, therefore a topic model based on the word network could effectively find word groups that correspond to rare topics. Based on the considerations above, we propose our word network topic model (WNTM). In order to keep the new model simple and general to employ at different scenarios, we take a similar approach as Keith et al. did in [15] to discover latent word groups in word network.
3.2 Word network topic model (WNTM)
The standard Gibbs sampling for LDA can be used to discover latent word groups in large word network. While in order to reserve the standard Gibbs sampling, we first have to represent the word co-occurrence network back to a pseudo-document set. We assume the word network as undirected and weighted. As illustrated in Fig. 1, each word in the network can be treated as a pseudo-document with content constituted by the list of its adjacent words.
Obviously since the word network is weighted, the adjacent words may occur multiple times in the text of the pseudo-document.
Although WNTM uses the same Gibbs Sampling with LDA, the rationalities underlying the generative process of them are different. LDA learns to generate a collection of documents by using topics and words under those topics. However, WNTM learns to generate each word’s adjacent word-list in the network by using latent word groups and words belonging to those groups. More specifically, WNTM learns the statistical relations between words, latent word groups and words’ adjacent word-lists by assuming that each word’s adjacent word-list is generated semantically by a particular probabilistic model. It first supposes that there is a fixed set of latent word groups in the word network, and each latent word group z is associated with a multinomial distribution over the vocabulary Φz, which is drawn from a Dirichlet prior Dir(β ). The generative process of the whole pseudo-document collection converted from the word network can be interpreted as follows:
1. For each latent word group z, draw Φz ∼ Dir(β ), a multinomial distribution over words for z 2. Draw Θi ∼ Dir(α), a latent word group distribution for the adjacent word-list Li of the word wi 3. For each word w j ∈ Li: (a) select a latent word group z j ∼Θi (b) select the adjacent word w j ∼Φz j
In WNTM, the Θ distributions represent the probability of latent word groups appearing in each word’s adjacent word-list and the Φ distributions stand for the probability of words belonging to each latent word group. Given the observed corpus, WNTM first converts it to a word network, then generate the pseudo-document set and finally the same Gibbs sampling implementation that developed for conventional LDA is employed to infer values of the latent variable in both Φ and Θ . Because each word’s adjacent word-list actually represents its global context information, so different from previous LDA-like approaches, WNTM models the distribution over latent word groups for each word instead of the distribution over topics for each document.
3.3 Inferring topics in a document
As discussed in the previous section, WNTM does not model the document generation process. Therefore, we cannot directly obtain topics in a document from the result of Gibbs sampling. Since WNTM models the generation process of each word’s adjacent word-list which stands for the word’s global contextual information, we can take topic proportions of word wi’s adjacent word-list Θi as topic proportions in wi. Given topic proportions for all words, topics of each document can be obtained accordingly. Specifically, to infer topics in a document, we assume that the expectation of the topic proportions of words generated by a document equals to the topic proportions of the document, i.e.,
P(z|d) = ∑ wi P(z|wi)P(wi|d), (1)
where P(z|wi) equals to Θi,z, which has been learned in WNTM. As to P(wi|d), we simply take the empirical distribution of words in the document as a estimation, i.e.,
P(wi|d) = nd(wi) Len(d) , (2)
where nd(wi) is the word frequency of wi in document d and Len(d) is the length of d. It is worthy noting the above strategy is straight-forward and easy to implement, which guarantees WNTM’s simplicity further.
To sum up, when texts are short and sparse, learning topics in word-by-document space will suffer from the severe sparsity problem, while learning topics in a word-word space has a theoretical guarantee for topic coherence, which has been proved in [2]. Meanwhile, as the distribution over documents for each topic is imbalanced, rare topics tend to be ignored by LDA-like models. However, we conjecture that words related to rare topics would still form a semantically compact latent group in the word co-occurrence network. So latent groups standing for rare topics could also be detected by WNTM. Therefore, the rich contextual information in word-word space facilitates WNTM to discover topics in word co-occurrence network other than directly reveal topics from document collection.
Motivation. Lexical semantic knowledge in the form of term taxonomies has been beneficial in a variety of NLP tasks, including inference, textual entailment, question answering and information extraction [3]. This widespread utility of taxonomies has led to multiple large-scale manual efforts towards taxonomy induction, such asWordNet [22] and Cyc [21]. However, such manually constructed taxonomies suffer from low coverage [15] and are unavailable for specific domains or languages. Therefore, in recent years, there has been substantial interest in extending existing taxonomies automatically or building new ones [4, 5, 19, 34, 38, 40].
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM’17 , November 6–10, 2017, Singapore, Singapore © 2017 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery. ACM ISBN 978-1-4503-4918-5/17/11. . . $15.00 https://doi.org/10.1145/3132847.3133041
Approaches towards automated taxonomy induction consist of two main stages:
(1) extraction of hypernymy relations (i.e., “is-a" relations between a term and its hypernym such as apple→fruit) (2) structured organization of terms into a taxonomy, i.e., a coherent tree-like hierarchy.
Extraction of hypernymy relations has been relatively wellstudied in previous works. Its approaches can be classified into two main categories: Distributional and Pattern-based approaches.
Distributional approaches use clustering to extract hypernymy relations from structured or unstructured text. Such approaches draw primarily on the distributional hypothesis [12], which states that semantically similar terms appear in similar contexts. The main advantage of distributional approaches is that they can discover relations not directly expressed in the text. In contrast, Pattern-based approaches utilize pre-defined rules or lexico-syntactic patterns to extract terms and hypernymy relations from text [13, 26]. Patterns are either chosen manually [13, 20] or learnt automatically via bootstrapping [35]. Pattern-based approaches usually result in higher accuracies. However, unlike the distributional approaches, which are fully unsupervised, they require a set of seed surface patterns to initiate the extraction process.
Early work on the second stage of taxonomy induction, namely the structured organization of terms into a taxonomy, focused on extending existing partial taxonomies such asWordNet by inserting missing terms at appropriate positions [34, 39, 40]. Another line of work focused on taxonomy induction fromWikipedia by exploiting the semi-structured nature of the Wikipedia category network [7, 10, 24, 30, 31, 36].
Subsequent approaches to taxonomy induction focused on building lexical taxonomies entirely from scratch, i.e., from a domain corpus or the Web [1, 2, 19, 25, 28, 38].
Automated taxonomy induction from scratch is preferred because it can be used over arbitrary domains, including highly specific or technical domains, such as Finance or Artificial Intelligence [25]. Such domains are usually under-represented in existing taxonomic resources. For example, WordNet is limited to the most frequent and the most important nouns, adjectives, verbs, and adverbs [11, 23]. Similarly,Wikipedia is limited to popular entities [18], and its utility is further diminished by slowed growth [37].
Past approaches to taxonomy induction from scratch either assume the availability of a clean input vocabulary [28] or employ a time-consuming manual cleaning step over a noisy input vocabulary [38]. For example, Figure 1 shows the pipeline of a typical
ar X
iv :1
70 4.
07 62
6v 4
[ cs
.A I]
1 4
Se p
20 17
taxonomy induction approach from a domain corpus [38]. An initial noisy vocabulary is automatically extracted from the domain corpus using a term extraction tool, such as TermExtractor [32], and is further cleaned manually to produce the final vocabulary. This requirement severely limits the applicability of such approaches in an automated setting because clean vocabularies are usually unavailable for specific domains.
To handle these limitations, we designed our approach to induce a taxonomy directly from a noisy input vocabulary. Consequently, it is the first work to fully automate the taxonomy induction process for arbitrary domains.
Contributions. In this paper, we present a novel, semi-supervised approach for building lexical taxonomies given an input vocabulary of (potentially noisy) seed terms. We leverage the existing work on hypernymy relations extraction and focus on the second stage, i.e. the organization of terms into a taxonomy. Our main contributions are as follows:
● We propose a novel probabilistic framework for extracting longer hypernym subsequences from hypernymy relations, as well as a novel minimum-cost flow based optimization framework for inducing a tree-like taxonomy from a noisy hypernym graph. ● We empirically show that our approach outperforms stateof-the-art taxonomy induction approaches across four different languages, while achieving >32% relative improvement in F1-measure over the Food domain. ● We demonstrate that our subsequence-based model is robust to the presence of noisy terms in the input vocabulary, and achieves a 65% relative improvement in precision over an edge-based model while maintaining similar coverage. To the best of our knowledge, this is the first approach towards taxonomy induction from a noisy input vocabulary.
The rest of the paper is organized as follows. In Section 2, we describe our taxonomy induction approach. In Section 3, we discuss our experiments and performance results. In Section 4, we discuss related work. We conclude in Section 5.
L‟équivalence entre les sujets et les évènements est détectée par la phase d‟inférence textuelle et d‟ancrage entre les expressions temporelles (2 o‟clock et the afternoon) est détecté par l‟application de la régle R5 de la base de règles d‟inférences qui stipule que si les différentes conditions se réunissent c‟est-à-dire :
 détecter une équivalence entre les deux évènements (e1, e2)
 l‟événement e1 est relié avec une relation TLINK e1t2 et l‟événement e1 est relié avec la même relation TLINK à une durée e2d.
 inclusion entre la date t1 et la durée d.
Nous aurons une inférence temporelle et textuelle entre les segments T et H.
Des deux résultats précédents le superviseur décide qu‟il y a une inférence textuelle entre les segments T et H.
Cette figure représente les différentes conditions nécessaires à une inférence textuelle.
T ′,H ′ ⊢R∗ (¬(∀r)ψ → ¬(∀s)¬¬ψ[s/r]) → ϕ definition of ∃ T ′,H ′ ⊢R∗ (¬(∀r)ψ → ¬(∀s)ψ[s/r]) → ϕ Equivalence (Lemma A.3) T ′,H ′ ⊢R∗ ϕ Renaming (Lemma A.3)
Therefore T,H ′ ⊢R∗ ϕ. Applying the induction hypothesis, T ⊢R ϕ.
Now, given a theory T in L(R), define:
T0 = T Ti+1 = (Ti)
∗ for i ∈ ω Tω = ⋃ i∈ω Ti
R0 = R Ri+1 = (Ri)
∗ for i ∈ ω Rω = ⋃ i∈ω Ri
Lemma A.10 (Henkin). Let R be at least countably infinite and T be a theory in L(R). Then Tω is a Henkin theory that is a conservative extension of T .
Proof. Take a closed ¬(∀r)ϕ ∈ L(Rω). Then there exists i ∈ ω such that ¬(∀r)ϕ ∈ L(Ri). But then there is a witness rϕ ∈ L(Ri+1) ⊆ L(Rω) such that
¬(∀r)ϕ → ¬ϕ[rϕ/r] ∈ Ti+1 ⊆ Tω .
So Tω is a Henkin theory. By induction on i ∈ ω, we prove that Ti is a conservative extension of T . Base case: T0 = T and the result is immediate. Induction step: Ti+1 is a conservative extension of Ti by Lemma A.9; that is, Ti+1 ∩ L(Ri) = Ti. By the induction hypothesis, Ti ∩ L(R) = T . But then, since L(R) ⊆ L(Rj) ⊆ L(Rk) if j < k, we have
Ti+1 ∩ L(R) = (Ti+1 ∩ L(Ri)) ∩ L(R) = Ti ∩ L(R) = T .
So each Ti is a conservative extension of T . But then
Tω ∩ L(R) = ( ⋃ i∈ω Ti) ∩ L(R) = ⋃ i∈ω(Ti ∩ L(R)) = T ,
which shows that Tω is a conservative extension of T .
By the usual Lindenbaum argument (using Zorn’s Lemma) [16, §3.1], for each R, any consistent set in L(R) may be extended to a maximal L(R)-consistent set. Hence for a consistent theory T in L(R), the theory Tω in L(Rω) is consistent and may be extended to a maximal L(Rω)-consistent set T ′ω. This set is a theory in L(Rω). Further, this theory is Henkin because Tω ⊆ T ′ ω, both theories are in the same language, Tω is Henkin by Lemma A.10, and any extension of a Henkin theory within the same language is still Henkin (because all Henkin axioms are already present).
To prove completeness of QRBB, we take θ such that QRBB 0 θ. We construct a structure Mc = (W, [·], N, V ) as in the proof of Theorem A.1 except that our set of worlds W is defined differently. First, let M0 be the set of all maximal L(R)-consistent sets; each such set is a theory in L(R). For each theory T ∈ M0, define Mω(T ) to be the set of all maximal L(Rω)-consistent extensions of Tω. As we have seen, each member of Mω(T ) is a maximal L(Rω)-consistent Henkin theory that is conservative over T (Lemma A.10). Define the set
M := ⋃
T∈M0 Mω(T )
whose members are maximal L(Rω)-consistent extensions of Tω for some T ∈ M0. It follows that {¬θ} can be extended to a T θ ∈ M0 and hence neither Mω(T
θ) nor M is empty. We define W := M × {1, 2} and write (Γ, i) ∈ W in the abbreviated form Γi. Since M is nonempty, W is nonempty. The remaining components of Mc are defined as in the proof of Theorem A.1 except that all language-specific aspects of definitions are extended to the larger language L(Rω).
The proof that Mc satisfies (pr), (d), and (rb) is as in the proof of Theorem A.1. So all that remains is to prove the Truth Lemma: for each formula ϕ ∈ L(Rω) and world Γi ∈ W , we have ϕ ∈ Γ iff Mc,Γi |= ϕ. The proof is by induction on the construction of formulas. The arguments for all but two cases are as in the proof of Theorem A.1. All that remains are the equality and quantifier inductive step cases.
• Inductive step: (s = r) ∈ Γ iff Mc,Γi |= s = r.
By (EP) and (EN), we have (s = r) ∈ Γ iff s = r. But the latter holds iff Mc,Γi |= s = r.
• Inductive step: (∀r)ϕ ∈ Γ iff Mc,Γi |= (∀r)ϕ.
If (∀r)ϕ ∈ Γ, then it follows by maximal L(Rω)-consistency and (UI) that ϕ[s/r] ∈ Γ for each s ∈ Rω that is free for r in ϕ. By the induction hypothesis, we have Mc,Γi |= ϕ[s/r] for each such s ∈ R. But this is what it means to have Mc,Γi |= (∀r)ϕ.
Conversely, if Mc,Γi |= (∀r)ϕ, then it follows that Mc,Γi |= ϕ[s/r] for all s ∈ Rω free for r in ϕ. By the induction hypothesis, we have ϕ[s/r] ∈ Γ for all such s. Since Γ is a Henkin theory, there is a Henkin constant rϕ ∈ Rω for ¬(∀r)ϕ. Hence ϕ[rϕ/r] ∈ Γ. But Γ contains the Henkin axiom
¬(∀r)ϕ → ¬ϕ[rϕ/r] ,
and so we have by L(Rω)-consistency that (∀r)ϕ ∈ Γ.
This completes the proof of the Truth Lemma. To complete the proof of completeness, we recall that we obtained T θ ∈ M0 as a maximal L(R)-consistent extension of {¬θ}. Hence there exists Γθ ∈ Mω(T θ) ⊆ M . But Γθ is a maximal L(Rω)-consistent extension of (T θ)ω, and (T
θ)ω is a conservative extension of T θ by Lemma A.10. Therefore, since θ /∈ T θ by consistency, we have θ /∈ Γθ. Applying the Truth Lemma, Mc,Γ θ 1 6|= θ. Completeness follows.
A.8 Conservativity of QRBB Over RBB
As a corollary of Theorems A.1 and A.5, we have the following.
Corollary A.11. QRBB is a conservative extension of RBB: for each ϕ ∈ F ,
QRBB ⊢ ϕ iff RBB ⊢ ϕ .
Proof. The right-to-left direction is obvious (QRBB contains all the axioms and rules of RBB). The left-to-right direction follows by QRBB soundness (Theorem A.5) and RBB completeness (Theorem A.1).
A.9 QRBBσ and QRBB + σ Soundness and Completeness
Recalling the semantics for QRBBσ and for QRBB + σ from §A.3, we prove the following theorem.
Theorem A.12 (QRBBσ and QRBB + σ Soundness and Completeness). Assume R contains the symbol σ.
• QRBBσ is sound: QRBBσ ⊢ ϕ implies |=σ ϕ for each ϕ ∈ F ∀.
• if R is at least countably infinite, then QRBBσ is sound and complete: for each ϕ ∈ F ∀,
QRBBσ ⊢ ϕ iff |=σ ϕ .
• analogous soundness and completeness results hold for QRBB+σ with respect to the satisfaction relation |=+σ .
Soundness is proved as in Theorem A.2. Completeness is proved as in Theorem A.5, except that provability is taken with respect to either QRBBσ or QRBB + σ and one must show (using an argument as in the completness portion of Theorem A.2) that Mc satisfies the relevant properties.
The primary Emacs-related ACL2 enhancement is a new utility introduced in Version 6.4, ACL2-Doc, for browsing documentation inside Emacs. This utility takes the place of Emacs Info, which is no longer supported because of the transition to XDOC discussed in Section 6. Emacs users will find this utility to be a nice alternative to using :doc at the terminal. It can be used to browse either the acl2+books combined manual or the ACL2 User’s Manual. This utility is loaded automatically into Emacs when loading the standard file emacs/emacs-acl2.el.
We first formalize the main research addressed in this paper, including the task-joint probability and generalization bounds for RMTL.
For approximate analysis of sample states, there exists a simple (and rather obvious) alternative to TorchLight’s causal graph based technology. One can use search to determine whether or not a given sample state s is a local minimum, and what its exit distance is. Since we cannot compute h+ effectively, such a search-based analysis is necessarily approximate. The straightforward method is to replace h+ with a relaxed-plan based approximation. Herein, we replace h+ with hFF, i.e., with FF’s heuristic function. Precisely, given a state s, we run a single iteration of FF’s Enforced Hill-Climbing, i.e., a breadth-first search for a state with better heuristic value. In this search, like FF does, we use helpful actions pruning to avoid huge search spaces. Unlike FF, to focus on the detection of states not on local minima, we allow only monotone paths (thus restricting the search space to states s′ where hFF(s′) = hFF(s)). We refer to this technique as search probing, SP in brief. We also experiment with a variant imposing a 1 second runtime cut-off on the search. We refer to this as limited search probing, SP1s in brief. SP and SP1s are run on the same set LS of states as TorchLight’s local analyses (II,III).
As it turns out, empirically – in the present benchmarks – SP and SP1s are very competitive with TorchLight’s analysis (III). Since that analysis is a main focus of our experiments, it is relevant to understand the commonalities and differences between these techniques.
As far as analysis quality guarantees are concerned, all 3 techniques – analysis (III), SP, SP1s – have similar properties: there are no guarantees whatsoever. Each may report
success although s is a local minimum (false positives), and each may fail although s is not a local minimum (false negatives). In all cases, false positives are due to the use of non-optimal relaxed plans (hFF instead of h+). False negatives are inherent in analysis (III) because this covers only certain special cases; they are inherent in SP1s due to the search limit. SP can have false negatives due to helpful actions pruning, however that could in principle be turned off; the more fundamental source of false negatives are the non-optimal relaxed plans. These are also responsible for a lack of connections across the techniques. The only implication is the trivial one that SP1s success on a state s implies SP success on s. In particular, if analysis (III) correctly identifies s to not be a local minimum, then this does not imply that SP will do so as well. The causal graph analysis may be less affected by irregularities in the hFF surface. This happens, for example, in the Transport domain of IPC 2008, resulting in higher success rates for analysis (III).
There are some obvious – but important – differences regarding runtime performance and the danger of false negatives. SP runtime is worst-case exponential in the size of the (grounded) input, whereas analysis (III) and SP1s runtime is low-order polynomial in that size. For SP, decreasing the number R of sample states merely reduces the chance of hitting a “bad” state (a sample state on a large flat region), whereas analysis (III) and SP1s scale linearly in R. On the other hand, both analysis (III) and SP1s buy their efficiency with incompleteness, i.e., increased danger of false negatives. Analysis (III) simply recognizes only special cases. SP1s effectively bounds the lookahead depth, i.e., the search depth in which exit states can be detected.
As indicated, SP and SP1s turn out to be competitive in the benchmarks. Large search spaces are rare for SP. The success rates of SP and SP1s are similar, and as far as predictive capability is concerned are similarly informative as those of analysis (III). Thus goodquality success rates can be obtained with much simpler techniques than TorchLight.13 This notwithstanding, (a) TorchLight has other functions – the guaranteed analyses (I,II) as well as diagnosis – that cannot be simulated, and (b) results in benchmarks only ever pertain to these examples. TorchLight’s analysis (III) offers unlimited lookahead depth at low-order polynomial cost. This does not appear to matter much in the present benchmarks, but there are natural cases where it does matter. We get back to this below.
As discussed in the previous section, small scale fading is the result of interference and has an effect on the length scale of the wavelength. It leads to fluctuations with periodic character with a periodicity of the wavelength of the wireless signal (which is dependent on the used channel, i.e., the base frequency). This effect is deterministic and can lead to local minima in the signal strength, which would violate some convergence conditions (see Sec. 4.2).
An example measurement is depicted in Fig. 1. The deterministic fluctuations on the scale of the wavelength (12.5 cm for 2.4 GHz in this case) can clearly be seen. These effects can be
stronger than the noise sources and have to be dealt with in order to ensure convergence of the presented algorithms.
For a flying robot, this is typically not a problem because it cannot hold position with a precision of this magnitude. This means that because of the stochastic movement (induced by aerodynamics and external factors like wind) these deterministic fluctuations are turned into additional noise. For ground based robots, this effect has to be mitigated by for example taking several samples and averaging over an area larger than the wavelength. This will also be demonstrated in our experiments in Sec. 5.
The proposed HRNN based CLMs are evaluated with two text datasets: the Wall Street Journal (WSJ) corpus (Paul & Baker, 1992) and One Billion Word Benchmark (Chelba et al., 2013). Also, we present an end-to-end speech recognition example, where HLSTM CLMs are employed for prefix tree-based beam search decoding and greatly improves recognition accuracies.
The RNNs are trained with truncated backpropagation through time (BPTT) (Werbos, 1990; Williams & Peng, 1990). Also, ADADELTA (Zeiler, 2012) and Nesterov momentum (Nesterov, 1983) is applied for weight update. No regularization method, such as dropout (Hinton et al., 2012), is employed. The training is accelerated using GPUs by training multiple sequences in parallel (Hwang & Sung, 2015).
One of the difficulties in training DSFs is obtaining a sufficient amount of training data. It would be useful therefore to have an strategy to easily and cheaply obtain as much training data as desired. In the spirit of the empirical success of DNNs, this section suggests one strategy for doing this.
The goal is to learn a map from a vector x ∈ Rd to a b-bit vector via a function hθ : Rd → {0, 1}b, anywhere hθ is parameterized by θ. The reason for doing this is to take data objects (e.g., images, documents, music files, etc.) that are represented in the input space Rd and map them to binary space {−1, 1}b where b < d and, moreover, since the space is binary, operations such nearest neighbor search are faster. There are existing approaches that can learn this mapping automatically, sometimes using neural networks (e.g., [56]). Often, hθ : Rd → {−1, 1}b rather than hθ : Rd → {0, 1}b, but this should not be of any consequence.
This section describes a strategy for learning hash functions that utilizes DSFs, the Lovász extension, and the submodular Hamming metric [50]. Let f : 2V → R be a submodular function and let f̆ be its Lovász extension. Also, let df (A,B) = f(A4B) be the submodular Hamming metric between A and B parameterized by submodular function f . We are given a large (and possibly unlabeled) data set D = {xi}i∈D and a corresponding distance function between data pairs (d(xi, xj) is the distance between item xi ∈ Rd and xj ∈ Rd). The goal is to produce a mapping hθ : Rd → {0, 1}b so that distances in the ambient space d(xi, xj) are preserved in the binary space. One approach adjusts hθ to ensure that d(xi, xj) =∑b `=1 1hθ(xi)(`)6=hθ(xj)(`). That is, we adjust hθ(xi) so that the Hamming distance preserves the distances in the ambient space. In general, this problem is made more difficult by the rigidity of the Hamming distance. In order to relax this constraint, we can use a submodular Hamming metric parameterized by a DSF fw (which itself is parameterized by w). Hence, the hashing problem can be seen as finding θ and w so that the following is true as much as possible.
d(xi, xj) = dfw(hθ(xi), hθ(xj)) (114)
The function hθ maps to binary vectors, and dfw is a function on two sets. This makes it difficult to pass derivatives through these functions in a back-propagation style learning algorithm. To address this issue, we can further relax this problem in the following way:
• Given A,B ⊆ V , the Hamming distance is |A4B| and we can represent this as (1A ⊗ (1V − 1B) + 1B ⊗ (1V − 1A))(V ) where ⊗ : Rn × Rn → Rn is the vector element multiplication operator (i.e.,
[x⊗ y](j) = x(j)y(j)). In other words, we define a vector zA4B ∈ {0, 1}V with
zA4B = 1A ⊗ (1V − 1B) + 1B ⊗ (1V − 1A) (115) = 1A + 1B − 21A ⊗ 1B (116)
and |A4B| = zA4B(V ) = ∑ i∈V zA4B(i). Hence, the submodular hamming metric is f(A4B) = f̆(zA4B), which holds since the Lovász extension is tight at the vertices of the hypercube.
• For two arbitrary vectors z1, z2 ∈ [0, 1]V , we can define a relaxed form of metric as follows: d(z1, z2) = f̆(z1 + z2 − 2z1 ⊗ z2), and for a DSF, this can be expressed as df̆w(z1, z2) = f̆w(z1 + z2 − 2z1 ⊗ z2).
• Let us suppose that h̃θ : Rd → [0, 1]b is a mapping from real vectors to vectors in the hypercube (e.g., h̃θ might be expressed with a deep model with a final layer of b sigmoid units at the output to ensure that each output is between zero and one). Then we can construct a distortion between xi and xj via
dw,θ(xi, xj) , df̆w(h̃θ(xi), h̃θ(xj)) = f̆w(h̃θ(xi) + h̃θ(xj)− 2h̃θ(xi)⊗ h̃θ(xj)) (117)
Hence, dw,θ is a parametric family of distortion functions that uses two maps, one via the DNN h̃θ and another via the DSF fw using the Lovasz extension f̆w.
• Assuming the original unlabeled data set D is large, and the distance function in the ambient space is accurate, it may be possible to learn both w and θ by forming an objective function to minimize:
J(w, θ) = ∑ i,j∈D ‖d(xi, xj)− dw,θ(xi, xj)‖. (118)
Learning (minw,θ J(w, θ)) can utilize stochastic gradient steps and the entire arsenal of DNN training methods.
The approach learns both the mapping function h̃θ and the submodular function fw simultaneously in a way that preserves the original distances. It may therefore be that h̃θ can be used as a feature transformation (i.e. a way to map data objects x into feature space via h̃θ), and at the same time we obtain a submodular function fw over those features that, perhaps, can useful for summarization, all without needing labeled training data as in Section 7.1.
The SP theory of intelligence, which has been under development since about 1987,1 aims to simplify and integrate concepts across artificial intelligence, mainstream computing and human perception and cognition, with information compression as a unifying theme.
The name ‘SP’ is short for Simplicity and Power, because compression of any given body of information, I, may be seen as a process of reducing informational ‘redundancy’ in I and thus increasing its ‘simplicity’, whilst retaining as much as possible of its non-redundant expressive ‘power’. Likewise with Occam’s Razor (Section 2.3, below).
Aspects of the theory, as it has developed, have been described in several peer-reviewed articles.2 The most comprehensive description of the theory as it stands now, with many examples, is in Wolff (2006a).
But this book, with more than 450 pages, is too long to serve as an introduction to the theory. This article aims to meet that need, with a fairly full description of the theory and a selection of examples.3 For the sake of brevity, the book will be referred to as ‘BK’.
The next section describes the origins and motivation for the SP theory, Section 3 introduces the theory, Sections 4 and 5 fill in a lot of the details, while the following sections describe aspects of the theory and what it can do.
1Apart from the period between early 2006 and late 2012 when I was working on other things.
2See www.cognitionresearch.org/sp.htm#PUBS. 3Some of the text and figures in this article come from the book, with permission.
Details of other permissions are given at appropriate points in the article.
Let us consider the introduction of × in HT as the following derived operator:
F ×G def = F ∨ (¬F ∧G) (13)
Although in classical logic, (13) ≡c F ∨ G, this equivalence does not hold in HT, that is (13) 6≡s F ∨ G. To see why, adding G, the two disjunctions have different equilibrium models: {F∨G,G} has one equilibrium model {G}, whereas {F ×G,G} also has a second equilibrium model {F,G}. We discuss now some basic properties of × operator.
Proposition 2 (Negation). The negation of an ordered and a regular disjunction are strongly equivalent:
¬(F ×G) ≡s ¬F ∧ ¬G ≡s ¬(F ∨G)
Proof. Applying De Morgan laws, ¬(F ∨¬F ∧G) amounts to ¬F ∧ (¬¬F ∨¬G) which, by distributivity, is equivalent to ¬F ∧ ¬¬F ∨ ¬F ∧ ¬G, but the first disjunct can be removed, since ¬F ∧ ¬¬F is inconsistent in HT by (5). ✷
Proposition 3 (Truth constants). These are some strongly equivalent simplifications:
(F × F ) ≡s F (14)
(⊥× F ) ≡s F (15)
(F ×⊥) ≡s F (16)
(⊤× F ) ≡s ⊤ (17)
(F ×⊤) ≡s (F ∨ ¬F ) (18)
✷
Note that the main difference with respect to ordinary disjunction when dealing with truth constants is (18). Equivalences (15) and (16) show that ⊥ acts as a neutral element for ordered disjunction. This means that we can safely consider an empty ordered disjunction as ⊥ (as happens with ∨ too).
Distributivity with respect to conjunction is satisfied in the following cases:
F ∧ (G×H) ≡s (F ∧G)× (F ∧H)
(F ×G) ∧H ≡s (F ∧H)× (G ∧H)
F × (G ∧H) ≡s (F ×G) ∧ (F ×H)
but the following pair of formulas
(F ∧G)×H (F ×H) ∧ (G×H)
are not strongly equivalent. In fact, they have different answer sets. The first rule has equilibrium models {F,G} and {H} whereas the second has two additional ones {F,H} and {G,H}. Distributivity between disjunctions only holds in the following case:
F × (G ∨H) ≡s (F ×G) ∨ (F ×H)
but the following list shows pairs of formulas that are not strongly equivalent:
F ∨ (G×H) (F ∨G)× (F ∨H) (19)
(F ×G) ∨H (F ∨H)× (G ∨H) (20)
(F ∨G)×H (F ×H) ∨ (G×H) (21)
Take line (19), for instance. Adding the atom H , the left rule yields two equilibrium models {G,H} and {H}, whereas the second rule allows a third equilibrium model {F,H}. Intuitively, adding H to F ∨ (G ×H) will always make the second disjunct to be true (both when G holds and when it does not). So, there is no need to make F true. By a similar reason, in the case of line (20), after adding atom G, the first rule yields two equilibrium models, {F,G} and {G} (there is no need to make H true), while the second rule yields a third solution {G,H}. Finally, for line (21), if we add the atom H to the first expression, we get three equilibrium models, {F,H}, {G,H}, and {H}, whereas for the second expression, the addition of H only yields the equilibrium model {H} itself.
Proposition 4 (Associativity). The × operator is associative, that is:
F × (G×H) ≡s (F ×G)×H
Proof.
(De)compression units. While we expect that the existing GPU compression units can be leveraged for cDMA to minimize design overheads, we assume that the cDMA (de)compression hardware supplements existing hardware for a conservative area estimate. Nonetheless, our cDMA unit can allow existing DRAM compression schemes optimized to minimize DRAM bandwidth to also take place. We use the FreePDK [44] 45 nm process design kit and scaled the resulting area using a conservative cell size reduction of 0.46× from 45 nm to 28 nm. Assuming a 50% cell area utilization due to the design being dominated by wires and MUXes, the six (de)compression units are estimated to incur a 0.31mm2 area overhead.
Buffer sizing. The DMA engine must also maintain a buffer large enough to hold the bandwidth-delay product of the memory sourcing the data to prevent bubbles in the output stream. As we detail in Section VII-B, DNN computations are highly compute-bound so the required average memory bandwidth is measured at less than 100 GB/sec, leaving
more than (336 − 100)=236 GB/sec for cDMA to fetch data without affecting performance. Our experiments show that provisioning 200 GB/sec of bandwidth for cDMA reaps most of the benefits of sparse compression. As a result, based on a 350 ns latency from the time the DMA engine requests data from GPU memory to the time it arrives at the DMA engine [45] and the 200GB/sec compression read bandwidth, the DMA engine needs a 70KB (200GB/sec×350 ns) buffer, shown as block “B” in Figure 9. It may seem counterintuitive that cDMA would need this large a buffer, since it is receiving only compressed requests at an overall rate of 16 GB/sec from the crossbar. The reason why the buffer needs to be overprovisioned is because the cDMA engine does not know a priori which responses will be compressed or not. Since it must launch sufficient requests to keep the PCIe bus busy even with highly-compressed data, a large number of requests will be in-flight. In the event that these requests are not compressed, the buffer is required to hold the large amount of data returned until it can be streamed out over the PCIe interface. This buffer size is not a significant source of area (approximately 0.21mm2 in 28 nm according to CACTI 5.3 [46]). Compared to the 600mm2 of a NVIDIA Titan X chip used for the evaluations in this paper, the added overheads of (de)compression units and DMA buffers are negligible.
We used Theano [24] with Lasagne [1] for rapid prototyping and testing of different parameters.1 To evaluate different hyperparameters, we trained multiple network configurations with various context lengths (500ms to 2000ms), context strides (1 to 4 frames), network depths (one to four hidden layers), layer sizes (15 to 125 neurons), activation functions (tanh and relu), optimization methods (SGD, Adadelta and Adam [9]), weight initialization methods (constant zero and Glorot [2]), and layer types (feed forward and LSTM).
The LSTM networks we tested were prone to overfitting quickly. We tried two methods of regularization to overcome this. The first was Dropout training, where we randomly dropped a specific portion of neuron outputs in each layer for each training batch [21]. We evaluated dropout layer combinations from 0 to 50% while increasing layer sizes proportionately, but this did not improve the results. The second was adding L2-Regularization with a constant factor of 0.0001. This greatly reduced overfitting and slightly improved the results.
Data visualization methods have been part of statisti s and data analysis resear h for many years. This resear h on entrated primarily on plotting one or more independent variables against a dependent variable in support of exploratory data analysis (Tukey, 1977; Lee, Ong, & Quek, 1995; Unwin, 2000).
The visualization of analysis results has, however, gained only re ently some attention with the proliferation of data mining (Card, Ma kinlay, & Shneidermann, 1999; Fayyad, Grinstein, & Wierse, 2002; Keim & Kriegel, 1996; Simo , Noirhomme-Fraiture, & Boehlen, 2001). The visualization of analysis results primarily serves four purposes: better illustrate the pattern to the end user, enable the omparison of patterns, in rease pattern a eptan e, and enable pattern editing and support for \what-if questions". The re ent interest in the visualization of analysis results was spawned by the often overwhelming number and
omplexity of data mining results.
Readers interested in omparing the visualization method proposed in this paper with other subgroups visualization methods an nd the visualization of subgroups A1{C1 in the joint work by Gamberger, Lavra , and Wetts here k (2002).

ar X
iv :1
30 7.
30 40
v2 [
cs .A
I] 3
1 M
ar 2
01 4
Between Sense and Sensibility Declarative narrativisation of mental models as a basis and benchmark for visuo-spatial cognition and computation focussed collaborative cognitive systems
In order to better understand users through their past actions, and assuming only geospatial data is available at the point of data collection, this section describes the process of trajectory augmentation that combines raw trajectories with land usage data. A trajectory is a temporally ordered sequence of data points that locate an individual or entity:
T = (p1, p2, p3, ..., pn)
where pi = {ti, li, ai} is an individual trajectory point, consisting of time (ti), location (li, e.g. a < lat, lng > pair) and accuracy (ai, typically measured in metres).
In addition to such trajectories, land usage data can also be used for identifying locations and entities that are meaningful to the user. Land usage data is assumed to be sets of entities with associated information. An entity, in this case, directly maps to a single real-world object, feature, or area, such as an individual postbox, field, or building. It can also refer to a collection of such entities that form a larger designation, such as a university campus or residential housing area. Each of these elements is expected to be associated with a set of geographical coordinate pairs that represent its shape and location, in addition to a set of tags in the form of ‘key:value’ pairs that describe properties of the element, including its type and usage (e.g. a house may be tagged as ‘building:residential’).
rable corpora are texts that, while not parallel in the strict sense, are somewhat related and convey overlapping information. For example, multilingual news feeds produced by news agencies and different versions of the same articles in Wikipedia could be treated as comparable corpora. There has been a considerable amount of work on bilingual corpora to extract parallel sentences. These include treating the web as a source of semi-parallel sentences (Fung, Prochasson & Shi 2010; Resnik & Smith 2003) or using other sources of comparable corpora such as extracted documents from different news agencies. (Resnik & Smith 2003) uses their structural filtering system STRAND which filters candidate parallel pairs by determining a set of pair-specific structural values from the underlying HTML page. They report a precision of 98% and a recall of 61% on their developed English-Chinese parallel corpus. (Zhao&Vogel 2002) proposes amaximum likelihood criterionwhich combines sentence length model and a statistical translation lexicon model extracted from an already existing aligned parallel corpus. An iterative process was applied to retrain the translation lexicon model with the extracted data. Their selected languages were Chinese and English. (Utiyama & Isahara 2003) use cross language information retrieval techniques and dynamic programming to extract parallel sentences from an English-Japanese news corpus. The authors first try to find similar article pairs, and then, they treat these pairs as parallel texts, align their sentences on a sentence pair similarity score. Subsequently, they use dynamic programming to find the minimumcost alignment over each document pair. They use the BM25 similarity measure for their algorithm. (Yang & Li 2003) proposed a parallel sentences extraction schema to identify Chinese and English title pairs based on dynamic programming. (Fung & Cheung 2004) present a method to extract parallel sentences from very non-parallel corpora by exploiting bootstrapping on top of IBM Model 4 (Brown et al. 1993). They claim that their “find-one-get-more” strategy principle allows them to add more parallel sentences from dissimilar documents, to the baseline set. Primary steps of their method is alike the former approaches while they uses similarity metric same other approaches. Then they used an iterative bootstrapping framework based on the principle of “find-one-get-more”, which claims that documents found to contain one pair of parallel sentences must contain others even if the documents are judged to be of low similarity. They rematch documents with using extracted sentence pairs, and refine the mining process iteratively until convergence. (Munteanu & Marcu 2005), first used a dictionary to translate some of the
6
1 Using English as Pivot to Extract Persian-Italian Parallel Sentences from Non-Parallel Corpora
words of the source sentences, and then used these translations to query a database for findingmatching translation candidates and extracting final parallel sentences. In other work, (Munteanu & Marcu 2006) train a maximum entropy classifier to extract parallel corpus in Arabic, English and French languages. They show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus. (Kumano, Tanaka & Tokunaga 2007), proposed a method of extracting phrasal alignments from comparable corpora by using an extended phrase-based joint probability model for statistical machine translation (SMT). They have indicated that their method does not require preexisting dictionaries or splitting documents into sentences in advance. (Abdul-Rauf & Schwenk 2009) present another technique similar to (Munteanu & Marcu 2005)’s and use a statistical machine translation system instead of the bilingual dictionary. In their approach they used an IR system to find the best candidates from translated sentences. Moreover, they used well-known evaluation metrics WER (Word Error rate), TER (Translation Error Rate) and TERp (Translation Error Rate plus) to decide the degree of parallelism between candidate sentences. (Diep, Besacier & Castelli 2010) presents an unsupervisedmethod and use statistical translation system to detect parallel French-Vietnamese parallel sentences with mining comparable corpora. An iterative process was implemented to increase the number of extracted parallel sentence pairs which improved the overall quality of the translation. Some other approaches are focused on parallel corpus production to improve SMT systems (Fung, Prochasson & Shi 2010; Smith, Quirk & Toutanova 2010; Hewavitharana & Vogel 2011; Ion 2012). (Fung, Prochasson & Shi 2010) use the web as a comparable resource to extract potential parallel sentences with using web crawler. They propose a sentence extraction architecture inspired by various pieces of earlier works. They’ve reported interesting results using French and English Wikipedia articles as their comparable resource. (Smith, Quirk & Toutanova 2010) propose a parallel sentences extraction method by modeling the document level alignment. Their work is motivated by the observation that parallel sentence pairs are often found in close proximity. They used the same documents in different languages in Wikipedia as their document level aligned corpora. In addition, they used some features which created by using the additional annotation given by Wikipedia, and features made by an automatically induced lexicon model. (Hewavitharana & Vogel 2011) used and evaluated three phrase alignment approaches to detect parallel phrase pairs embedded in comparable sentences: First approach relies on the Viterbi path, second one does not
7
Deep neural networks have achieved state-of-the-art performance on computer vision problems, such as classification [22, 36, 37, 12, 13], detection [7, 33, 1], etc. However, their complexity is an impediment to widespread deployment in many applications of real world interest, where either memory or computational resource is limited. This is due to two main issues: large model sizes (50MB for GoogLeNet [37], 200M for ResNet-101 [13], 250MB for AlexNet [22], or 500M for VGG-Net [36]) and large computational cost, typically requiring GPU-based implementations. This generated interest in compressed models with smaller memory footprints and computation.
Several works have addressed the reduction of model
size, through the use of quantization [3, 28, 26], low-rank matrix factorization [19, 6], pruning [11, 10], architecture design [27, 17], etc. Recently, it has been shown that weight compression by quantization can achieve very large savings in memory, reducing each weight to as little as 1 bit, with a marginal cost in classification accuracy [3, 28]. However, it is less effective along the computational dimension, because the core network operation, implemented by each of its units, is the dot-product between a weight and an activation vector. On the other hand, complementing binary or quantized weights with quantized activations allows the replacement of expensive dot-products by logical and bitcounting operations. Hence, substantial speed ups are possible if, in addition to the weights, the inputs of each unit are binarized or quantized to low-bit.
It appears, however, that the quantization of activations is more difficult than that of weights. For example, [4, 32] have shown that, while it is possible to binarize weights with a marginal cost in model accuracy, additional quantization of activations incurs nontrivial losses for large-scale classification tasks, such as object recognition on ImageNet [35]. The difficulty is that binarization or quantization of activations requires their processing with non-differentiable operators. This creates problems for the gradient descent procedure, the backpropagation algorithm, commonly used to learn deep networks. This algorithm iterates between a feedforward step that computes network outputs and a backpropagation step that computes the gradients required for learning. The difficulty is that binarization or quantization operators have step-wise responses that produce very weak gradient signals during backpropagation, compromising learning efficiency. So far, the problem has been addressed by using continuous approximations of the operator used in the feedforward step to implement the backpropagation step. This, however, creates a mismatch between the model that implements the forward computations and the derivatives used to learn it. In result, the model learned by the backpropagation procedure tends to be sub-optimal.
In this work, we view the quantization operator, used in the feedforward step, and the continuous approximation, used in the backpropagation step, as two functions
1
that approximate the activation function of each network unit. We refer to these as the forward and backward approximation of the activation function. We start by considering the binary ±1 quantizer, used in [4, 32], for which these two functions can be seen as a discrete and a continuous approximation of a non-linear activation function, the hyperbolic tangent, frequently used in classical neural networks. This activation is, however, not commonly used in recent deep learning literature, where the ReLU nonlinearity [30, 39, 12] has achieved much greater preponderance. This is exactly because it produces much stronger gradient magnitudes. While the hyperbolic tangent or sigmoid nonlinearities are squashing non-linearities and mostly flat, the ReLU is an half-wave rectifier, of linear response to positive inputs. Hence, while the derivatives of the hyperbolic tangent are close to zero almost everywhere, the ReLU has unit derivative along the entire positive range of the axis.
To improve the learning efficiency of quantized networks, we consider the design of forward and backward approximation functions for the ReLU. To discretize its linear component, we propose to use an optimal quantizer. By exploiting the statistics of network activations and batch normalization operations that are commonly used in the literature, we show that this can be done with an half-wave Gaussian quantizer (HWGQ) that requires no learning and is very efficient to compute. While some recent works have attempted similar ideas [4, 32], their design of a quantizer is not sufficient to guarantee good deep learning performance. We address this problem by complementing this design with a study of suitable backward approximation functions that account for the mismatch between the forward model and the back propagated derivatives. This study suggests operations such as linearization, gradient clipping or gradient suppression for the implementation of the backward approximation. We show that a combination of the forward HWGQ with these backward operations produces very efficient low-precision networks, denoted as HWGQ-Net, with much closer performance to continuous models, such as AlexNet [22], ResNet [13], GoogLeNet [37] and VGG-Net [36], than other available low-precision networks in the literature. To the best of our knowledge, this is the first time that a single low-precision algorithm could achieve successes for so many popular networks. According to [32], theoretically HWGQ-Net (1-bit weights and 2-bit activations) has ∼32× memory and ∼32× convolutional computation savings. These suggest that the HWGQ-Net can be very useful for the deployment of state-of-the-art neural networks in real world applications.
A key difficulty in applying machine learning techniques to computer algebra is the lack of suitable datasets. CAD problem sets such as [56] do not have anywhere near a sufficient number of problems to perform the experiment. In our previous study on choosing the variable ordering for CAD [41] we used the nlsat-dataset [58], which although developed for non-linear arithmetic SAT-solvers, contained many suitable problems.
For the present experiment we need problems that are expressed with a conjunction of at least two equalities in order to build a non-trivial GB. From the nlsat dataset 493 three-variable problems and 403 four-variable problems fit this criteria, which should have been a sufficient number. GB preconditioning was applied to each problem and cell counts from computing the CAD with the original polynomials and their replacement with the GB were computed and compared. For each one of these problems the GB preconditioning was beneficial or made no difference; surprising as the experiments on much smaller datasets [15], [57] had shown much greater volatility. This points to an undetected uniformity within the current nlsat dataset. It would need to be widened if it is to be used more extensively for computer algebra research.
1http://www.regularchains.org
Since existing datasets were not suitable for the present experiment, we had no choice but to generate our own problems. The generation process aimed for an unbiased data set which would be computationally feasible for computing multiple CADs, and have some comparable structure (number of terms and polynomials) to existing CAD problems.
In total, 1200 problems were generated using the random polynomial generator randpoly in MAPLE-17. Each problem has two sets of three polynomials; the first to represent conjoined equalities and the second for the other polynomial constraints (respectively E and F from the description in Section I-B). The number of variables was at most 3, labelled x, y, z and under ordering x ≺ y ≺ z; the number of terms per polynomial at most 2; the coefficients were restricted to integers in [−20, 20]; and the total degree was varied between 2, 3 and 4 (with 400 problems generated for each).
A time limit of 300 CPU seconds was set for each CAD computation (all GB computations completed quickly) from which 1062 problems finished to constitute the final dataset. Of these, 75% benefited from GB preconditioning. So our randomly generated dataset matched the previously found results of [15], [57] with most problems benefiting but not all and so is suitable for the purpose of this experiment.
To simplify the notations, we will consider discrete state spaces. Our methods generalizes to general state spaces straightforwardly. Let V = {x1, . . . , xn} be a set of variables where each variable takes values from some set S. Let π(·) be a distribution defined on SV .
Let σ ∈ SV be a configuration, namely σ : V → S. Let σx,s be the configuration that agrees with σ except at x, where σx,s(v) = s for s ∈ S. In other words, for any y ∈ V ,
σx,s(y) := { σ(y) if x 6= y; d if x = y.
The (lazy) Gibbs sampler is defined in Algorithm 1. Let n = |V | be the total number of variables. The transition kernel PRU (where RU stands for “random updates”) of the sampler above is defined as:
PRU (σ, τ) =  1 2n · π(σv,s)∑ s∈S π(σ v,s) if τ = σ v,s for some x ∈ V and s ∈ S;
1/2 if τ = σ; 0 otherwise,
(6)
where σ, τ are two configurations. It is not hard to see, for example, by checking the detailed balance condition (1), that π(·) is the stationary distribution of PRU . Note that this Markov chain is lazy, i.e., it remains at its current state with probability at least 1/2. This self-loop move has in fact probability higher than 1/2 because in the first case of (6) the probability of remaining in σ is positive. Lazy chains are usually studied in the literature since the self-loop eliminates (potential) periodicity. Moreover, all eigenvalues of the transition matrix PRU are non-negative.
Definition 4. A distribution π(·) is bipartite, if all variables can be partitioned into two sets V1 = {x1, . . . , xn1} and V2 = {y1, . . . , yn2} where n = n1 + n2, such that conditioned on the values of V2, all variables in V1 are mutually independent, and vice versa.
In the following we consider a particular systematic scan sampler for bipartite distributions. For a configuration σ, let σi := σ|Vi be its projection on Vi where i = 1, 2. The alternating-scan sampler is given in Algorithm 2.
In other words, the alternating-scan sampler sequentially resamples all variables in V1, and then resamples all variables in V2. Note that since we are considering a bipartite distribution, in order to resample xi ∈ V1, we only need to condition on σ2. In other words, for any i ∈ [n1], the
Algorithm 2 Alternating-scan sampler Input: Starting configuration σ = σ0 for t = 1, . . . , Tmix do for i = 1, . . . , n1 do Set σ ← σxi,s with probability π(σ
xi,s)∑ s∈S π(σ
xi,s) . end for for j = 1, . . . , n2 do Set σ ← σyj ,s with probability π(σ
yj,s)∑ s∈S π(σ yj,s) .
end for end for return σ
distribution {
π(σxi,s)∑ s∈S π(σ xi,s) } s∈S
that we draws from depends only on σ2. Similarly, resampling yj ∈ V2 only depends on σ1. We will denote the transition kernel of the alternating-scan sampler as PAS , where AS stands for “alternating scan”.
An unusual feature of systematic-scan samplers (including the alternating-scan sampler) is that they are not reversible. Namely the detailed balance condition (1) does not in general hold. This is because updating variables x and y in order is in general different from updating y and x in order. This imposes a technical difficulty as most of the theoretical tools of analyzing these chains are not suitable for irreversible chains, such as the Dirichlet form [5] or conductance bounds [14].
On the other hand, the scan sampler is aperiodic. Any potential state σ of the chain must be in the state space Ω. Therefore π(σ) > 0 and the probability of staying in σ is strictly positive. Moreover, if the Gibbs sampler is irreducible (namely the state space Ω is connected via single variable flips), then so is the scan sampler. This is because any single variable update can be simulated in the scan sampler, with small but strictly positive probability. Hence if the Gibbs sampler is ergodic, then so is the scan sampler.
We restate our main theorem here in formal terms.
Theorem 1. For any bipartite distribution π, if PRU is ergodic, then so is PAS. Moreover,
Trel(PAS) ≤ Trel(PRU ).
Due to the space limit, we provide a proof sketch here.
Proof sketch. The first statement is straightforawd. For the second, let Sπ be the projection matrix of the stationary distribution, namely
Sπ(σ, τ) = π(τ).
If P is reversible, then we can rewrite the spectral gap in terms of an operator norm, namely,
λ(P ) = 1− ‖P − Sπ‖π ,(7)
where ‖·‖π is the operator norm with respect to the distribution π. The transition matrix of updating a particular variable x is the following
Tx(σ, τ) =
{ π(σx,s)∑ s∈S π(σ x,s) if τ = σ x,s for some s ∈ S;
0 otherwise.
Moreover, let I be the identity matrix that I(σ, τ) = 1(σ, τ). Then we have that
PRU = I
2 +
1
2n ∑ x∈V Tx; PAS = n1∏ i=1 Txi n2∏ j=1 Tyj .
We consider an artificial but equivalent variant of PAS , where after updating all variables in V1, we do a random update according to PRU , and then proceed to update all variables in V2. This
is equivalent to PAS since the extra random update is either redundant with the updates in V1 or with those in V2. To put it formally,
PAS = n1∏ i=1 Txi · PRU · n2∏ j=1 Tyj .
Using the equation above, we can show that
‖PAS − Sπ‖π ≤ ‖PRU − Sπ‖π .
However, this is not enough as (7) only applies to reversible Markov chains. Instead, we estabilish a similar inequality for the multiplicative reversibilization R(P ) and conclude using (7).
Remark. It is easy to check that the proof above also works if we consider the non-lazy version of PRU . To do so, we just replace I2 + 1 2n ∑ x∈V Tx with 1 n ∑ x∈V Tx and the rest of the proof goes through without changes.
Remark. The proof above can also handle the case of general state spaces, such as Gaussian variables. However, for general state spaces, in order to apply Theorem 1 on mixing times, we need to replace Theorem 2 and Theorem 3 with their continuous counterparts. See for example [16].
Using Theorem 2 and Theorem 3, we translate Theorem 1 in terms of the mixing time.
Corollary 5. For a Markov random field defined on a bipartite graph, let PRU and PAS be the transition kernels of the random-update Gibbs sampler and the alternating-scan sampler, respectively. Then,
Tmix(PAS) ≤ log ( 4e2
πmin
) (Tmix(PRU ) + 1) ,
where πmin = minσ∈Ω π(σ).
Since n variables are updated in each epoch of PAS , one might hope to strengthen Theorem 1 so that nTrel(PAS) is also no larger than Trel(PRU ). Unfortunately, this is not the case and we give an example (similar to the “two islands” example in [11]) where Tmix(PAS) Tmix(PRU ) and Trel(PAS) Trel(PRU ). This example implies that Theorem 1 is asymptotically tight. However, it is still possible that Corollary 5 is loose by a factor of log π−1min.
Example 6. Let G = (L ∪R,E) be a complete bipartite graph Kn,n and we want to sample an uniform independent set in G. In other words, each vertex is a Boolean variable and a valid configuration is an independent set I ⊆ L ∪ R. The state space is Ω = {I | I ⊆ L or I ⊆ R} and the measure π is uniform on Ω. Under single-site updates, the state space Ω is composed of two independent copies of the Boolean hypercube {0, 1}n with the two origins identified. The random-update Gibbs sampler has mixing time O(2n) because the hitting time of the Boolean hypercube is O(2n) and the mixing time is upper bounded by the hitting time multiplied by a constant [17, Eq. (10.24)]. The relaxation time is also O(2n) by Theorem 2. In fact, it is not hard to see that both quantities are Θ(2n).
On the other hand, the alternating-scan sampler has mixing time Ω(2n) and relaxation time Ω(2n). For the mixing time, we partition the state space Ω into ΩL = {I | I ⊂ L} and ΩR = {I | I ⊂ R and I 6= ∅}. Consider the alternating scan projected down to ΩL and ΩR. If the current state is in ΩL, then there is 2−n probability to go to ∅ after updating all vertices in L, and then with probability 1− 2−n the state goes to ΩR after updating all vertices in R. Similarly, going from ΩR to ΩL has also probability O(2−n). Thus in each epoch of the alternating scan, the probability to go between ΩL and ΩR is Θ(2n) and the mixing time is thus Θ(2−n). The relaxation time can be similarly bounded using a standard conductance argument [14].
In summary, for this distribution π, we have that Trel(PAS) Trel(PRU ) and Tmix(PAS) Tmix(PRU ). Therefore, Theorem 1 is asymptotically tight and Corollary 5 is tight up to the factor log π−1min.
We note that in the example above, the alternating scan is not the best scan order. Indeed, as shown in [11], if we scan vertices alternatingly from the left and right, rather than scanning variables layerwise, the mixing time is smaller by a factor of n. Thus, although Theorem 1 and Corollary 5 provide certain guarantees of the alternating-scan sampler, the layerwise alternating order is not necessarily the best one.
In this section, we outline various existing approaches to automated lipreading.
Automated lipreading: Most existing work on lipreading does not employ deep learning. Such work requires either heavy preprocessing of frames to extract image features, temporal preprocessing of frames to extract video features (e.g., optical flow or movement detection), or other types of handcrafted vision pipelines (Matthews et al., 2002; Zhao et al., 2009; Gurban & Thiran, 2009; Papandreou et al., 2007; 2009; Pitsikalis et al., 2006; Lucey & Sridharan, 2006; Papandreou et al., 2009). Generalisation across speakers and extraction of motion features is considered an open problem, as noted in a recent review article (Zhou et al., 2014). LipNet addresses both of these issues.
Classification with deep learning: In recent years, there have been several attempts to apply deep learning to lipreading. However, all of these approaches perform only word or phoneme classification, whereas LipNet performs full sentence sequence prediction. Approaches include learning multimodal audio-visual representations (Ngiam et al., 2011), learning visual features as part of a traditional speech-style processing pipeline (e.g. HMMs, GMM-HMMs, etc.) for classifying words and/or phonemes (Almajai et al., 2016; Takashima et al., 2016; Noda et al., 2014; Koller et al., 2015), or combinations thereof (Takashima et al., 2016). Many of these approaches mirror early progress in applying neural networks for acoustic processing in speech recognition (Hinton et al., 2012).
Chung & Zisserman (2016a) propose spatial and spatiotemporal convolutional neural networks, based on VGG, for word classification. The architectures are evaluated on a word-level dataset BBC TV (333 and 500 classes), but, as reported, their spatiotemporal models fall short of the spatial architectures by an average of around 14%. Additionally, models cannot handle variable sequence lengths and they do not attempt sentence-level sequence prediction.
Chung & Zisserman (2016b) train an audio-visual max-margin matching model for learning pretrained mouth features, which they use as inputs to an LSTM for 10-phrase classification on the OuluVS2 dataset, as well as a non-lipreading task.
Wand et al. (2016) introduce LSTM recurrent neural networks for lipreading but address neither sentence-level sequence prediction nor speaker independence. This work holds the previous stateof-the-art in the GRID corpus with a speaker-dependent accuracy of 79.6%.
Garg et al. (2016) apply a VGG pre-trained on faces to classifying words and phrases from the MIRACL-VC1 dataset, which has only 10 words and 10 phrases. However, their best recurrent model is trained by freezing the VGGNet parameters and then training the RNN, rather than training them jointly. Their best model achieves only 56.0% word classification accuracy, and 44.5% phrase classification accuracy, despite both of these being 10-class classification tasks.
Sequence prediction in speech recognition: The field of automatic speech recognition (ASR) would not be in the state it is today without modern advances in deep learning, many of which have occurred in the context of ASR (Graves et al., 2006; Dahl et al., 2012; Hinton et al., 2012). The connectionist temporal classification loss (CTC) of Graves et al. (2006) drove the movement from deep learning as a component of ASR, to deep ASR systems trained end-to-end (Graves & Jaitly, 2014; Maas et al., 2015; Amodei et al., 2015). As mentioned earlier, much recent lipreading progress has mirrored early progress in ASR, but stopping short of sequence prediction.
No lipreading work (based on deep learning or not) has performed sentence-level sequence prediction. LipNet demonstrates the first sentence-level results by using CTC. Furthermore, it does not require alignments to do so.
Lipreading Datasets: Lipreading datasets (AVICar, AVLetters, AVLetters2, BBC TV, CUAVE, OuluVS1, OuluVS2) are plentiful (Zhou et al., 2014; Chung & Zisserman, 2016a), but most only contain single words or are too small. One exception is the GRID corpus (Cooke et al., 2006), which has audio and video recordings of 34 speakers who produced 1000 sentences each, for a total of 28 hours across 34000 sentences. Table 1 summarises state-of-the-art performance in each of the main lipreading datasets.
We use the GRID corpus to evaluate LipNet because it is sentence-level and has the most data. The sentences are drawn from the following simple grammar: command(4) + color(4) + preposition(4) + letter(25) + digit(10) + adverb(4), where the number denotes how many word choices there are for each of the 6 word categories. The categories consist of, respectively, {bin, lay, place, set}, {blue, green, red, white}, {at, by, in, with}, {A, . . . , Z}\{W}, {zero, . . . , nine}, and {again, now, please, soon}, yielding 64000 possible sentences. For example, two sentences in the data are “set blue by A four please” and “place red at C zero again”.
Rules with existentially quantified variables in the head—so-called generating rules—require the introduction of fresh individuals. Cyclic applications of generating rules may prevent the chase from terminating, and in fact determining whether chase terminates on a set of rules and facts is undecidable (Deutsch, Nash, & Remmel, 2008). However, several decidable classes of existential rules have been identified, and the existing proposals can be classified into two main groups. In the first group, rules are restricted such that their possibly infinite universal models can be represented using finitary means. This group includes rules with universal models of bounded treewidth (Baget et al., 2011a), guarded rules (Cal̀ı et al., 2010a), and ‘sticky’ rules (Cal̀ı, Gottlob, & Pieris, 2011). In the second group, one uses a sufficient (but not necessary) acyclicity notion that ensures chase termination.
Roughly speaking, acyclicity notions analyse the information flow between rules to ensure that no cyclic applications of generating rules are possible. Weak acyclicity (WA) (Fagin et al., 2005) was one of the first such notions, and it was extended to notions such as safety (Meier, Schmidt, & Lausen, 2009), stratification (Deutsch et al., 2008), acyclicity of a graph of rule dependencies (aGRD) (Baget, Mugnier, & Thomazo, 2011b), joint acyclicity (JA) (Krötzsch & Rudolph, 2011), and super-weak acyclicity (SWA) (Marnette, 2009). Syntactic acyclicity criteria have also been investigated in the context of logic programs with function symbols in the rule heads, where the goal is to recognise logic programs with finite stable models. Several such notions have been implemented in state of the art logic programming engines, such as omega-restrictedness (Syrjänen, 2001) from the Smodels system (Syrjänen & Niemelä, 2001), lambda-restrictedness from the ASP grounder GrinGo (Gebser, Schaub, & Thiele, 2007), argument-restrictedness (Lierler & Lifschitz, 2009) from the DLV system (Leone, Pfeifer, Faber, Eiter, Gottlob, Perri, & Scarcello, 2006), and many others (Calimeri, Cozza, Ianni, & Leone, 2008; Greco, Spezzano, & Trubitsyna, 2012; De Schreye & Decorte, 1994).
employ the proposed method for translating the English query words and utilize the translated queries to retrieve Persian documents. We use two parallel corpora: TEP [4] and 20M, UTPECC comparable corpus version 2.0 [5] and a bilingual English-Persian dictionary as the resources of extracting features. Also, we use Wikipedia parallel corpus [6] for constructing the training data and labeling the translation candidates. After constructing the ranking model using LTR approach, we employ the learned model to translate English queries and construct queries in Persian. We use the constructed Persian queries for retrieving documents in Persian. We employ Hamshahri collection [7], which is used in CLEF 2008 and 2009 as test collection. This corpus contains English queries, corresponding Persian queries, and about 166,000 documents in Persian. The results show that using LTR approach for query translation significantly outperforms all single resource based CLIR methods. Also, our results show that the proposed methods outperform the linear combination method which is one of the most used methods for translation resource combination in CLIR. We analyze the impact of different features in constructing the ranking model. Our results show that translation relation based features alongside context based features resolve most of main problems of single resource based CLIR methods.
The remainder of this paper is organized as follows. In Section II, we review the previous work on CLIR and using LTR for CLIR. In Section III, we describe the proposed LTR approach for query translation. Section IV explains the features used in this paper. Section V explains the design of experiments and the results of different CLIR methods. Also, in this section, we discuss impact of different features on constructing the LTR based translation model and the effect of size and quality of corpora on the accuracy of CLIR. Finally, Section VI concludes the paper and describes the future work.
II. RELATED WORK Different translation resources have been used for
query translation in CLIR. In fact, each resource that could provide a translation relation between source and target language words could be employed for query translation. Four translation resources have been widely used in CLIR for query translation [2]: 1) Dictionaries 2) Comparable corpora 3) Parallel corpora and 4) Machine translators. In this research, we did not employ machine translators for query translation because machine translators usually are used for translating a complete sentence and using them for translating queries, which are usually a set of keywords, will not have good performance in CLIR. The mentioned translation resources usually provide term to term matching between source and target language words. However, by using them, the meaning of the query in semantic level will not be considered. Consequently, some problems such as ambiguity remain in translation process, which could have a bad effect on the performance of CLIR system. To overcome this problem, in some researches, approaches such as word sense disambiguation have been utilized while in other researches semantically
annotated resources such as ontologies in query translation process have been employed.
Among different translation resources, parallel corpora have been used more than others in CLIR [1, 2]. Using parallel corpora, we can extract translation relations between source and target language words. Different methods have been employed for extracting translations from parallel corpora. Among these methods, IBM model-1[8] is the most used method for translation knowledge extraction. This method provides a probabilistic mapping between source language words to target language words and vice versa. In fact, the output of this method is two probabilistic lexicons, in which for each pair of source language word, e, and target language word, f, probabilities of translationsp(f|e) and p(e|f) are provided. Using these probabilistic lexicons, we can translate each query term to target language and after translating all query terms, we can construct the query in target language. After constructing the query in target language, the traditional IR methods such as Okapi BM25 method could be employed for retrieving documents [9]. Also, the translation probabilities could be employed in language model based IR approaches for calculating the relatedness scores of documents to queries. Berger and Lafferty [10] used the translation probabilities for estimating the language model of query in target language. After estimating the query language model in target language, they used language model based IR methods for retrieving documents in target language. Similar research used this approach for CLIR such as the research done in [11]. Also, in another study, Lavrenko et al. [12] used language modeling approach for CLIR. However, instead of using IBM model-1 for extracting translation knowledge from parallel corpora, they directly estimated the probability of relevance of each target language word with regard to a given source language query and calculated the language model of query in target language.
Comparable corpora are other useful resources for query translation in CLIR. After extracting translation knowledge from a comparable corpus, the methods described for parallel corpora based CLIR could be employed for comparable corpora based CLIR. Different approaches have been proposed for extracting translation from comparable corpora ([13]- [15]). The main intuition behind these methods is that a pair of source and target language words that usually co-occur in aligned documents are more likely to be translations of each other. Using this intuition, Tao and Zhai [13] estimated the associations between source and target language words. They estimated the probability distribution of words in documents of comparable corpora and considered the source and target language words that have similar probability distributions to be translations of each other. Talvensaari et al. [14] used similar approach for extracting translation knowledge from comparable corpora. In addition to co-occurrence information, they used similarity scores of aligned documents for calculating the association score of pairs of sourcetarget language words. Rahimi and Shakery [15] proposed a method based on language modeling framework for extracting translations from comparable
Building on linguistic and cognitive findings, the phonological representation of speech lies at the center of human speech processing. Speech analysis is performed at different time granularities broadly categorized as segmental and supra-segmental levels. The phonological classes define the sub-phonetic and phonetic attributes recognized at the segmental level whereas the syllables, lexical stress and prosodic accent are the basic supra-segmental events - c.f. Figure 3. The phonological representations are often studied at segmental level and their supra-segmental properties are not investigated. This supra-segmental characterization of phonological posteriors will be explored in this work.
The orthogonal matching pursuit (OMP) algorithm [36, 64] is an improvement of the MP algorithm. The OMP employs the process of orthogonalization to guarantee the orthogonal direction of projection in each iteration. It has been verified that the OMP algorithm can be converged in limited iterations [36]. The main steps of OMP algorithm have been summarized in Algorithm 1.
Algorithm 1. Orthogonal matching pursuit algorithm Task: Approximate the constraint problem: α̂ = argminα ‖α‖0 s.t. y = Xα Input: Probe sample y, measurement matrix X , sparse coefficients vector α Initialization: t = 1, r0 = y, α = 0, D0 = φ, index set Λ0 = φ where φ denotes empty set, τ is a small constant. While ‖rt‖ > τ do
Step 1: Find the best matching sample, i.e. the biggest inner product between rt−1 and xj (j 6∈ Λt−1) by exploiting λt = argmaxj 6∈Λt−1 |〈rt−1,xj〉|.
Step 2: Update the index set Λt = Λt−1 ⋃
λt and reconstruct data set Dt = [Dt−1,xλt ].
Step 3: Compute the sparse coefficient by using the least square algorithm α̃ = argmin ‖y −Dtα̃‖22.
Step 4: Update the representation residual using rt = y −Dtα̃. Step 5: t = t + 1.
End Output: D, α
We apply a competition mechanism among CAUs. The competition encourages the unit standing for the minimum matching error to pop out. As a result, the relation representation can be more easily decoded by the readout units. A classic competition mechanism is the winner-take-all (WTA), defined as
h′k =
{ 1, if hk = min(h),
0, otherwise. (6)
WTA is of conceptual interest, which is demonstrated in Section 3.2. However, WTA is not differentiable and discards too much information. In practice, we can use softmin competition, defined as
h′k = e−hk∑ i e −hi . (7)
In all our experiments, adding the softmin competition significantly improves the results of neural networks with CAUs.
Datasets with large corpora of “paired” images and sentences have enabled the latest advance in image captioning. Many novel networks [8, 21, 17, 32] trained with these paired data have achieved impressive results under a domain-specific setting – training and testing on the same domain. However, the domain-specific setting creates a huge cost on collecting “paired” images and sentences in each domain. For real world applications, one will prefer
Source Caption (MSCOCO)
Target Caption (CUB-200)
Generated Caption before adapt after adapt
Source Ground Truth Target Ground Truth
Generated (before adapt) Generated (after adapt)
A family of ducks swimming in the water.
A hummingbird close to a flower trying to eat.
This bird has wings that are brown and has red eyes.
A small bird with orange flank and a long thin black bill.
a “cross-domain” captioner which is trained in a “source” domain with paired data and generalized to other “target” domains with very little cost (e.g., no paired data required).
Training a high-quality cross-domain captioner is challenging due to the large domain shift in both the image and sentence spaces. For instance, MSCOCO [23] mostly consists of images of large scene with more object instances, whereas CUB-200-2011 [33] (shortened as CUB-200 in the following) consists of cropped birds images. Moreover, sentences in MSCOCO typically describe location, color and size of objects, whereas sentences in CUB-200 describe
1
ar X
iv :1
70 5.
00 93
0v 1
[ cs
.C V
] 2
M ay
parts of birds in detail (Fig. 1). In this case, how can one expect a captioner trained on MSCOCO to describe the details of a bird on CUB-200 dataset?
A few works propose to leverage different types of unpaired data in other domains to tackle this challenge. [14, 30] propose to leverage an image dataset with category labels (e.g., ImageNet [7]) and sentences on the web (e.g., Wikipedia). However, they focus on the ability to generate words unseen in paired training data (i.e., word-level modification). Anderson et al. [2] propose to leverage image taggers at test time. However, this requires a robust crossdomain tagger. Moreover, they focus on selecting a few different words but not changing the overall style.
Inspired by Generative Adversarial Networks (GANs) [10], we propose a novel adversarial training procedure to leverage unpaired images and sentences. Two critic networks are introduced to guide the procedure, namely domain critic and multi-modal critic. The domain critic assesses whether the generated captions are indistinguishable from sentences in the target domain. The multi-modal critic assesses whether an image and its generated caption is a valid pair. During training, the critics and captioner act as adversaries – captioner aims to generate indistinguishable captions, whereas critics aim at distinguishing them. Since the sentence is assessed only when it is completed (e.g., cannot be assessed in a word by word fashion), we use Monte Carlo rollout to estimate the assess of each generated word. Then, we apply policy gradient [29] to update the network of the captioner. Last but not least, we propose a novel critic-based planning method to take advantage of the learned critics to compensate the uncertainty of the sentence generation policy with no additional supervision (e.g., tags [2]) in testing.
To evaluate, we use MSCOCO [23] as the source domain and CUB-200 [33, 27], Oxford-102 [25, 27], Flickr30k [36] and TGIF [22] as target domains. Our method consistently performs well on all datasets. In particular, on CUB-200, we achieve 21.8% CIDEr-D improvement after adaptation. Utilizing critic during inference further gives another 4.5% boost. The contributions of the paper are summarized below:
• We propose a novel adversarial training procedure for cross-domain captioner. It utilizes critics to capture the distribution of image and sentence in the target domain.
• We propose to utilize the knowledge of critics during inference to further improve the performance.
• Our method achieves significant improvement on four publicly available datasets compared to a captioner trained only on the source domain.
In this paper, we presented two new local search strategies for the minimum vertex cover (MVC) problem, namely two-stage exchange and edge weighting with forgetting. The two-stage exchange strategy yields an efficient two-pass move operator for MVC local search algorithms, which significantly reduces the time complexity per step. The forgetting mechanism enhances the edge weighting scheme by decreasing weights when the averaged weight reaches a threshold, to periodically forget earlier weighting decisions. Based on these two strategies, we designed a slight, yet effective MVC local search algorithm called NuMVC. The NuMVC algorithm was evaluated against the best known heuristic algorithms for MVC (MC,MIS) on standard benchmarks, i.e., the DIMACS and BHOSLIB benchmarks. The experimental results show that NuMVC is largely competitive on the DIMACS benchmark and dramatically outperforms other state-of-the-art heuristic algorithms on all BHOSLIB instances.
Furthermore, we showed that NuMVC is characterized by exponential RTDs, which means it is robust w.r.t. the cutoff parameters and the restart time, and hence has close-to-optimal parallelization speedup. We also performed further investigations to provide further insights into the two new strategies and their effectiveness. Finally, we conducted an experiment to study the performance of NuMVC with different parameter settings, and the results indicate that NuMVC is not sensitive to its parameters.
The two-stage exchange strategy not only has a lower time complexity per step, but also has the flexibility to allow us to employ specific heuristics in different stages. An interesting research direction is thus to apply this idea to other combinatorial problems whose essential tasks are also to seek for an optimal subset with some fixed cardinality.
Messages posted on social media vary greatly in terms of information they contain. For example, users post messages of personal nature, messages useful for situational
awareness (e.g. infrastructure damage, causalities, individual needs), or not related to the crisis at all. Depending on their information needs, different humanitarian organizations use different annotation schemes to categories these messages. In this work, we use a subset of the annotations used by the United Nations Office for the Coordination of Humanitarian Affairs (UN OCHA). The 9 category types (including two catch-all classes: “Other Useful Information” and “Irrelevant”) used by the UN OCHA are shown in the below-presented annotation scheme. For most of the datasets we have performed annotations by employing volunteers and paid workers. To perform volunteered-based annotations, messages were collected from Twitter in real-time and passed through a deduplication process. Only unique messages were considered for human-annotation. We use Stand-By-Task-Force (SBTF)1 volunteers to annotate messages using our MicroMappers platform.2 The real-time annotation process helps train machine learning classifiers rapidly, which are then used to classify new incoming messages. This process helps address time-critical information needs requirement of many humanitarian organizations. After the first round of annotations, we found that some categories are small in terms of number of labels thus showing high class-imbalance. A dataset is said to be imbalanced if at least one of the classes has significantly fewer annotated instances than the others. The class imbalance problem has been known to hinder the learning performance of classification algorithms. In this case, we performed another round of annotations for datasets that have high class imbalance using the paid crowdsourcing platform CrowdFlower.3 In both annotation processes, an annotation task consists of a tweet and the list of categories listed below. A paid worker or volunteer reads the message and selects one of the categories most suitable for the message. Messages that do not belong to any category but contain some important information are categorized as “Other Useful Information”. A task is finalized (i.e. a category is assigned) when three different volunteers/paid workers agree on a category. According to the Twitter’s data distribution policy, we are not allowed to publish actual contents of more than 50k tweets. For this reason, we publish all annotated tweets, which are less than 50k, along with tweet-ids of all the unannotated messages at http://CrisisNLP.qcri. org/. We also provide a tweets retrieval tool implemented in Java, which can be used to get full tweets content from Twitter. In below we show the annotation scheme used for crisis events caused by natural disasters. For other events, details regarding their annotations are available with the published data. Annotation scheme: Categorizing messages by information types
• Injured or dead people: Reports of casualties and/or injured people due to the crisis
1http://blog.standbytaskforce.com/ 2http://micromappers.org/ 3http://crowdflower.com/
Table 1: Crises datasets details including crisis type, name, year, language of messages, country, # of tweets.
Crisis type Crisis name Country Language # of Tweets Start-date End-date Earthquake Nepal Earthquake Nepal English 4,223,937 2015-04-25 2015-05-19 Earthquake Terremoto Chile Chile Spanish 842,209 2014-04-02 2014-04-10 Earthquake Chile Earthquake Chile English 368,630 2014-04-02 2014-04-17 Earthquake California Earthquake USA English 254,525 2014-08-24 2014-08-30 Earthquake Pakistan Earthquake Pakistan English 156,905 2013-09-25 2013-10-10 Typhoon Cyclone PAM Vanuatu English 490,402 2015-03-11 2015-03-29 Typhoon Typhoon Hagupit Phillippines English 625,976 2014-12-03 2014-12-16 Typhoon Hurricane Odile Mexico English 62,058 2014-09-15 2014-09-28 Volcano Iceland Volcano Iceland English 83,470 2014-08-25 2014-09-01 Landslide Landslides worldwide Worldwide English 382,626 2014-03-12 2015-05-28 Landslide Landslides worldwide Worldwide French 17,329 2015-03-12 2015-06-23 Landslide Landslides worldwide Worldwide Spanish 75,244 2015-03-12 2015-06-23 Floods Pakistan Floods Pakistan English 1,236,610 2014-09-07 2014-09-22 Floods India Floods India English 5,259,681 2014-08-10 2014-09-03 War & conflict Palestine Conflict Palestine English 27,770,276 2014-07-12 2014-10-02 War & conflict Peshawar Attack Pakistan Pakistan English 1,135,655 2014-12-16 2014-12-28 Biological Middle East Respiratory Syndrome Worldwide English 215,370 2014-04-27 2014-07-14 Infectious disease Ebola virus outbreak Worldwide English 5,107,139 2014-08-02 2014-10-27 Airline accident Malaysia Airlines flight MH370 Malaysia English 4,507,157 2014-03-11 2014-07-12
• Missing, trapped, or found people: Reports and/or questions about missing or found people
• Displaced people and evacuations: People who have relocated due to the crisis, even for a short time (includes evacuations)
• Infrastructure and utilities damage: Reports of damaged buildings, roads, bridges, or utilities/services interrupted or restored
• Donation needs or offers or volunteering services: Reports of urgent needs or donations of shelter and/or supplies such as food, water, clothing, money, medical supplies or blood; and volunteering services
• Caution and advice: Reports of warnings issued or lifted, guidance and tips
• Sympathy and emotional support: Prayers, thoughts, and emotional support
• Other useful information: Other useful information that helps understand the situation
• Not related or irrelevant: Unrelated to the situation or irrelevant
[1] Yao Qian, Zhizheng Wu, Xuezhe Ma, and Frank Soong, “Automatic prosody prediction and detection with conditional random field (crf) models,” in Proceedings of ISCSLP, 2010, pp. 135–138.
[2] Jingwei Sun, Jing Yang, Jianping Zhang, and Yonghong Yan, “Chinese prosody structure prediction based on conditional random fields,” in Proceedings of ICNC, 2009, vol. 3, pp. 602–606.
[3] Je Hun Jeon and Yang Liu, “Automatic prosodic events detection using syllable-based acoustic and syntactic features,” in Proceedings of ICASSP, 2009, pp. 4565– 4568.
[4] Vivek Rangarajan, Shrikanth Narayanan, and Srinivas Bangalore, “Exploiting acoustic and syntactic features for prosody labeling in a maximum entropy framework,” in Proceedings of NAACL HLT, 2007, pp. 1–8.
[5] Philipp Koehn, Steven Abney, Julia Hirschberg, and Michael Collins, “Improving intonational phrasing with syntactic information,” in Proceedings of ICASSP, 2000, pp. 1289–1290.
[6] Min Chu and Yao Qian, “Locating boundaries for prosodic constituents in unrestricted mandarin texts,” Computational linguistics and Chinese language processing, pp. 61–82, 2001.
[7] Xin Nie and Zuo-ying Wang, “Automatic phrase break prediction in chinese sentences,” Journal of Chinese information Processing, pp. 39–44, 2003.
[8] Jian-Feng Li, Guoping Hu, and Ren-hua Wang, “Chinese prosody phrase break prediction based on maximum entropy model,” in Proceedings of INTERSPEECH, 2004, pp. 729–732.
[9] Gina-Anne Levow, “Automatic prosodic labeling with conditional random fields and rich acoustic features,” in Proceedings of IJCNLP, 2008, pp. 217–224.
[10] John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira, “Conditional random fields: Probabilistic models for segmenting and labeling sequence data,” in Proceedings of 18th ICML, 2001, pp. 282–289.
[11] Zhao Sheng, Tao Jianhua, and Cai Lianhong, “Learning rules for chinese prosodic phrase prediction,” in Proceedings of ACL, 2002, pp. 1–7.
[12] Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu, “Deep learning for chinese word segmentation and pos tagging.,” in Proceedings of EMNLP, 2013, pp. 647–657.
[13] Wenzhe Pei, Tao Ge, and Chang Baobao, “Maxmargin tensor neural network for chinese word segmentation,” in Proceedings of ACL, 2014, pp. 293–303.
[14] Mike Schuster and Kuldip K Paliwal, “Bidirectional recurrent neural networks,” IEEE Transactions on Signal Processing, vol. 45, no. 11, pp. 2673–2681, 1997.
[15] Sepp Hochreiter and Jürgen Schmidhuber, “Long shortterm memory,” Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[16] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin, “A neural probabilistic language model,” The Journal of Machine Learning Research, vol. 3, pp. 1137–1155, 2003.
[17] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean, “Distributed representations of words and phrases and their compositionality,” in Proceedings of NIPS, 2013, pp. 3111–3119.
[18] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig, “Linguistic regularities in continuous space word representations.,” in Proceedings of HLT-NAACL, 2013, pp. 746–751.
[19] Mairgup Mansur, Wenzhe Pei, and Baobao Chang, “Feature-based neural language model and chinese word segmentation,” Proceedings of 6th IJCNLP, vol. 1, no. 2.3, pp. 2–3, 2013.
[20] Shin-ichi Horikawa, Takeshi Furuhashi, and Yoshiki Uchikawa, “On fuzzy modeling using fuzzy neural networks with the back-propagation algorithm,” IEEE transactions on Neural Networks, vol. 3, no. 5, pp. 801– 806, 1992.
[21] Ronald J Williams and David Zipser, “Gradient-based learning algorithms for recurrent networks and their computational complexity,” Back-propagation: Theory, architectures and applications, pp. 433–486, 1995.
[22] Alex Graves, Navdeep Jaitly, and A-R Mohamed, “Hybrid speech recognition with deep bidirectional lstm,” in Proceedings of ASRU, 2013, pp. 273–278.
[23] Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa, “Natural language processing (almost) from scratch,” The Journal of Machine Learning Research, vol. 12, pp. 2493– 2537, 2011.
[24] Felix Weninger, Johannes Bergmann, and Björn Schuller, “Introducing currennt–the munich opensource cuda recurrent neural network toolkit,” Journal of Machine Learning Research, vol. 15, 2014.
We embed a Monte-Carlo Tree Search (MCTS) module from MctsAi (Yoshida et al. 2016) to our AI. This module analyzes game state and recommends n candidate actions to the proposed AI, in which one out of them will be selected as an optimal action. The search module provides the best action by considering strength of action under a given game situation. It is noted that if n is set to 1, HP-AI will only use the strongest action, and if the value of n is too large, HP-AI will be weaken; value of n can be used to control game difficulty and its value is set at 3 in this study. MCTS is a combination of tree search algorithm and Monte-Carlo method; it uses random sampling in exploration of the decision space. There are four major steps in MCTS: selection, expansion, simulation and backpropagation. The four steps are repeated until a given amount of time is elapsed. An overview of MCTS is shown in Figure
5, where the root node represents the current game situation while child nodes represent actions. A path from a root node to a leaf node is a sequence of AI actions.
• Selection: UCB1 is employed as the selection criterion
of nodes. Reward used for evaluation is computed by using changes in hit points before and after the actions is executed (denoted as HPafter – HPbefore = ΔHP); hitpoints considered are that of the AI and that of the player. The selection criterion is given as Eq. (2); considering the ith node, C is a balance parameter, Ni is the number of visits at that node, NiP is the number of visits at its parent node, and Xi is the average reward (see (3) and (4)).
1 2 (2)
1 (3)
Δ Δ (4) • Expansion: By the time a leaf node is reached, if the
depth of the path is shallower than a threshold and the number of visits of the leaf node is larger than a threshold, child nodes will be created from the leaf node.
• Simulation: A simulation is done by using a sequence of actions in the path from the root node to the leaf node as AI actions. Consequentially, it uses random actions of the same number of those in the path for the opponent’s actions.
• Backpropagation: An update from simulation is performed to obtain UCB1 for nodes that were traversed in the path.
ar X
iv :1
20 4.
12 31
v3 [
cs .A
I] 1
4 A
In this paper, we propose a framework to study a general class of strategic behavior in voting, which we call vote operations. We prove the following theorem: if we fix the number of alternatives, generate n votes i.i.d. according to a distribution π, and let n go to infinity, then for any ǫ > 0, with probability at least 1 − ǫ, the minimum number of operations that are needed for the strategic individual to achieve her goal falls into one of the following four categories: (1) 0, (2) Θ( √ n), (3) Θ(n), and (4) ∞. This theorem holds for any set of vote operations, any individual vote distribution π, and any integer generalized scoring rule, which includes (but is not limited to) almost all commonly studied voting rules, e.g., approval voting, all positional scoring rules (including Borda, plurality, and veto), plurality with runoff, Bucklin, Copeland, maximin, STV, and ranked pairs.
We also show that many well-studied types of strategic behavior fall under our framework, including (but not limited to) constructive/destructive manipulation, bribery, and control by adding/deleting votes, margin of victory, and minimum manipulation coalition size. Therefore, our main theorem naturally applies to these problems.
Keywords: Computational social choice; generalized scoring rules; vote operations
∗Corresponding author. Tel:+1-617-495-1246. Email address: lxia@seas.harvard.edu (Lirong Xia)
Preprint submitted to Artificial Intelligence August 15, 2012
As the same as the concept of neural network synaptic weight functions changes can be interpreted as biological neural learning, we can have a general definition of pattern learning as:
Pattern learning is the changing of pattern recognition identification set.
This definition is useful because it is equivalent to the intensive studied standard neural connectionist and artificial intelligence learning models. Also because it can be a generalization of these learning mechanisms as a basic pattern recognition process as learning been simply the recognition of new patterns.
A simple example can be the recognition of a new pattern of numbers sequence when a person dial a phone, or recognition of a new face image pattern to a new person.
A consequence of this definition is that the concepts of learning and memory are equivalent. When we memorize we are learning new patterns, and when we learn we are memorizing new patterns. This equivalence is not always clear in the cognitive science literature.
A difference between learning and memorization can be established if one define different categories of patterns as for example simple recorded patterns of known processing patterns as memory and new processing patterns recognition not previously known as learning, but it is only a matter of categorization of the basic mechanism of pattern recognition.
A conclusion we can propose is that the learning concept is a subset of the pattern recognition concept.
The major components of our system are the distributed web servers, asynchronous task dispatcher, data warehouse, search engine, text pre-processing engine, modularized concurrent topic modeling engine, update pool, and quality evaluator.
The workflow of the system is as follows: After a query is accepted by one of the web servers, a request is dispatched to the review search engine. The search engine determines if pre-processed results already exist in the data warehouse, and if not, how much further pre-processing is required. Requests of pre-processing tasks are created and dispatched to the text processing engine, where background workers are constantly pre-processing fresh reviews while assigning highest priority to new requests from the search engine. When pre-processed data is ready, the results are dispatched to the concurrent topic modeling engine. Multiple topic modeling instances are created at the same time, each emitting updates every few Gibbs sampling iterations to a pool where the results are
further evaluated for quality. After evaluation, the best result is selected and sent back to the asynchronous task dispatcher, where it is later routed back to the initial web server. The web server then packages the result and returns it for presentation to the end user.
Domain similarity (DS) was designed to capture the topic (the field, area, or domain) of a word. The following experiments use the domain matrix from Turney (2012).11 To make the domain matrix, Turney (2012) first constructed a frequency matrix, in which the rows correspond to n-grams in WordNet and the columns correspond to nouns that were observed near the row n-grams in the Waterloo corpus. The hypothesis was that the nouns near a term characterize the topics associated with the term. Given an n-gram, a, the Waterloo corpus was sampled for phrases containing a and the phrases were processed with a partof-speech tagger, to identify nouns. If the noun b was the closest noun to the left or right
9. The PPMI matrix is available from the author on request. 10. Turney et al. (2011) did not normalize PPMI in their experiments. They used Equation 6, whereas we
use Equation 9 here. 11. The domain matrix is available from the author on request.
of a, where a corresponds to the i-th row of the frequency matrix and b corresponds to the j-th column of the matrix, then the frequency count for the i-th row and j-th column was incremented.
The word-context frequency matrix for domain space has about 114,000 rows (WordNet terms) and 50,000 columns (noun contexts, topics), with a density of 2.6%. The frequency matrix was converted to a PPMI matrix and then processed with singular value decomposition (SVD).12 The SVD yields three matrices, U, Σ, and V. A term in domain space is represented by a row vector in UkΣ p
k. The parameter k specifies the number of singular values in the truncated singular value decomposition; that is, k is the number of latent factors in the low-dimensional representation of the term (Landauer & Dumais, 1997). We generate Uk and Σk by deleting the columns in U and Σ corresponding to the smallest singular values. The parameter p raises the singular values in Σk to the power p (Caron, 2001). When p is zero, all of the k factors have equal weight. When p is one, each factor is weighted by its corresponding singular value in Σ. Decreasing p has the effect of making the similarity measure more discriminating (Turney, 2012).
The similarity of two words in domain space, DS(a, b, k, p), is computed by extracting the row vectors in UkΣ p k that correspond to the n-grams a and b, and then calculating their cosine. Optimal performance requires tuning the parameters k and p for the task (Bullinaria & Levy, 2012; Turney, 2012). For Comp and Decomp, we use the parameter settings given by Turney (2012). For Super, we generate features with a wide range of parameter settings and let the supervised learning algorithm decide how to use the features.
If either a or b does not correspond to a row in UkΣ p
k, then DS(a, b, k, p) is set to zero. Since the rows in the domain matrix correspond to n-grams in WordNet, pseudo-unigrams do not present a problem. Given a pseudo-unigram, a b, we compute DS(a b, c, k, p) by extracting the row vectors in UkΣ p k that correspond to the bigram ab and the unigram c and then calculating the cosine of the vectors.
We chose the MNIST dataset for it’s large number of results to compare to. Of these, deep learning methods typically fall into one of two categories, 1) those that are completely unsupervised and have a simple classifier on top, or 2) those that are fine-tune discriminatively with labels. Our method falls into the first category as it is completely unsu-
pervised during training, and only the linear SVM applied on top has access to the label information of the training set. We do not back propagate this information through the network, but this would be an interesting future direction to pursue. Table 5 shows our method is competitive with other deep generative models, even surpassing several which use discriminative fine tuning.
Naïve Bayes is one of the most effective and efficient classification algorithms. In machine learning problems, a learner attempts to construct a classifier from a given set of training examples with class labels. Assume that F1, F2, F3.., Fn are n attributes. An example E is represented by a vector (f1,f2,….fK), where fi is the value of Fi. Let C represent the class variable which takes values excellent, good, average and poor. We use QE to represent the value that C
takes. A naïve Bayesian classifier is defined as follows-
P(QE|F1,F2,F3,F4,……FN) = ( , , ,. .| )∗ ( )
( , , , ,…… ) (1)
QE = argmaxP(F1|QE)*P(F2|QE)*P(F3|QE).*P(FN|QE)* P(QE) (2)
Where, P(QE|F) is the posterior probability of class (target) given predictor (attribute). P(QE) is the prior probability of class. P(F|QE) is the likelihood which is the probability of predictor given class. P(F) is the prior probability of predictor. The value of (fi/QE) can be estimated from the training example which can be easily implemented by Naïve Bayes classifier.
In the script test.m, we provide the following parameters:
ALPHA = 1e-2; BETA = 1e-2; OMEGA = 0.05; N = 20; M = 1; SEED = 1; OUTPUT = 1; J = 10;
We explain these parameters as follows:
1. ALPHA and BETA are Dirichlet hyperparamters. In real-world applications, the asymmetric prior ALPHA may have substantial advantages over the symmetric prior, while the asymmetric prior BETA may not. Generally, the hyperparameters determine the sparseness of multinomial parameters THETA and PHI that influence the topic modeling performance. However, for simplicity, we assume that the hyperparameters are symmetric and are provided by users as prior knowledge in this toolbox. In many cases, we often use the smoothed LDA with fixed symmetric Dirichlet hyperparameters ALPHA = 0.01 and BETA = 0.01.
2. OMEGA is a balancing weight OMEGA ∈ [0, 1] for relational topic models (RTM) [1], which balances messages from document content and document links. When OMEGA = 0, RTM reduces to the standard LDA.
3. N is the number of learning iterations.
4. M is the number of inner iterations in the VB algorithm.
5. SEED is the seed for random number generation.
6. OUTPUT = 0 denotes that no output is printed on the screen. OUTPUT = 1 prints the number of iterations and perplexity on the screen.
7. J is the total number of topics pre-defined by the user.
For all tools, there are three common output parameters:
1. phi is a J×W matrix for the unnormalized topic multinomial parameters over vocabulary.
2. theta is a J ×D matrix for the unnormalized document-specific topic proportions.
3. mu is a J × NZ matrix for the topic distribution over word index, where NZ is the total number of non-zero elements in the word count matrix cora_wd. Another similar output is z, which is a 1×NZ vector for the discrete assignment of topic label over word indices.
Turkish is an Ural-Altaic language, having agglutinative word structures. Turkish has a productive morphology with inflectional and derivational processes. Theoreticaly, there are infinite number of words in Turkish as one root may generate hundreds of new reflected, derived words. For instance take the verbal root kazmak and see its reflections: kazdın, kazdım, kazdı, kazdık, kazdılar, kazdınız, kazarken, kazıyorken, kazmazken, kazmıyorken, kazmazdı, kazmazdım… Turkish morphology has been well studied by Oflazer [13]. A two-level morphological analyzer for Turkish was built by using XRCE finite state tools. This analyzer segments words into a series of lexical morphemes. Because of complex morphophonological processes, we'll annotate classes of graphemes e.g. A represents unrounded back vowels a and e, H is for high vowels ı, i, u, ü and D stands for d and t.
To learn a policy through classification, it is first necessary to generate an appropriate training data set. For our problem, this data set must associate the states of the batteries and the current load with an appropriate decision (which battery to use to service the load). We construct the training set by building a sample of profiles from the stochastic description of the expected loads. The distributions we used to describe amplitude, duration and frequency of loads are shown in Figure 13. The deterministic solutions to these problems are constructed as described in Section 4. Training data is then generated from these plans by simulating their execution and recording the battery
states, load and battery choice at a fixed time increment throughout the plan. For example, if the increment is 0.01 minutes then the training data generated from a plan will record the battery states of charge (available and bound), load and currently selected battery (which might or might not have changed from the previous time increment) at every 0.01 minute interval throughout the plan. In our experiments we selected the time increment to be the same as the smallest increment used in the variable discretisation described in Section 4.4, but this is not a requirement of the approach. The choice of time increment determines the frequency of the decision-cycle for the learned policy. The time increment also determines how much training data is generated from a single plan, according to the makespan of the plan. In order to reduce the volume of training data for fine-grained time increments used with long makespan batteries, it is possible to randomly sample from the set of state-battery-selection pairs across multiple plans. In our experiments we did not need to do this.
Once the training data is generated, a classifier can be learned using a standard machine learning approach. WEKA (Hall, Frank, Holmes, Pfahringer, Reutemann, & Witten, 2009) is a machine learning framework, developed at the University of Waikato, that provides a set of classification and clustering algorithms for data-mining tasks. WEKA takes as input a training set, comprising a list of instances sharing a set of attributes. In order to perform the classification on the battery usage problem data, we consider instances of the following form:
τ = (σ1, γ1, . . . , σN , γN , B, L)
where σi and γi denote the available charge and total charge of the ith battery, respectively, B is the currently active battery and L is the current load (this is essentially the state of the MDP but without the time label, since we want our policy to operate independently of time). In this setting, the attribute used as the class is the battery B.
The stochastic load profiles have been defined with a distribution of:
• the load amplitude l ∈ [100 . . . 750] mA;
• the load/idle period duration d ∈ [0.1 . . . 5] min;
• the load frequency f ∈ [0.3 . . . 0.7].
The probability distributions are shown in Figure 13.
This leads to load profiles that are very irregular (see the bottom of Figure 14) and therefore harder to handle than the very regular profiles considered by Jongerden et al. We generated a set of stochastic load profiles and for each of them we produced a near-optimal plan using the deterministic solving described in Section 4. This set of plans has been used as the training set for the classification process.
In order to select the most suitable classification algorithm, we applied all the classifiers provided by WEKA to a data set of 10,000 training examples. We first evaluated their performance as the number of correctly classified instances during the cross-validation. We discarded classifiers providing less than 70% correctness. We then considered the memory and the time required to use the classifier. The output of the classification process is a model encoding the resulting decision tree. In some cases, the generated model requires significant memory to store (more than 500Mb of RAM memory), or it is too slow to be used. These parameters have also been used to determine the number of training examples to classify, as the bigger the training set, the better the performance and the higher the memory and time requirements. Some of the classifiers with their performance are reported in Table 3.
According to these criteria, we selected the J48 classifier, which implements the machine learning algorithm C4.5 (Quinlan, 1993). The output is a decision tree whose leaves represent, in our case
study, the battery to be used (a fragment of the tree is shown in Figure 15). For the cardinality of the training set, an empirical evaluation showed that the best result is obtained using 250,000 training examples (note that this involves considering about 4 · 106 real values characterising the states and battery selections in these training examples) since further extending the training set does not make any significant improvement in the performance but increases memory and time requirements.
In the approaches described so far, the structures in the graphical models were either defined using expert knowledge or were learned directly from data using some form of structural learning. Both can be problematic since appropriate expert domain knowledge might not be available, while structural learning can be very time consuming and possibly results in local optima which are difficult to interpret. In this context, the advantage of relational latent variable models is that the structure in the associated graphical models is purely defined by the entities and relations in the domain.
The additional complexity of working with a latent representation is counterbalanced by the great simplification by avoiding structural learning. In the following discussion, we assume that data is in triple format; generalizations to relational databases haven been described [41, 19].
[Wu et al., 2016] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural machine translation system:
Bridging the gap between human and machine translation. In arXiv:1609.08144.
[Hochreiter and Schmidhuber, 1997] Hochreiter, Sepp and Schmidhuber, Jürgen. Long short-term memory. Neural computation, 9(8):1735–1780, 1997. [Cho et al., 2014]Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, Yoshua Bengio. On the Properties of
Neural Machine Translation: Encoder-Decoder Approaches. In EMNLP 2014, pages 103-111.
[Lecun et al., 1998] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
[Burt, 1988] Burt P J. Attention mechanisms for vision in a dynamic world. International Conference on Pattern
Recognition. Pages 977-987, 1988, vol.2.
[Denil et al., 2012] Denil M, Bazzani L, Larochelle H, et al. Learning where to attend with deep architectures for
image tracking. Neural Computation, pages 2151-2184, 2012, 24(8).
[Bahdanau et al., 2015] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation
by jointly learning to align and translate. ICLR 2015.
[Luong et al., 2015] Thang Luong, Hieu Pham, and Christopher D. Manning. Effective Approaches to Attention
based Neural Machine Translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural
Language Processing, pages 1412–1421.
[Chung et al., 2014] Chung J, Gulcehre C, Cho K H, et al. Empirical Evaluation of Gated Recurrent Neural Networks
on Sequence Modeling. Eprint Arxiv, 2014.
[Graves, 2013] Graves, Alex. Generating sequences with recurrent neural networks. arXiv preprint
arXiv:1308.0850, 2013.
[Kim et al., 2016]Yoon Kim, Yacine Jernite, David Sontag and Alexander M. Rush. Character-aware neural language
models. In AAAI 2016, pages 2741-2749.
[Srivastava et al., 2014]Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: a simple way to prevent neural
networks from overfitting[J]. Journal of Machine Learning Research, 2014, 15(1):1929-1958.
[Papineni et al., 2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for
automatic evaluation of machine translation. In ACL 2002, 311-318.
[Hinton et al., 2012] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving
neural networks by preventing coadaptation of feature detectors. arXiv:1207.0580, 2012.
[Collins,2002]Michael Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and
Experiments with Perceptron Algorithms. In Proceedings of EMNLP, pages 1–8, Stroudsburg, PA, USA.
[Wang et al., 2015] Wang P, Qian Y, Soong F K, et al. A Unified Tagging Solution: Bidirectional LSTM Recurrent
Neural Network with Word Embedding. arXiv preprint arXiv:1511.00215, 2015.
AlphaGo and Atari agents choose one action from a finite set of possible actions. There are also articles on continuous reinforcement learning where action is a vector [69] [70] [71] [72] [73] [74] [75] [76] [77].
AlphaGo is a powerful AGI in it’s own very simple world which is just a board with stones and a bunch of simple rules. If we improve AlphaGo with continuous reinforcement learning (and if we manage to make it work well on hard real-world tasks) than it would be real AGI in a real world. Also, we can begin with pretraining it on virtual videogames[93]. Videogames often contain even much more interesting challenges than real life gives for an average human. Millions of available books and videos contain the concentrated life experience of millions of people. While AlphaGo is a successful example of pairing modern RL with modern CNN, RL can be combined with neural chat bots and reasoners [94][95].
6 do we have powerful unsupervised learning?
Yes, we do. DCGAN [52] generates reasonable pictures[53]. Language generation models which minimize perplexity are unsupervised and were improved very much recently (table 1). ”Skip-thought vectors”[103] generate vector representation for sentences allowing to train linear classifiers over those vectors and their
cosine distances to solve many supervised problems at state-of-the-art level. Generative nets gained deep new horizons at the edge of 2013-14 [78][79]. Recent work [51] continues progress in ”computer vision as inverse graphics” approach.
In this section, we compare the efficacy of the proposed algorithms via simulations. For simulations, we use the minimum knapsack problem described
in the previous section which is solved using the greedy algorithm GA. We compare the regret of four algorithms namely, CCB-NS, CCB-S, CCB-SE and a variant of the εt−greedy algorithm. The εt−greedy algorithm [3] solves the classical multi-armed bandit problem which involves the selection of the single best arm. In the classical version, a random arm is explored with probability εt and the optimal arm (with the highest empirical mean) is selected with probability 1− εt. We extend the algorithm to the AAB setting by exploring all the workers with probability εt and with probability (1−εt), we select the minimum cost worker subset which meets the target constraint with the empirically estimated qualities. The parameter εt = min{1, 100t } decreases with time so as to give more weight to exploitation than exploration. Note that, the εt algorithm is not strategyproof.
In the simulations, we have selected the number of agents to be 1100. To emphasize the fact that CCB-SE algorithm identifies bad workers early, out of the 1100 workers, 600 workers are chosen with cost as 20 and quality as 2/3 whereas, the other 500 workers are chosen with the costs uniformly drawn between 10 and 20 and the quality uniformly drawn between 2/3 and 1. The required target accuracy is chosen to be 0.9 with α = 0.1. Since the value of ∆ can be arbitrarily low, we adopt the following strategy for the implementation. We solve the optimization problem with UCB for a target accuracy of 0.95 but check the lower confidence bound with target accuracy 0.9. This ensures that the constraint is never violated, however, it may result in extra cost of workers for the rest of the rounds. Since the costs of the workers are not adversarially chosen, the expected difference between the optimal set with accuracy 0.9 and 0.95 is not large. In general, if the requester gives a target accuracy range of (1−α, 1−α+ξ) such that the upper confidence bound is solved using accuracy 1 − α + ξ but the lower bound is checked with accuracy 1 − α, then it is possible to control the number of non-optimal rounds and it can be shown that the number of
non-optimal rounds is at most min (
1 16(h−1(ξ))2 ln ( 2n µ ) , 2(h−1(∆))2 ln ( 2n µ )) . In
the εt−greedy algorithm, the worker set is chosen such that the target accuracy of 0.9 is achieved with respect to the estimated qualities. For simulations, we have chosen T to be 104. However, if T is large, one can choose a smaller value of ξ. Over 1200 runs of simulations, we observed that none of the four algorithms violated the stochastic constraint with respect to the true qualities.
With ξ = 0.05, the comparison of the average regret and the negative social welfare is given in Figures 2 and 3 respectively. The regret is compared against the greedy solution returned by GA algorithm with true qualities. We ran 1000 samples to generate the graphs. We see that the algorithm CCB-NS converges much faster when compared to the εt−greedy algorithm. We also see that the cost of CCB-SE algorithm reduces significantly in a few iterations only. We also compare the total cost between CCB-NS algorithm and εt−greedy algorithm with change in the number of workers (Figure 4). The simulations show that the CCB-NS algorithm outperforms the εt−greedy algorithm even when there are fewer number of workers.
In order to train the text classifiers, we used wordaligned biblical texts in English and each of the languages considered along with the labels from WALS. For each rule discussed in Section 3.2, we select dependencies and create normalized feature vectors according to the discussed method. This forms the Text feature set in Section 4.
The natural place to begin a study of Dyna-style planning is with the policy evaluation problem of estimating a statevalue function from a linear model of the world. The model consists of a forward transition matrix F ∈ Rn × Rn (incorporating both environment and policy) and an expected reward vector b ∈ Rn, constructed such that Fφ and b>φ can be used as estimates of the feature vector and reward that follow φ. A Dyna algorithm for policy evaluation goes through a sequence of planning steps, on each of which a starting feature vector φ is generated according to a probability distribution µ, and then a next feature vector φ′ = Fφ and next reward r = b>φ are generated from the model. Given this imaginary experience, a conventional modelfree update is performed, for example, according to the linear TD(0) algorithm (Sutton 1988):
θ ← θ + α(r + γθ>φ′ − θ>φ)φ, (1)
or according to the residual gradient algorithm (Baird 1995):
θ ← θ + α(r + γθ>φ′ − θ>φ)(φ− γφ′), (2)
where α > 0 is a step-size parameter. A complete algorithm using TD(0), including learning of the model, is given in Algorithm 1.
This work demonstrated how standard sEMG based grasp classification benefits from the integration of the affordances of the manipulated objects. We proposed a method to automatically extract the object affordances from a first-person video recording of the scene and an estimate of the gaze position. The method identifies relevant gaze fixations on the
base of ocular and muscular activity. The objects observed during such fixations are segmented and their affordances are encoded into high-level visual features, extracted by an off-the-shelf Convolutional Neural Network. Despite we only conducted an offline evaluation of the method, the fixation detection has been designed to follow an online execution paradigm.
The method was evaluated on the data collected from intact subjects performing several of the most common grasps in activities of daily living. The acquisition protocol has been designed to simulate the prosthesis usage in a realistic environment. To ensure variability, we considered
grasps both in a static setting as well as when used to perform a functional task, while we took the limb position effect into account by repeating the movements while seated and standing. Furthermore, the same objects were associated to multiple grasps to enforce a many-to-many relationship between grasps and objects, and multiple objects were placed in the user’s field of view to encourage realistic gaze behavior.
Our tests confirmed that the integration of object affordances to the muscular activity of the forearm is indeed useful for grasp classification. The average prediction accuracy went from 80%, when using only the EMG cue, to 84%, when integrating EMG and vision. This improvement was considerable, as it involved uniformly all the subjects and all the grasp types. As expected, the contribution of vision was higher at the onset and the offset of the grasp, when the myoelectric cue is affected by motion artifacts. Finally, the analysis of the Movement Error Rate suggested that the performances of the multimodal classifier can be further reduced with a majority vote of the predictions at no expense of the prediction delay.
To test the symmetry-breaking hypothesis in more realistic networks, we conducted several experiments with deep fully-connected feedforward networks. In this section, we present the results of these experiments. We recall that the equations describing the plain networks are given by:
xl+1 = f(Wlxl + bl+1) (24)
The equations for the residual networks are given by:
xl+1 = f(Wlxl + bl+1) + Qlxl (25)
where Ql denotes the skip connectivity matrix, which can be different from the identity matrix, and the equations describing the hyper-residual networks are given by:
xl+1 = f(Wlxl + bl+1) + Qlxl + 1
l − 1
[ Ql−1xl−1 + . . .+ Q1x1 ] (26)
where every layer projects to all layers above itself. We divided the contribution from the non-adjacent layers by l− 1, because we found that this performed better than the non-normalized version. As usual, f(·) is chosen to be the ReLU nonlinearity. The networks all have 30 fully-connected hidden layers (20 layers in Figure 11) with n = 128 hidden units in each hidden layer.
Compared to existing architectures on influence diagrams, MCDAGs can be exponentially more efficient by strongly decreasing the constrained induced-width (cf Section 2.3), thanks to (1) the duplication technique, (2) the analysis of extra reordering freedoms, and (3) the use of normalizations conditions. One can compare these three points with existing works:
• The idea behind duplication is to use all the decompositions (independences) available in influence diagrams. An influence diagram actually expresses independences on one hand on the global probability distribution PC |D, and on the other hand on the global utility function. MCDAGs separately use these two kinds of independences, whereas a potential-based approach uses a kind of weaker “mixed” independence relation. Using the duplication mechanism during the MCDAG building is better, in terms of induced-width, than using it “on the fly” as in [Dechter, 2000].4
• Weakening constraints on the elimination order can be linked with the usual notion of relevant information for decision variables. With MCDAGs,
4E.g., for the quite simple influence diagram introduced in Section 3.1.1, the algorithm in [Dechter, 2000] gives 2 as an induced-width, whereas MCDAGs give an inducedwidth 1. The reason is that MCDAGs allow to eliminate both x1 before x2 in the subproblem corresponding to Ud,x2 and x2 before x1 in the subproblem corresponding to Ud,x1 .
this notion is not used only for decision rules conciseness reasons: it is also used to reveal reordering freedoms, which can decrease the time complexity. Note that some of the ordering freedom here is obtained by synergism with the duplication.
• Thanks to simplification rule S1Σ, the normalization conditions enable us not only to avoid useless computations, but also to improve the architecture structure (S1Σ may indirectly weaken some constraints on the elimination order). This is stronger than Lazy Propagation architectures [Madsen and Jensen, 1999], which use the first point only, during the message passing phase. Note that with MCDAGs, once the DAG of computation nodes is built, there are no remaining normalization conditions to be used.
Compared to existing architectures, MCDAGs actually always produce the best decomposition in terms of constrained induced-width, as Theorem 1 shows.
Theorem 1. Let wGp( p) be the constrained inducedwidth associated with the potential-based approach (cf Section 2.2). Let wmcdag be the induced-width associated with the MCDAG (cf Section 3.2). Then, wmcdag ≤ wGp( p).
Last, the MCDAG architecture contradicts a common belief that using division operations is necessary to solve influence diagrams with VE algorithms.
Having introduced all the preliminaries, we can finally define the measures used in our sample implementation of CoCoE. The first type of measures is based on complexity of the graph representations. We distinguish between local and global complexities. The global ones are associated with the graphs as a whole, and we compute specifically graph diameters, average shortest paths and node distributions along walks. The local measures associated with the walk envelopes are: (A) envelope size in nodes; (B) envelope size in biconnected components; (C) average component size in nodes; (D) average clustering coefficient of the walk nodes w.r.t. the envelope graph.
The coherences of walks are based on similarities. Let us assume a sequence of v1, v2, . . . , vn walk nodes. Then the particular coherences are: (E) taxonomybased start/end coherence simtax(v1, vn); (F) taxonomy-based product coherence Πi∈{1,...,n−1} simtax(vi, vi+1); (G) average taxonomy-based coherence 1 n−1∑
i∈{1,...,n−1} simtax(vi, vi+1); (H) distributional start/end coherence simcos(v1, vn); (I) distributional product coherence Πi∈{1,...,n−1}simcos(vi, vi+1); (J) av-
erage distributional coherence 1n−1 ∑ i∈{1,...,n−1} simcos(vi, vi+1). This family of measures helps us to assess how topically convergent (or divergent) are the walks. To compute walk entropies, we use the Tw, Ts taxonomies. By definition, the higher the entropy of a variable, the more information the variable contains. In our context, a high entropy value associated with a walk means that there is a lot of information available for agents to possibly utilise when processing the graph. The entropy measures we use relate to the following sets of nodes and types of clusters representing the context of the walks: (K) walk nodes only, top clusters; (L) walk nodes only, specific clusters; (M) walk and envelope nodes, top clusters; (N) walk and envelope nodes, specific clusters. The entropies of the sets (K-N) are defined using the notion of cluster size (cs(. . . )) introduced before. Given a set Z of nodes of interest, the entropy H(Z) is computed as
H(Z) = − ∑ Ci∈C?(Z) cs(Ci)∑ Cj∈C?(Z) cs(Cj) · log2 cs(Ci)∑ Cj∈C?(Z) cs(Cj) , where ? is one of T, S, for top or specific clusters, respectively.
We perform clustering experiments with the baseline document features (AW2V), tf-idf and our PH signatures. Figure 3 shows the B-Cubed precision, recall and F1-Score of each method (metrics as defined in Amigó et al. (2009)). To further assess the utility of PH embeddings, we concatenate them with AW2V to obtain a third representation, AW2V+PH.
With GMM and AW2V+PH, the F1-Score of clustering is 0.499. In terms of F1 and precision, we see that tf-idf representations perform better than PH, for reasons that we will discuss in later sections. In terms of recall, PH as well as AW2V perform fairly well. Importantly, we see that all the metrics for PH are significantly above the random baseline, indicating that some valuable information is contained in them.
The analysis here is similar to that for Lemma 1. Recall that in the proof of Theorem 2, we have proved that
γλ ≥ 2‖A⊤w∗‖2 √ c
m log
4n
δ ≥ 2 ∥∥∥(RR⊤ − I)A⊤w∗ ∥∥∥ ∞
(32)
holds with a probability at least 1− δ. Define L̂(λ) = −h(λ)− ŵ⊤ÂR⊤λ− γλ‖λ‖1. Using the fact that λ̂ maximizes L̂(·) over the domain ∆ and h(·) is β-strongly convex, we have
〈 λ̂− λ∗,∇h(λ∗) +RR⊤A⊤ŵ 〉 + β
2 ‖λ∗ − λ̂‖22 + γλ‖λ̂Ω̄λ‖1 ≤ γλ‖λ̂Ωλ − λ∗‖1. (33)
On the other hand, we have 〈 λ̂− λ∗,∇h(λ∗) +RR⊤A⊤ŵ 〉
=〈λ̂− λ∗,∇h(λ∗) +A⊤w∗〉+ 〈 λ̂− λ∗, (RR⊤ − I)A⊤w∗ 〉 + 〈 λ̂− λ∗, RR⊤A⊤(ŵ −w∗) 〉
(21) ≥ − ‖λ̂− λ∗‖1 ∥∥∥(RR⊤ − I)A⊤w∗ ∥∥∥ ∞ − ‖λ̂− λ∗‖2 ∥∥∥RR⊤A⊤(ŵ −w∗) ∥∥∥ 2 (32)
≥ − γλ 2 ‖λ̂− λ∗‖1 − ‖λ̂− λ∗‖2 ∥∥∥RR⊤A⊤(ŵ −w∗) ∥∥∥ 2 .
(34)
From (33) and (34), we have
β 2 ‖λ∗ − λ̂‖22 + γλ 2 ‖λ̂Ω̄λ‖1
≤3γλ 2
‖λ̃Ωλ − λ∗‖1 + ‖λ̂− λ∗‖2 ∥∥∥RR⊤A⊤(ŵ −w∗) ∥∥∥ 2
≤3γλ √ rλ
2 ‖λ̃Ωλ − λ∗‖2 + ‖λ̂− λ∗‖2 ∥∥∥RR⊤A⊤(ŵ −w∗) ∥∥∥ 2
≤‖λ̂− λ∗‖2 ( 3γλ √ rλ 2 + ∥∥∥RR⊤A⊤(ŵ −w∗) ∥∥∥ 2 )
which implies
‖λ∗ − λ̂‖2
≤ 2 β
( 3γλ √ rλ
2 + ∥∥∥RR⊤A⊤(ŵ −w∗) ∥∥∥ 2
)
≤ 2 β
( 3γλ √ rλ
2 + ‖A⊤(ŵ −w∗)‖2 + ∥∥∥(RR⊤ − I)A⊤(ŵ −w∗) ∥∥∥ 2
)
≤ 2 β
( 3γλ √ rλ
2 +
( 1 + ‖RR⊤ − I‖2 ) ‖A⊤(ŵ −w∗)‖2 ) .
In this method, we systematically occlude subsets of the input, forward propagate the masked input through the VQA model, and compute the change in the probability of the answer predicted with the unmasked original input. Since there are 2 inputs to the model, we focus on one input at a time, keeping the other input fixed (mimicing partial derivatives). Specifically, to compute importance of a question word, we mask that word by dropping it from the question, and feed the masked question with original image as inputs to the model. The importance score of the question word is computed as the change in probability of the original predicted answer.
We follow the same procedure on the images to compute importance of image regions. We divide the image into a grid of size 16 x 16, occlude one cell at a time with a gray patch2, feed in the perturbed image with the entire question to the model, and compute the decrease in the probability of the original predicted answer. The generated importance maps are shown in Fig. 2.
More results and interactive visualizations can be found on authors’ webpages.3
When selecting corpora for testing our methods, we primarily consider the labelling quality of the corpora, because the corpora’s quality heavily influences the quality of the ACD tools trained on them [Habernal and Gurevych, 2015]. The inter-rater agreement (IRA) score is a widely used metric to evaluate the reliability of annotations and quality of corpora.
Fleiss’ kappa [Fleiss, 1971] is among the most widely used IRA metrics, because it can compute the agreement between two or more raters, and it considers the possibility of the agreement occurring by chance, thus giving more “robust” measure than simple percentage agreement. If the Fleiss’ kappa score equals 1, it suggests the raters have “perfect agreement”; the lower the score, the poorer the agreement. In this work, all IRA scores reported are Fleiss’ kappa values.
Since there exist few well-annotated and publicly available argumentation corpora, we create our own argumentation corpus.1 We randomly sampled 200 hotel reviews of appropriate length (50 - 200 words) in the hotel review dataset provided by [Wachsmuth et al., 2014]. We presented these hotel reviews on a crowdsourcing platform, and asked five workers to independently annotate each review. Similar to [Wachsmuth et al., 2015], we viewed each sub-sentence as a clause. We asked the workers to label each clause as one of the following six categories: • major claim: summarises the main opinion of a review; • claim: an opinion on a certain aspect of a hotel; • premise: a reason/evidence supporting a claim; • background: an objective description that does not give
direct opinions but provides some background information; for example “this is my second staying at this hotel”, “we arrived at at midnight”; • recommendation: a positive or negative recommenda-
tion for the hotel, e.g. “do not come to this place if you want a luxury hotel”, ‘I would definitely come to this hotel again the next time I visit London’; and • others, for all the other clauses.
A detailed annotation guide and some examples were presented to the workers before they started their labelling. We asked the workers to give one and only one major claim for each hotel review, and informed them that a claim can have no premises, but each premise must support some claim. The annotating process lasts for 4 weeks, with 216 workers in total participated. We removed the annotations with obvious mistakes, and finally obtained annotations for 105 hotel reviews. In total, the corpus contains 1575 sub-sentences and 14756 tokens; some statistics are given in Table 1. Since the IRA for type others is lower than 0.5, we manually checked and calibrated all others annotations. Except for type others, all types have IRA scores above 0.6, suggesting that the agreement is substantial [Landis and Koch, 1977].
Another corpus we used to test our approach is the persuasive essays corpus proposed in [Stab and Gurevych, 2016]. This corpus contains 402 essays on a variety of different topics, and it has three argument component types: major claim,
1Details of the creation of our hotel corpus is presented in a separate paper, which is currently under review.
claim and premise; the IRA scores for these three argumentative types are 0.88, 0.64 and 0.83, resp.; however, the IRA for type others is not reported.
To the best of our knowledge, these two corpora are among the most well-annotated argumentation corpora (in terms of IRA scores). Some larger corpora, e.g. the one in [Levy et al., 2014], have much lower IRA scores (.39); the legal texts corpus proposed in [Palau and Moens, 2009] is not publicly available, and the web texts corpus proposed in [Habernal et al., 2014] has relatively low IRA (below .50) for most argument component types.
In this paper, we proposed a novel method for fast face recognition called ⁄ Regularized Sparse Representation using Hierarchical Feature Selection (HSR). By employing hierarchical feature selection, we can extract the local features from image, which improves recognition rate because local features are less sensitive to the facial variation. More importantly, the global dictionary can be easily compressed in the dimension and scale by hierarchical feature selection, which speeds up the computation of sparse representation. To be more specific, it is feasible to compress the scale of Gabor-feature based occlusion dictionary via sparse coding. And high-dimensional images and global dictionary can be rapidly compressed into low-dimensional feature space via ELM-AE. By introducing ⁄ regularized sparse representation, our method can produce sparser representation than regularized SRC, which in turn speeds up the face recognition. Besides, our method can also produce more robust representation than regularized SRC, which is more suitable to identify occluded faces such as AR sunglasses and scarves. We evaluated our method on a variety of face databases. Experimental results have demonstrated the great advantage of our method for computational cost in comparison with SRC and GSRC. Besides, we also achieve approximate or even better recognition rate. Therefore, our method has a great potential for the application of fast face recognition like real-time surveillance. Our future work will focus on two aspects. First, we will extend ELM-AE into Multi-Layer ELM-AE, which may extract more representative features in order to improve the recognition rate. Second, we will optimize the ⁄ regularization algorithm in order to reduce the computational cost further.
This study was partially supported by the LC536 grant of MŠMT ČR.
We now explain our approximate learning algorithm for the energy sharing problem. It is based on Q-learning and state aggregation. Although the straightforward Q-learning algorithm described in Section 4 requires complete state information and is not computationally efficient with respect to large state-action spaces, its state-aggregation based counterpart requires significantly less computation and memory space. Also our experiments show that we do not compromise much on the policy obtained either (see Fig. 9b).
For coordinate descent with SRRT in Algorithm 3, when λ = 0 we have
βk = (L+D)−1[XTy − Usk−1] (66)
sk = (1− αk)βk−1 + αkβk. (67)
When k ≥ 3, it can be shown that
βk − βk−1 = G[(1− αk−2)(βk−2 − βk−3) + αk−1(βk−1 − βk−2)]. (68)
When k = 2, we have β2 − β1 = α1G(β1 − β0). (69)
Using the recursion in Equation (68), we can get
β3 − β2 = [(1− α1)G+ α1Gα2G](β1 − β0). (70)
β4 − β3 =[α1G(1− α2)G+ (1− α1)Gα3G + α1Gα2Gα3G](β1 − β0).
(71)
Generally speaking, we can write
βk − βk−1 = PT k−1P−1(β1 − β0), (72)
where T k = diag(tk1 , t k 2 , . . . , t k p) is a diagonal matrix. For t k i , it is a polynomial function of δi; that is, tki = φk(δi), where
φk(t) = t× . . .× t︸ ︷︷ ︸ dk/2e k−dk/2e∑ i=0 ci × t× . . .× t︸ ︷︷ ︸ i ), (73)
and c0, c1, . . . , ck−dk/2e are dependent on α1, α2, . . . , αk−1. When k = 2, we have
c0 = α 1. (74)
When k = 3, we have c0 = 1− α1, c1 = α 1α2. (75)
When k = 4, we have
c0 = α 1(1− α2) + α3(1− α1), c1 = α 1α2α3.
(76)
When k = 5, we have
c0 = (1− α2)(1− α3), c1 = α
1(1− α2)α4 + α3(1− α1)α4 + α1α2(1− α3), c2 = α 1α2α3α4. (77)
For the coordinate descent with SRRT in Algorithm 3, we have
t291 = 0, t 29 2 < 0.000001, t 29 3 < 0.000001,
t294 = 0.000002, t 29 5 = 0.000393,
which are smaller than the ones in the traditional coordinate descent shown in Section 5.1.
In Section 4 we have shown that it is possible to learn neural networks over encrypted data and to apply neural networks to encrypted data. However, some scenarios may be infeasible due to excessive computational complexity. In this section we discuss practical considerations in more details.
While HE schemes allow the evaluation of polynomial functions, these computations are much slower than computations done on plain data. Furthermore, in current implementations of HE, high degree polynomials are slower to compute than lower degree polynomials. The reason for that, in a nut shell, is that as part of the encryption process some random noise is added to the message. When adding two numbers via the ⊕ operation, the noise in the resulting ciphertext increases linearly with respect to the number of additions, however, when multiplying, the noise grows super–linearly. For an FHE scheme, when the noise size reaches a certain level, a time consuming cleaning process is performed which slows down the entire process. For HE schemes as the one considered in this work, the parameters of the scheme have to be chosen to accommodate the noise growth incurred by the desired computation. A higher complexity requires larger parameters, which leads to slower execution of the algorithms. Therefore, special considerations should be taken to approximate the neural network with polynomials with the lowest degree possible.
Let N be a neural network with l layers. If the composition of the activation function and pooling functions in each layer is approximated by a polynomial of degree d then the polynomial approximation of N will be a polynomial of degree dl since when composing polynomials, the degrees of the polynomials multiply. Therefore, in order to end up with low degree polynomials, we need both d and l to be small. Minimizing d, the degree of the polynomial approximation to non-linear functions, is a standard exercise in approximation theory. Tools, such as, Chebyshev polynomials, can be used to find optimal or close to optimal approximations. Even more significant is minimizing the number of layers l. This goes against the current trend of learning deep neural networks. However, recent work on model compression (Bucilu et al., 2006; Ba & Caruana, 2014) show that deep nets can be closely approximated by shallow nets (1-2 hidden layers). These studies suggest that the success of deep nets might be due to better optimization and not necessarily from the kind of function space spanned by deep nets. Therefore, once you have a deep net, you can use it to train a shallow net by labeling a large set of unlabeled instances. This procedure converts deep nets to shallow, but wider, nets. In terms of polynomials, the deep nets convert to high degree polynomials while the shallow but wide nets convert to low degree polynomials with many monomials. Hence this conversions results in polynomials that are faster to execute on encrypted data.
While inference using crypto-nets may be feasible, learning is a more difficult to scale tasks. Training neural networks is a computational intensive task. Even without encryption, high throughput computing units such as GPUs or multi-node clusters are needed to make learning neural nets feasible on large datasets (Dean et al., 2012; Coates et al., 2013). Furthermore, assuming, as before, that the neural network has l layers such that each layer is approximated by a polynomial of degree d results in the neural network of degree dl. The gradient of this network, with respect to the weight vector, is a polynomial of the same degree. To make gradient step, the gradient polynomial is evaluated on the value predicted by the current network. Therefor, the gradient step is a polynomial of degree d2l. On top of that, the loss function needs to be taken into account which will make the degree even higher. Hence, learning from encrypted data in the way proposed here is feasible only for small datasets or for simple models such as linear models.
Additionally, we follow Hasan and Ng (2010) and analyse the precision-recall curves of TopicRank, KEA++ and TopicCoRank. To generate the curves, we vary the number of evaluated keyphrases (cutoff) from 1 to the total number of extracted/assigned keyphrases and compute the precision and recall for each cut-off. Such representation gives a good appreciation of the advantage of a method compared to others, especially if the other methods achieve performances in the Area Under the Curve (AUC).
Figure 2 shows the precision/recall curves of TopicRank, KEA++ and TopicCoRank on each dataset. The final recall for the methods does not reach 100% because the candidate selection method does not provide keyphrases that do not occur within the document, as well as candidates that do not fit the POS tag pattern /(N|A)+/. Also, because TopicRank and TopicCoRank topically cluster keyphrase candidates
and output only one candidate per topic, their final recall is lowered every time a wrong keyphrase is chosen over a correct one from the topic.
We observe that the curve for TopicCoRank is systematically above the others, thus showing improvements in the area under the curve and not just in point estimate such as f1-score. Also, the final recall of TopicCoRank is much higher than the final recall of TopicRank and KEA++.
The quantifying of how ultrametric a data set is by Rammal et al. (1985, 1986) was influential for us in this work. The Rammal ultrametricity index is given by ∑
x,y(d(x, y) − dc(x, y))/ ∑
x,y d(x, y) where d is the metric distance being assessed, and dc is the subdominant ultrametric. The latter is also the ultrametric associated with the single link hierarchical clustering method. The Rammal et al. index is bounded by 0 (= ultrametric) and 1. As pointed out in Rammal et al. (1985, 1986), this index suffers from “the chaining effect and from sensitivity to fluctuations”. The single link hierarchical clustering method, yielding the subdominant ultrametric, is, as is well known, subject to such difficulties.
For our experiments, we build a model as the one described in Section 3 using Theano (Bergstra et al., 2010). We choose the following parameters, context window size 2n + 1 = 5, vocabulary |V | = 100, 000, word embedding size M = 64, and hidden layer size H = 32. The intuition, here, is to maximize the relative size of the embeddings compared to the rest of the network. This might force the model to store the necessary information in the embeddings matrix instead of the hidden layer. Another benefit is that we will avoid overfitting on the smaller Wikipedias. Increasing the window size or the embedding size slows down the training speed, making it harder to converge within a reasonable time.
The examples are generated by sweeping a window over sentences. For each sentence in the corpus, all unknown words are replaced with a special token 〈UNK〉 and sentences are padded with 〈S〉, 〈/S〉 tokens. In case the window exceeds the edges of a sentence, the missing slots are filled with our padding token, 〈PAD〉.
To train the model, we consider the data in minibatches of size 16. Every 16 examples, we estimate the gradient using stochastic gradient descent (Bottou, 1991), and update the parameters which contributed to the error using backpropagation (Rumelhart et al., 2002). Calculating an exact gradient is prohibitive given that the dataset size is in millions of examples. We calculate the development error by sampling randomly 10000 minibatches from the development dataset.
For each language, we set the batch size to 16 examples, and the learning rate to be 0.1. Following, (Collobert et al., 2011)’s advice, we divide each layer by the fan in of that layer, and we consider the embeddings layer to have a fan in of 1. We divide the corpus to three sets, training, development and testing with the following percentages 90, 5, 5 respectively.
One disadvantage of the approach used by (Collobert et al., 2011) is that there is no clear stopping criteria for the model training process. We have noticed that after a few weeks of training, the model’s performance reaches the point where there is no significant decrease in the average loss over the development set, and when this occurs we manually stop the training. An interesting property of this model is that we did not notice any sign of overfitting for large Wikipedias. This could be explained by the infinite amount of examples we can generate by randomly choosing the re-
placement word in the corrupted phrase. Figure 2 shows a typical learning curve of the training. As the number of examples have been seen so far increased both the training error and the development error go down.
Several works have studied the problem of classifier fusion [1] [5], [13] [7] [16] [12]. A particularly popular formalism for combining outputs of classifiers is stacking [15]. Stacking in general is implied in any method which involves “learning” to combine the base classifiers. The fundamental idea of stacking is that the problem of combining the base classifiers can be cast as another learning problem. The outputs (say probabilities) of the base classifiers are treated as an input space to the stacking function, while the output space of the function remains the same as that of the base classifiers [9] [14]. The stacking framework learns the parameters of the stacking function to optimize classification accuracy, generally on some labeled training or held-out data. Our approach, on the other hand, does not optimize classification accuracy – the objective that is optimized is
an index called clarity, and makes no reference to the true labels of the data that it is optimized over. The combination function is optimized in an unsupervised manner over the actual test data. Moreover, we preform the optimization separately for each test instance.
To the best of our knowledge, few recent works have actually looked into instance-specific weight learning [8] [17]. Some of the most promising results are reported in [8]. The basic idea in this work is to propagate fusion weights of labeled instances to the individual unlabeled instances along a graph built on low-level features. The method has been shown to outperform other fusion methods on a variety of datasets. However, although the learned weights are instance specific, the method not only still requires a held-out set for which labels are known, it also requires knowledge of the low-level features of instances. On the other hand, our method does not require held-out data. Moreover, our solution is a meta algorithm that requires no knowledge of the low-level features of the instances. Another issue with [8] is that the weights learned for different test instances are not disjoint from each other. This has the undesirable aspect that newer test instances cannot be independently introduced into the set.
Given the distinctness of our approach, we focus on introducing and investigating our proposed instance-specific weight-learning paradigm, rather than demonstrating improvements over several other global fusion strategies. Unlike the other methods mentioned earlier, our solution does not require a separate held-out set. Also, the optimization of weights for each test instance is disjoint from other test instances. Finally, our method is as true meta algorithm that makes no reference to low-level features or how the classifiers were trained.
We also analyze important aspects of the fusion such as selecting only a group of good classifiers for an instance and the effects of noisy classifiers on the weight learning scheme and show that our proposed method is quite robust.

Let X be a crisp set. In the framework of Zadeh theory [17], a fuzzy set A is defined by the membership function ]1,0[→: XA . The non-membership function ]1,0[→: XA is obtained by negation and thus both functions define a partition of unity, namely:
1=+ AA  (2.1)
Atanassov has extended the fuzzy sets to the intuitionistic fuzzy sets [1]. Atanassov
has relaxed the condition (2.1) to the following inequality:
1 AA  (2.2)
He has used the third function, the intuitionistic fuzzy index A that verifies the equality:
AAA  1 (2.3)
Similarly, we can consider instead of (2.1) the following condition:
1AA+νμ (2.4)
Thus, we obtain the paraconsistent fuzzy set [14]. One can define the index of contradiction:
1AAA +ν=μκ (2.5)
More generally, in this paper, we will consider as bifuzzy set a set A , defined by two functions totally independent ]1,0[→: XA and ]1,0[→: XA . We consider the following two parameters:
the net truth: AAA   (2.6) the definedness 1 AAA  (2.7)
When 0A the information is inconsistent (overdefined) and when 0A the information is incomplete (undefined). In addition, from (2.6) and (2.7) it results the following inequality:
1||||  AA  (2.8)
We must observe that ||  is a distance between  and  . Also, ||  is a distance
between  and 1 . One can generalize the formulae (2.6) and (2.7). Let there be
2]1,0[, ba and aa 1 , bb 1 . We can consider the following equality:
1 111  
 

 


ba
ba
ba
ba
ba
ba (2.9)
If ||  a and |1|  b one obtains a new distance:
  |1|||1 |1|1|| 1 ),(          ba ba D (2.10)
and its equivalent form:  
|1|||1
||1|1|
1 )1,(

 

 

 
ba
ba D
Vasile Pătraşcu
It results, new formulae for  and  :
)( |1|||1
|1|1         (2.11)
)1( |1|||1
||1 

  

  (2.12)
The standard solutions to k-SVD include the power iteration algorithm and the Krylov subspace methods. Their time complexities are considered to be Õ(mnk), where the Õ notation hides parameters such as the spectral gap and logarithm of error tolerance. Here we introduce a simplified version of the block Lanczos method [19]1 which costs time O(mnkq), where q = log n is the number of iterations, and the inherent constant depends weakly on the spectral gap. The block Lanczos algorithm is described in Algorithm 5.1 can be implemented in 18 lines of MATLAB code.
1 f unc t i on [U, S , V] = BlockLanczos (A, k , q ) 2 s = 2 ∗ k ; % can be tuned 3 [m, n ] = s i z e (A) ; 4 C = A ∗ randn (n , s ) ; 5 Krylov = ze ro s (m, s ∗ q ) ; 6 Krylov ( : , 1 : s ) = C; 7 f o r i = 2 : q 8 C = A’ ∗ C; 9 C = A ∗ C;
10 [C, ˜ ] = qr (C, 0) ; % opt i ona l 11 Krylov ( : , ( i −1)∗ s +1: i ∗ s ) = C; 12 end 13 [Q, ˜ ] = qr ( Krylov , 0) ; 14 [ Ubar , S , V] = svd (Q’ ∗ A, ’ econ ’ ) ; 15 Ubar = Ubar ( : , 1 : k ) ; 16 S = S ( 1 : k , 1 : k ) ;
1We introduce this algorithm because it is easy to understand. However, as q grows, columns of the Krylov matrix gets increasingly linearly dependent, which sometimes leads to instability. Thus there are many numerical treatments to strengthen stability (see the numerically stable algorithms in [23]).
25
Algorithm 5.1 k-SVD by the Block Lanczos Algorithm. 1: Input: an m× n matrix A and the target rank k. 2: Set s = k +O(1) be the over-sampling parameter; 3: Set q = O(log n ) be the number of iteration; 4: Draw a n× s sketching matrix S; 5: C = AS; 6: Set K = [ C, (AAT )C, (AAT )2C, · · · , (AAT )q−1C ] ;
7: QR decomposition: [ QC︸︷︷︸ m×sq ,RC] = qr( K︸︷︷︸ m×sq ); 8: SVD: [ Ū︸︷︷︸ sq×sq , Σ︸︷︷︸ sq×sq , V︸︷︷︸ n×sq ] = svd(QTCA︸ ︷︷ ︸ s×n );
9: Retain the top k components of Ū, Σ, and V to form sq × k, k × k, n× k matrices; 10: U = QŪ ∈ Rm×k; 11: return UΣVT ≈ Ak.
17 V = V( : , 1 : k ) ; 18 U = Q ∗ Ubar ;
Although the block Lanczos algorithm can attain machine precision, it inevitably goes many passes through A, and it is thus slow when A does not fit in memory.
Facing large-scale data, we must trade off between precision and computational costs. We are particularly interested in approximate algorithm that satisfies:
1. The algorithm goes constant passes through A. Then A can be stored in large volume disks, and there are only constant swaps between disk and memory.
2. The algorithm only keeps a small-scale sketch of A in memory.
3. The time cost is O(mnk) or lower.
5.2 Prototype Randomized k-SVD Algorithm
This section describes a randomized algorithm that computes the k-SVD of A up to 1 + Frobenius norm relative error. The algorithm is proposed by [14], and it is described in Algorithm 5.2.
To create a rectified setup, we have fixed both cameras on a mounting plate and adjusted the RGB camera lens’ focal length to best match the field-of-view size of the multispectral camera.
In a first step the mosaic multispectral image is converted from its 2D data layout to a multispectral cube of 217 × 409 pixels and 25 channels. When overlaying the images some distortion differences become visible. To correct this, we infer a geometric transformation using the local weighted mean transform (LWMT) with the 12 closest points used to deduce a 2nd degree polynomial transformation for each control point pair based on a total of 33 correspondence pairs scattered all across the image. We use this transform to warp the multispectral image cube to the RGB image using bicubic interpolation, aligning the pixels of the two image sources, such that they can be stacked to a 28 channel image. Finally, the resulting image cube is cropped to 1082 × 1942 pixel, such that only areas where data from both sources is available remain as illustrated in Figure 2.
With the above mentioned procedure, we do not perform any debayering/demosaicking, for which a large variety of algorithms exist to make the visual perception of RGB images as pleasing as possible. Many of them cannot easily be adapted to non-RGB data, and the most straight-forward one would be to use bilinear interpolation for this as well. We have decided not to do so, because such an interpolation step can also be represented in the first convolution layer of a ConvNet, such that doing this explicitly before would primarily add to the overall computational effort without much benefit. The first convolution layer can also compensate for varying sensitivity of the individual spectral bands.
In this section, we analyze the asymptotic error rate for any positional ranking rule. We start by showing a tighter bound on the general error rate that can be derived from the proof of Theorem 2.
Fig. 1: An example of Petrie polygon (skew orthogonal projections) of three candidates. Three hyperplanes, under Borda count ranking rule, separate the simplex into six polytopes.
Lemma 3. An upper bound for the ranking error rate of any ( , δ)-differentially private positional ranking system with M candidates and N voters is( M
2
)√ 2(M !− 1)Q ( Nτ
2 √ 2 ln(2/δ)
) τ +Q ( Nτ√
2 ln(2/δ) ) for ∀τ > 0.
Proof: Since the Q-function is convex on [0,+∞), by Jensen’s Inequality, from Lemma 1 and Lemma 2, we have
PMe (N) ≤ ( M
2
) · 2 τ∫ 0 pD(l)Q ( l σ̂ ) dl +Q ( τ σ̂ )
≤ ( M
2
) · 2 τ∫ 0 pD(0)Q ( l σ̂ ) dl +Q ( τ σ̂ ) ≤ ( M
2
) · 2pD(0)Q ( τ 2σ̂ ) +Q ( τ σ̂ ) = ( M
2
)√ 2(M !− 1)Q ( Nτ
2 √ 2 ln(2/δ)
) τ
+Q ( Nτ√
2 ln(2/δ)
) . (28)
Lemma 3 slightly improves the bound in Theorem 2. We use this lemma to assist the proof of the following Theorem.
Theorem 3. For any positional ranking aggregation algorithm with M candidates, taking input from the ( , δ)-differentially private system defined in Section II-C,
lim N→∞
PMe (N) = 0
6 for any given and δ.
Proof: This directly follows from Lemma 3 and the Bounded Convergence Theorem.
We have discussed every basic concept to frame the question in a way akin to analysis. Mental states are physical states. The brain states in a human constitute its subjective experience. The question is whether a particular whole brain simulation, will have experience, and if it does, how similar this experience is to the experience of a human being. If the proponents of pan-experientialism are right, then this is nothing special, it is a basic capability of every physical resource (per the scientifically plausible, physicalist variant of pan-experientialism). However, we may question what physical states are part of human experience. We do not usually think that, for instance, the mitochondrial functions inside neurons, or the DNA, is part of the experience of the nervous system, because they do not seem to be directly participating in the main function of the nervous system: thinking. They are not part of the causal picture of thought. Likewise, we do not assume that the power supply is part of the computation in a computer.
This analogy might seem out of place, initially. If pan-experientialists are right, experience is one of the basic features of the universe. It is then all around us, however, most of it is not organized as an intelligent mechanism, and therefore, correctly, we do not call them conscious. This is the simplest possible explanation of experience that has not been disproven by experiment, therefore it is a likely scientific hypothesis. It does not require any special or strange posits, merely physical resources organized in the right way so as to yield an intelligent functional mind. Consider my “evil alien” thought experiment. If tonight, an evil alien arrived and during your sleep shuffled all the connections in your brain randomly, would you still be intelligent? Very unlikely, since the connection pattern determines your brain function. You would most likely lose all of your cognition, intelligence and memory. However, one is forced to accept that even in that state, one would likely have an experience, an experience that is probably meaningless and chaotic, but an experience nonetheless. Perhaps, that is what a glob of plasma experiences. The evil alien thought experiment supports the distinction between experience and consciousness. Many philosophers mistakenly think that consciousness consists
in experience. That, when we understand the “mystery” of experience, we will understand consciousness. However, this is not the case. Experience is part of human-like consciousness, indeed, however, consciousness also includes a number of high-level cognitive functions such as reasoning, prediction, perception, awareness, self-reflection and so forth [Minsky, 2006, Section 4]. I suggest that it is possible that there are minds without human-like consciousness and with experience (e.g., a special purpose neural network adding numbers), and experience without any recognizable mentality.


In this section, we analyze the IRM-SA method from the perspective of its automation. We identify individual steps that can be automated using natural language processing techniques, and propose suitable methods.
The individual goals of such an automation are: (i) (semi)automatically generate invariants in the IRM-SA model from the requirements document (and thus make synchronization and traceability between the requirements and IRM-SA model more robust and faster to obtain), (ii) (semi)automatically validate the resulting IRM-SA model.
TimeML (Pustejovsky et al., 2004) is a temporal annotation language. It may be used to annotate events, time expressions or timex’s (times, dates, durations), temporal relations between events and times (such as before or during), and signal expressions – words or phrases (such as conjunctions, adverbials) that provide information about temporal relations. TimeBank (Pustejovsky et al., 2003) is currently the largest TimeML-annotated gold standard corpus available, including over 6 000 temporal relation annota-
tions, as well as events, times and signals. It consists of around 65 000 tokens of English newswire text. TimeML offers the following definition of temporal signal. From the annotation guidelines1 :
A signal is a textual element that makes explicit the relation holding between two entities (timex and event, timex and timex, or event and event). Signals are generally:
• Temporal prepositions: on, in, at, from, to, before, after, during, etc.
• Temporal conjunctions: before, after, while, when, etc.
• Prepositions signaling modality: to.
• Special characters: “-” and “/”, in temporal expressions denoting ranges.
In cases where a specific duration occurs as part of a complex qualifier-head temporal signal, e.g. two weeks after, TimeBank has followed the convention that the signal head alone is annotated as a signal and the qualifier is annotated as a TIMEX of type DURATION.
2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.2 Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2.1 Feature-Based Representations . . . . . . . . . . . . . . . . . . . . 9 2.2.2 Semantic Networks . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.2.3 Semantic Space Representations . . . . . . . . . . . . . . . . . . . 11
2.3 Distributional Representations . . . . . . . . . . . . . . . . . . . . . . . . 12 2.3.1 Learning distributional representations . . . . . . . . . . . . . . . . 13 2.3.2 Weighting Techniques . . . . . . . . . . . . . . . . . . . . . . . . 14 2.4 Neural Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.5 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.5.1 LSA, LSI and LDA . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.5.2 Dimensionality Reduction Techniques . . . . . . . . . . . . . . . . 17 2.5.3 Similarity Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.6 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
Nous avons utilisé le vote à la majorité pour l’agrégation des résultats au sein d’une phrase. Nous avons testé deux approches différentes pour séparer les ex aequo :
Ordonnancement d’entités à l’âge des deux web 131
(i) le maximum de la moyenne de la précision des contributeurs (la précision d’un contributeur est une métrique proposée par la plateforme CrowdFlower et que nous avons mentionnée plus haut), (ii) la valeur la plus élevée. Nous avons pu constater a posteriori que ces deux choix donnaient des résultats très similaires lorsque le jeu de données est utilisé pour comparer des algorithmes d’ordonnancement d’entités étant donné un besoin d’information exprimé par une liste de mots clés. Nous avons utilisé la même stratégie de vote à la majorité pour agréger les résultats au niveau d’une page web.
Dans la prochaine section, nous introduisons LDRANK, un algorithme pour l’ordonnancement guidé par une requête d’entités du LOD. Le jeu de données dont nous venons de décrire la construction sera utilisé à la section 4.6 pour évaluer LDRANK et le comparer à l’état de l’art.
As described above, KRE will make accurate predictions for level i if the nodes at that level actually obey the unconditional heuristic distribution P (v). As i increases, the distribution of heuristic values will start to converge to P (v). The rate of convergence depends upon the state space. It is believed to be fairly slow for the sliding-tile puzzles, but faster for
Rubik’s Cube. If the convergence occurs before the IDA* threshold is reached KRE will provide accurate predictions for any set of start states (including single start states).
In order to experimentally test this we repeated the KRE Rubik’s Cube experiment but, in addition to using a large set of random start states, we also looked at the individual performance of two start states, s6, which has a low heuristic value (6), and s11, which has the maximum value for the heuristic used in this experiment (11). As in KRE we used the 8-6-6 heuristic which takes the maximum of 3 different PDBs (one based on all 8 corner cubies and two based on 6 edge cubies each). This heuristic is admissible and consistent. Over the billion random states we sampled to estimate P (v) the maximum value was 11 and the average value was 8.898.
Table 3 presents the results. The KRE column presents the KRE prediction and the Multiple start states columns presents the actual number of states generated (averaged over a set of random start states) for each IDA* threshold. Both columns are copied from the KRE journal paper (Korf et al., 2001). The Ratio columns of Table 3 shows the value predicted by the KRE formula divided by the actual number of nodes generated. This ratio was found to be very close 1.0 for multiple start states, indicating that KRE’s predictions were very accurate.
The results for the two individual start states we tested are shown in the “Single start state” part of the table. Note that both states are optimally solved at depth 17, but, as in KRE, the search at that depth was run to completion. In both cases the KRE formula was not accurate for small thresholds but the accuracy of the prediction increased as the threshold increased. At threshold d = 17 the KRE prediction was roughly a factor of 2 too small for s6 and about 10% too large for s11. This is a large improvement over the smaller thresholds. These predictions will become even more accurate as depth continues to increase.
The reason the predictions improve for larger values of d is that at deeper depths the heuristic distribution within a single level converges to the unconditional heuristic distribution. Using dashed and dotted lines of various types, Figure 5(a) shows the distribution of heuristic values seen in states that are 0, 1, 2 and 4 moves away from s6. The solid line in Figure 5(a) is the unconditional heuristic distribution. The x-axis corresponds to different heuristic values and the y-axis shows the percentage of states at the specified depth with heuristic values less than or equal to each x value. For example for depth 0 (which includes
the start state only) only a heuristic value of 6 was seen (leftmost curve). For depth 1, heuristic values of 5, 6 and 7 were seen (second curve from the left), and so on. The figure shows that the heuristic distribution at successive depths converges to the unconditional heuristic distribution (rightmost curve in Figure 5(a)). At depth 17 (not shown), the heuristic distribution is probably quite close to the unconditional heuristic distribution, making the KRE prediction quite accurate even for this single start state.
Figure 5(b) shows the heuristic distributions for nodes that are 0, 1, 2, and 4 moves away from s11. In this case the unconditional heuristic distribution is to the left of the heuristic distributions for the shallow depths, with the heuristic distribution for depth 0 being the rightmost curve in this figure. Comparing parts (a) and (b) of Figure 5 we see that the convergence to the unconditional heuristic distribution is faster for s11 than for s6, which explains why the KRE prediction in Table 3 is more accurate for s11.
4. Conditional Distribution and the CDP Formula
We now present our new formula CDP (Conditional Distribution Prediction), which overcomes the two shortcomings of KRE described in the previous section. An important feature of CDP is that it extends the unconditional heuristic distribution of heuristic values P (v) used in KRE to be a conditional distribution.
Timeline Generation. The task of timeline generation is firstly proposed by Swan and Allan[27] by using named entities for sentence selection in different time epochs. Since then, researchers tried to generate timeline by considering different respects such as usefulness and novelty[3] or interest and burstiness[10]. Yan et al.[39] extended the graph based sentence ranking algorithm in multi-document summarization (MDS) to timeline generation by projecting sentences from different temporal corpus into one plane. Another work from Yan et al.[38] transforms
this task to an optimization problem by considering the relevance, coverage and coherence of the sentences. Existing approaches seldom explore the topic information lied in the corpus or the structure of news information. Li et al [17] exploited topic model to capture the dynamic evolution of topics in timeline generation.
Topic Models. Recently, bayesian topic models such as LDA[6] or HDP[29] have shown the power text mining for its clear probabilistic interpretation. In topic model, each document is denoted as a mixture of different topics and each topic is presented as a distribution over words. hLDA [15] extends LDA model to a multi-level tree structure where each node is associated with a topic distribution over words. It can be inferred from a nested Chinese restaurant process based one MCMC strategy. HDP can be used as an LDA-based topic model where the number of clusters can be automatically inferred from data. Therefore, HDP is more practical when users have little knowledge about the content to be analyzed.
Learning from time-correlated corpus. Learning evolutionary topics from a time-correlated corpus aims to preserve the smoothness of clustering results over time, while fitting the data of each epoch [1, 8, 9, 19, 36, 37]. Among exsting works, the approaches in [1, 37, 36] utilized time-dependent DP for topic modeling, while others focused on extending LDA to dynamic topic models [5, 31, 33] . In fact, incorporating time dependencies into DP mixture models is a hot topic in the research of Bayesian nonparametric [7, 22, 24, 34, 42]. For instance, Wang et al. [34] focused on detecting the simultaneous busting of some topics in multiple text streams. Zhang et al. [35] further extended [34] where they adjusted the timestamps of all documents to synchronize multiple streams and then learned a common topic model. Wang et al. [42] introduced an approach based on Hierarchical Dirichlet Process [29] to discover interesting cluster evolutionary pattern from correlated time-varying text corpora.
The cell of type B is a special case of the LeakyLP cell. When we set ypω0 = y p ω1 = 0.5 there is a direct relation between the cutoff frequency of a discrete Butterworth lowpass filter and the activation of ypφ: Let fcutoff be the frequency, where amplitude response is reduced to 1√ 2 of the
maximal gain. We can calculate fcutoff by
fcutoff = 1
π arctan ( 1− ypφ 1 + ypφ ) (42)
⇔ ypφ = 1 + tan(πfcutoff)
1− tan(πfcutoff)
with the bounds fcutoff ∈ (0, 0.5) and ypφ ∈ (−1, 1) (for more details see Schlichthärle, 2000, 2.2;6.4.2). For ypφ ∈ (0, 1) we get fcutoff ∈ (0, 0.25). In Figure 7 (left) we can see, that even for a negative value of ypφ and a highpass characteristic of H1(z) the impulse response H(z) has a lowpass characteristic.
Theano offers the ability to define symbolic loops through use of the Scan Op, a feature useful for working with recurrent models such as recurrent neural networks, or for implementing more complex optimization algorithms such as linear conjugate gradient.
Scan surmounts the practical difficulties surrounding other approaches to loop-based computation with Theano. Using Theano’s symbolically-defined implementations within a Python loop prevents symbolic differentiation through the iterative process, and prevents certain graph optimizations from being applied. Completely unrolling the loop into a symbolic chain often leads to an unmanageably large graph and does not allow for “while”-style loops with a variable number of iterations.
The Scan operator is designed to address all of these issues by abstracting the entire loop into a single node in the graph, a node that communicates with a second symbolic graph representing computations inside the loop. Without going into copious detail, we present a list of the advantages of our strategy and refer to section 4.3 where we empirically demonstrate some of these advantages. Tutorials available from the Theano website offer a detailed description of the required syntax as well as example code.
1. Scan allows for efficient computation of gradients and implicit “vector-Jacobian” products. The specific algorithm used is backpropagation through time Rumelhart et al. (1986), which optimizes for speed but not memory consumption.
2. Scan allows for efficient evaluation of the R-operator (see Pearlmutter (1994)), required for computing quantities such as the Gauss-Newton approximation of Hessian-vector products.
3. The number of iterations performed by Scan can itself be expressed as a symbolic variable (for example, the length of some input sequence) or a symbolically specified condition, in which case Scan behaves as a “do while” statement. If the number of steps is fixed and equal to 1, the Scan node is “unrolled” into the outer graph for better performance.
4. Any loop implemented with Scan can be transparently transferred to a GPU (if the computation at each iteration can itself be performed on the GPU).
5. The body of Scan (which involves computing indices of where to pick input slices and where to put the output of each iteration) is implemented with Cython to minimize the overhead introduced by necessary bookkeeping between each iteration step.
6. Whenever possible, Scan detects the amount of memory necessary to carry out an operation: it examines intermediate results and makes an informed decision as to whether such results are needed in subsequent iterations in order to partially optimize memory reuse. This decision is taken at compilation time.
7. Loops represented as different Scan instances are merged (given that certain necessary conditions are respected, e.g., both loops perform the same number of steps). This aids not only in reducing the overhead introduced by each instance of Scan, but also helps optimize the computation performed at each iteration of both loops, e.g. certain intermediate quantities may be useful to the body of each individual loop, and will be computed only once in the merged instance.
8. Finally, whenever a computation inside the loop body could be performed outside the loop, Scan moves said computation in the main graph. For example element-wise operations are moved outside, where, given that they are done by a single call to an Elementwise operations, one can reduce overhead. Another example is dot products between a vector and a matrix, which can be transformed outside of the loop into a single matrix-matrix multiplication. Such optimizations can lead to significant speed improvement and in certain cases to the elimination of the Scan node completely.
All of these features make it easier for a user to implement a variety of recurrent neural networks architectures, and to easily change the equations of the model without having to derive gradients by hand or worry about manually optimizing the implementation.
This chapter discusses how to scale up the EASGD method to hundreds and thousands of processors. In Section 6.1, we first propose a tree-structured extension of the EASGD method called EASGD Tree. The basic idea and the design principle are discussed in Section 6.1.1, and the numerical results are presented in Section 6.1.2. We present two different communication schemes for the EASGD Tree method. As we had seen the advantage of EAMSGD, we also accelerate EASGD Tree with Nesterov ’s momentum method. In Section 6.2, we unify the DONWPOUR method and the EASGD method by considering a Gauss-Seidel reformulation of the EASGD update rules in the synchronous scenario. This unification suggests the possibility of using both DONWPOUR and EASGD under the EASGD Tree. It also suggests that in-between the DONWPOUR and the EASGD method there may be some even better method.
6.1 EASGD Tree
The original motivation of EASGD Tree is to run SGD at multiple time scales, where each scale corresponds to the use of a different learning rate. It naturally gives rise to a hierarchical tree structured organization of the processors. In literature, the tree
112
idea has shown up in various contexts. For example, the tree was used to scale up the asynchronous SGD method by aggregating the delayed gradients computed by the intermediate nodes and the leaf nodes [2]. It can also be used to efficiently implement the Broadcast/AllReduce operation as in MPI [1, 62]. The benefit of using a tree is that the number of links to connect a very large number of nodes is minimal. The trade-off is that the connectivity of the whole tree is not robust to the link failure. However, the tree structure has its own charm for its simplicity. The main theoretical challenge is to understand its convergence property in terms of the root of the tree. This still remains open.
To show the results both intuitively and qualitatively, we report results on the first six data in the form of table and the rest with figures. The results of different methods on different data sets are presented in Table 2 and Fig. 2 respectively.
There are several observations from these results.
(1) When we use the classifier trained in C-stage to assist learning in E-stage, the testing performance will increase significantly, especially when the training points in E-stage are rare. This is consistent with intuition since the assistance from C-stage will be weaker with the increase of training points.
(2) Compared with the accuracy of SVM(a), our results have a remarkable improvement. This validates that our methods could, to some extent, inherit the metric from C-stage.
(3) It seems that the improvement of our method with respect to other approaches is much larger in multi-class scenario. The reason may be that the binary classification accuracy is high enough and it is hard to make further improvement.
(4) Compared OPID with OPIDe, it seems that OPID performs slightly better than OPIDe in most data sets. The t-test results show that their performances tie in most cases. Nevertheless, their performances are also data dependent.
(5) It seems that our method achieves more significant improvement on biological data sets (DNA,
Splice, Protein) than image data sets (Mnist, Gisette and Satimage). It may be caused by the fact that the biological data is more time dependent than the image data and it is more consistent with our settings.
Prior work [6] has shown that using convolutional architectures for both the generator and discriminator networks is key to generating images with GANs. While individual discriminators in our framework see projected inputs that are lower-dimensional than the full image, their dimensionality is still large enough (a very small m would require a large number of discriminators) to make it hard to train discriminators with only fully-connected layers. Therefore, it is desirable to employ convolutional architectures for each discriminator.
To do so, the projection matrices WTk must be chosen to produce “image-like” data. Accordingly, we use downsampled convolutions with random filters to embody the projections WTk (as illustrated in Fig. 1). The elements of these convolution filters are drawn i.i.d. from a Gaussian distribution, and the filters are then scaled to have unit `2 norm. Note that this imposes a block-Toeplitz structure on WTk , and they are no longer purely random as assumed in above. We partially address this by choosing filter sizes that are larger than the downsampling rate to promote more “mixing” of the input coordinates (e.g., we use 8×8 filters when using a stride of 2). We find that our strategy works well in practice, and that the benefits of using convolutional architectures for each discriminator Dk outweigh the drawbacks of not using projections that are purely random.
∗. In submission to JAIR 1. We reported our initial work on detection of information-dense text in a paper published at AAAI
2014 (Yang & Nenkova, 2014) In this manuscript we have further extended the work by using much larger samples of New York Times articles for training and by analyzing the performance of a larger number of two-layer classifiers. Here we also introduce a new collection of manually annotated test data, for both domain-dependent and general content-density of texts. We make use of this collection for detailed evaluation of the content-density classifiers. With the publication of this manuscript, we will make the classifiers and data available for use by others In our AAAI 2014 paper, we use the term information-dense to describe the types of text we wish to detect. Here we switch the terminology to content-dense, to avoid confusion with work in the intersection of cognitive science and computational linguistics that uses the term information density in information-theoretic sense to describe the change in surprise in the linear processing of sentences (Jaeger, 2010; Pate & Goldwater, 2015).
ar X
iv :1
70 4.
00 44
0v 1
[ cs
.C L
] 3
A pr
Analagous to activation for knowledge chunks, the measure of memory strength of a production rule is the production strength, S. In [4], S is supposed to be the log-odds that the production rule will fire, and it is defined as:
S(t) = ln ∑ k t−dk +B (5)
where the k indexes events in which the production fires ([4], p. 293), and B is a constant. (It is unclear from [4] if the B in 5 is the same B as in 4.)
In this approach, we assume each ASCII grapheme acts as a phoneme. We assume that the DNN will learn to map these “phonemes” to speech sounds. We normalize the data to lowercase and remove all punctuation marks. This ensures that the phone-set contains 26 letters and an extra /sil/ phone to mark beginning and end of the sentence.
In this article, we presented the first study of algorithm portfolios for the TTP. We first studied the performance of 21 existing TTP algorithms on the full original set of 9720 TTP instances created by Polyakovskiy et al [41] and defined 55 instance features for TTP. Then, we studied various different approaches for the resulting algorithm selection problem, showing very substantial improvements over the single best algorithm and closing the gap between it and an omniscient oracle by 90%. Finally, we studied which algorithms contribute most to the portfolio, finding that the algorithms with best average performance (e.g.
the complex ones C3–C6 and MATLS, and the swarm-intelligence approaches that start with M) were quite important for the portfolio because of their performance on small and mid-sized TTP instances. Interestingly, the relatively simple heuristic S5 continues to dominate in particular on the large TTP instances and thus is one of the most important contributors to well-performing portfolios. Despite this general trend, the algorithm with the worst average performance, CS2SA, added substantial benefit on top of all other algorithms. An analysis of the feature importance revealed that the values for the five most important features can be extracted from the instance definition in constant time. The resulting portfolio that uses only this subset has a performance comparable to the one that uses all 55 features that can take hours to compute.
In future work, we aim to study which features make TTP instances hard for which algorithm and why, and whether we can identify a smaller representative subset of TTP instances to speed up future benchmarking studies.
Table 1 lists the features we use for discovering both the decision cue relations and decision content relations. We start with a collection of domainindependent BASIC FEATURES shown to be useful in relation extraction (Banko and Etzioni, 2008; Chen et al., 2011). Then we add MEETING FEATURES, STRUCTURAL FEATURES and SEMANTIC FEATURES that have been found to be good predictors for decision detection (Hsueh and Moore, 2007) or meeting and decision summarization (Gal-
ley, 2006; Murray and Carenini, 2008; Fernández et al., 2008; Wang and Cardie, 2011). Features employed only for argument’s are listed in the last category in Table 1.
After applying the features in Table 1 and the global constraints from Section 5 in preliminary experiments, we found that the extracted relation instances are mostly derived from decision cue relations. Sample decision cue relations and instances are displayed in Table 2 and are not necessarily surprising: previous research (Hsueh and Moore, 2007) has observed the important role of personal pronouns, such as “we” and “I”, in decision-making expressions. Notably, the decision cue is always followed by the decision content. As a result, we include two additional features (see Table 3) that rely on the cues to identify the decision content. Finally, we disallow content relation instances with an argument containing just a personal pronoun.

In this section, we describe details of the experiments for the proposed TPRNmodel on question answering task using the Stanford’s SQuAD dataset [3]. The results of interest are the interpretations of the learned representations, discussed at length in Sec. 5. 1
Please note that the goal of this work is not to beat the state-of-the-art system on SQuAD (at the time of writing this paper, r-net, [12]), but to create a question answering system that is interpretable, by exploiting TPR. Therefore, as long as we have a system with TPR cells that performs reasonably well, we can use it to provide proof of concept for the interpretability claims in this work.
SQuAD is a reading comprehension dataset for question answering. It consists of more than 500 Wikipedia articles and more than 100,000 questionanswer pairs about them, which is significantly larger than previous reading comprehension datasets [3]. The questions and answers are human-generated. The answer to each question is determined by two pointers in the passage, one pointing to the start of the answer and the other one pointing to its end. Two metrics that are used to evaluate models on this dataset are Exact Match (EM) and F1 score.
1The codes are available at https://github.com/Palang2014/QA_TPR_Public
For the experiments, we used the same settings reported in [1] for all layers of TPRNexcept the phrase embedding layer. (Note that these hyperparameters had previously been optimized for the original model, and were not re-optimized for TPRN.) The phrase embedding layer is replaced by our proposed recurrent TPR cells. The full setting of the TPRNmodel for experiments is as follows:
• Questions and paragraphs were tokenized by PTB tokenizer.
• The concatenation of word embedding using GLOVE [9] and character embedding using Convolutional Neural Networks (CNNs) was used to represent each word. The embedding vector for each character was of length 8 (1-D input to the CNN) and the output of the CNN’s max-pooling layer over each word was a vector of length 100. The embedding size of word embedding using GLOVE was also set to 100.
• For TPR, 100 symbols and 20 roles were used. Embedding size of 10 for both symbol vectors and role vectors was used. This means that the matrix S in (1) was of size 10× 100 and the matrix R in (1) was of size 10× 20. We used vec(v(t)) as the output of our phrase embedding layer. vec(.) vectorizes the given matrix.
• The weight cQ = 0.00001 was used for both filler-related and rolerelated terms in (2).
• The optimizer used was AdaDelta [13] with 12 epochs.
Each experiment for the TPRNmodel took about 13 hours on a single Tesla P100 GPU. Performance results of our single model compared with a single model in [1] are presented in Table 1. From this table we observe that our proposed TPR based model underperforms [1] by about 2 points. This is good enough performance for our proposed TPRNmodel to assume that it has learned appropriate representations for questions and paragraphs. Focussing on queries, we explore the representations in the learned TPRs in considerable detail. Sec. 5.1.2 mentions a few selected, highly targeted performance comparisons against the POS tagging provided by the Stanford parser [2].
The refinement process used to steer the computation of an A-tSNE embedding works on a per-point basis, see Sec. 4.3. We propose three different strategies that can be used to select the points to be refined.
The very basic strategy is to refine the neighborhoods of all the points in X in a random order. When computational resources are sparse, however, it makes sense to steer the refinement process to increase precision in areas of the embedding that the analyst finds interesting, e.g., based on initial visual clusters appearing in the embedding.
In this paper, we constructed a novel CRNN model which employs prominent feature-filtering from Char-CNN and long-term sequence understanding from Char-RNN. Our framework automatically learns grammatical errors and misspelling through subword information. Our architecture also benefits from the latest development on GRU unit, which reduces algorithmic runtime without compromise of the performance.
In general, we showed the results independently for each benchmarking data stream. Evaluation on precision, recall, and F1 score were demonstrated for each of our data-sets. Our experiment was conducted on a cross-validated training and testing data split. The participants for our experiment are listed in Table 4. There were 20 frameworks in total for our experimental evaluation. The token random indicates the arbitrary initialisation of word vector.
As shown in Figure 3a, 3b, and 3c (Google-news), our model ranked fifth on precision rate, with 2.44% less than the best one. For the recall rate and F1 score, our model yielded first and third respectively. Our architecture achieved 2.62% more recall and 0.57% less F1 score compared with the best one.
For twenty-news-groups collection, we referred to Figure 3d, 3e, and 3f. Our algorithm achieved the optimal precision rate, recall ratio, and F1 score, leading the next best with 3.68%, 1.00%, and 2.20% respectively. Our algorithm yielded a similar precision rate and recall rate. For the F1 score, the lowest score 34.65% was from Word-RNN and random embeddings combination. Our algorithm obtained much better precision rate, recall ratio, and F1 score than Word-RNNs and Word-CNNs.
For Brown Corpus, observations can be derived from Figure 3g, 3h, and 3i. In general, our framework achieved the next best on precision rate, 0.54% less than the CharCNN. CRNN ranked third on recall rate, having 1.40% difference to the optimum one. Our model obtained the best F1 score, with 0.05% more than the next best. From Figure 3g, 3h, and 3i, word level neural networks was worse than our model.
For question classification corpus, precision rate, recall rate, and F1 score were displayed in Figure 3j, 3k, and 3l respectively. From Figure 3j, our model ranked ninth on precision rate, with 2.31% less than the best performer. For the recall ratio and F1 score, our algorithm ranked first on both, achieving 4.23% and 0.77% more than the next best algorithm.
We have previously mentioned about the performance comparison on our model over the competitors. We now analyse the performance difference on Word-CNNs, LinearSVMs [30], KNN-WMDs, and KNN series.
The average precision rate and F1 score for WordCNN+word2vec was 0.91% and 0.32% higher than WordCNN+GloVe. Based on such a small scale data-set, this kind of difference was significant. Similarly, for Word-RNNs, the mean precision and F1 score for word2vec based was 1.06% and 0.47% higher than the Glove sponsored. The precision rate, recall rate, and F1 score difference between WordCNN+GloVe and Word-CNN+random was 0.31%, 0.12%, and 0.28% respectively. For Word-RNN+random and WordRNN+GloVe, the performance difference on precision rate, recall ratio, and F1 score was 2.64%, 3.43%, and 3.10% respectively.
For non-neural networks, we emphasized the LinearSVMs, KNN-WMDs and traditional KNNs. Sent2vec [32] encoding schema tries to interpret the sentential information into a single skip-thought vector rather than word level embeddings. From the corresponding results in Figure 3, we observed that the performance of LinearSVM+sent2vec was the worst in its series for all four benchmarking collections. This phenomenon also applied to KNN-WMD+sent2vec and KNN+sent2vec.
We explain non-neural networks in the context of
word2vec and GloVe. For LinearSVM, both produced resemblant precision, recall, and F1 score. The only exception was question classification collection where word2vec encoding yielded 8.29% more recall and 5.11% more F1 score. For the KNN-WMDs, two embeddings posed almost identical impact under each measurement excepted the precision and F1 score on twenty-news-groups. Word2vec version yielded 6.15% more precision and 4.04% more F1 score. KNNs followed the above tendency. The only exception was twenty-news-groups of which word2vec version produced 5.99%, 7.00%, and 6.65% more precision, recall, and F1 score respectively.
We now analyse the evaluation results for KNN-WMDs and KNNs. The former one utilised WMD as the spatial distance function, the latter one applied plain cosine distance. From Figure 3, we can perceive that WMD dominant the contest over precision, recall, and F1 score. The average precision for the former one was 42.17, the latter one was 39.27. For the average recall rate, WMD sponsored models achieved 43.39% and plain KNNs obtained 37.43%. For the mean F1 score, the former one yielded 42.30%, the latter one produced 38.02%. It is obvious that WMD was much better at measuring the spatial dissimilarity.
Referring to Figure 3, LSTM, GRU, and MGU produced almost equivalent result across four data-sets. This inspired us to conduct an additional experiment involving runtime. As can be perceived from Figure 4, MGU had the minimal average runtime under each benchmarking test.
Suppose we have some equation candidate
0 = A1 + · · ·+As.
We then evaluate this equation for the r virtual experiments, obtaining the numerical values of all Fj , and thus Ai. As a result, we can evaluate for the constants k2, . . . , ks by solving the resulting matrix equation

  −A1,1 ...
−A1,r

  =

  A2,1 . . . As,1 ... . . . ...
A2,r . . . As,r

 

  k2 ... ks

  . (5)
Equation (5) is equivalent to the matrix equation

  0 ... 0

  =

  A2,1 . . . As,1 ... . . . ...
A2,r . . . As,r

 

  k2 ... ks

  −

  −A1,1 ...
−A1,r

  . (6)
Definition 2. Let a data matrix of equation candidate
0 = A1 + · · ·+As.
with r experiments be defined as

  A2,1 . . . As,1 ... . . . ...
A2,r . . . As,r

  (7)
Definition 3. Let A∗ denote the conjugate transpose of A such that if
A =

  A2,1 . . . As,1 ... . . . ...
A2,r . . . As,r

  ,
A∗ =

  A2,1 . . . A2,r ... . . . ...
As,1 . . . As,r

  . (8)
where Ai,j is defined as the complex conjugate of Ai,j .
Definition 4. The Moore-Penrose left pseudoinverse of A ∈ M(m,n,R) is defined as A+ ∈ M(n,m,R),m, n ∈ Z+ such that
A+A = I, (9)
the n× n identity matrix [19]. If the columns of A are linearly independent, then the Moore-Penrose left pseudoinverse is calculated as
A+ = (A∗A)−1A∗ (10)
such that A+A = ((A∗A)−1A∗)A = (A∗A)−1(A∗A) = I ([19], Theorem 2).
We discuss some practical issues of the two models. Convergence. In the above two models, after plugging in the reinforced random walk process within the EM or Gibbs sampling process, there are no explicit objective functions any more. We empirically prove that both the number of active topics and the data likelihood will converge (see Figure 2(d) and 2(h)). We leave the theoretical justification of the convergence as the future work. How to set the parameter α? The parameter α controls the transition probability to the neighbor topics or staying on itself. It takes similar effect as the step size in the gradient descent method, and hence in practice a small α (e.g., 0.1) can be used. How to set the parameters γ and K? We empirically show that the performance of the two models are not sensitive to the parameters γ and K (See Figure 5 and 6). In practice, K can be set to be very large to let the data overfitted. γ can be usually set within [1, 2] for DivPLSA and [0.6, 1.5] for DivLDA. Scalability. Both DivPLSA and DivLDA can be easily scaled by making use of existing large-scale topic modeling techniques. The E-step of DivPLSA can be parallelized by assigning the documents to different processors or nodes. A scaled version of DivLDA can be built on top of the existing large scale LDA model, e.g., the yahoo-LDA model in [2].
While training BNNs on the ImageNet dataset we noticed that we could not force the training set error rate to converge to zero. In fact the training error rate stayed fairly close to the validation error rate. This observation led us to investigate a more relaxed activation quantization (more than 1-bit). As can be seen in Table 2, the results are quite impressive and illustrate an approximate 5.6% drop in performance (top-1 accuracy) relative to floating point representation, using only 1-bit weights and 2-bit activation. Following Miyashita et al. (2016), we also tried quantizing the gradients and discovered that only logarithmic quantization works. With 6-bit gradients we achieved 46.8% degradation. Those results are presently state-of-the-art, surpassing those obtained by the DoReFa net (Zhou et al., 2016). As opposed to DoReFa, we utilized a deterministic quantization process rather than a stochastic one. Moreover, it is important to note that while quantizing the gradients, DoReFa assigns for each instance in a mini-batch its own scaling factor, which increases the number of MAC operations.
While AlexNet can be compressed rather easily, compressing GoogleNet is much harder due to its small number of parameters. When using vanilla BNNs, we observed a large degradation in the top-1 results. However, by using QNNs with 4-bit weights and activation, we were able to achieve 66.5% top-1 accuracy (only a 5.5% drop in performance compared to the 32-bit floating point architecture), which is the current state-of-the-art-compression result over GoogleNet. Moreover, by using QNNs with 6-bit weights, activations and gradients we achieved 66.4% top-1 accuracy. Full implementation details of our experiments are reported in Appendix A.6.
The literals P (o1, o2, · · · , ok) occurring in the causal model use some predicates P applied to classes of objects oi. The ontological model consists of specialization/generalization links between classes of objects
o1 is-a−→ o2,
where is-a−→ denotes the usual specialization link between classes. E.g., we have Hurri is-a−→ SWind, House1FPA is-a−→ HouseFPA and HouseFPA is-a−→ BFPA : a “hurricane” (Hurri) is a specialization of a “strong wing” (SWind), and the class of “low houses with one level only in the flood-prone area” (House1FPA) is a specialization of the class of “houses in the flood-prone area” (HouseFPA), which itself is a specialization of the class of “buildings in this area” (BFPA). A part of the ontological model for our Xynthia example is given in Fig. 1 (each white-headed arrow labelled with is-a represents an is-a−→ link).
A〈i〉ι , E
〈i〉 ι are empty matrices for i /∈ Iι, A〈i〉 = [A
〈i〉 1 , A 〈i〉 2 , . . . , A 〈i〉 c ],
E〈i〉 = [E 〈i〉 1 , E 〈i〉 2 , . . . , E 〈i〉 c ].
The above objective can be solved similarly as for (19). Namely, we alternately update the part dictionaries and the tree-structured shape models. After solving (24), we have the part dictionaries D〈i〉ι . = Dι◦σι◦T 〈i〉(ν̌ι) and the shape model parameters {Zι}cι=1. Given a gallery set, ideally an mCPA model should be learned from face images of this gallery. However, in some cases the gallery does not contain non-frontal/non-neural face images to learn the corresponding component models of the mCPA. To remedy this problem, we first learn an mCPA model using a separate training set that contains face images of all the interested poses and expressions of a few subjects, and then learn the part dictionaries on the gallery set of interest while fixing the learned tree-structured shape model from that separate training set.
With a learned mCPA model, we use the same method as that of the standard CPA model to recognize a probe face, which requires us to select the correct component corresponding to its pose and expression before hand. In a fully automatic face recognition system, we may figure out the pose and expression by off-the-shelf methods, which, we will discuss in Section VI-D.
VI. EXPERIMENTS
In this section, we present experiments to evaluate the proposed CPA method in the context of face recognition across illuminations, poses, and expressions. We used images of frontal face with neutral expression as the gallery, and face images with illumination, pose, and expression variations as the probes. We used the CMU Multi-PIE [32] and MUCT [33] datasets to conduct our experiments. The CMU Multi-PIE dataset contains face images with well controlled illumination, pose, and expression variations, and is thus intensively used for controlled experiments.
We designed an mCPA model whose component for frontal view and neutral expression consists of 21 facial parts of varying sizes. The basic constellation of the 21 parts is listed in Table I. For the other components, the availability of a part is determined by its visibility. The parameters used for the mCPA learning are set as λ̂ = 1, η̂ = 0.02, and ϑ = 0.25 for all the experiments reported in this section.
With learned mCPA models, we first evaluated our method in Section VI-A in the scenario of face recognition across pose and expression with illumination variation. In particular, we show the advantages of our CPA method over other alternatives, such as the methods of a holistic face alignment followed by a holistic or part-based face recognition. We then demonstrate in Section VI-B the effectiveness of our CPA method when using part-based face recognition strategy. In Section VI-C, we test on face images with synthesized occlusions to demonstrate
Remark: The relative sizes are in terms of the conventional facial region aligned with the two eyes, i.e., 60×80 window with two outer eye corners at (5,22) and (56,22); and, the absolute sizes are measured in pixels for the part dictionaries.
the robustness of our method. Finally, we compared with the state-of-the-art across-pose face recognition methods in Section VI-D.
For controlled experiments reported in Sections VI-A, VI-B, and VI-C, we initialized face locations by manually annotating eye corner points, and assumed that the pose and expression of each probe face are given. For practical experiments in Section VI-D that conduct fully automatic face recognition across pose, we used off-the-shelf face detector [41] and pose estimator [25] to initialize our method.
In theory, it is also possible to use a policy network to select the nonuniformly random moves during rollouts. Such a network can be called a rollout policy network and would take the place of traditional pattern-based rollout policies. However there are two related obstacles preventing convolutional rollout policy networks from working effectively: (1) convolutions are computationally expensive, and (2) UCB1 is a deterministic algorithm.
When performing inference with a convolutional network, one prefers to evaluate the input in batches to maximize throughput on hardware platforms such as modern GPUs. While batching convolutions is a well known technique and forms the basis of modern minibatch stochastic gradient methods, for batching to be effective, the input states need to be sufficiently unique. If one were to naively explore the Monte Carlo search
tree in batches using UCB1, then many of the states within a batch would be duplicate states. Asynchronous parallel versions of UCT have also encountered this problem, getting around it by using heuristic “virtual losses” to introduce variance during tree exploration (Chaslot et al., 2008a).
Instead, we substitute for UCB1 the probabilistic bandit algorithm Thompson sampling (Thompson, 1933) as the search policy in MCTS, a choice justified by recent empirical evidence (Chapelle & Li, 2011), as well as proofs of its comparable regret bounds to those of UCB1 (Agrawal & Goyal, 2012; Kaufmann et al., 2012). Specifically, we use Thompson sampling with Bernoulli rewards (or beta prior) as described below in (2), in which the optimal action at each time step is selected by choosing the argmax of the randomly sampled values qj :
argmax j qj where qj ∼ Beta(wj + 1, nj − wj + 1). (2)
We incorporate Thompson sampling into a batched MCTS algorithm, with pseudocode described in Figure 2.
In practice, we execute the game rule logic on the CPU and synchronously run the convolutional network
on the GPU. Although this incurs communication overhead between main memory and GPU memory, we believe that splitting the work between the two is optimal, especially on combined multicore and multi-GPU systems.
This paper contributes to a full stack design of predictive analytics platform in DCIM Software. The major contribution of this paper is as follows:
• The model of a computing unit, Fault Engine, which leverages the log-data from all concerned devices available in the device chain and employs a Markov Process based Failure Model to predict
whether the failure is permanent or transient hence raising alarm with proper severity. It also indicates the recovery probability at any given time stamp after the failure has occurred.
• The Engine identifies the root cause devices in a situation of failure. It assigns probability to devices to be a probable root cause hence narrows down the search space for the DC management team.
• The Engine is capable of identifying communities of correlated devices based on the correlated failures.
In this section, we show the performance and scalability of Sparse Allreduce performance and scalability by running PageRank algorithm on clusters of different size and different systems. PageRank is implemented on top of BIDMat, an interactive matrix library written in Scala that fully utilize hardware accelerations (Intel MKL). So the computation is already an order of magnitude faster than pure Java. the Twitter follower’s graph and Yahoo web graph.
The scaling of Sparse Allreduce is illustrated in Figure 8. The figure plots the total runtime in the first 10 iteration against cluster size. The configuration is optimally tuned individually for different cluster size. We also present the runtime breakdown (into computation and communication). As shown in the figure, the system scales well up to 64 nodes. However, communication starts to dominate the runtime for larger clusters. Particularly, for the 64 node cluster, communication takes up to 80% of overall runtime.
It is also worth pointing out that the overall achieved bandwidth is around 2Gb/s on EC2 which is much smaller than the rated 10Gb/s of the network. This is not a bad number for the communication technology used (pure Java sockets). Socket performance in HPC has been extensively studied, and it is well-known that Java sockets achieve a maximum of a few Gbits/sec. There are several technologies available which would better this figure, however at this time
Figure 8: Sparse Allreduce scaling and compute/comm break down. v
Figure 9: PageRank runtime comparison (log scale).
there are barriers to their use on commodity clusters. RDMA over Converged Ethernet would be an ideal technology. This technology bypasses copies in several layers of the TCP/IP stack and uses direct memory-to-memeory copies to delivers throughputs much closer to the limits of the network itself. It is available currently for GPU as GPUdirect (which communicates directly from on GPU’s memory to another over a network), and in Java as Sockets Direct. However, at this time both these technologies are only available for Infiniband networks. We will monitor these technologies, and we also plan to experiment with some open source projects like RoCE (RDMA over Converged Ethernet) which offer more modest gains.
Finally, we compare our system with other popular distributed learning systems: Hadoop/Pegasus, GraphX and GraphLab. Figure 9 plots the first 10 iteration runtime for different systems. There’s no data available for Mahout and GraphX for the Yahoo dataset. The y-axis of the plot is logscale. Sparse Allreduce spends 6 seconds for 10 PageRank
iterations on the Twitter followers’ graph and 23 seconds for the Yahoo graph. As seen from the figure, each system provide half to one order magnitude improvement from right to left.
It also worth mentioning that PowerGraph uses greedily partitioned graph which produces shorter vertex lists (and communication) on each node. Our benchmarks use random partitioning, and should improve by about 15-20 % using greedy partition.
There are several commercial solutions that allow handling the temperature of specific places. Two of this solutions are Loxone [35] and tado◦ [36]. Moreover, there are also some researches that combine the IoT and fuzzy logic like Vitruvius [6, 21]
Knowledge Base Debugging
In this chapter we will give a description of an algorithm for interactive KB debugging (Algorithm 5) which implements the entire functionality required by an interactive debugging system. All other algorithms presented so far will be subroutines of Algorithm 5 which are either directly or indirectly called by it. Before we explain and discuss Algorithm 5 in detail, we give the reader a rough and informal overview of the algorithm’s input, output and actions in the following section in order to make the details of the algorithm easier to digest.
Remark 9.1 Note, in the following, when we speak of the input DPI we refer to the DPI 〈K,B,P ,N 〉R that is provided as an input to Algorithm 5, by the current DPI we mean the DPI 〈K,B,P ∪ P ′,N ∪N ′〉R where P ′ and N ′, respectively, are all positive and negative test cases added to the input DPI from the start of the algorithm’s execution until the current point in time. Further on, an intermediate (or previous) DPI denotes a DPI 〈K,B,P ∪ P ′′,N ∪N ′′〉R which is not the current DPI and where ∅ ⊆ P ′′ ⊆ P ′ and ∅ ⊆ N ′′ ⊆ N ′. Finally, the last-but-one DPI corresponds to an intermediate DPI 〈K,B,P ∪ P ′′,N ∪N ′′〉R where either |P ′| = |P ′′|+ 1 or |N ′| = |N ′′|+ 1 is true, but not both.
The interpretation of δ–separation as irrelevance relation should be that if C δ–separates A from B in G then A is irrelevant for B given C. This is denoted by A irδ B|C.
Proposition 4.10 δ–separation as graphoid δ–separation satisfies the following properties:
(i) left redundancy,
(ii) left decomposition,
(iii) left and right weak union,
(iv) left and right contraction,
(v) left and right intersection.
Proof: Didelez (2000, pp.27).
With these results not only property (5) but also (7) hold for δ–separation. So far, however, the conditions for (6) and (8) are not satisfied because right redundancy does not hold. By definition right redundancy would imply that A irδ B|B ⇔ A\B irδ B|∅ which is only true if A\B and B are unconnected in (GBAn(A∪B))
m. A simple counterexample is given by the graph with V = {a, b} and E = {(a, b)}. Additionally, property (6) does not hold, as can be seen by another example shown in Figure 4. Let A = {a}, B = {b1, b2}, and C = {b1, c}. Then, A irδ B\C|C but not A irδ B|C since the latter only holds if A irδ B|C\B. In contrast, the converse does hold because it is a special case of the subsequent result paralleling Lemma 4.5.
Lemma 4.11 Special version of right decomposition Given a directed graph G, it holds that:
A irδ B | C, D ⊂ B ⇒ A irδ D | (C ∪B)\D
Proof: Let A∗ = A\(B∪C) and C∗ = C\B. Then, we have to show that A∗⊥⊥u B|C∗ in (GBAn(A∪B∪C))m implies A∗⊥⊥u D|C∗ ∪ (B\D) in (GDAn(A∪B∪C))m. Note that A∗⊥⊥u D|C∗ ∪ (B\D) in (GBAn(A∪B∪C))m holds due to weak union and decomposition of ⊥⊥u . Changing the graph to (GDAn(A∪B∪C)) m means that all edges
that are present in G as directed edges starting in B\D are added. Additionally, those edges have to be added which result from vertices in B\D having common children with other vertices. Since all these new edges involve vertices in B\D there can be no additional path between A∗ and D in (GDAn(A∪B∪C))
m not intersected by C∗ ∪ (B\D). Although (6) does not hold in full generality it is easily checked that (8) does.
Proposition 4.12 Alternative intersection property Property (8) holds for δ–separation, i.e.
In the rest of this subsection we show more trajectories of iterates from individual runs. The details of the experiments are as follows.
Figures 19-20: In these two figures we plotted the normalized distances to θ∗ of a trajectory of averaged iterates θ̄αt and original iterates θ α t , for each algorithm and each stepsize, using the data from one of the runs of the algorithms that produced the previous four figures. Our observations from these results are the same as those from Figures 10-11 in the case of Problem I: (i) the averaged iterates θ̄αt perform better than θ α t in that they vary less and can approach a smaller neighborhood of θ∗; (ii) the unperturbed algorithms do not seem to have any disadvantages compared with the perturbed algorithms for the same stepsize.
17
In this paper, we considered the problem of designing agents that autonomously integrate available heterogeneous information about their environment. We investigated how experimentation in complex simulated environments on the one hand, and causal models on the other, can help to address this problem. A next step would be to perform more sophisticated experiments, e.g., with agents of different “hardware”.
Tables VIII and IX show the results obtained when classifying using the middle 30 seconds of the songs and the comparison of those segments against using full songs, respectively. The overall accuracy was 81.36%, which means an 8.48 pp drop against the full songs baseline. This time, both Bass and Trance accuracies dropped 16.8 pp and 20.4 pp, respectively, getting confused with each other by the classifier. Having listened to the tracks that got confused this way, the conclusion is as expected: these middle segments correspond to what is called a breakdown section of the songs. These sections correspond to lower energy segments (though not as low as an intro) of the tracks which, again, are not the most
March 24, 2015 DRAFT
characteristic parts of both these genres and, in the particular case of Bass vs. Trance, they are timbrically very similar due to their Electronic nature. A human listener would, probably, also be unable to distinguish between these two genres if listening only to these segments. Although, for 3 of the 5 genres, classification performance did not drop pronouncedly, it did so for 2 of them, which means that, in general, taking the middle sections of the songs for classification is also not a good segment selection strategy.
It is necessary to remove aromatic compounds from heavy petroleum cuts to improve the quality of the produced lubricating base oils [1-2]. Extraction of aromatic compounds from lubricating oil cut is usually done through a liquid-liquid (solvent) extraction process. Several works have been done to find a suitable solvent for extraction of aromatic hydrocarbons from lube-oil cut [3-5]. Sulfolane is heavily used as solvent for this process due to high selectivity for aromatics extraction. This compound is suitable to extract light and heavy vacuum distillates [6-9]. In a liquid-liquid (solvent) extraction process, two streams of feed and solvent need to be kept in contact with one another within the column. Studies on the efficiency of conventional extraction contactors show that RDC (Rotating Disc Contactors) columns can be considered as the most efficient extraction column [10-13]. Research has been focused on predicting the efficiency of RDC column with respect to the physical properties of the inlet streams as well as the geometry and other characteristics of the system [14]. Performance of RDC columns has not been accurately predictable due the complexity of heat and mass transfer taken place in these components. Acritical Neural Network (ANN) can be
used to model processes in different areas of chemical engineering [15-22] especially in the function approximation. Function approximation is based upon the training of an ANN against sets of input–output data pairs in an attempt to determine the relationship between the input and output parameters. In this study, an ANN model is used to develop simulation framework for the parametric study of an RDC column with which the solvent extraction process for production of lubricating oils takes place.
Here we propose a method inspired by the SSA algorithm to guide the system to learn on the pathway toward the selected goal position yg. This instantiation of the SAGG-RIAC architecture uses algorithm 3 and considers evolving contexts, as explained below.
The truck speed is set to 40 km/h, UAV speed is 56 km/h. The flight endurance is set to 20 minutes. The time to launch and retrieve the UAV is 1 minute each. These settings are adapted from the work of [4]. To solve the MIP model, we use CPLEX 12.6.2. The TSP is solved (in both heuristics) by the state-of-the-art solver Concorde.
VI. Results
This section describes experimental results on all settings of our heuristics and FSTSP. The algorithm is implemented in Scala language with all the results are given on a computer system consists of 4 × AMD CPU at clock speed 1400 Mhz and 92GB of memory free. The rest of this section is organized as follows. Firstly, we compare the results of all heuristics with truck-only value to evaluate their overall performance. Secondly, we report the results of our heuristics under all settings against FSTSP. The next section evaluates the impact of instance options to the performance of our heuristics, where we also want to find the best combination of instance option and profit function.
I. Compare to truck-only TSP optimal solution
In this result, we want to observe the gap in percentage between FSTSP and 6 settings with truck-only TSP optimal solution. The result are demonstrated in Table 3, 4 The gap is calculated by:
gap = newObjective− tspObjective
tspObjective × 100 (8)
As can be seen, the negative average gaps are found in all of the settings. It means the combined method of truck and drone for delivery is clearly more cost-efficient than the traditional method. For a small number of customers like in Table 3, there are only slight differences between FSTSP and the best settings (obj-3 and rf). However, the performance of FSTSP decreases as the size of customers increases (from -41.39 downto -27.18), while it is opposite for our heuristic settings. Specifically, Set 2 shows a significant performance difference between FSTSP and the rest. Again, obj-3 and rf are two best settings with their average gaps close to each other (-42.79 and -42.29).
Both obj-3 and rf share one common characteristic: they both use profit function 1 with cr option 3. Hence, this combination is considered to be the best among others. It is logical when the profit function is calculated by taking the distance from UAV node to the center of all customers, as high profit means that the drone will be sent to service the furthest customers, saving the traveling cost for the truck.
II. Compare to FSTSP heuristic
We want to observe the performance of our models against FSTSP framework. We calculate the gap - the percentage that our models can outperform or being outperformed by FSTSP. The gap is calculated by:
gap = objective− f stsp
f stsp × 100 (9)
A positive average gap means that the setting is better than FSTSP. Furthermore, a winning rate in percentage is also demonstrated to see the performance of each setting. A value of more than 50% shows that the setting dominates FSTSP. Again, obj-3 and rf are the two best settings and outperform FSTSP in either Set 1 or Set 2 (see Table 5, 6).
III. Comparison between instance options
In this subsection, we want to evaluate the impact of instance options to the performance of all settings. From Table 7, among 9 options, obj-3 won 7 and rf has 2 winnings. At this point we can conclude that cluster first - route second heuristic performs better than route first - cluster second under the settings of profit function 1 and cr option 3.
Additionally, figure 9 shows detailed winning rate in percentage of each settings. Contrary to obj-3 and rf, obj-2 and obj-2w have the worst performance, meaning that shortest travel time of each UAV routes does not necessarily leads to a truck’s shortest total time.
VII. Conclusion
This paper proposed a MIP formulation to solve the cluster step of two new heuristic methods. In cluster first - route second, we first find the UAV routes set and then build the truck’s route given the fixed clusters. Conversely, route first - cluster second solves the truck-only TSP to get an initial route. It then runs the cluster step to rearrange that route. Experimental results demonstrate the effectiveness of both heuristics as the customer size increases, compare to either TSP objective or FSTSP. It also shows the impact of instance options to heuristics performances. In any heuristics, the settings with profit function 1 and cr option 3 dominate all others, strengthening our observations that the drones should service the furthers customers to help saving the truck’s travel time. Furthermore, cluster first - route second heuristic performs better than route first - cluster second.
As nature of a generalized version of a NP-Hard problem (TSP), larger instances of this problem cannot be solved by using a MIP solver for clustering, in a reasonable time. It resulted in the tuning of time span parameter to reduce the Ω size in MIP model. Hence, local search techniques should be developed. That is the next step in our developments for this problem. Additionally, we want to have more numerical analysis on the relation between different instance options to the performance of heuristics. They will clearly give us closer look into the nature of this problem. It is also obviously clear to extend this problem to a multiple trucks and multiple UAV problem. Also an online and dynamic version is also a logical direction.
Acknowledgement
The research of this paper is supported by National Foundation for Science and Technology (NAFOSTED), project ID FWO.102.2013.04
References
[1] Agatz, N., Bouman, P., Schmidt, M.: Optimization approaches for the traveling salesman problem with drone. Tech. rep. (2015)
[2] Banker, S.: Amazon and drones – here is why it will work (dec 2013), http://www.forbes. com/sites/stevebanker/2013/12/19/amazon-drones-here-is-why-it-will-work/
[3] Drexl, M.: Synchronization in vehicle routing-a survey of vrps with multiple synchronization constraints. Transportation Science 46(3), 297–316 (2012)
[4] Murray, C.C., Chu, A.G.: The flying sidekick traveling salesman problem: Optimization of drone-assisted parcel delivery. Transportation Research Part C: Emerging Technologies 54, 86–109 (2015)
[5] Reinelt, G.: Tsplib—a traveling salesman problem library. ORSA journal on computing 3(4), 376–384 (1991)
[6] Rizzoli, A.E., Montemanni, R., Lucibello, E., Gambardella, L.M.: Ant colony optimization for real-world vehicle routing problems. Swarm Intelligence 1(2), 135–151 (2007)
For the time being, it is possible to identify three categories of creative systems: humans, computers and computer-aided systems. Even if there are relevant internal differences between elements of the same category, still the structure of internal and external constraints is usually similar between systems of the same class. The elements of these three categories considered together form the creative social network described in the previous section. It is possible to identify the generation and the evaluation spaces for each of these homogeneous categories.
The generation space for humans is formally defined as A pHum = ∪ n i=1A p i . where A pi is the potential generation set of human creative systems considered as a whole. The space is the set union of the potential generation spaces of all n humans inhabiting Earth at a certain time. A pHum depends on I
Hum, EHum, which are the sets of all possible configurations of internal and external human constraints. Likewise, for computational creative systems we hold that A pCcs = ∪ni=1A p i identifies the potential generation set of computers. A p Ccs depends on ICcs, ECcs, which are the sets of all possible configurations of internal as well as external computational constraints. Computational systems can visit regions of the potential generation space faster than humans, since they are potentially able to modify their external constraints quicker than humans [19]. In this sense, computational creative systems are more flexible than humans. For computeraided systems, the potential generation space is described by the equation A pCad = A
p Hum ∪ A p Ext, where A p Ext is the set of artefacts that humans are not able to
generate without computer-aided systems. The following is always true: A
p Ext ⊂ A , A p Ext ∩ A p Hum = ∅
The set union of potential generation spaces of all three categories are a subset of the potential generation space A , since all systems have internal constraints that reduce the number of artefacts that can be generated.
(A pHum ∪ A p Cad ∪ A p Ccs) ⊂ A
The evaluation space for all humans is formally defined as: VHum = ⌊A , IHum, EHum⌉v = ∪∞i=1(d Hum C (c), f Hum+ (r), f Hum − (r)) in which the probability density functions are calculated over all humans. A single 3-tuple represents the evaluation for a single point of the space. A similar equation is used to calculate the evaluation space of computers:
VCcs = ⌊A , ICcs, ECcs⌉v = ∪∞i=1(d Ccs C (c), f Ccs+ (r), f Ccs − (r))
This paper applied two clustering techniques for clustering word senses in an effort to improve two state of the art lexical entailment techniques. The results showed that clustering word senses does provide some improvement, but care must be taken in combining the senses in order to make a classification decision. The experiments conducted in this paper show that tiered clustering, with an appropriate algorithm for combining sense, consistantly can give better results over the single prototype baseline.
There is a lot of room here for future work in this arena. For one, the clustering could still be improved. It would be interesting to see if there was a way to incorporate word relatedness into the tiered clustering model. By merging some of the similar clusters, this approach would be more resilient as the number of clusters would be more limited. Another way to limit the clusters would be to have a higher threshold of the amount of occurrences needed for a cluster to be kept. Perhaps tuning this parameter would have produced better results. There are other avenues for improvement as well such as using less noisy and more representative data, and finding better ways of use these clusters to produce a score.
The latter is especially deserving of further study. In conclusion, I think that this is a viable approach to improving lexical entailment, but further work must be done for the benefits of this approach to be worth the large computational costs.
Our model consists of two convolutional neural networks nete and netf as shown in (1). Both of them take the same form, so we detail only the target architecture.
We now discuss a few common adversarial network problems and their saddle-point formulations. Generative Adversarial Networks (GANs) fit a generative model to a dataset using a game in which a generative model competes against a discriminator [14]. The generator, G(z; θg), takes random noise vectors z as inputs, and maps them onto points in the target data distribution. The discriminator,
D(x; θd), accepts a candidate point x and tries to determine whether it is really drawn from the empirical distribution (in which case it outputs 1), or fabricated by the generator (output 0). During a training iteration, noise vectors from a Gaussian distribution G are pushed through the generator network G to form a batch of generated data samples denoted by Dfake. A batch of empirical samples, Dreal, is also prepared. One then tries to adjust the weights of each network to solve a saddle point problem, which is popularly formulated as,
min θg max θd
Ex∼Dreal f(D(x; θd)) + Ez∼G f(1−D(G(z; θg); θd)). (4)
Here f(.) is any monotonically increasing function. Initially, [14] proposed using f(x) = log(x).
Domain Adversarial Networks (DANs) [25, 12, 10] take data collected from a “source” domain, and extract a feature representation that can be used to train models that generalize to another “target” domain. For example, in the domain adversarial neural network (DANN [12]), a set of feature layers maps data points into an embedded feature space, and a classifier is trained on these embedded features. Meanwhile, the adversarial discriminator tries to determine, using only the embedded features, whether the data points belong to the source or target domain. A good embedding yields better task-specific objective on target domain while fooling the discriminator, and is found by solving
min θf ,θyk max θd ∑ k αkLyk ( xs; θf , θyk ) − λLd (xs,xt; θf , θd) . (5)
Here Ld is any adversarial discriminator loss function and Lyk denotes the task specific loss. θf , θd, and θyk are network parameter of feature mapping, discriminator, and classification layers.
True identification of a file format is a tedious task. There are catalogues containing several thousand of known file types [8], without having any global standard for the file types. File type detection methods can be categorized into three kinds: extension-based, magic bytes-based, and content-based methods, each of them has its own strengths and weaknesses, and none of them are comprehensive or foolproof enough to satisfy all the requirements. The fastest and easiest method of file type detection is the extension-based method. The Microsoft’s operating systems use such approach almost exclusively. All the file types, at least in the Windows-based systems, are generally accompanied by an extension. This approach can be applied to both binary and text files. While it does not need to open the files, it is by far the fastest way to classify the files. However, it has a great vulnerability while it can be easily spoofed by a simple file renaming.
The second method of file type detection that is devoted to the binary files is based on the magic bytes. The magic bytes are some predefined signatures in the header or trailer of binary files. A file header is the first portion of a computer file that contains metadata. Metadata is the background information that describes the content, quality, condition, and other appropriate characteristics of the data. The file header contains necessary information for the corresponding application to recognize and understand the file. Magic bytes may include some extra information regarding the tool and the tool’s version that is used to produce the file. Checking the magic bytes of a file is a little slower than just checking its extension since the file should be opened and its magic bytes should be read. The magic bytes method is taken by many UNIX-based operating systems. However, it has some drawbacks: the magic bytes are not used in all file types. They only work on the binary files and are not an enforced or regulated aspect of the file types. They vary in length for different file types and do not always give a very specific answer. There are several thousands file types for which magic bytes are defined and there are multiple lists of magic bytes that are not completely consistent. Since there are not any standard for what a file can contain, the creators of a new file type usually include something to uniquely identify their file types. Some programs or program developers may never put any magic bytes at the beginning of their file types. This approach can be also spoofed. Altering the magic bytes of a file may not disturb its functionality but can defeat the true file type detection. The third method of file type detection is to consider the file contents and using the statistical modeling
techniques. It is a new research area and is the only way to determine the spoofed file types. It can reveal the malicious file types that their contents do not match with their claimed types. It is based on the byte values inside of different computer files. Each computer byte consists of eight bits so it can accept 256 different values varying between 0 and 255. The BFD of a file can be simply found by reading the contents of that file and counting the number of times that each byte value occurs. It is believed that different files of the same type have the same characteristics, which can be extracted to be used in the file type detection. In this approach, several sample files of the same type is given and a fileprint, something similar to a fingerprint, is produced from the sample files. Whenever an unknown file is examined, its fileprint will be produced with the same process and it will be compared with the collection of previously produced fileprints. To produce a fileprint some features of the file type should be selected and extracted. There are some methods that can be used for the feature extraction. The original principle is to use the BFD of file contents and manipulate with its statistical features. Such statistical measurements together form a model of the chosen file type, sometimes called a centroid. It is also possible to produce several centroids from a file type (multi-centroids). The centroids are then compared to an unknown sample file or data fragment, and the distance between the sample and the centroids is then calculated. If such distance is lower than a predefined threshold, the examined file is categorized as being of the same file type that the centroid represents.
In Table 3 we compare the generative performance of eVAE with other models through their samples. Encoders and de-
coders have L = 2 layers of H = 1000 deterministic units. D = 8 for MNIST, and D = 15 for TFD. VAE, mVAE, and eVAE refer to the best performing models over all architectures from Table 1. For MNIST, the VAE model is (L,H,D) = (3, 500, 8), mVAE is (3, 1000, 24), and eVAE is (3, 500, 48). For TFD, the VAE model is (3, 500, 15), mVAE is (3, 1000, 50), and eVAE is (3, 500, 25). We observe that eVAE significantly improves over VAE and is competitive with several state-of-the-art models, notably Adversarial Autoencoders. Samples from eVAE on MNIST and TFD are shown in Fig. 8.
While there may be a number of settings that would benefit from cross-residual learning, we illustrate one natural setting here in multitask learning [3]. To implement a multitask network, a common approach [5, 10, 28, 35] is to introduce a branching point in the architecture that leads to one network head per task, e.g. see Figure 1. In Table 1 and Figure 4, we show 50-layer multitask residual networks with a branching point at the last input size reduction. The earlier in the network this branching point is introduced the larger the input feature map size is to the individual network heads, often resulting in multitask networks with a large memory footprint. On the other hand, if the branching point begins deeper in the network, the representational specialization available for each task is limited to a small space of high-level abstract features. In our design of a multitask cross-residual network (X-ResNet), we address this latter problem by allowing additional cross-task mixing via crossresidual weights which cheaply increases late-layer representational power without requiring large input feature spaces. While it is possible to completely forego a branching point in the network design and simply couple multiple network towers using cross-residual skip connections, this results in a composite network that is very memory intensive and only feasible in a multi-GPU environment (though this could be somewhat alleviated by freezing weights, e.g. in combination with greedy layerwise training).
In addition, to introduce some task specialization, at the branching point in our multitask network design and before the cross-residual layers, we move the last ReLU activation and batch normalization canonically present inside the residual building block outside, placing it after the elementwise addition such that there is one per task. This helps to produce a slightly different normalization for each task branch and in practice, slightly improves performance. As in most multitask networks with a branching point, the total network loss is taken to be a combination of each of the individual network head losses. While some tune the loss weight for each of these network heads, we simply use the unweighted sum over all the network head losses.
This sections looks at another way of defining low impact: undetectability. If, during the 19th century, there was an inhabitant of London, and there now remains no record whatsoever of their existence, it is likely that they had a very low impact. Presume that we are given a partial description of them, and asked, from the point of view of today and all the evidence we have, whether they existed or not. If we really cannot tell either way, this seems a definitional example of low impact: their potential impact is literally lost in the noise of the world10.
How could this be formalised? Suppose that the AI uses a probability estimator P ′. This estimator is bounded by practical concerns, as any real-world agent can only have bounded rationality. Its unbounded equivalent is P . And these estimators have some level of reflection [CYHB13], so P ′ can express some estimates over the outcome of P ′ (self-reflection) and P (modelling of a superior rational agent). Let b designate background information about the world (prior to X), and let G designate the set of events in a slice S of event X/¬X’s future light-cone – maybe the slice 50 (time-like) years in the future.
Then what we would want is that P (g|X, b) = P (g|¬X, b) for g ∈ G – that the likely future is identical whether or not the AI is turned on or not (as assessed by the unbounded version of the agent, with access to all of S). If we designate the expectation estimator derived from P ′ as E′, this reduces to wanting:
R = E′(|P (g|X, b)− P (g|¬X, b)|) = 0.
This expectation is both over P (the AI’s estimate of its unbounded estimator) and over the likely g ∈ G (the likely future; this also depends on the AI’s policy choices).
This works if the impact of the AI is really lost through entropy, such that even P cannot detect it. However, an unbounded agent with full access to the slice G, might be able to always detect the presence of an AI, no matter how low impact it attempts to have.
utility functions, or some near enough approximation. That is, we want U more or less to contain representations of everything humans may really care about. If the AI does not, in expectation, affect the elements of U that much, then it counts as low impact in the desired sense of the term. Just as in the coarse graining case, we’d want to add a variety of other utilities to U , in case there are vulnerabilities that have not occurred to us. We of course will need to exclude utility functions that care directly about whether X for this approach to work.
10Of course, some care must still be taken when we decide how to describe this person. For instance, we can’t tell now whether there really was somebody called ‘Jack the Ripper’ who was a serial killer and had exactly 100,000 hairs on his head at midnight on 1 January 1888. The more formal development of this approach below will eliminate this issue.
In that situation, we can consider the change in P as it’s fed information from G, one bit at a time. When P has all of G, it will detect the presence of the AI, but before that point, it will have doubts. An AI that optimises the world is likely to leave many traces all over G; one that changes little will leave corresponding few choices. For 0 ≤ ρ ≤ 1, designate by gρ a random sampling of G where we have randomly chosen a proportion ρ of all the data in G. We can designate a new measure of R:
R = min ρ
{ E′ ( P (gρ|X, b) P (gρ|¬X, b) ) > 10, or E′ ( P (gρ|¬X, b) P (gρ|X, b) ) > 10 } (3)
Here R is a measure of how much information P is likely to need before detecting the difference between X and ¬X (‘difference’ being somewhat arbitrarily defined as the expected ratio of probabilities, or inverse ratios, being higher than 10).
Hinton & Salakhutdinov (2006) achieved breakthrough results by training deeper multi-layer networks through preconditioning early layers by greedy training each layer in succession. More recently, Yosinski et al. (2014) initialized from known successful weights thereby dropping the network immediately into a lower point of the loss surface and avoiding getting stuck in the many local optima that arise with deeper and larger networks. Distillation (Hinton et al., 2015) is another technique that uses existing cumbersome models to extract structure from the data before transferring it to a less cumbersome model.
GradNets embody this philosophy by uniting stages of training that are easy to optimize and powerful with the additional benefit that training can be done jointly in a single training phase.
The goal of the experiments is to compare the four extraelimination heuristics to elimination. For each graph and each of the four extra-elimination heuristics, 19642 triangulations were generated and 488 of these were timed. The same procedure was repeated four times using pure elimination. There are two reasons for this repetition. First, it provides a fair comparison between elimination and the overall best of the four extra-elimination methods. Second, it ensures that a large part of the elimination search space has been explored. Note that the elimination triangulations have a significant advantage over the four extra-elimination methods individually since 4× as many cases were considered.
The 19642 triangulations were generated using a variety of state of the art elimination heuristics. 20 one-step look ahead heuristics were used, including minimum weight, fill, size, and various combinations and repetitions of these. For each look ahead heuristic there was an additional parameter from 1-3 where the next node in the order is chosen randomly from the top x choices. This parameter is similar to the Stochastic-Greedy Algorithm given in [15]. Maxi-
mum cardinality search was also used, bringing the total to 61. For each of the 61 methods one triangulation was chosen for timing by having the lowest state space from separate pools of 100, 50, 10, and 1 triangulations. All of the above were repeated using a modified state space heuristic.
The triangulations were timed on a calculation of the probability of evidence, stopping if more than 1 Gb of memory was used. Our probability computation engine is hybrid inference/search [10] where message passing is done over the junction tree and search is done within a clique. For search, we use an algorithm that consists of backtracking and an optimized static variable order.
The first set of graphs is composed of 356 randomly generated Bayesian networks. Each graph has 30 nodes, a maximum in-degree of 4, and the set of edges is chosen uniformly over all graphs fulfilling the constraints. Each node has a 0.5 probability of being deterministic and a 0.1 probability of being observed. The stochastic variables have cardinalities between 2 and 5 and the observed variables have a cardinality of 50. The deterministic variables have cardinalities between 2 and the product of their parents’ cardinalities, with a upper bound of 125. For each graph, the fastest triangulation from each of the five methods is chosen. Table 1 gives counts of the number of graphs where each method was the best overall, and the number of times each method was various orders of magnitude slower than the best. Figure 4 compares the number of graphs where the best triangulation could and could not have been created using elimination (determined using Algorithm 1). Table 2 gives results over sets of graphs with a fixed number of deterministic variables. With little determinism, there is less opportunity for improvement over elimination, and with much determinism the total state space of the graph is small and the solution can be found quickly regardless of the triangulation.
The second set of results (Table 3) uses 10 real-world dynamic Bayesian networks. In addition to what was done on the random graphs, a set of 488 triangulations was generated using one instance of maximum cardinality search, minimum weight, fill, or size (labeled once). This is to compare our elimination baseline to a typical baseline. The
following are descriptions of the graphs. Aurora Decoding: whole word model for speech recognition, [5]. Edit Distance training 1, 2, decoding: learns edit distance parameters from data [14]. Feature Detect: extracts phonetic features from speech data. Image Concept Detect: for image classification. Mandarin: speech recognition graph modeling tonal phones using asynchronous feature streams [33]. MultiStream: speech recognition training graph with asynchronous feature streams based on [34]. PhoneFree 1, 2: word pronunciation scoring using a phone-free model.
On the randomly generated graphs, all-extra was the overall winner scoring the best on over half of the graphs. Sampled-extra was the second best overall, followed by some-extra. Lo-extra and elimination performed poorly overall. All-extra performs well when there is a high percentage of determinism (as in this set of random graphs). One might conclude that all-extra is the only method that should ever be considered, but in one case it was 15 times as slow as the best (which was an elimination graph). Sampled-extra has the potential to perform very well as it subsumes all of the other methods, but the large number of fill-in choices keep it, on average, slower than all-extra. The results on the real-world DBNs were much less dramatic. This because the cliques that are necessarily formed when partitioning the DBNs can account for a majority of the compute time and make the graphs fairly dense to begin with (see [4]). The extra-elimination heuristics gave significant improvement on 4 graphs with 2 more than doubling in speed. The median performance of the new heuristics was much better in many cases, but poor in others.

Business Week article, available at: http: //www.businessweek.com/investor/content/ oct2007/pi2007102_394204.h%tm, 2007.
This paper presents a new model of WordNet that is used to disambiguate the correct sense of polysemy word based on the clue words. The related words for each sense of a polysemy word as well as single sense word are referred to as the clue words. The conventional WordNet organizes nouns, verbs, adjectives and adverbs together into sets of synonyms called synsets each expressing a different concept. In contrast to the structure of WordNet, we developed a new model of WordNet that organizes the different senses of polysemy words as well as the single sense words based on the clue words. These clue words for each sense of a polysemy word as well as for single sense word are used to disambiguate the correct meaning of the polysemy word in the given context using knowledge based Word Sense Disambiguation (WSD) algorithms. The clue word can be a noun, verb, adjective or adverb.
KEYWORDS
Word Sense Disambiguation, WordNet, Polysemy Words, Synset, Hypernymy, Context word, Clue Words
Although most uses of machine learning in music employ supervised or unsupervised learning algorithms, other algorithmic families exist. For example, in semi-supervised learning, some training examples include output labels but others do not. This approach is mo-
tivated by the fact that providing output labels for every input in the training set can be difficult and time consuming. Our hand position instrument designer might create a large unlabelled example set by moving her hand in front of the camera without providing any additional information, then select a few still images from this dataset and add labels specifying what note should be played for those hand positions. She might then apply a semi-supervised learning algorithm to build her hand position classifier, with the algorithm using the labelled examples to learn how inputs should be matched to notes, but also benefitting from the numerous unlabelled examples that provide further information about the nature of inputs it is likely to encounter.
In reinforcement learning, an algorithm learns a strategy of action to maximise the value of some reward function. This reward could be an explicit value specified by a human user in response to each action of the algorithm. A simple example is a melody generation program that could be trained to produce “good” melodies by a human user who presses a “thumbs up” button (positive reward value) when he likes a melody and a “thumbs down” (penalty or negative reward value) when he dislikes the melody. The reward could alternatively be computed, for instance using an estimate of how well the melody fits with current musical material generated by human collaborators.
We start with the introduction of linear SVMs for the solution of a binary problem. We refer to the two types of vectors as positive and negative ones, i.e. yi ∈ Y = {1;−1}. The aim of a classification problem is to find an hyperplane which separates positive from negative vectors. For a point that lies on this hyperplane the following equation holds:
w · x + b = 0, (4.3)
where w identifies the hyperplane (it is the direction vector of the plane) and |b| ‖w‖ is the normal distance from the hyperplane to the origin. Usually there are many hyperplanes that a learning algorithm can choose to solve a problem. The SVMs algorithm returns the hyperplane with the maximum margin m. The margin is the maximum distance between the separating hyperplane and the closest points of each sets. All these concepts are reported in Figure 4.1.
If we define marginal hyperplanes as w · x + b = ±1, the following inequalities hold :
w · xi + b ≥ 1 for yi = 1, w · xi + b ≤ −1 for yi = −1.
(4.4)
The vectors that lie on marginal hyperplanes are called support vectors. The equations 4.4 can be combined in order to obtain a unique inequality:
36
4.3. SUPPORT VECTOR MACHINES
yi(w · xi + b)− 1 ≥ 0. (4.5)
Using equations of marginal hyperplanes we can find that m = 1‖w‖ , thus it is clear that searching the hyperplane that maximizes the margin is equivalent to minimize ‖ w ‖ with constrain (4.5).
In most of classification problems, data present a component of noise, an example is shown in Figure 4.2.
To take into account the possible noise in the data, we can introduce the slack variables ξi. This variable measures the distance by which vector xi violates the desired inequality: yi(w · xi + b) ≥ 1. At this point we can define the minimization problem to solve as:
min w,b,ξ
1 2 ‖ w ‖2 +C p N∑ i=1 ξpi
subject to: yi(w · xi + b) ≥ 1− ξi ∧ ξi ≥ 0, ∀i ∈ [1, N ]. (4.6)
This can be formulated as a Lagrangian problem as follows:
L(w, b,α, ξ,β) = 1 2 ‖ w ‖2 +C p N∑ i=1 ξpi − N∑ i=1 αi[yi(xi ·wi+b)−1+ξi]− N∑ i=1 βiξi.
(4.7)
37
CHAPTER 4. FROM THEORY TO LEARNING ALGORITHMS
First and second terms of sum are the objective function that we want to minimize. The others are the constraints multiplied by different Lagrangian multipliers α and β. In this optimization problem, we minimize the amount of slack variables and ‖ w ‖ . Generally, these two requests are conflicting. The parameter that sets this trade-off is C. The parameter p describes different penalties for misclassified vectors as we explain in the end of this paragraph. Until objective function and constrains are convex and differentiable, the KKT conditions can be applied at the optimum:
5wLp = 0 ⇒ w = N∑ i=1 αiyixi, (4.8)
5bLp = 0 ⇒ N∑ i=1 αiyi = 0, (4.9) 5ξiLp = 0 ⇒ αi + βi = Cξ p−1 i , (4.10)
∀i, αi[yi(w · xi + b)− 1 + ξi] = 0 ⇒ αi = 0 ∨ yi(w · xi + b) = 1− ξi, (4.11)
∀i, βiξi = 0 ⇒ βi = 0 ∨ ξi = 0. (4.12)
The first equation shows that w is a liner combination of training vectors. The forth equation indicates that the vectors that really appear in that combination are only the support vectors. Indeed for other vectors the Lagrangian multiplier is zero. In this case we have two different types of support vectors. The first ones are vectors that lie on margin hyperplane (they have ξi = 0). The second ones are called outlier and are the misclassified vectors for which ξi 6= 0. The task of determine a classifier is equivalent to find the parameters w and b of the model. In the test phase, given a new vector x with unknown label, the output hypothesis is:
h(x) = sgn(w · x + b) = sgn( N∑ i=1 αiyi(xi · x) + b). (4.13)
The second equality can be simply obtained using equation (4.8). The extension of the theory to the multiclass case, i.e. yi ∈ Y = {1, 2, ..., G}, is straightforward. We solve different optimization problems, one for each
38
4.3. SUPPORT VECTOR MACHINES
class g = [1, 2, ..., G]. Thus, during the training phase, a pair (wg, bg) is found for each class. The considered class takes the label y = 1 and the others y = −1 (one-vs-all approach), in this way we solve G different binary problems. In the test phase, given a vector x, we choose the class g that gives the maximum output according to:
h(x) = argmax g∈G (wgx + bg) = argmax g∈G ( N∑ i=1 αi,gyixix + bg ) . (4.14)
Loss Function. As anticipated in section 4.1 the loss function is the penalty associated to a misclassified vector. For a binary problem the most common loss functions are the hinge loss and the quadratic hinge loss. These are respectively associated to p = 1 and p = 2 in equation (4.1). As shown Figure 4.3, penalties are different depending on the value of p chosen.
We have no loss if the prediction falls in the right part of the hypersurface (i.e. distance between point and hyperplane is greater than 1). When the prediction falls into the margin we have a loss 0 ≤ l ≤ 1. If the loss is greater than one the prediction is in the wrong part of the hypersurface.
39
CHAPTER 4. FROM THEORY TO LEARNING ALGORITHMS
We now recap the important definitions of [7]. A (Dung) argumentation framework is a directed graph 〈A, C〉, where A is the set of arguments and C ⊆ A2 is the conflict relation over A. For arguments A, B ∈ A we write C(A, B)⇔ (A, B) ∈ C ⇔ A conflicts with B, i.e. A is used as a counterargument against B. Note that C can denote either an attack relation defined by a set of instantiating formulae, or the defeat relation defined by determining which attacks succeed as defeats.
In what follows let S ⊆ A be a set of arguments and A, B ∈ A. S conflicts with B iff (∃A ∈ S) C(A, B). S is conflict-free (cf) iff C∩S2 = ∅. S defends A iff (∀B ∈ A) [C(B, A)⇒ S conflicts with B]. Let Def(S) := {A ∈ A S defends A}. Then, S is an admissible set iff S is cf and S ⊆ Def(S). An admissible set S is:
• a complete extension iff Def(S) ⊆ S;
• a preferred extension iff S is a ⊆-maximal complete extension;
• the grounded extension iff S is the ⊆-least complete extension;
• a stable extension iff S is complete and conflicts with all arguments in A− S.
S := {complete, preferred, grounded, stable} is the set of Dung semantics. An argument A ∈ A is sceptically justified under the semantics s ∈ S iff A belongs to all s extensions.
2.2 The ASPIC+ Framework
Dung’s framework provides an intuitive calculus of opposition for determining the justified arguments based on conflict alone. However, it abstracts from the internal logical structure of arguments, the nature of defeats and how they are determined by preferences, and consideration of the conclusions of the arguments. However, these features are referenced when studying whether any given logical instantiation of a framework yields complete extensions that satisfy the rationality postulates of [6]. ASPIC+ [12, 13] provides a structured account of abstract argumentation, allowing one to reference the above features, while at
the same time accommodating a wide range of instantiating logics and preference relations. ASPIC+ then identifies conditions under which the instantiation (given arguments, attacks and preferences) results in complete extensions that satisfy the rationality postulates of [6]; such instantiations are normatively rational.
This section gives a brief overview of the main ideas and results of the method. Details will be discussed subsequently.
We adopt the concept of NN blocks used in several deep network architectures [4]. Network blocks each contain one or more NN layers that can be stacked in a modular fashion just like standard NN layers. In this paper, we introduce fused binary blocks, which reorder the computation of multiple NN layers within a block in order to minimize the space required for intermediate temporary results. Figure 1 shows three fused binary blocks we use in this paper, operating on a 28 x 28 input sample: a Fused Binary Fully Connected (FC) Block, a Fused Binary Convolution Block and a Fused Binary Convolution-Pool Block. Internally, each block rearranges the order of computation to use only a single floatingpoint accumulator, Taccum, and outputs binary rather than floating-point results, Tres, which are then used as input to the next block.
Figure 2 shows the order of computation of a binary convolution layer followed by a pooling layer, as in BNN, compared to the fused binary convolution-pool block, as in eBNN. In BNN, all the convolution results are first computed and stored as floating-point temporaries (the green block). After convolution is complete, pooling takes place on the output matrix from convolution. By doing the convolution and pooling layers in two phases, the network must store the output of the convolution layer in the floating-point representation requiring 26x26 float-point storage of temporaries.
In contrast, under eBNN, in the Binary Convolution-Pool block, the convolution and pooling operations are fused so that only a single convolution result is stored (in floatingpoint format) at a time. In the case of max pooling, only the maximum of the local pool window is stored. Once the entire pooling region has been accumulated into Taccum, the maximum passes through through batch normalization and a binary activation function and is stored as a single binary value in the Tres result matrix. In the case of overlapped pooling, we recompute the values in the convolution result matrix which are used multiple times, in order to keep the lowest memory footprint possible. Figure 2 shows the computation required for a single result in Tres with a single filter. In the general case of multiple filters, the process is repeated for each filter, and the output Tres will have dimensions of filters x width x height. Taccum is reused for each filter, as they are computed one at a time. This reordering prioritizes low memory usage over parallelism and is tailored specifically for embedded devices with small memory capacity.
We report here a protein classification experiment carried out using the Protein Classification Benchmark Collection (PCBC) [18] [1]. This benchmark contains structural and functional annotations of proteins. The two datasets that we have exploited, SCOP95 and CATH95, are available at http://hydra.icgeb.trieste.it/benchmark. The entries of the SCOP95 dataset are characterized by sequences with variable lengths and relatively little sequence similarity (less than 95% sequence identity) between the protein families. The CATH95 dataset contains near-identical protein families of variable lengths in which the proteins have a high sequence similarity (more than 95% sequence identity). Basically, the considered classification tasks involve protein domain sequence and structure comparisons at various levels of the structural hierarchies. We have considered the following 14 PCB subsets:
• PCB00001 SCOP95 Superfamily Family • PCB00002 SCOP95 Superfamily 5fold • PCB00003 SCOP95 Fold Superfamily • PCB00004 SCOP95 Fold 5fold • PCB00005 SCOP95 Class Fold • PCB00006 SCOP95 Class 5fold • PCB00007 CATH95 Homology Similarity • PCB00008 CATH95 Homology 5fold • PCB00009 CATH95 Topology Homology • PCB00010 CATH95 Topology 5fold • PCB00011 CATH95Architecture Topology
TABLE 1 Dataset sizes and meta parameters used in conjunction with Ked, Keip and Kdtw kernels
DATASET length|#class|#train|#test Ked : C, σ KDeip : ν,C, σ KDTW : C, σ Synthetic control 60|6|300|300 1.0;0.125 .1;.25;.0625 8.0;4.0 Gun-Point 150|2|50|150 256;.5 0.01;256;.5 16.0;0.0312 CBF 128|3|30|900 8;1.0 .001;4.0;.0312 1.0;1.0 Face (all) 131|14|560|1690 4;0.5 .1;8.0;.5 2.0;0.25 OSU Leaf 427|6|200|242 2;0.125 .1;4;0.125 4.0;0.062 Swedish Leaf 128|15|500|625 128;0.125 10;8;.0625 4.0;0.031 50 Words 270|50|450|455 32;0.5 0.01;32;0.5 4.0;0.062 Trace 275|4|100|100 8;0.0156 .001;256;.0625 4;0.25 Two Patterns 128 |4|1000|4000 4.0;0.25 .01;1;.0312 0.25,0.125 Wafer 152|2|1000|6174 4.0;0.5 0.01;32;.5 1.0;0.016 face (four) 350|4|24|88 8.0;2.0 .01;16;2 16;0.5 Ligthing2 637|2|60|61 2.0;0.125 .001;128;2 2.0;0.031 Ligthing7 319|7|70|73 32.0;256.0 .1;16;.5 4;0.25 ECG200 96|2|100|100 8.0;0.25 0, 1024, .0625 2;0.62 Adiac 176|37|390|391 1024.0;0.125 10;256.0;.0312 16;0.0039 Yoga 426|2|300|3000 64.0;0.125 1;32;.0625 4;0.008 Fish 463|7|175|175 64.0;1.0 .01;256;1 8;0.016 Coffee 286|2|28|28 128.0;4.0 .01;1024;4 8;0.062 OliveOil 570|4|30|30 2.0;0.125 .01;64;2 2;0.125 Beef 470|5|30|30 128.0;4.0 0;64;.5 16;0.016
TABLE 2 Comparative study using the UCR datasets: classification error rates (in %) obtained using the first near neighbor (1-NN) classification rule and a SVM classifier for the Ked, Keip and Keip kernels. Two scores are given S1|S2: the first one, S1, is evaluated on the training data, while the second one, S2, is evaluated on the test data. For the each classification methods (1-NN and SVM) the rank of each classifier is given in parenthesis ((1): best classifier, (2): 2nd best classifier, (3): 3rd best classifier
DATASET 1-NN δed 1-NN δeip 1-NN δdtw SVM Ked SVM Keip SVM Kdtw Synthetic control 9(3)|12(3) .67(1)|1(2) 1.0(2)|0.67(1) 3(3)|2.33(3) .33(2)|.67(1) 0(1)|1(2) Gun-Point 4.0(1)|8.67(1) 4.0(1)|8.67(1) 18.36(3)|9.3(3) 4.0(3)|6.0(3) 2.0(2)|2.67(2) 0(1)|1.33(1) CBF 16.67(3)|14.78(3) 3.33(2)|4.22(2) 0(1)|0.33(1) 3.33(2)|10.89(3) 3.33(1)|5(1) 3.33(1)|5.44(2) Face (all) 11.25(3)|28.64(3) 7.5(2)|26.33(2) 6.8(1)|19.23(1) 9.82(3)|16.45(3) 6.25(2)|24.91(2) .54(1)|16.98(1) OSU Leaf 37.0(2)|48.35(2) 37(2)|48.35(2) 33.17(1)|40.9(1) 35(3)|44.21(2) 34.5(2)|44.21(2) 20(1)|23.55(1) Swedish Leaf 26.6(3)|21.12(3) 24.4(1)|20.96(2) 24.65(2)|20.8(1) 15(2)|8.64(2) 15(2)|8.64(2) 7(1)|5.6(1) 50 Words 34.47(3)|36.32(3) 32.2(1)|32.73(2) 33.18(2)|31(1) 33.56(3)|30.99(3) 31.78(2)|29.67(2) 15.21(1)|17.58(1) Trace 18(3)|24(2) 16(2)|24(2) 0(1)|0(1) 9(3)|19(3) 1(2)|7(2) 0(1)|2(1) Two Patterns 8.5(3)|9.32(3) 4.3(2) |3.62(2) 0(1)|0(1) 8.6(3)|7.45(3) 5.5(2)|3.52(2) 0(1)|0(1) Wafer 0.7(2)|0.45(2) .5(1)|.42(1) 1.4(3)|2.01(3) .7(3)|.7(3) .2(2)|.68(2) 0(1)|0.39(1) face (four) 37.5(3)|21.59(3) 33.33(2)|19.31(2) 26.09(1)|17.05(1) 20.84(3)|19.31(3) 16.67(2)|13.63(2) 8.33(1)|5.68(1) Ligthing2 25.0(3)|24.59(3) 20(2)|16.39(2) 13.56(1)|13.1(1) 21.77(3)|31.14(3) 20(2)|26.22(2) 8.33(1)|19.67(1) Ligthing7 35.71(3)|42.47(3) 30.0(1)|32.87(2) 33.33(2)|27.4(1) 37.14(3)|36.98(3) 34.29(2)|35.61(2) 17.14(1)|16.43(1) ECG200 14.0(2)|12.0(2) 1.0(1)|2.0(1) 23.23(3)|23(3) 8.0(3)|9.0(2) 3.0(1)|7.0(1) 7(2)|13(3) Adiac 41.28(3)|38.87(1) 39.59(1)|38.87(1) 40.62(2)|39.64(3) 26.67(3)|24.04(1) 25.13(2)|24.04(1) 24.61(1)|25.32(3) Yoga 22.67(3)|16.9(2) 21.67(2)|22.26(3) 16.37(1)|16.4(1) 17.66(3)|14.43(2) 15.33(2)|14.4(2) 11(1)|11.2(1) Fish 24.0(1)|21.71(2) 24.0(1)|21.71(2) 26.44(3)|16.57(1) 14.86(3)|13.14(3) 13.14(2)|12.57(2) 6.86(1)|4.57(1) Coffee 21.43(2)|25.0(2) 21.43(2)|25.0(2) 14.81(1)|17.86(1) 0(1)|0(1) 0(1)|7.14(2) 10.71(3)|17.86(3) OliveOil 13.33(1)|13.33(1) 13.33(1)|13.33(1) 13.79(3)|13.33(1) 10.0(1)|10.0(1) 10.0(1)|10.0(1) 13.33(3)|16.67(3) Beef 46.67(1)|46.67(1) 46.67(1)|46.67(1) 55.17(3)|50(3) 37.67(2)|30(1) 37.67(2)|30(1) 32.14(1)|42.85(3) Average Rank (2.4)|(2.25) (1.45)|(1.75) (1.85)|(1.5) (2.65)|(2.4) (1.8)|(1.7) (1.25)|(1.6)
• PCB00012 CATH95 Architecture 5fold • PCB00013 CATH95 Class Architecture • PCB00014 CATH95 Class 5fold
We evaluate the elastic cosine similarity based on the eip defined for symbolic sequences (Def.3.6, Eq.7) comparatively to five other similarity measures commonly used in Bioinformatics:
• BLAST [2]: the Basic Local Alignment Search Tool is a very popular family of fast heuristic search methods used for finding similar regions between two or more nucleotides or amino acids.
• SW [17]: The SmithWaterman algorithm is used for performing local sequence alignment, for determining similar regions between two nucleotide or protein sequences. Instead of looking at the sequence globally as NW does, the SmithWaterman algorithm compares subsequences of all possible lengths. • NW [11]: The Needleman Wunsch algorithm performs a maximal global alignment of two strings. It is commonly used in bioinformatics to align protein sequences or nucleotides. • LA kernel [12]: The Local Alignment kernel is used
to detect local alignment between strings by convolving simple basic kernels. Its construction mimic the local alignment scoring schemes proposed in the
SW algorithm. • PRIDE [5]: The PRIDE score is estimated as the
PRobability of IDEntity between two protein 3D
structures. The calculation of similarity between two proteins, is based on the comparison of histograms of the pairwise distances between C − α residues whose distribution is highly characteristic of protein folds.
The average AUC (area under the ROC Curve) measure is evaluated for 1-NN classifiers exploiting respectively BLAST, SW, NW, LA, PRIDE and eCOS(ν) as alignment methods. One can notice that these datasets are quite well suited for global alignment since, as shown in table 3, the NW algorithm performs better than the SW algorithm. The eip structure that considers global alignment is thus well adapted to the task. We show on these experiments that for ν = .05 the eCOS classifier in average performs significantly better than BLAST heuristics [2] and LA [12], the local alignment kernel for string. Furthermore, it performs almost as well as the SW and NW algorithms. The PRIDE method [5] gets the best results, but it uses the tertiary structure of the proteins while all the other methods exploit the primary structure. Here again, a ranking based on eCOS similarity has a complexity that could be maintained linear at exploitation stage (i.e. when testing numerous sequences against massive datasets). These very positive results offer perspective in fast filtering of biological symbolic sequences.
The basic high-level strategy here is as follows: We first present a candidate solution that satisfies (12) and carefully quantify the achievable objective function value for α ∈ (0, ᾱ], and ᾱ small. We then analyze a lower bound on the VAE cost and demonstrate that no solution can do significantly better, namely, any solution that can match the performance of our original proposal must necessarily also satisfy (12). Given that this is a lower bound, this implies that no other solution can both minimize the VAE objective and not satisfy (12). We now proceed to the details. Define µ(i)z , µz ( x(i);φ ) and Σ(i)z , Σz ( x(i);φ ) . We first note that if z = µ(i)z + S(i)z , with
S(i)z satisfying Σ (i) z = S (i) z ( S(i)z )> , and ∼ p( ) = N ( ; 0, I), then z ∼ qφ ( z|x(i) ) . With this
reparameterization and
µ(i)x , Wµ(i)z +WS(i)z , diag[Σ(i)x ] , ν ( µ(i)z + S (i) z ;θ ) for some function ν
µ(i)z , f(x(i);φ) for some function f (31) S(i)z , g(x(i);φ) for some function g,
the equivalent VAE objective becomes L(θ,φ) = ∑
i
{ Ep( ) [( x(i) −Wµ(i)z −WS(i)z )> ( Σ(i)x )−1 ( x(i) −Wµ(i)z −WS(i)z )]
+ Ep( ) [ log ∣∣∣Σ(i)x ∣∣∣ ] + tr [ Σ(i)z ] − log ∣∣∣Σ(i)z ∣∣∣+ ‖µ(i)z ‖22 } (32)
when b = 0 as stipulated.8 For now assume that κ, the dimension of the latent z, satisfies κ = rank[U ] (later we will relax this assumption).
D.1 A Candidate Solution
Here we consider a candidate solution that, by design, satisfies (12). For the encoder parameters we choose
µ̂(i)z = π (i), Σ̂
(i)
z = αI. (33) where α is a non-negative scalar and π(i) is defined in conjunction with a matrix Ψ such that
suppα [ x(i) −Ψπ(i) ] = supp [ s(i) ]
span [U ] = span [Ψ] . (34) 8The extension to arbitrary b is trivial but clutters the presentation.
All quantities in (33) can be readily computed via X applied to an encoder module provided that κ = dim[z] = rank [U ] as stipulated, and sufficient representational complexity for µz and Σz . Additionally, for the encoder we only need to define the posterior moments at specific points x(i), hence the indexing via i in (33).
In contrast, for the decoder we consider the solution defined over any z given by
Ŵ = Ψ
µ̂x = Ŵz
diag [ Σ̂x ] = Λ(hπ(z)), (35)
where Λ(i) ∈ Rd×d is a diagonal matrix with [ Λ(i)
] jj = { α, if s(i)j = 0, 1, otherwise, ∀j. (36)
and hπ : Rκ → {1, . . . , n} is a function satisfying hπ(z) , arg min
i∈{1,...,n} ‖z − π(i)‖2. (37)
Again, given sufficient capacity, this function can always be learned by the decoder such that (35) is computable for any z. Given these definitions, then the index-specific moments µ̂(i)x and Σ̂ (i)
x are of course reduced to functions of given by
µ̂(i)x = µ̂x ( µ̂(i)z + Ŝ (i) z ;θ )
Σ̂ (i)
x = Σ̂x ( µ̂(i)z + Ŝ (i) z ;θ ) . (38)
.
We next analyze the behavior of (32) at this specially parameterized solution as ¯alpha becomes small, in which case by design all covariances will be feasible by design. For this purpose, we first consider the integration across all cases where Σ̂ (i)
x does not reflect the correct support, meaning /∈ S(i), where
S(i) , { : [ Σ(i)x ] jj = α iff s(i)j = 0, ∀j } . (39)
With this segmentation in mind, the VAE objection naturally partitions as
L(θ,φ) = ∑
i
{ L(i)(θ,φ; /∈ S(i)) + L(i)(θ,φ; ∈ S(i)) } (40)
where L(i)(θ,φ; /∈ S(i)) denotes the cost for the i-th sample when integrated across those samples not in S(i), and L(i)(θ,φ; ∈ S(i)) is the associated complement.
D.2 Evaluation of L(i)(θ,φ; /∈ S(i))
First we define ρ = min
i,j∈{1,...,n},i6=j 1 2‖π(i) − π(j)‖2, (41)
which is just half the minimum distance between any two distinct coefficient expansions. If any z is within this distance of π(i), it will necessarily be quantized to this value per our previous definitions. Therefore if ‖Ŝ(i)z ‖2 < ρ, we are guaranteed that the correct generating support pattern will be mapped to Σ̂ (i)
x , and so it follows that
P ( /∈ S(i) ) ) ≤ P (∥∥∥Ŝ(i)z ∥∥∥ 2 > ρ ) = P (‖√α ‖2 > ρ) (42)
at our candidate solution. We also make use of the quantity
η , max i∈{1,...,n} ‖x(i) −Ψπ(i)‖22, (43)
which represents the maximum data-fitting error. Then for the i-th sample we have
L(i)(θ,φ; /∈ S(i))
=
∫
/∈S(i)
[( x(i) − Ŵ µ̂(i)z − Ŵ Ŝ (i) z )> ( Σ̂ (i) x )−1 ( x(i) − Ŵ µ̂(i)z − Ŵ Ŝ (i) z )
+ log ∣∣∣Σ̂(i)x ∣∣∣+ tr [ Σ̂ (i) z ] − log ∣∣∣Σ̂(i)z ∣∣∣+ ‖µ̂(i)z ‖22 ] p( )d
≤ ∫
‖√α ‖2>ρ
[( x(i) − Ŵ µ̂(i)z − Ŵ Ŝ (i) z )> ( Σ̂ (i) x )−1 ( x(i) − Ŵ µ̂(i)z − Ŵ Ŝ (i) z )
+ log ∣∣∣Σ̂(i)x ∣∣∣+ tr [ Σ̂ (i) z ] − log ∣∣∣Σ̂(i)z ∣∣∣+ ‖µ̂(i)z ‖22 ] p( )d
≤ ∫
‖√α ‖2>ρ
[ 1 α ( x(i) −Ψπ(i) −√αΨ )> ( x(i) −Ψπ(i) −√αΨ )
+ κα− κ logα+ ‖π(i)‖22 ] p( )d , (44)
where the second inequality comes from setting Σ̂ (i)
x = αI (its smallest possible value) in the inverse
term and Σ̂ (i)
x = I (its largest value) in the log-det term. Next, given that
‖x(i) −Ψπ(i)‖22 ≤ η∫
‖√α ‖2>ρ
( π(i) )> Ψ>Ψ · p( )d = 0 (45)
∫
‖√α ‖2>ρ ‖Ψ ‖22p( )d ≤ tr
[ Ψ>Ψ ] ,
it follows that the bound from (44) can be further reduced via
L(i)(θ,φ; /∈ S(i)) ≤ tr [ Ψ>Ψ ] +
∫
‖√α ‖2>ρ
[ 1 αη + κα− κ logα+ ‖π(i)‖22 ] p( )d
= Θ(1) + [
1 αη + κα− κ logα+ ‖π(i)‖22
] ∫
‖√α ‖2>ρ p( )d
≤ Θ(1) + [
1 αη + κα− κ logα+ ‖π(i)‖22 ] α ρ2
= Θ(1) + Θ(α2)−Θ(α logα) = Θ(1) as α→ 0, (46)
where the second inequality holds based on the vector version of Chebyshev’s inequality, which ensures that ∫
‖√α ‖2>ρ p( )d = P (‖√α ‖2 > ρ) ≤ αρ2 . (47)
Clearly then, as α becomes small, we have established that
L(i)(θ,φ; /∈ S(i))→ O (1) . (48)
D.3 Evaluation of L(i)(θ,φ; ∈ S(i))
In analyzing L(i)(θ,φ; ∈ S(i)), we note that ∫
∈S(i)
( x(i) − Ŵ µ̂(i)z − Ŵ Ŝ (i) z )> ( Σ̂ (i) x )−1 ( x(i) − Ŵ µ̂(i)z − Ŵ Ŝ (i) z ) p( )d
≤ ∫ ( x(i) −Ψπ(i) −√αΨ )> ( Λ(i) )−1 ( x(i) −Ψπ(i) −√αΨ ) p( )d ≤ ∫ ( x(i) −Ψπ(i) )> ( Λ(i) )−1 ( x(i) −Ψπ(i) ) p( )d + tr [ Ψ>Ψ ] ≤ η + tr [ Ψ>Ψ ]
= Θ(1) (49)
given the alignment of Λ(i) with zero-valued elements in x(i) −Ψπ(i). Furthermore, the remaining terms in L(i)(θ,φ; ∈ S(i)) are independent of giving
∫
∈S(i)
[ log ∣∣∣Σ̂(i)x ∣∣∣+ tr [ Σ̂ (i) z ] − log ∣∣∣Σ̂(i)z ∣∣∣+ ‖µ̂(i)z ‖22 ] p( )d
= [ log ∣∣∣Λ(i) ∣∣∣+ κα− κ logα+ ‖π(i)‖22 ] ∫
‖√α ‖2<ρ p( )d
= [ (r(i) − κ) logα+ κα+ ‖π(i)‖22
] ∫
‖√α ‖2<ρ p( )d
= [ (r(i) − κ) logα
] ∫
‖√α ‖2<ρ p( )d +O(α) +O(1), (50)
where r(i) , ∣∣∣ { j : Λ (i) jj = α }∣∣∣ = d− ‖s(i)‖0. (51) Therefore, since
∫ ‖√α ‖2<ρ p( )d → 1 as α becomes small, we may conclude that
L(i)(θ,φ; ∈ S(i))→ ( d− κ− ‖s(i)‖0 ) logα+O(1), (52)
D.4 Compilation of Candidate Solution Cost
After combining (48) and (52) across all i we find that
L(θ,φ)→ ∑
i
( d− κ− ‖s(i)‖0 ) logα+O(1) (53)
for any α ∈ (0, ᾱ] as ᾱ becomes small. If d > κ+ ‖s(i)‖0, then this expression will tend towards minus infinity, indicative of an objective value that is unbounded from below, certainly a fertile region for candidate minimizers. Note that per the theorem statement, L = UV and S must represent a unique feasible solution to
min L,S
d · rank[L] + ‖S‖0 s.t. X = L+ S. (54)
Given that each column x(i) has d degrees of freedom, then with U fixed there will be an infinite number of feasible solutions x(i) = Uv(i) + s(i) such that dim[v(i)] + ‖s(i)‖0 = κ+ ‖s(i)‖0 > d and a combinatorial number such that k + ‖s(i)‖0 = d. Therefore for uniqueness we require that k + ‖s(i)‖0 < d, so it follows that indeed L(θ,φ) will be unbounded from below as ᾱ and therefore α becomes small, with cost given by (53) as a candidate solution satisfying the conditions of the theorem.
Of course it still remains possible that some other candidate solution could exist that violates one of these conditions and yet still achieves (53) or an even lower cost. We tackle this issue next. For this purpose our basic strategy will be to examine a lower bound on L(θ,φ) and show that essentially any candidate solution violating the theorem conditions will be worse than (53).
D.5 Evaluation of Other Candidate Solutions
To begin, we first observe that if granted the flexibility to optimize Σ(i)x independently over all values of inside the integral for computing L(θ,φ), we immediately obtain a rigorous lower bound.9 For this purpose we must effectively solve decoupled problems of the form
inf γ>α
c γ + log γ, (55)
to which the optimal solution is just
γ∗ = ξα(x) , [c− α]+ + α, (56) 9Note that this is never possible in practice, even with an infinite capacity network for computing Σ(i)x , since it would require a unique network for each data sample; however, it nonetheless serves as a useful analysis tool.
where the operator [·]+ retains only the positive part of its argument, setting negative values to zero. Plugging this solution back into (55), we find that
inf γ>α
c γ + log γ = log ξα(c) +O(1). (57)
In the context of our bound, this leads to
L(θ,φ) ≥ ∑
i
  Ep( )  ∑
j
log ξα
([ x
(i) j −wj·µ(i)z −wj·S(i)z
]2)  
+ tr [ Σ(i)z ] − log ∣∣∣Σ(i)z ∣∣∣+ ‖µ(i)z ‖22 } +O(1). (58)
From this expression, it is clear that the lowest objective value we could ever hope to obtain cannot involve arbitrarily large values of Σ(i)z and µ (i) z since the respective trace and quadratic terms grow faster than log-det terms. Likewise µ(i)z cannot be unbounded for analogous reasons. Therefore, optimal solutions to (58) that will be unbounded from below must involve the first term becoming small, at least over a range of values with significant probability measure. Although the required integral admits no closed-form solution, we can simplify things further using refinements of the above bound.
For this purpose consider any possible candidate solution Ŵ = Ψ and µ̂(i)z = π (i) (not necessarily one that coincides with U and the optimal subspace), and define
∆(i)α (Ψ,π) , suppα [ x(i) −Ψπ ] . (59)
Without loss of generality we also specify that
S(i)z , Ξ(i)D(i), (60)
where Ξ(i) ∈ Rd×κ has orthonormal columns andD(i) is a diagonal matrix with [ D(i)
] kk = ξ√α ( σ (i) k ) , (61)
and σ(i) = [σ(i)1 , . . . , σ (i) κ ]> ∈ Rκ+ is an arbitrary non-negative vector. Any general Σ(i)z =
S(i)z ( S(i)z )> , with singular values bounded by α, is expressible via this format. We then reexpress
(58) as
L(θ,φ) ≥ ∑
i
   Ep( )  
∑
j∈∆(i)α (Ψ,π(i))
log ξα
([ x
(i) j −ψj·π(i) −ψj·Ξ(i)D(i) ]2)  
+ Ep( )  
∑
j /∈∆(i)α (Ψ,π(i))
log ξα ([ ψj·Ξ (i)D(i) ]2)   (62)
+ tr [ Ξ(i) ( D(i) )2 ( Ξ(i) )>] − log ∣∣∣∣Ξ (i) ( D(i) )2 ( Ξ(i) )>∣∣∣∣+ ‖π(i)‖22 } +O(1),
= ∑
i
   Ep( )  
∑
j∈∆(i)α (Ψ,π(i))
log ξα
  [ x
(i) j −ψj·π(i) −
∑
k
ψ̄jk · ξ√α ( σ (i) k ) · k ]2   
+ Ep( )  
∑
j /∈∆(i)α (Ψ,π(i))
log ξα
  [∑
k
ψ̄ (i) jk · ξ√α
( σ
(i) k ) · k ]2    (63)
+ ∑
k
ξα
[( σ
(i) k
)2] − ∑
k
log ξα
[( σ
(i) k )2] + ‖π(i)‖22 } +O(1),
where ψ̄(i)jk is the k-th element of the vector ψj·Ξ (i). We can now analyze any given point {Ψ,π(i),Ξ(i),σ(i)}ni=1 as α becomes small. The first term can be shown to be Θ(1) with all other variables fixed,10 leading to the revised bound
L(θ,φ) ≥ ∑
i
   Ep( )  
∑
j /∈∆(i)α (Ψ,π(i))
log ξα
  [∑
k
ψ̄ (i) jk · ξ√α
( σ
(i) k ) · k ]2    (64)
− ∑
k
log ξα
[( σ
(i) k
)2] }
+ Θ(1).
where the terms ∑ k ξα [( σ (i) k )2] and ‖π(i)‖22 have also been absorbed into Θ(1).
Given that
Ep( ) [ log ξα ([ a> ]2)] = log ξα [ max k (ak) 2 ] +O (1) ≥ logα+O (1) (65)
for any vector a, we have the new bound
L(θ,φ) (66)
≥ ∑
i
  
∑
j /∈∆(i)α (Ψ,π(i))
log ξα ( max k [ ψ̄ (i) jk · ξ√α ( σ (i) k )]2) − ∑
k
log ξα
[( σ
(i) k )2]    + Θ(1).
If then we choose σ(i)k = 0 for all i = 1, . . . n and k = 1, . . . , κ, then
max k
[ ψ̄
(i) jk · ξ√α
( σ
(i) k
)]2 = logα+ Θ(1) (67)
and we obtain the lower bound
L(θ,φ) ≥ ∑
i
( d− κ− ∣∣∣∆(i)α ( Ψ,π(i) )∣∣∣ ) logα+ Θ(1). (68)
Additionally, if any set ∆(i)α ( Ψ,π(i) ) exists such that
∑
i
( d− κ− ∣∣∣∆(i)α ( Ψ,π(i) )∣∣∣ ) ≤ ∑
i
( d− κ− ‖s(i)‖0 ) , (69)
then s(i) cannot be part of the unique, feasible solution to (10), i.e., we could use the support pattern from each ∆(i)α ( Ψ,π(i) ) to find a different feasible solution with equal or lower value of n · rank [L] + ‖S‖0, which would violate either the uniqueness or optimality of the original solution. Therefore, we have established that with σ(i)k = O(α) for all i and k, the resulting bound on L(θ,φ) is essentially no better than (53), or the same bound we had before from our feasible trial solution. Moreover, the resulting Ŵ = Ψ that maximizes this bound, as well as the implicit
Σ̂ (i)
x ( µ̂z [ x(i) ]) = diag [( x(i) −Ψπ(i) )2] , (70)
will necessarily satisfy (12). We then only need consider whether other choices for σ(i)k can do better.
Let Ψ̃ (i) denote the the rows of Ψ(i) associated with row indeces j /∈ ∆(i)α ( Ψ,π(i) ) , meaning the indices at which we assume no sparse corruption term exists. Additionally, defineB(i) , Ψ̃(i)Ξ(i). 10This is ultimately because
∫ |c1+x|>α log |c1 + x|N (x; 0, 1)dx < c2 ∫ |c1+x|>α log |c1 + x|dx = Θ(1) for
any c1 and a c2 sufficiently large.
This implies that ∑
j /∈∆(i)α (Ψ,π(i))
log ξα ( max k [ ψ̄ (i) jk · ξ√α ( σ (i) k )]2) − ∑
k
log ξα
[( σ
(i) k
)2] (71)
= ∑
j
log ξα ( max k [ B (i) jk · ξ√α ( σ (i) k )]2) − ∑
k
log ξα
[( σ
(i) k
)2] .
Contrary to our prior assumption σ(i) = 0, now consider any solution with ‖σ(i)‖0 = β > 0. For the time being, we also assume thatB(i) is full column rank. These conditions imply that
∑
j
log ξα ( max k [ B (i) jk · ξ√α ( σ (i) k )]2) ≥ ( d− β − ∣∣∣∆(i)α ( Ψ,π(i) )∣∣∣ ) logα+ Θ(1) (72)
since at least β elements of the summation over j must now be order Θ(1). By assumption we also have ∑ k log ξα [( σ (i) k )2] = (κ − β) logα + Θ(1). Combining with (72), we see that such
a solution is equivalent or worse than (68). So the former is the best we can do at any value of {Ψ,π(i),Ξ(i),σ(i)}ni=1, provided that B(i) is full rank, and obtaining the optimal value of ∆ (i) α ( Ψ,π(i) ) implies that (12) holds.
However, if B(i) is not full rank it would indeed entail that (71) could be reduced further, since a nonzero element of σ(i) would not increase the first summation, while it would reduce the second. But if such a solution were to exist, it would violate the uniqueness assumption of the theorem statement. To see this, note that rank[B(i)] = rank[Ψ̃ (i) ] since Ξ(i) is orthogonal, so if the former is not full column rank, neither is the latter. And if Ψ̃ (i)
is not full column rank, there will exist multiple solutions such that ‖x(i) −Ψπ(i)‖0 = ‖s(i)‖0 or equivalently ‖x(i) −Uv(i)‖0 = ‖s(i)‖0 in direct violation of the uniqueness clause.
Therefore to conclude, a lower bound on the VAE cost is in fact the same order as that obtainable by our original trial solution. If this lower bound is not achieved, we cannot be at a minimizing solution, and any solution achieving this bound must satisfy (12).
D.6 Generalization to Case where κ > rank[U ]
Finally, we briefly consider the case where κ > rank[U ] , τ , meaning thatW contains redundant columns that are unnecessary in producing an optimal solution to ( ??). The candidate solution described in Section D.1 can be expanded via Ŵ = [ Ψ, 0[d×(κ−τ)] ] ,µ(i)z = [ (π(i))>, 0[1×(κ−τ)] ]> , and Σ̂ (i) z = diag [ α1>[τ×1], 1 > [(κ−τ)×1] ] such that the same objective function value is obtained.
Now consider the general case where κ ≥ rank[Ŵ ] > τ . If we review the lower bound described in Section D.5, with this general Ŵ replacing Ψ, it can be shown that Σ̂ (i)
z will be forced to have additional diagonal elements lowered to α, increasing the achievable objective by at least − logα per sample. The details are not especially enlightening and we omit them here for brevity. Consequently, at any minimizer we must have rank[Ŵ ] = τ .
[193] Pellizzoni, L. (2003). Uncertainty and participatory democracy. Environmental Values, 12(2):195–224.
[194] Pieters, W., Padget, J., and Dechesne, F. (2013). Obligations to enforce prohibitions: on the adequacy of security policies. In Proceedings of the 6th International Conference on Security of Information and Networks, pages 54–61.
[195] Pieters, W., Padget, J., Dechesne, F., Dignum, V., and Aldewereld, H. (2015). Effectiveness of qualitative and quantitative security obligations. Journal of Information Security and Applications, 22:3 – 16.
[196] Pinker, S., Bizzi, E., Brenner, S., Chomsky, N., Minsky, M., Partee, B. H., and Winston, P. H. (2016). The Golden Age – A Look at the Original Roots of Artificial Intelligence, Cognitive Science, and Neuroscience (partial transcript).
[197] Pitt, J. and Artikis, A. (2015). The open agent society: retrospective and prospective views. Artificial Intelligence and Law, 23(3):241 – 270.
[198] Pitt, J. and Diaconescu, A. (2015). Structure and Governance of Communities for the Digital Society. In IEEE International Conference on Autonomic Computing (ICAC), pages 279–284.
[199] Pnueli, A. (1977). The Temporal Logic of Programs. In Foundations of Computer Science, 1977., 18th Annual Symposium on, pages 46–57. IEEE.
[200] Prakken, H. and Sartor, G. (1996). A dialectical model of assessing conflicting arguments in legal reasoning. Artificial Intelligence and Law, 4:331–368.
[201] Prakken, H. and Sergot, M. (1996). Contrary-to-duty obligations. Studia Logica, 57(1):91–115.
[202] Rao, A. S. (1996). AgentSpeak(L): BDI Agents speak out in a logical computable language. In European Workshop on Modelling Autonomous Agents in a Multi-Agent World, pages 42 – 55. Springer Berlin Heidelberg.
[203] Reiter, R. (1980). A logic for default reasoning. Artificial Intelligence, 13(1-2):81– 132.
[204] Republic of Italy (1947). Constitution of the Italian Republic.
[205] R.H.Bordini, J.F.Hubner, and M.Wooldridge (2007). Programming Multi-Agent Systems in AgentSpeak Using Jason. John Wiley & Sons.
[206] Ricciardi, M. (1997). Constitutive rules and institutions. In meeting of the Irish Philosophical Club, Ballymascanlon.
[207] Ross, A. Tû-Tû. Harvard Law Review (1956 - 1957), 70:812–825.
[208] Ruiter, D. (1997). A basic classification of legal institutions. Ratio Juris, 10(4):357 – 372.
REFERENCES 199
[209] Ryu, Y. U. and Lee, R. M. (1994). Defeasible deontic reasoning: A logic programming model. In Deontic logic in computer science, pages 225–241. John Wiley and Sons Ltd.
[210] Scharpf, F. W. (1997). Introduction: the problem-solving capacity of multi-level governance. Journal of European Public Policy, 4(4):520–538.
[211] Schlosser, M. (2015). Agency. In Zalta, E. N., editor, The Stanford Encyclopedia of Philosophy (Fall 2015 Edition). The Metaphysics Research Lab Center for the Study of Language and Information, Stanford University Stanford, CA 94305-4115.
[212] Searle, J. R. (1964). How to Derive “Ought” from “Is”. The Philosophical Review, 73(1):43–58.
[213] Searle, J. R. (1969). Speech acts: An essay in the philosophy of language. Cambridge university press.
[214] Searle, J. R. (1995). The Construction of Social Reality. The Free Press, New York.
[215] Searle, J. R. (2005). What is an institution? Journal of Institutional Economics, 1:1–22.
[216] Sergot, M. (1988). Representing legislation as logic programs. Oxford University Press.
[217] Sergot, M. J. (1982). Prospects for representing the law as logic programs. Logic Programming, pages 33–42.
[218] Shams, Z., Vos, M. D., Oren, N., and Padget, J. (2016). Normative Practical Reasoning via Argumentation and Dialogue. In Proceedings of the 25th International Conference on Artificial Intelligence. AAAI Press.
[219] Smith, M. E. (2004). Toward a Theory of EU Foreign Policy Making: Multi-level Governance, Domestic Politics, and National Adaptation to Europe’s Common Foreign and Security Policy. Journal of European Public Policy, 11(4):740–758.
[220] Suber, P. (1990). The Paradox of Self-Amendment: A Study of Law, Logic, Omnipotence, and Change. Peter Lang International Academic Publishers.
[221] The United States of America. The United States Constitution.
[222] Tinnemeier, N. A. M., Dastani, M. M., Meyer, J. J. C., and Van Der Torre, L. (2009). Programming normative artifacts with declarative obligations and prohibitions. Proceedings - 2009 IEEE/WIC/ACM International Conference on Intelligent Agent Technology, IAT 2009, 2:145–152.
[223] Tosatto, S. C., Boella, G., van der Torre, L., and Villata, S. (2012a). Abstract Normative Systems: Semantics and Proof Theory. In Principles of Knowledge Representation and Reasoning: Proceedings of the Thirteenth International Conference, pages 358– 368.
DOI:10.5121/ijfcst.2014.4308 87
the opinion word is “good” which is positively orientated. Semantic orientation is a task of determining whether a sentence has either positive, negative orientation or neutral orientation [6][14]. Opinion mining is performed at three levels [3]:
 Document level: At this level the whole document is classified as positive, negative or neutral.  Sentence level: At this level the whole sentence is classified as positive, negative or neutral.
 Aspect level: At this level the whole document/sentence is classified as positive, negative or neutral for each feature present in the document/sentence.
Document level and sentence level only classify the whole document or sentence, it does not identify the aspect present in the document/sentence i.e. if the polarity of the document is positive/negative it doesn’t mean that document possess positive/negative opinion for each aspect. To determine the opinion on every aspect, opinion mining at aspect level is performed. Two types aspects are found in user reviews explicit and implicit [2].
 Explicit aspects are those that are easily identified in the reviews, explicit aspects are noun and noun phrase. For example, “The voice quality of this phone is great, here voice quality is an explicit aspect and it can be directly seen in the sentence.  Implicit aspects are those that are not easily identified in the reviews, explicit aspects are not noun and a noun phrase. For example, “This phone is not fit in my pockets”, here not fit in my pockets is an implicit aspect it indicate the aspect size that cannot be directly identified in the sentence.
In this paper an Aspect based Opinion Mining system named as “Aspect based Sentiment Orientation System” is proposed which extracts the feature and opinions from sentences and determines whether the given sentences are positive, negative or neutral for each feature. Negation is also handled by the system. To determine the semantic orientation of the sentences a dictionary based technique of the unsupervised approach is adopted. To determine the opinion words and their synonyms and antonyms WordNet is used as a dictionary. The rest of the paper is organized as follows: Section 2 discusses related work. Section 3 describes the proposed approach. Section 4 shows the experimental results of the system. Section 5 concludes the paper.
Nous avons constaté que le comportement de l'algorithme est indépendant du type de définition. Cela montre que l'algorithme peut être utilisé pour n'importe quel type de texte et pas seulement pour des définitions.
Le nombre de groupes, la précision et le rappel sont proportionnels au seuil. Il faut mentionner que, mis à part le type de définition, le comportement de l'algorithme peut être divisé en zones par rapport à la valeur de seuil par distance :
1. La zone 1 où 0 ≤ α ≤ 0.7 : dans cette zone on obtient une très bonne précision (supérieure au 90%) et un rappel bas (inférieur à 40%). Il faut utiliser une valeur α dans cet intervalle pour obtenir des acceptions courantes, un nombre réduit de groupes (environ 5) avec peu de définitions et sans la présence d’intrus dans les groupes générés.
4 La cellule est composée d’un noyau enveloppé dans le protoplasme, autour duquel il y a une membrane qui sépare la cellule de son milieu. 5 La cellule est composée d’une membrane cellulaire qui enveloppe une masse visqueuse et granuleuse appelée le protoplasme, dans lequel on trouve toutes les organelles cellulaires, y compris le noyau.
EvalECD'2010 - 9 -
2. La zone 2 où 0.75 ≤ α ≤ 0.85 se caractérise par un haut degré de précision (autour de 80%) et un rappel moyen (autour de 50%). Cet intervalle maintient un bon équilibre entre la précision et le nombre de groupes générés (environ 10) ainsi qu’un rappel acceptable. 3. La zone 3 où 0.85 ≤ α ≤ 0.99, obtient une précision moyenne (environ 50%) et un rappel élevé (environ 80%). Le nombre de groupes générés dans cette zone est trop élevé (supérieur à 20) -il y a plus de groupes que d’acceptions-, par contre chaque groupe est très précis en termes de la cohérence de sa signification.

In this section we give a (necessarily brief) review of the main background concepts from the literature. We henceforth assume a countable set of propositional variables PROP and a finite set of agents AG. We let GR be the set of all non-empty groups, i.e., GR =℘(AG)\ /0. An epistemic model over PROP and AG (or just a model) M= (S,∼,V ) where S is a set of states (or worlds), V : PROP→ 2S associates a set of states V (p) with each propositional variable p, and ∼ is a function that maps each agent to a binary equivalence relation on S. We write ∼i for ∼(i).
s ∼i t means that agent i cannot discern between states s and t – if we are in s she doesn’t know whether we are in t, and vice versa. Considering the distributed knowledge of a group G – a key concept in the following – we define a derived relation ∼G= ⋂ a∈G ∼a (it is easy to see that ∼G is an equivalence relation). Intuitively, someone who has all the knowledge of all the members of G can discern between two states if and only if at least one member of G can discern between them. We will also consider common knowledge. A similar relation modeling the common knowledge of a group is obtained by taking the transitive closure of the union of the individual relations: vCG= ( ⋃ i∈G ∼i)∗.
Definition 1 Below are several languages from the literature.
(ELD) ϕ ::= p | ¬ϕ | ϕ ∧ϕ | Kiϕ | DGϕ (ELCD) ϕ ::= p | ¬ϕ | ϕ ∧ϕ | Kiϕ | DGϕ |CGϕ (PACD) ϕ ::= p | ¬ϕ | ϕ ∧ϕ | Kiϕ | DGϕ |CGϕ | [ϕ]ϕ,
where p ∈ PROP, i ∈ AG and G ∈ GR. We use the usual propositional derived operators, as well as EGϕ for ∧
i∈G Kiϕ . ELD and ELCD are static epistemic languages with distributed knowledge, and with distributed and common knowledge, respectively. These are the languages we will extend with resolution operators in the next section. We will also be interested in PACD, the language for public announcement logic with both common knowledge and distributed knowledge, when we look at completeness proofs.
Satisfaction of a formula ϕ of any of these languages in a state m of a model M, denoted M,m |= ϕ , is defined recursively by the following clauses:
M,m |= p iff m ∈V (p) M,m |= ¬ϕ iff M,m 6|= ϕ M,m |= ϕ ∧ψ iff M,m |= ϕ & M,m |= ψ M,m |= Kaϕ iff ∀n ∈ S. (m∼a n⇒M,n |= ϕ) M,m |= DGϕ iff ∀n ∈ S. (m∼G n⇒M,n |= ϕ) M,m |=CGϕ iff ∀n ∈ S. (m( ⋃ i∈G ∼i)∗n⇒M,n |= ϕ) M,m |= [ψ]ϕ iff M,m |= ψ ⇒M|ψ,m |= ϕ.
where R∗ denotes the transitive closure of R and M|ψ is the submodel of M restricted to {m∈M |M,m |= ψ}. Validity is defined as usual: |= ϕ means that M,m |= ϕ for all M and m.
We now define some axiom schemata and rules. The classical “S5” proof system for multi-agent epistemic logic, denoted (S5), consists of the following axioms and rules:
(PC) instances of tautologies (K) Ki(ϕ → ψ)→ (Kiϕ → Kiψ) (T) Kiϕ → ϕ (4) Kiϕ → KiKiϕ (5) ¬Kiϕ → Ki¬Kiϕ (MP) from ϕ and ϕ → ψ infer ψ (N) from ϕ infer Kiϕ .
Axioms for distributed knowledge, denoted (DK):
(KD) DG(ϕ → ψ)→ (DGϕ → DGψ) (TD) DGϕ → ϕ (5D) ¬DGϕ → DG¬DGϕ (D1) Kiϕ ↔ Diϕ (D2) DGϕ → DHϕ , if G⊆ H.
Axioms and rules for common knowledge, denoted (CK):
(KC) CG(ϕ → ψ)→ (CGϕ →CGψ) (TC) CGϕ → ϕ (C1) CGϕ → EGCGϕ (C2) CG(ϕ → EGϕ)→ (ϕ →CGϕ) (NC) from ϕ infer CGϕ .
The system that consists of (S5) and (DK) over the language ELD, denoted S5D, is a sound and complete axiomatization of all ELD validities. The system that consists of (S5), (DK) and (CK) over the language ELCD is a sound and complete axiomatization of all ELCD validities.
Several versions of the Multi-Access Broadcast Channel (MABC) problem can be found in the literature. We will use the description given by Hansen et al. (2004) that allows this problem to be formalized as a DEC-POMDP.
In the MABC, we are given two nodes (computers) which are required to send messages to each other over a common channel for a given duration of time. Time is imagined to be split into discrete periods. Each node has a buffer with a capacity of one message. A buffer that is empty in a period is refilled with a certain probability in the next period. In a period, only one node can send a message. If both nodes send a message in the same period, a collision of the messages occurs and neither message is transmitted. In case of a collision, each node is intimated about it through a collision signal. But the collision
signaling mechanism is faulty. In case of a collision, with a certain probability, it does not send a signal to either one or both nodes.
We are interested in pre-allocating the channel amongst the two nodes for a given number of periods. The pre-allocation consists of giving the channel to one or both nodes in a period as a function of the node’s information in that period. A node’s information in a period consists only of the sequence of collision signals it has received till that period.
In modeling this problem as a DEC-POMDP, we obtain a 2-agent, 4-state, 2-actionsper-agent, 2-observations-per-agent DEC-POMDP whose components are as follows.
• Each node is an agent.
• The state of the problem is described by the states of the buffers of the two nodes. The state of a buffer is either Empty or Full. Hence, the problem has four states: (Empty, Empty), (Empty, Full), (Full, Empty) and (Full, Full).
• Each node has two possible actions, Use Channel and Don’t Use Channel.
• In a period, a node may either receive a collision signal or it may not. So each node has two possible observations, Collision and No Collision.
The initial state of the problem α is (Full, Full). The state transition function P, the joint observation function G and the reward function R have been taken from Hansen et al. (2004). If both agents have full buffers in a period, and both use the channel in that period, the state of the problem is unchanged in the next period; both agents have full buffers in the next period. If an agent has a full buffer in a period and only he uses the channel in that period, then his buffer is refilled with a certain probability in the next period. For agent 1, this probability is 0.9 and for agent 2, this probability is 0.1. If both agents have empty buffers in a period, irrespective of the actions they take in that period, their buffers get refilled with probabilities 0.9 (for agent 1) and 0.1 (for agent 2).
The observation function G is as follows. If the state in a period is (Full, Full) and the joint action taken by the agents in the previous period is (Use Channel, Use Channel), the probability that both receive a collision signal is 0.81, the probability that only one of them receives a collision signal is 0.09 and the probability that neither of them receives a collision signal is 0.01. For any other state the problem may be in a period and for any other joint action the agents may have taken in the previous period, the agents do not receive a collision signal.
The reward function R is quite simple. If the state in a period is (Full, Empty) and the joint action taken is (Use Channel, Don’t Use Channel) or if the state in a period is (Empty, Full) and the joint action taken is (Don’t Use Channel, Use Channel), the reward is 1; for any other combination of state and joint action, the reward is 0.
We have evaluated the various algorithms on this problem for three different horizons (3, 4 and 5) and the respective optimal policies have a value of 2.99, 3.89 and 4.79. Results are detailed in Table 8 where, for each horizon and algorithm, the value and the computation time for the best policy found are given.
The results show that the MILP compares favorably to more classical algorithms except for GMAA* that is always far better for horizon 4 and, for horizon 5, roughly within the
same order of magnitude as MILP with the more pertinent heuristics. As expected, apart for the simplest setting (horizon of 3), NLP based resolution can not find the optimal policy of the DEC-POMDP, but the computation time is lower than the other methods. Among MILP methods, MILP-2 is better than MILP even with the best heuristics for horizon 3 and 4. When the size of the problem increases, heuristics are the only way for MILPs to be able to cope with the size of the problem. The table also shows that, for the MABC problem, pruning extraneous histories using the LOC heuristic is always a good method and further investigation revealed that 62% of the heuristics proved to be locally extraneous. As far are cutting bounds are concerned, they don’t seem to be very useful at first (for horizon 3 and 4) but are necessary for MILP to find a solution for horizon 5. For this problem, one must also have in mind that there is only one optimal policy for each horizon.
In this section, we tackle the problem of learning from data a Mahalanobis distance for supervised classification and compare our methods to state-of-the-art Mahalanobis metric learning algorithms.
We develop a condition on TBoxes, called unraveling tolerance, that is sufficient for the TBox to be Datalog6=-rewritable for PEQ, and thus also sufficient for PEQ-evaluation w.r.t. the TBox being in PTIME. Unraveling tolerance strictly generalizes syntactic ‘Horn conditions’ such as the ones used to define the DL Horn-SHIQ, which was designed as a (syntactically) maximal DL with PTIME query evaluation [HMS07, EGOS08].
Unraveling tolerance is based on an unraveling operation on ABoxes, in the same spirit as the unfolding of an interpretation into a tree interpretation we discussed above. Formally, the unraveling Au of an ABox A is the following (possibly infinite) ABox:
• Ind(Au) is the set of sequences b0r0b1 · · · rn−1bn, n ≥ 0, with b0, . . . , bn ∈ Ind(A) and r0, . . . , rn−1 ∈ NR∪N − R such that for all i < n, we have ri(bi, bi+1) ∈ A and (bi−1, r − i−1) 6=
(bi+1, ri) when i > 0; • for each C(b) ∈ A and α = b0r0 · · · rn−1bn ∈ Ind(Au) with bn = b, C(α) ∈ Au; • for each α = b0r0 · · · rn−1bn ∈ Ind(Au) with n > 0, rn−1(b0r0 · · · rn−1bn−1, α) ∈ Au.
For all α = b0 · · · bn ∈ Ind(Au), we write tail(α) to denote bn. Note that the condition (bi−1, r − i−1) 6= (bi+1, ri) is needed to ensure that functional roles can still be interpreted in a functional way after unraveling.
Definition 20 (Unraveling Tolerance). A TBox T is unraveling tolerant if for all ABoxes A and ELIQs q, we have that T ,A |= q implies (T ,Au) |= q.
It is not hard to prove that the converse direction ‘T ,Au |= q implies T ,A |= q’ is true for all ALCFI-TBoxes. Note that it is pointless to define unraveling tolerance for queries that are not necessarily tree shaped, such as CQs.
Example 21. (1) The ALC-TBox T1 = {A ⊑ ∀r.B} is unraveling tolerant. This can be proved by showing that (i) for any (finite or infinite) ABox A, the interpretation I+A that is obtained from A by extending BI +
A with all a ∈ Ind(A) that satisfy ∃r−.A in A (when viewed as an interpretation) is an ELIQmaterialization of T1 and A; and (ii) I + A |= C(a) iff I + Au |= C(a) for all ELIQs C(x) and a ∈ Ind(A). The proof of (ii) is based on a straightforward induction on the structure of the ELIconcept C . As illustrated by the ABox A = {r(a, b), A(a)} and the fact that Au,T |= B(b), the use of inverse roles in the definition of Au is crucial here despite the fact that T1 does not use inverse roles.
(2) A simple example for an ALC-TBox that is not unraveling tolerant is
T2 = {A ⊓ ∃r.A ⊑ B,¬A ⊓ ∃r.¬A ⊑ B}.
For A = {r(a, a)}, it is easy to see that we have T2,A |= B(a) (use a case distinction on the truth value of A at a), but T2,Au 6|= B(a).
Before we show that unraveling tolerance indeed implies PTIME query evaluation, we first demonstrate the generality of this property by relating it to Horn-ALCFI , the ALCFI-fragment of HornSHIQ. Different versions of Horn-SHIQ have been proposed in the literature, giving rise to different versions of Horn-ALCFI [HMS07, KRH07, EGOS08, Kaz09]. As the original definition from [HMS07] based on polarity is rather technical, we prefer to work with the following equivalent and less cumbersome definition. A Horn-ALCFI-TBox T is a finite set of concept inclusions L ⊑ R and functionality assertions where L and R are built according to the following syntax rules:
R,R′ ::=⊤ | ⊥ | A | ¬A | R ⊓R′ | L → R | ∃r.R | ∀r.R
L,L′ ::=⊤ | ⊥ | A | L ⊓ L′ | L ⊔ L′ | ∃r.L
with r ranging over NR ∪ N − R
and L → R abbreviating ¬L ⊔ R. Whenever convenient, we may assume w.l.o.g. that T contains only a single concept inclusion ⊤ ⊑ CT where CT is built according to the topmost rule above.
By applying some simple transformations, it is not hard to show that every Horn-ALCFITBox according to the original polarity-based definition is equivalent to a Horn-ALCFI-TBox of the form introduced here. Although not important in our context, we note that even a polynomial time transformation is possible.
Theorem 22. Every Horn-ALCFI-TBox is unraveling tolerant.
Proof. As a preliminary, we give a characterization of the entailment of ELIQs in the presence of Horn-ALCFI-TBoxes which is in the spirit of the chase procedure as used in database theory [FKMP05, CGK13] and of consequence-driven algorithms as used for reasoning in Horn description logics such as EL++ and Horn-SHIQ [BBL05, Kaz09, Krö10b].
We use extended ABoxes, i.e., finite sets of assertions C(a) and r(a, b) with C a potentially compound concept. An ELIU⊥-concept is a concept that is formed according to the second syntax rule in the definition of Horn-ALCFI . For an extended ABox A′ and an assertion C(a), C an ELIU⊥-concept, we write A′ ⊢ C(a) if C(a) has a syntactic match in A′, formally:
• A′ ⊢ ⊤(a) is unconditionally true; • A′ ⊢ ⊥(a) if ⊥(b) ∈ A′ for some b ∈ Ind(A); • A′ ⊢ A(a) if A(a) ∈ A′; • A′ ⊢ C ⊓D(a) if A′ ⊢ C(a) and A′ ⊢ D(a); • A′ ⊢ C ⊔D(a) if A′ ⊢ C(a) or A′ ⊢ D(a); • A′ ⊢ ∃r.C(a) if there is an r(a, b) ∈ A′ such that A′ ⊢ C(b).
Now for the announced characterization. Let T = {⊤ ⊑ CT } be a Horn-ALCFI-TBox and A a potentially infinite ABox (so that the characterization also applies to unravelings of ABoxes). We produce a sequence of extended ABoxes A0,A1, . . . , starting with A0 = A. In what follows, we use additional individual names of the form ar1C1 · · · rkCk with a ∈ Ind(A0), r1, . . . , rk roles that occur in T , and C1, . . . , Ck subconcept of concept in T . We assume that NI contains such names as needed and use the symbols a, b, . . . also to refer to individual names of this compound form. Each extended ABox Ai+1 is obtained from Ai by applying the following rules in a fair way:
R1 if a ∈ Ind(Ai), then add CT (a). R2 if C ⊓D(a) ∈ Ai, then add C(a) and D(a); R3 if C → D(a) ∈ Ai and Ai ⊢ C(a), then add D(a); R4 if ∃r.C(a) ∈ Ai and func(r) /∈ T , then add r(a, arC) and C(arC); R5 if ∃r.C(a) ∈ Ai, func(r) ∈ T , and r(a, b) ∈ Ai, then add C(b);
R6 if ∃r.C(a) ∈ Ai, func(r) ∈ T , and there is no r(a, b) ∈ Ai, then add r(a, arC) and C(arC); R7 if ∀r.C(a) ∈ Ai and r(a, b) ∈ Ai, then add C(b).
Let Ac = ⋃ i≥0Ai be the completion of the original ABox A. 2 Note that Ac may be infinite even if A is finite, and that none of the above rules is applicable in Ac. We write ‘Ac ⊢ ⊥’ instead of ‘Ac ⊢ ⊥(a) for some a ∈ NC’. If A 6⊢ ⊥, then Ac corresponds to an interpretation Ic in the standard way, i.e.,
∆Ic = Ind(Ac) AIc = {a | A(a) ∈ Ac} for all A ∈ NC rIc = {r(a, b) | r(a, b) ∈ Ac} for all r ∈ NR
where in Ic we assume that only the individual names in Ind(A) are elements of NI.
Claim 1. If Ac 6⊢ ⊥, then Ic is a PEQ-materialization of T and A.
To prove Claim 1 it suffices to show that there is a homomorphism h preserving Ind(A) into any model J of T and A and that Ic is a model of T and A. The homomorphism h can be constructed inductively following the construction of Ac. Similarly, following the construction of Ac it is readily checked that Ic is a model of T and A. Using Claim 1 and the easily proved fact that Ac 6⊢ ⊥ iff A is consistent w.r.t. T one can now prove
Claim 2. T ,A |= C(a) iff Ac ⊢ C(a) or Ac ⊢ ⊥, for all ELIQs C(x) and a ∈ Ind(A).
We now turn to the actual proof of Theorem 22. Consider the application of the above completion construction to both the original ABox A and its unraveling Au. Recall that individuals in Au are of the form a0r0a1 · · · rn−1an. Consequently, individuals in Auc take the form a0r0a1 · · · rn−1ans1C1 · · · skCk. For a ∈ Ind(Ac) and α ∈ Ind(Auc ), we write a ∼ α if a and α are of the form ans1C1 · · · skCk and a0r0a1 · · · rn−1ans1C1 · · · skCk, respectively, with k ≥ 0. Note that, in particular, a ∼ a for all a ∈ Ind(A). The following claim can be shown by induction on i.
Claim 3. For all a ∈ Ind(Ai) and α ∈ Ind(Aui ) with a ∼ α, we have (1) Ai ⊢ C(a) iff Aui ⊢ C(α) for all ELI-concepts C; (2) C(a) ∈ Ai iff C(α) ∈ Aui for all subconcepts C of concepts in T . Now, unraveling tolerance of T follows from Claims 2 and 3. ❏
Theorem 22 shows that unraveling tolerance and Horn logic are closely related. Yet, the next example demonstrates that there are unraveling tolerant ALCFI-TBoxes that are not equivalent to any Horn sentence of FO. Since any Horn-ALCFI-TBox is equivalent to such a sentence, it follows that unraveling tolerant ALCFI-TBoxes strictly generalize Horn-ALCFI-TBoxes. This increased generality will pay off in Section 5 when we establish a dichotomy result for TBoxes of depth one.
Example 23. Take the ALC-TBox
T = {∃r.(A ⊓ ¬B1 ⊓ ¬B2) ⊑ ∃r.(¬A ⊓ ¬B1 ⊓ ¬B2)}.
One can show as in Example 21 (1) that T is unraveling tolerant; here, the materialization is actually A itself rather than some extension thereof, i.e., as far as ELIQ (and even PEQ) evaluation is concerned, T cannot be distinguished from the empty TBox.
2Order of rule application has an impact on the shape of Ac, but is irrelevant for the remainder of the proof.
It is well-known that FO Horn sentences are preserved under direct products of interpretations [CK90]. To show that T is not equivalent to any such sentence, it thus suffices to show that T is not preserved under direct products. This is simple: let I1 and I2 consist of a single r-edge between elements d and e, and let e ∈ (A⊓B1⊓¬B2)I1 and e ∈ (A⊓¬B1⊓B2)I2; then the direct product I of I1 and I2 still has the r-edge between (d, d) and (e, e) and satisfies (e, e) ∈ (A⊓¬B1 ⊓¬B2)I , thus is not a model of T .
We next show that unraveling tolerance is indeed a sufficient condition for Datalog6=-rewritability (and thus for PTIME query evaluation). In Section 6, we will establish a connection between query evaluation under DL TBoxes and constraint satisfaction problems (CSPs). The Datalog6= program constructed in the proof of the following result resembles canonical monadic Datalog programs for CSPs [FV93]. Note that it is not clear how to attain a proof of Theorem 24 via the CSP connection: first, as we will also see in Section 6, functional roles break this connection; and second, canonical Datalog programs are available only for Boolean queries while we construct programs for unary queries.
Theorem 24. Every unraveling tolerant ALCFI-TBox is Datalog6=-rewritable for PEQ.
Proof. Let T be an unraveling tolerant ALCFI-TBox. By Theorem 19, it suffices to show that T is Datalog6=-rewritable for ELIQ. Let A be an ABox and q = C0(x) an ELIQ. Recall from the proof of Theorem 19 that cl(T , C0) denotes the closure under single negation of the set of subconcepts of T and C0. For an interpretation I and d ∈ ∆I , we use tIT ,q(d) to denote the set of concepts C ∈ cl(T , C0) such that d ∈ CI . A T , q-type is a subset t ⊆ cl(T , C0) such that for some model I of T , we have t = tIT ,q(d). We use tp(T , q) to denote the set of all T ,q-types. For t, t ′ ∈ tp(T , q) and r a role, we write t r t′ if there are a model I of T and d, d′ ∈ ∆I such that tIT ,q(d) = t, tIT ,q(d ′) = t′, and (d, d′) ∈ rI .
Introduce, for every set T ⊆ tp(T , C0) a unary IDB relation PT . Define a Datalog6= program Π that consists of the following rules:
(1) PT (x) ← A(x) for all concept names A ∈ cl(T , C0) and T = {t ∈ tp(T , q) | A ∈ t}; (2) PT (x) ← PT0(x) ∧ r(x, y) ∧ PT1(y) for all T0, T1 ⊆ tp(T , q) and all role names r that
occur in cl(T , C0) and their inverses, where T = {t ∈ T0 | ∃t′ ∈ T1 : t r t′}; (3) PT0∩T1(x) ← PT0(x) ∧ PT1(x) for all T0, T1 ⊆ tp(T , q); (4) goal(x) ← PT (x) for all T ⊆ tp(T , q) such that t ∈ T implies C0 ∈ T ; (5) goal(x) ← P∅(y); (6) goal(x) ← r(y, z1) ∧ r(y, z2) ∧ z1 6= z2 for all func(r) ∈ T .
To show that Π is a rewriting of the OMQ (T , C0(x)), it suffices to establish the following.
Claim. A |= Π(a0) iff T ,A |= C0(a0) for all ABoxes A and a0 ∈ Ind(A).
The “if” direction is straightforward: by induction on the number of rule applications, one can show that whenever Π derives PT (a), then every model of T and A satisfies tIT ,q(a) ∈ T . By definition of the goal rules of Π, A |= Π(a0) thus implies that every model of T and A makes C0(a0) true or that A is inconsistent w.r.t. T . Consequently, T ,A |= C0(a0).
For the “only if” direction, it suffices to show that A 6|= Π(a0) implies T ,Au 6|= C0(a0) since T is unraveling tolerant. Because of the rules in Π of the form (3), for every a ∈ Ind(A) we can find a unique minimal Ta such that PTa(a) is derived by Π. Observe that, A(α) ∈ A
u, tail(α) = a, and t ∈ Ta implies A ∈ t because of the rules of the form (1) in Π and by construction of Au.
We first associate with every α ∈ Ind(Au) a concrete T , q-type tα ∈ Ttail(α). To start, we choose ta ∈ Ta arbitrarily for all a ∈ Ind(A). Now assume that tα has already been chosen and that
β = αrb ∈ Ind(Au). Then r(tail(α), b) ∈ A. Because of the rules in Π of the form (2) and (5), we can thus choose tβ ∈ Tb such that tα r tβ . In this way, all types tα will eventually be chosen. We now construct an interpretation I , starting with
∆I = Ind(Au)
AI = {α | A ∈ tα} for all concept names A
rI = {(α, β) | r(α, β) ∈ Au} for all role names r.
Next, consider every α ∈ Ind(Au) and every ∃r.C ∈ tα such that Au does not contain an assertion r(α, β) with C ∈ tβ . First assume that func(r) 6∈ T . There must be a T , q-type t such that tα r t and C ∈ t. Choose a model Jα,∃r.C of T and D = ⊓ ta ⊓ ∃r.⊓ t, a d ∈ DJα,∃r.C , and an e ∈ (⊓ t)Jα,∃r.C with (d, e) ∈ rJα,∃r.C . W.l.o.g., we can assume that Jα,∃r.C is tree-shaped with root d. Let J−α,∃r.C be obtained from Jα,∃r.C by dropping the subtree rooted at e. Now disjointly add J−α,∃r.C to I , additionally including (a, d) in r I . Now assume that func(r) ∈ T . Then, if there exists r(α, β) ∈ Au, then C ∈ tβ as otherwise we do not have tα r tβ . Thus, assume there is no r(α, β) ∈ Au. There must be a T , q-type t such that tα r t and C ∈ t. We then have D ∈ t for all ∃r.D ∈ tα and so construct only a single J − α,∃r.C for the role r and disjointly add J − α,∃r.C to I , additionally including (a, d) in rI . This finishes the construction of I . The following claim can be proved by induction on C , details are omitted.
Claim. For all C ∈ cl(T , C0) : (a) α ∈ CI iff C ∈ tα for all α ∈ Ind(Au) and
(b) d ∈ CJα,∃r.D iff d ∈ CI for all Jα,∃r.D and all d ∈ ∆ J− α,∃r.D .
By construction of I and since A(α) ∈ Au implies A ∈ tα, I is a model of A. Due to the rules in Π that are of the form (4), Point (a) of the claim yields I 6|= C0(a0). Finally, we observe that I is a model of T . The concept inclusions in T are satisfied by the above claim, since C ⊑ D ∈ T means that C ∈ t implies D ∈ t for every T , q-type t, and since each Jα,∃r.C is a model of T . Due to the rules in Π that are of the form (6) and since each Jα,∃r.C is a model of T , all functionality assertions in T are satisfied as well. Summing up, we have shown that T ,Au 6|= C0(a0), as required. ❏
Together with Theorems 19 and 22, Theorem 24 also reproves the known PTIME upper bound for the data complexity of CQ-evaluation in Horn-ALCFI [EGOS08].
By Theorems 18 and 24, unraveling tolerance implies materializability unless PTIME = NP. Based on the disjunction property, this implication can also be proved without the side condition.
Theorem 25. Every unraveling tolerant ALCFI-TBox is materializable.
Proof. We show the contrapositive using a proof strategy that is very similar to the second step in the proof of Theorem 18. Thus, take an ALCFI-TBox T that is not materializable. By Theorem 16, T does not have the disjunction property. Thus, there are an ABox A∨, ELIQs C0(x0), . . . , Ck(xk), and a1, . . . , ak ∈ Ind(A∨) such that T ,A∨ |= C0(a0) ∨ · · · ∨ Ck(ak), but T ,A∨ 6|= Ci(ai) for all i ≤ k. Let Ai be Ci viewed as a tree-shaped ABox with root bi, for all i ≤ k. Assume w.l.o.g. that none of the ABoxes A∨,A0, . . . ,Ak share any individual names and reserve a fresh individual name b and role names r, r0, . . . , rk that do not occur in T . Construct an ABox
A = A∨ ∪ A0 ∪ · · · ∪ Ak ∪ {r(b, b0), . . . , r(b, bk)} ∪ {r0(bj , b0), . . . , rj−1(bj , bj−1), rj+1(bj , bj+1), . . . , rk(bj , bk)}
∪ {r0(b0, a0), . . . , rk(bk, ak)}
and an ELIQ q = ∃r.(∃r0.C0 ⊓ · · · ⊓ ∃rk.Ck)(x). By the following claim, A and q witness that T is not unraveling tolerant.
Claim. T ,A |= q(b), but T ,Au 6|= q(b).
Proof. “T ,A |= q(b)”. Take a model I of T and A. By construction of A, we have bIi ∈ (∃rj.Cj) I whenever i 6= j. Due to the edges r0(b0, a0), . . . , rk(bk, ak) and since T ,A∨ |= C0(a0) ∨ · · · ∨ Ck(ak), we thus find at least one bi such that bIi ∈ (∃ri.Ci) I . Consequently, I |= q(b).
“T ,Au 6|= q(b)” (sketch). Consider the elements brbiriai in Au. Each such element is the root of a copy of the unraveling Au∨ of A∨, restricted to those individuals in A∨ that are reachable from ai. Since T ,A∨ 6|= Ci(ai), we find a model Ii of T and A∨ with a Ii i /∈ C Ii i . By unraveling I , we obtain a model Iui of T and A u ∨ with a Iui i /∈ C Iui i . Combining the models I u 0 , . . . ,I u k in a suitable way, one can craft a model I of T and Au∨ such that brbiria I i /∈ C I i for all i ≤ k. Consequently, I 6|= q(b). ❏
Keywords geo-semantic relatedness · geo-semantic similarity · gold standards · geo-semantics · cognitive plausibility · GeReSiD
In this case study, we compare the relative impact of several of our proposed extensions to lattice regression. The business entity resolution problem is to determine if two business descriptions refer to the same real-world business. This problem is also treated by Dalvi et al. (2014), where they focus on defining a good title similarity. Here, we consider only the problem of fusing different similarities (such as a title similarity and phone similarity) into one score that predicts whether a pair of businesses are the same business. The learned function is required to be monotonically increasing in seven attribute similarities, such as the similarity between the two business titles and the similarity between the street names. There are two other features with no monotonicity constraints, such as the geographic region, which takes on one of 14 categorical values. Each sample is derived from a pair of business descriptions, and a label provided by an expert human rater indicating whether that pair of business descriptions describe the same real-world business. We measure accuracy in terms of whether a predicted label matches the ground truth label, but in actual usage, the learned function is also used to rank multiple matches that pass the decision threshold, and thus a strictly monotonic function is preferred to a piecewise constant function. The training and test sets, detailed in Table 3, were randomly split from the complete labeled set. Most of the samples were drawn using active sampling, so most of the samples are difficult to classify correctly.
Table 4 reports results. The linear model performed poorly, because there are many important high-order interactions between the features. For example, the pair of businesses might describe two pizza places at the same location, one of which recently closed, and the other recently opened. In this case, location-based features will be strongly positive, but the classifier must be sensitive to low title similarity to determine the businesses are different. On the other hand, high title similarity is not sufficient to classify the pair as the same, for example, two Starbucks cafes across the street from each other in downtown London.
The lattice regression model was first optimized using cross-validation, and then we made the series of minor changes (with all else held constant) listed in Table 4 to illustrate the impact of these changes on accuracy. First, removing the monotonicity constraints resulted in a statistically significant drop in accuracy of half a percent. Thus it appears the monotonicity constraints are successfully regularizing given the small amount of training data and the known high Bayes error in some parts of the feature space. Lattice regression without the monotonicity constraints performed similarly to random forests (and not statistically significantly better), as expected due to the similar modeling abilities of the methods.
The cross-validated lattice was 3 × 3 × 3 × 26, where the first three features used a missing data vertex (so the non-missing data is interpolated from a 29 lattice). Calibrating the missing values for those three features instead of using missing data vertices statistically significantly dropped the accuracy from 81.9% to 80.7%. (However, if one subsamples the training set down to 3000 samples, then the less flexible option of calibrating the missing values works better than using missing data vertices.)
The cross-validated calibration used five changepoints for two of the four continuous features, and no calibration for the two other continuous features. Figure 8 shows the calibrations learned in the optimized lattice regression. Removing the continuous signal calibration resulted in a statistically significant drop in accuracy.
Another important proposal of this paper is calibrating categorical features to realvalued features. For this problem, this is applied to a feature specifying which of 14 possible geographical categories the businesses are in. Removing this geographic feature statistically significantly reduced the accuracy by half a percent.
The amount of torsion regularization was cross-validated to be 10−4. Changing to graph Laplacian and re-optimizing the amount of regularization decreased accuracy slightly, but not statistically significantly so. This is consistent with what we often find: torsion is often slightly better, but often not statistically significantly so, than the graph Laplacian regularizer.
Changing the multilinear interpolation to simplex interpolation (see Section 5.2) dropped the accuracy slightly, but not statistically significantly. For some problems we even see simplex interpolation provide slightly better results, but generally the accuracy difference between simplex and multilinear interpolation is negligible.
The appendix collects miscellaneous results that are needed in the main body of the text.
A.1 Proof of the Second Part of Theorem 1
We provide here a full proof of the second part of Theorem 1. First, we need some background. Let X = (X ,A) be a measurable space, Θ ⊂ RK open, p ≡ p(·; θ)θ∈Θ be a family of densities with respect to ν, a σ-finite measure on X such that p(·; θ) is defined on the closure Θ̄ of Θ and p is measurable on the product σ-algebra ofX ×Θ where Θ is equipped with the σ-algebra of Borel sets. Denote by F (θ) = ∫ (∂ log p∂θ (x; θ))( ∂ log p ∂θ (x; θ))
>p(x; θ)ν(dx) be the Fisher information matrix of p at θ. The family p is called regular if the following hold:
(a) p(x; θ) is a continuous function on Θ for ν-almost all x;
(b) p possesses finite Fisher’s information at each point θ ∈ Θ; (c) the function ψ(·; θ) is continuous in the space L2(ν). Theorem 5 (Cramer-Rao Lower Bound). Let p = (p(x; θ))x∈X ,θ∈Θ be a regular family of densities with information matrix F (θ) 0, θ ∈ Θ. Pick θ ∈ Θ and assume that ψ : Θ→ R, t : X → R are measurable such that u 7→ ∫ (t(x)− ψ(u))2p(x;u)ν(dx) is bounded in a neighborhood of θ and ψ
is differentiable. Then, the bias d(u) = ∫ t(x)p(x;u)ν(dx)−ψ(u) is continuously differentiable in a neighborhood of the point θ ∈ Θ and
E [ (t(X)− ψ(θ))2 ] ≥ (ψ′(θ) + d′(θ))> F−1(θ) (ψ′(θ) + d′(θ)) + ‖d′(θ)‖22 , (8)
where X ∼ p(·; θ)ν(·).
The proof follows closely that of Theorem 7.3 of Ibragimov and Has’minskii (1981), which states this result for ψ(θ) = θ (and thus k = K) only, and is hence omitted.
With this, we can present the details of the proof of the second part of Theorem 1. Choose X = A × R, p(a, y; θ) = πD(a)ϕ(y; r(a), σ2(a)), where θ = (r(a))a∈A is the unknown parameter to be estimated, and ϕ(·;µ, σ2) is the density of the normal distribution with mean µ and variance σ2, Θ = R. It is easy to see that p = (p(·; θ)θ∈Θ) is a regular family. Let the quantity to be estimated be ψ(θ) = ∑ a π(a)r(a). By Theorem 5, for any estimator A, if v̂n is the estimate constructed by A based on the data Dn generated from p(·; θ) in an i.i.d. fashion, the bias dn(θ) = Eθ[v̂n] is differentiable on Θ and
MSE (v̂) ≥ 1 n (ψ′(θ) + d′n(θ)) > F−1(θ) (ψ′(θ) + d′n(θ)) + ‖d′n(θ)‖ 2 2 , (9)
where F (θ) is the Fisher information matrix underlying p(·; θ). If MSE (v̂n) 6→ 0 then lim supn→∞ MSE(v̂n) V1/n
= +∞. Hence, it suffices to consider A such that MSE (v̂n) → 0. Then, by (9), 0 ≤ ‖d′n(θ)‖ 2 2 ≤ MSE (v̂n), hence we also have ‖d′n(θ)‖ 2 2 → 0.
Now, a direct calculation shows that F (θ) = diag(. . . , πD(a)/σ2(a), . . .) and ψ′(θ) = π. Hence, ψ′(θ)>F−1(θ)ψ′(θ) = V1 and using again (9),
lim sup n→∞
MSE (v̂n)
V1/n ≥ 1− 2 lim sup n→∞
(d′n(θ)) >F−1(θ)ψ′(θ)
V1 = 1 ,
finishing the proof.
A.2 Proof for Proposition 1
In the proof, we use the shorthand v̂LR for v̂LR(π, πD, Dn). As already noted, the estimator is unbiased, so its MSE equals its variance. Since samples in Dn are independent, we have
V(v̂LR) = 1 n V ( π(A) πD(A) R ) .
The law of total variance implies
V(v̂LR) = 1 n E [ V ( π(A) πD(A) R|A )] + 1 n V [ E ( π(A) πD(A) R|A )] .
The first term equals
1 n E [( π(A) πD(A) )2 σ2(A)|A )] = 1 n ∑ a πD(a) π2(a) π2D(a) σ2(a) = V1 n .
The second term is
1 n V [ π(A) πD(A) rΦ(A) ] = 1 n [∑ a π2(a) πD(a) r2Φ(a)− (vπΦ) 2 ] = V2 n .
Combining the two above completes the proof.
A.3 Proof for Proposition 2
We note that the MSE is equal to the sum of the variance and the squared bias. Let us abbreviate v̂Reg(π,D n) by v̂Reg. First, notice that this estimate is (slightly) biased:
E[v̂Reg] = ∑ a π(a)E[r̂(a)]
= ∑ a π(a)E[E[r̂(a)|n(a)]]
= ∑ a π(a)E[rΦ(a)I{n(a) > 0}+ 0× I{n(a) = 0}]
= ∑ a π(a)rΦ(a)(1− pa,n).
Thus, the squared bias can be bounded as follows:
(E[v̂Reg]− vπΦ) 2 = (∑ a π(a)rΦ(a)pa,n )2 .
For the variance term, we again use the law of total variance to yield:
V(v̂Reg) = E[V(v̂Reg|n(1), . . . , n(K))] + V(E[v̂Reg|n(1), . . . , n(K)]).
Now, conditioned on n(1), . . . , n(K), the estimates {r̂(a)}a∈A are independent, so, by distinguishing the case n(a) > 0 (for which the variance of r̂(a) is σ2(a)/n(a)) from the other case n(a) = 0 (for which this variance is 0), we have
V(v̂Reg|n(1), . . . , n(K)) = ∑ a π2(a) (σ2(a) n(a) I{n(a) > 0}+ 0× I{n(a) = 0} ) .
Thus,
E[V(v̂Reg|n(1), . . . , n(K))] = ∑ a π2(a)σ2(a)E [ 1 n(a) I{n(a) > 0} ] .
For the second variance term, we also distinguish the case n(a) > 0, for which E[r̂(a)|n(a)] = rΦ(a), from the case n(a) = 0, for which E[r̂(a)|n(a) = 0], thus
E[v̂Reg|n(1), . . . , n(K)] = ∑ a π(a)(rΦ(a)I{n(a) > 0}+ 0× I{n(a) = 0}),
Hence, V(E[v̂Reg|n(1), . . . , n(K)]) = V( ∑ a π(a)rΦ(a)I{n(a) > 0}), which by Lemma 2 implies
V( ∑ a π(a)rΦ(a)I{n(a) > 0}) ≤ ∑ a π2(a)r2Φ(a) pa,n(1− pa,n) .
The proof of the upper bound is then completed by adding squared bias to variance, and using definitions of V0,n, V1, and V3.
For the lower bound, use Theorem 5. As mentioned in Appendix A.1, the Fisher information matrix is F (θ) = diag(. . . , πD(a)/σ2(a), . . .) and if the target is ψ(θ) = ∑ a π(a)r(a), ψ
′(θ) = π. Calculating the derivative of the bias and plugging into (8), we get the result.
A.4 Proof for Lemma 1
For convenience, the lemma is restated here.
Lemma 1. Let X1, . . . , Xn be n independent Bernoulli random variables with parameter p > 0. Letting Sn = ∑n i=1Xi, p̂ = Sn/n, Z = I{Sn>0} p̂ − 1 p , we have for any n and p that
E [Z] ≤ 4 p . (10)
Further, when np ≥ 34,
E [Z] ≤ 2 p
√ 2
np
(√ 3
2 ln (np 2 ) + 1 ) . (11)
Proof. According to the multiplicative Chernoff bound for the low tail (cf. Lemma 3 in the Appendix), for any 0 < δ ≤ 1, with probability at least 1− δ, we have
p̂ ≥ p− √ 2p
n ln
1 δ .
Denote by Eδ the event when this inequality holds. Assuming 2
np ln
1 δ ≤ 1/4 , (12)
thanks to 1/(1− x) ≤ 1 + 2x which holds for any x ∈ [0, 1/2], on Eδ we have
Z ≤ 1 p̂ − 1 p ≤ 1 p  1 1− √ 2 np ln 1 δ − 1  ≤ 2 p √ 2 np ln 1 δ .
Then, since Z ≤ n, we have for every δ satisfying (12) that
E[Z] ≤ 2 p
√ 2
np ln
1 δ + δn = 2 p
(√ 2
np ln
1 δ + np 2 δ
) = 2 p f (np 2 , δ ) , (13)
where f(u, δ) = √
1 u ln 1 δ + uδ. Hence, it remains to choose δ to approximately minimize f(u, δ)
subject to the constraint δ ≥ e−u/4 (due to (12)). First, note that if we choose δ = e−u/4, then f(u, e−u/4) ≤ 12 + ue −u/4 < 2, showing that EZ ≤ 4/p, proving the first part of the result.
To get the second part, we choose δ = u−3/2, which satisfies (12) since u−3/2 ≥ e−u/4 for u ≥ 17. Then, f(u, u−3/2) = u−1/2 (√ 3 2 ln(u) + 1 ) . Plugging this into (13) finishes the proof.
A.5 Technical Lemmas
Lemma 2. Using notation from Section 2.3, and wa = π(a)rΦ(a) one has
V ∗ := V (∑ a π(a)rΦ(a)I{n(a) > 0} ) ≤ ∑ a∈A w2a pa,n(1− pa,n)
provided that r(a) ≥ 0 for all action a ∈ A.
Proof. Let Xa = I{n(a) > 0}. First, note that E [Xa] = pa,n and so
V (∑ a∈A waI{n(a) > 0} ) = E [{∑ a∈A wa(Xa − pa,n) }2]
= ∑ a,b∈A wawb E [(Xa − pa,n)(Xb − pb,n)]
≤ ∑ a∈A w2a E [ (Xa − pa,n)2 ] (negative association)
= ∑ a∈A w2a pa,n(1− pa,n) .
Lemma 3 (Multiplicative Chernoff Bound for the Lower Tail, Theorem 4.5 of Mitzenmacher and Upfal (2005)). LetX1, . . . , Xn be independent Bernoulli random variables with parameter p, Sn =∑n i=1Xi. Then, for any 0 ≤ β < 1,
P ( Sn n ≤ (1− β)p ) ≤ exp ( −β 2np 2 ) .

The backend used in the PDS monitoring system is Oracle 11g. There are total five database tables:
Message DB Filter Word DB Ontology DB Phishing Rules DB Phishing Words DB
It is known that the Alzheimer’s language is generally fluent and grammatical but in order to maintain the fluency the deficiencies in semantic or episodic memory are compensated with empty speech (Nicholas et al., 1985), such as repetitions, both on the word level but also on the idea, sentence or narrative level. DEPID easily enables to track repeated ideas in the narrative. We consider a proposition as repetition of a previous idea when the deprel(DEPENDENT LEMMA, HEAD LEMMA) tuples of the two propositions match. For instance, a sentence “I had a happy life.” contains three propositions: nsubj(I, HAVE), dobj(LIFE, HAVE) and amod(HAPPY, LIFE). Another sentence “I’ve had a very happy life.” later in the same narrative only adds a single proposition to the total count— advmod(VERY, HAPPY)—as this is the only new piece of information that was added.
We modify DEPID to exclude the repetitive ideas of a narrative by only counting the proposition types expressed with the lexicalised de-
1Similar to Brown et al. (2008), we exclude the example 17, but for examples 18, 54, 55, 56, we include all paraphrases.
prel(DEPENDENT LEMMA, HEAD LEMMA) dependency arcs. We call this modified version of dependency-based PID computation method DEPID-R. The relation between DEPID-R and DEPID is that DEPID counts the tokens of the same propositions.
Unfolding autoencoders are an extension of recursive autoencoders, where each reconstruction step is applied recursively until an original input is reconstructed (Socher et al., 2011a). Figure 4.7 demonstrates this: where a standard RAE would stop the reconstruction step from y2 at y′1, the unfolding autoenocoder continues its recursive reconstruction by unfolding y′1 into x ′′ 1 and x ′′ 2, thereby reconstructing data from the input layer.
Unfolding autoencoders have a number of nice properties. Particularly, the unfolding prevents an RAE from degenerating, as standard recursive autoencoders are incentivised to learn small weights for all internal layers, thereby shrinking the overall reconstruction error. As the unfolding autoencoder measures its error function always by comparing with input weights, this strategy becomes void. Of course, if input weights are updated as part of the learning process, the issue of degenerating all weights to zero still persists, and needs to be addressed separately.
We conducted three different experiments dealing with different aspects of speech recognition system for better accuracy and practical implementation. In order to match the Test template to the reference template of the same speaker, we have record voices of speakers, converted them to sequence vectors of Mel cepstral coefficient and stored them into binary file as reference template. Now, for matching the stored reference template with the test template, we use DTW algorithm. Experiment 1: Matching test and reference template of same speaker. In this experiment first repetition of 10 words are used as the reference and next 3 repetitions of the words are used as test template. The 30(3*10) template were matched using DTW algorithm implemented in mat lab. The accuracy for other speakers in the similar manner was:
Set theory as a branch of human endeavour was developed by the efforts of many mathematicians [ Kam ]. Such a theory found many applications in science, technology and other fields. In an effort to capture uncertainity in human reasoning, Zadeh formulated and studied the theory of fuzzy sets. The theory of fuzzy sets found applications in many branches of science and technology. Pawlak, a computer scientist proposed and studied the concept of rough set in an effort to capture other aspects of uncertainity arising in applications such as database design. The theory of rough sets is found to have complemented the theory of fuzzy sets.
The author, in his research efforts related
to the fusion problem in Wireless Sensor Networks ( WSN ) discovered the idea of “graded set” ( discussed in Section 2 ) as a generalization of the idea of rough set. When understanding the details of rough set theory, the author discovered the idea of “granular set”. The basic motivation for such sets is discussed below. Motivation for Granular Sets:
In biological systems such as trees, when the tissue
is examined under a microscope at different resolutions, different cells/parts are observed. It is very clear that as the resolution increases, finer granular structure is observed. Our goal is to arrive at a mathematical abstraction of such sets observed in biological systems ( physical, chemical, biological etc ) as well as artificial systems ( such as databases ). It is expected that a detailed theory of such sets will find many applications as in the case of rough sets, fractal sets etc.
Disaggregation in real homes lead to several challenges such as the presence of similar appliances, multiple instances of same appliances and presence of low power electronics. Recent work has looked into fusing data from several pervasive sensors to improve NILM accuracy for such cases. Pathak et al. [25] and Saha et al. [27] discuss the potential of using acoustic and user WiFi data to localise appliance usages. On similar lines, Akshay et al. [1] improve the accuracy of existing NILM approaches by leveraging the relationship between a user’s location as obtained by WiFi localisation and appliance usage. The key idea in such approaches is to constrain the search space of the disaggregation problem by adding contextual information, such as the physical location of loads in the home.
We measure the degree of correlation between the system and the users with Spearman’s footrule [30] and the M-measure variant [21]. These non-parametric measures describe the degree of correlation between two ranked lists, and provide similar results to other correlation measures including Spearman’s ρ and Kendall’s τ [32]. In our case the two ranked lists are represented by the SLM ranking r1, and the user consensus ranking r2. We discuss each of the correlation measures in more detail, where we will abbreviate Spearman’s footule to
simply Footrule throughout the rest of this section.
The Footrule is calculated by summing the result of the absolute differences between the rank positions of the documents for each individual ranked list. The Footrule, denoted by Fr, is more formally defined as follows:
Fr(r1, r2) = k∑ i=1 |(r1(i)− r2(i))| (10)
where r1 and r2 are two ranked lists assumed to contain the same set of documents, and k is the size of the ranked list, in our case k = 10, which represents the top-10 ranked documents. In order to use the Footrule as a metric, we need to normalise the result by calculating the maximum possible value, through:
F = 1− Fr(r1, r2) maxFr(k)
(11)
where maxFr represents the maximum value, which when k is an even number maxFr = 12k
2, and if k is an odd number then maxFr = 12 (k+ 1)(k− 1). This ensures the resulting Footrule falls in the range of 0 and 1 where a value close to 1 means that the two ranked lists are highly similar.
When evaluating search results, however, we may wish to consider the fact that documents in the top ranks are often considered the most relevant to the users information need than documents appearing in lower ranks [21]. To give more weight to the top ranked documents, we apply the M-measure, which was designed to place more emphasis on ranked lists containing identical or nearidentical sets of documents in the top rank positions. Due to the fact that the ranked lists contain the same set of documents, we can drop the terms S and T mentioned in [21], which record the set of documents unique to r1 and r2, respectively, and reformulate the M-measure more precisely as:
m = k∑ i=1 | 1 r1(i) − 1 r2(i) |, (12)
where we calculate the sum of the absolute differences between each document’s SLM rank and consensus rank. Next we calculate the maximum value max M, which is defined through:
maxM = k∑ i=1 |1 i − 1 k − i+ 1 | (13)
Lastly, we normalise m by deducting 1 from the result of the division of m by the maximum value maxM to obtain a metric ranging between 0 and 1:
M = 1− m maxM (k)
(14)
The average by query is calculated by summing the scores for each correlation measure, Footrule and M-measure, and then dividing the result by the total number of queries. We repeat this process for each user by first gathering the per query correlation scores for each user and then dividing the result by the total number of queries, and then take a further average for each user. The average user agreement represents the degree of correlation between each user and the consensus ranking, in other words, the user agreement describes the average correlation between an individual user and what we could consider to be the “wisdom” or “opinion” of the crowd. We produce a consensus ranking for each query, and measure the correlation between this ranking and the individual user ranking for the given query. Next, we calculate the average correlation per query for each user, as we did before, and then compute the average consensus based on the per user averages.
The experimental validation of self-CMA-ES investigates the performance of the algorithm comparatively to CMA-ES on the BBOB noiseless problems [4]. Both algorithms are launched in IPOP scenario of restarts when the CMA-ES is restarted with doubled population size once stopping criteria [3] are met5.
5 For the sake of reproducibility, the source code is available at https://sites.google.com/site/selfcmappsn/
The population size λ is chosen to be 100 for both CMA-ES and self-CMAES. We choose this value (about 10 times larger than the default one, see the default parameters of CMA-ES in Algorithm 1) to investigate how sub-optimal the other CMA-ES hyper-parameters, derived from λ, are in such a case, and whether self-CMA-ES can recover from this sub-optimality.
The auxiliary CMA-ES is concerned with optimizing hyper-parameters c1, cµ and cc (Algorithm 1), responsible for the adaptation of the covariance matrix of the primary CMA-ES. These parameters range in [0, .9] subject to 0 ≤ c1+ cµ ≤ 0.9; the constraint is meant to enforce a feasible C update for the primary CMA-ES (the decay factor of C should be in [0, 1]). Infeasible hyper-parameter solutions get a very large penalty, multiplied by the sum of distances of infeasible hyper-parameters to the range of feasibility.
We set wsel,i = 1/µ for i = 1, . . . , µ and µ = ⌊λ/2⌋ to 50. The internal computational complexity of self-CMA-ES thus is λh = 20 times larger than the one of CMA-ES without lazy update (being reminded that the internal time complexity is usually negligible compared to the cost per objective function evaluation).
We here present the proofs of main theorems. The omitted ones can be found in appendices.
Solution space
Priority space
p
s
Construct
p’
Construct
s’
Figure 4: Coupled search spacescapture information about the structure of the search space in the vicinity of the solution.As swo constructs a new solution from scratch, the priorities can be thought of as pro-viding information about pitfalls common to the current region of the solution space. Ifsome elements of the solution have tended to be sources of di culty over some number ofiterations, increasing their priority makes it more likely that the constructor will handlethose elements in a good way.One consequence of the coupled search spaces is that a small change in the sequence ofelements generated by the prioritizer may correspond to a large change in the correspondingsolution generated by the constructor, compared to the solution from the previous itera-tion. Moving an element forward in the sequence can signi cantly change its state in theresulting solution. In addition, any elements that now occur after it in the sequence mustaccommodate that element's state. For example, in the scheduling domain, moving a taskearlier in the priority sequence may allow it to be placed on a di erent manufacturing line,thus possibly changing the mix of jobs that can run on that line, and on the line it wasscheduled on in the previous iteration. One small change can have consequences for anyelement that follows it, with lower-priority tasks having to \ ll in the gaps" that are leftafter higher-priority tasks have been scheduled.The result is a large move that is \coherent" in the sense that it is similar to what wemight expect from moving the higher priority task, then propagating the e ects of thatchange by moving lower priority tasks as needed. This single move may correspond to alarge number of moves for a search algorithm that only looks at local changes to the solution,and it may thus be di cult for such an algorithm to nd.The fact that swo makes large moves in both search spaces is one obvious di erencebetween swo and traditional local search techniques, such as wsat (Selman, Kautz, &Cohen, 1993). Another di erence is that with swo, moves are never selected based on theire ect on the objective function. Instead, unlike hillclimbing techniques, each move is made357
Joslin & Clementsin response to \trouble spots" found in the current solution. The resulting move may beuphill, but the move is always motivated by those trouble spots.In priority space the only \local optima" are those in which all elements of a solution areassigned equal blame. swo tends to avoid getting trapped in local optima, because analysisand prioritization will always (in practice) suggest changes in the sequence, thus changingthe solution generated on the next iteration. This does not guarantee that swo will notbecome trapped in a small cycle, however. In our implementations we have introducedsmall amounts of randomness in the basic cycle. We also restart swo periodically with anew initial sequence.Another aspect of local search is that typically each point in the solution space is as-sociated with a single value, the objective function score for that solution. When we talkabout hillclimbing, we generally refer to the \terrain" described by this objective functionscore, over the space of solutions. The process of analysis in swo can be thought of assynthesizing a more complex description of that terrain, by breaking a solution down intoits component elements and assigning a score to each. Prioritization then translates theanalysis into a \strategy" that the constructor can use to generate the next solution.Assigning scores to the individual elements of a solution allows swo to take advantageof the fact that real problems often combine some elements that are di cult to get right,plus others that are easy. In the scheduling problems presented below, some tasks can beassigned to just a few production lines, while others allow for much more exibility. Somehave due dates close to their release time, while others have a lot of leeway. It is sometimespossible to identify \di cult" elements of a problem with static analysis, but interactionscan be complex, and elements that are causing di culty in one part of the search space maybe no trouble at all in another. Rather than trying to identify elements that are globallydi cult by analyzing the entire problem, swo analyzes individual solutions in order to ndelements that are locally di cult. Globally di cult elements tend to be identi ed over time,as they are di cult across large parts of the search space.By assigning blame and adjusting priorities based on identi ed problems in actual so-lutions, swo avoids dependence on complex, domain dependent heuristics. It is our beliefthat this independence is particularly important in complex domains where even the bestheuristics will miss some key interactions and therefore inhibit the search from exploringgood areas that the heuristic incorrectly labels as unpromising. swo uses actual solutionsto discover which areas of the search space are promising and which are not.3. SWO for schedulingThis section describes an application of swo to a ber-optic production line schedulingproblem, derived from data provided by Lucent Technologies. In this particular plant, acable may be assembled on any one of 13 parallel production lines. For each cable type,only a subset of the production lines are compatible, and the time required to produce thecable will depend on which of the compatible lines is selected. Each cable also has a setuptime, which depends on its own cable type and that of its predecessor. Setups betweencertain pairs of cable types are infeasible. Task preemption is not allowed, i.e. once a cablehas started processing on a line, it nishes without interruption.358
\Squeaky Wheel" OptimizationEach cable is assigned a release time and due date. Production cannot begin before therelease time. The objective function includes a penalty for missing due dates, and a penaltyfor setup times.3.1 ImplementationWe describe the implementation in terms of the three main components of swo:Constructor. The constructor builds a schedule by adding tasks one at a time, in theorder they occur in the priority sequence. A task is added by selecting a line and aposition relative to the tasks already in that line. A task may be inserted betweenany two tasks already in the line or at the beginning or end of that line's schedule.Changes to the relative positions of the tasks already in the line are not considered.Each task in the line is then assigned to its earliest possible start time, subject to theordering, i.e., a task starts at either its release time, or immediately after the previoustask on that line, whichever is greater.For each of the possible insertion points in the schedule, relative to the tasks already ineach line, the constructor calculates the e ect on the objective function, and the taskis placed at the best-scoring location. Ties are broken randomly. After all tasks havebeen placed, the constructor applies swo to the individual line schedules, attemptingto improve the score for each line by reordering the cables that were assigned to it.Analyzer. To assign blame to each task in the current schedule, the analyzer rst calculatesa lower bound on the minimum possible cost that each task could contribute to anyschedule. For example, if a task has a release time that is later than its due date,then it will be late in every schedule, and the minimum possible cost already includesthat penalty. Minimum possible setup costs are also included. For a given schedule,the blame assigned to each task is its \excess cost," the di erence between its actualcost and its minimum possible cost. Excess lateness costs are assigned to tasks thatare late, and excess setup costs are split between adjacent tasks.Prioritizer. Once the blame has been assigned, the prioritizer modi es the previous se-quence of tasks by moving tasks with non-zero blame factors forward in the sequence.Tasks are moved forward a distance that increases with the magnitude of the blame.To move from the back of the sequence to the front, a task must have a high blamefactor over several iterations. We call this a \sticky sort."Our current implementation has considerable room for improvement. The analysis andfeedback currently being used are very simple, and the construction of schedules could takevarious heuristics into account, such as preferring to place a task in a line that has more\slack," all other things being equal.3.2 Experimental resultsWe have six sets of test data, ranging in size from 40 to 297 tasks, all with 13 parallelproduction lines. The largest problem was the largest that the manufacturer required inpractice. We compare the following solution methods:359
Joslin & Clements swo tabu ipData Best Avg AvgSet Obj Obj Time Obj Time Obj Time40 1890 1890 48 1911 425 1934 2050 3101 3156 57 3292 732 3221 17560 2580 2584 87 2837 1325 2729 614470 2713 2727 124 2878 2046 2897 4950148 8869 8927 431 10421 17260 | |297 17503 17696 1300 | | | |Table 1: Experimental results: schedulingswo Applies the swo architecture to the problem, running for a xed number of iterationsand returning the best schedule it nds.tabu Uses tabu search (Glover & Laguna, 1997), a local search algorithm in which movesthat increase cost are permitted to avoid getting trapped at local optima. To avoidcycling, when an \uphill" move is made, it is not allowed to be immediately undone.ip Applies an Integer Programming (IP) solver, using an encoding described in (Clementset al., 1997).On the 297 task problem, swo was far more e ective than either tabu or ip. tabu,for example, failed to nd a feasible schedule after running for over 24 hours. On thesmallest problems, tabu and ip were able to nd solutions, but swo outperformed both bya substantial margin.Table 1 presents results on each problem for swo, tabu and ip. For swo, ten trialswere run and the results averaged. The tabu and ip implementations were deterministic,so only the results of a single run are shown. The second column of the table shows the bestobjective function value we have ever observed on each problem. The remaining columnsshow the objective function value and running times for swo, tabu and ip. All but the ipexperiments were run on a Sun Sparcstation 10 Model 50. The ip experiments were run onan IBM RS6000 Model 590 (a faster machine).The best values observed have been the result of combining swo with ip, as reportedin (Clements et al., 1997). In that work, swo generated solutions, running until it hadproduced a number of \good" schedules. An ip solver was then invoked to re-combineelements of those solutions into a better solution. Although the improvements achievedby the IP solver were relatively small, on the order of 1.5%, it achieved this improvementquickly, and swo was unable to achieve the same degree of optimization even when givensubstantially more time. While noting that the hybrid approach can be more e ective thanswo alone, and much more e ective than ip alone, here we focus on the performance of theindividual techniques.We also note that our very rst, fairly naive implementation of swo for these schedulingproblems already outperformed both tabu and ip. Moreover, our improved implementation,360
\Squeaky Wheel" Optimization 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 0 2 4 6 8 10
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 0
2
4
6
8
10
Position in priority sequence
# of
li ne
s jo
b in
th at
p os
iti on
c an
r un
o n
Position in priority sequence Order based on # of lines job can run on
Order after 14th iteration, producing a solution 0.05% over best knownFigure 5: Comparison of heuristic priorities and priorities derived by sworeported above, is still fairly simple, and is successful without relying on domain-dependentheuristics. We take this as evidence that the e ectiveness of our approach is not dueto cleverness in the construction, analysis and prioritization techniques, but due to thee ectiveness of the swo cycle at identifying and responding to whatever elements of theproblem happen to be causing di culty in the local region of the search.It is also instructive to compare the results of a good heuristic ordering, with the se-quence derived by swo. A good heuristic for this scheduling domain (and the one thatis used to initially populate the priority sequence) is to sort the tasks by the number ofproduction lines on which a task could be feasibly assigned in an empty schedule. A taskthat can be scheduled on many lines is likely to be easier to schedule than one that iscompatible with only a small number of lines, and should therefore be expected to need alower priority. The top graph in Figure 5 shows the sequence of tasks, as determined bythis heuristic. The lower graph illustrates the changes in priority of these tasks, after swohas run for fourteen iterations (enough to improve the solution derived from the sequenceto within 0.05 percent of the best known solution).As the gure illustrates, the heuristic is generally accurate, but swo has had to movesome tasks that are compatible with most of the production lines to positions of relativelyhigh priority, re ecting the fact that contrary to the heuristic, these tasks turned out to berelatively di cult to schedule well. Other tasks that are compatible with only a few pro-duction lines are actually easy to schedule well, and have moved to relatively low priorities.361
Joslin & Clements Iterations Feasible < 18000 < 17700per Success Mean Success Mean Success Mean SampleRestart Rate Cost Rate Cost Rate Cost Size10 0.8542 5.9 0.0504 195.3 0.0002 49994.5 1000020 0.9722 6.0 0.2052 90.9 0.0006 33328.3 500030 0.9955 5.8 0.3812 67.5 0.0030 9895.5 330040 0.9996 5.8 0.5488 56.7 0.0060 6658.2 250050 0.9995 6.0 0.6330 57.0 0.0160 3112.7 200060 1.0000 5.7 0.7242 52.9 0.0188 3170.4 165070 1.0000 5.7 0.8079 50.2 0.0350 1973.5 140080 1.0000 6.2 0.8552 49.5 0.0296 2670.0 125090 1.0000 5.8 0.8827 48.9 0.0300 2965.3 1100100 1.0000 5.9 0.8840 52.4 0.0400 2452.3 1000200 1.0000 6.0 0.9680 53.0 0.0600 3204.3 500300 1.0000 5.3 0.9967 50.1 0.0567 5090.8 300400 1.0000 5.8 1.0000 52.9 0.0720 5320.2 250500 1.0000 5.8 1.0000 52.8 0.1000 4692.6 200600 1.0000 5.8 1.0000 57.2 0.0867 6590.8 150700 1.0000 6.1 1.0000 42.4 0.1200 5472.4 100800 1.0000 5.6 1.0000 53.0 0.1200 6210.3 100900 1.0000 5.3 1.0000 45.8 0.1700 4691.6 1001000 1.0000 6.0 1.0000 45.4 0.1800 4838.1 100Table 2: Experimental results: restarts in the scheduling domain3.3 RestartsThe swo solver used to produce the results reported in Table 1 restarted the priority queueevery n=2 iterations, where n is the number of jobs in the problem. The same noisy heuristicthat was used to initially populate the priority queue was also used to restart it. This restartcuto was picked in a rather ad hoc manner. A more careful analysis of di erent restartcuto values might lead to producing better solutions faster, and to some additional insighton the workings of swo.Restarts are often used in non-systematic search to avoid getting trapped in local optimaor in cycles. (See Parkes and Walser, 1996, for an empirical study of wsat and furtherreferences.) Restarts have also been used in systematic search to escape exponentially largeregions of the search space that do not contain a solution (Gomes, Selman, & Kautz, 1998).Local optima pose little threat to swo, since it is not directly driven by uphill/downhillconsiderations. swo, through its use of large coherent moves, also tends to escape un-promising parts of the search space quickly. However, swo is open to getting trapped in acycle, and restarts are used as a means to escape them.For these scheduling problems, swo is unlikely to get into a tight cycle where priorityqueues and solutions repeat exactly. This is due to the presence of random tie breaking inseveral places, and to the presence of noise in the prioritizer. However, it is our belief thatswo can get trapped in a cycle where similar priority queues and solutions repeat.We ran a series of experiments with the 297 task problem to determine the impact ofvarious restart cuto s. The results are summarized in Table 2. Restart cuto s ranged fromafter every 10 iterations to after every 1000 iterations. The success rate and mean cost are362
\Squeaky Wheel" Optimizationshown for each value for each of three di erent solution qualities. The success rate indicatesthe probability that a solution of at least the given quality was found in a given pass. Themean cost is the average number of total iterations to get a solution of that quality.For the feasible and 18000 solution thresholds, swo reaches a 100 percent success ratewell before reaching the maximum restart cuto of 1000 used in these experiments. In somesense, it is easy for swo to produce solutions that are at least of these qualities. The resultsfor these 2 thresholds indicate that when it is easy for swo to solve the problem, any cuto greater than the average number of uninterrupted iterations it takes to produce a solutioncan be used to solve the problem at minimum cost. For such \easy" problems, it appearsthat too small a restart cuto can hurt, but that too big a cuto will not.The numbers for the 17700 solution quality threshold, tell a di erent story. The successrate is still climbing when the experiment ends, and the mean cost has actually risen aboveits minimum. For this solution quality, the restart cuto that minimizes mean cost fallsaround the range of 70 to 100. Mean costs rise steeply for restart cuto s below this range,and slowly for cuto s larger than that. This is an example of a hard problem for swo, and itshows that some care needs to be taken when choosing a restart strategy for such problems.Additional research is needed to determine how to set the restart cuto automatically forarbitrary problems.This data indicates that swo does bene t from restarts, up to a point. With the 17700threshold, for restart cuto s up to 100, each increase in the cuto in general led to asuperlinear increase in the success rate. (This is also another indicator that swo is learningfrom iteration to iteration.) Above 100 iterations per restart, the success rate initiallyclimbs sublinearly and then appears to level out. It is an open question what this tells usabout the search space.4. SWO for graph coloringWe have also applied swo to a very di erent domain, graph coloring. Here the objectiveis to color the nodes of a graph such that no two adjoining nodes have the same color,minimizing the number of colors.4.1 ImplementationThe priority sequence for graph coloring consists of an ordered list of nodes. The solver isalways trying to produce a coloring that uses colors from the target set, which has one lesscolor than was used to color the best solution so far. Again, we describe the implementationin terms of the three main components of swo:Constructor. The constructor assigns colors to nodes in priority sequence order. If anode's color in the previous solution is still available (i.e. no adjacent node is usingit yet), and is in the target set, then that color is assigned. If that fails, it tries toassign a color in the current target set, picking the color that is least constraining onadjacent uncolored nodes, i.e. the color that reduces the adjacent nodes' remainingcolor choices the least. If none of the target colors are available, the constructor triesto \grab" a color in the target set from its neighbors. A color can only be grabbedif all neighbor nodes with that color have at least one other choice within the target363
Joslin & Clementsset. If multiple colors can be grabbed, then the least constraining one is picked. If nocolor in the target set can be grabbed then a color outside the target set is assigned.Nodes that are early in the priority sequence are more likely to have a wide range ofcolors to pick from. Nodes that come later may grab colors from earlier nodes, butonly if the earlier nodes have other color options within the target set.Analyzer. Blame is assigned to each node whose assigned color is outside the target set,with the amount of blame increasing for each additional color that must be addedto the target set. We ran experiments with several di erent variations of color-basedanalysis. All of them performed reasonably.Prioritizer. The prioritizer modi es the previous sequence of nodes by moving nodes withblame forward in the sequence according to how much blame each received. This isdone the same way it is done for the scheduling problems. The initial sequence is a listof nodes sorted in decreasing degree order, with some noise added to slightly shu ethe sort.4.2 Experimental resultsWe applied swo to a standard set of graph coloring problems, including random graphs andapplication graphs that model register allocation and class scheduling problems. These werecollected for the Second DIMACS Implementation Challenge (Johnson & Trick, 1996), whichincludes results for several algorithms on these problems (Culberson & Luo, 1993; Glover,Parker, & Ryan, 1993; Lewandowski & Condon, 1993; Morgenstern, 1993). Problems rangefrom 125 nodes with 209 edges to 4000 nodes with 4,000,268 edges.Glover et al. (1993) is the only paper that is based on a general search technique, tabuwith branch and bound, rather than a graph coloring speci c algorithm. This approachhad the worst reported average results in the group. Morgenstern (1993) used a distributedimpasse algorithm and had the best overall colorings, but also required that the targetnumber of colors, as well as several other problem speci c parameters be passed to thesolver. Lewandowski & Condon (1993) also found good solutions for this problem set.Their approach used a hybrid of parallel impasse and systematic search on a 32 processorCM-5. Culberson & Luo (1993) used an Iterated Greedy (ig) algorithm that bears somesimilarity to swo. ig is the simplest algorithm in the group. Its solution quality fallsbetween the impasse algorithms and tabu but solves the entire set in 1 to 2 percent of thetime taken by the other methods. Both ig and impasse are discussed further under relatedwork.Table 3 compares swo with the results for ig (Culberson & Luo, 1993), distributedimpasse (Morgenstern, 1993), parallel impasse (Lewandowski & Condon, 1993), and tabu(Glover et al., 1993). For each, one column shows the number of colors required for eachproblem, and the run time (in CPU seconds). Bold face indicates that the number of colorsis within 0.5 of the best result in the table.We used a Pentium Pro 333MHz workstation running Linux for the swo graph coloringexperiments. The times shown for the other four algorithms are based on those reported in(Johnson & Trick, 1996). The results for ig, impasse and tabu are normalized to our times364
\Squeaky Wheel" Optimization swo ig Dist. impasse Par. impasse tabuProblem colors time colors time colors time colors time colors timeDSJC125.5 18.3 1.6 18.9 2.5 17.0 6.3 17.0 4043.6 20.0 153.3DSJC250.5 31.9 8.3 32.8 6.9 28.0 268.5 29.2 4358.1 35.0 3442.2DSJC500.5 56.3 40.9 58.6 18.2 49.0 8109.1 53.0 4783.9 65.0 3442.2DSJC1000.5 101.5 208.6 104.2 67.6 89.0 41488.7 100.0 5333.8 117.0 3442.2C2000.5 185.7 1046.2 190.0 272.4 165.0 14097.9 | | | |C4000.5 341.6 4950.8 346.9 1054.1 | | | | | |R125.1 5.0 0.2 5.0 2.0 5.0 0.2 5.0 64.6 5.0 0.4R125.1c 46.0 5.1 46.0 1.1 46.0 0.2 46.0 85.0 46.0 0.9R125.5 36.0 2.8 36.9 1.9 36.0 0.2 37.0 33.0 36.0 0.7R250.1 8.0 0.5 8.0 7.0 8.0 0.2 8.0 22.0 8.0 0.2R250.1c 64.0 30.6 64.0 4.6 64.0 0.5 64.0 278.2 65.0 46.4R250.5 65.0 14.7 68.4 8.3 65.0 82.2 66.0 39.9 66.0 59.0DSJR500.1 12.0 2.0 12.0 21.1 12.0 0.2 12.0 26.6 12.0 0.5DSJR500.1c 85.2 96.9 85.0 14.6 85.0 59.1 85.2 5767.7 87.0 3442.2DSJR500.5 124.1 68.7 129.6 26.1 123.0 175.3 128.0 90.5 126.0 395.1R1000.1 20.0 8.0 20.6 87.2 20.0 8.2 20.0 49.9 20.0 1.7R1000.1c 101.7 433.2 98.8 49.1 98.0 563.3 102.6 3940.0 105.0 3442.2R1000.5 238.9 574.5 253.2 102.9 241.0 944.0 245.6 215.9 248.0 3442.2 at300 20 0 25.3 16.4 20.2 3.8 20.0 0.2 20.0 274.3 39.0 3442.2 at300 26 0 35.8 12.0 37.1 7.7 26.0 10.0 32.4 6637.1 41.0 3442.2 at300 28 0 35.7 11.9 37.0 9.6 31.0 1914.2 33.0 1913.5 41.0 3442.2 at1000 50 0 100.0 203.9 65.6 146.3 50.0 0.2 97.0 7792.7 | | at1000 60 0 100.7 198.0 102.5 87.3 60.0 0.2 97.8 6288.4 | | at1000 76 0 100.6 208.4 103.6 79.6 89.0 11034.0 99.0 6497.9 | |latin sqr 10 111.5 369.2 106.7 59.7 98.0 5098.0 109.2 6520.1 130.0 3442.2le450 15a 15.0 5.5 17.9 17.0 15.0 0.2 15.0 162.6 16.0 17.8le450 15b 15.0 6.1 17.9 16.2 15.0 0.2 15.0 178.4 15.0 28.4le450 15c 21.1 8.0 25.6 14.5 15.0 57.2 16.6 2229.6 23.0 3442.2le450 15d 21.2 7.8 25.8 13.5 15.0 36.3 16.8 2859.6 23.0 3442.2mulsol.i.1 49.0 5.9 49.0 4.2 49.0 0.2 49.0 27.2 49.0 0.3school1 14.0 8.4 14.0 10.5 14.0 0.2 14.0 46.3 29.0 90.7school1 nsh 14.0 7.2 14.1 8.9 14.0 0.2 14.0 66.4 26.0 31.2Table 3: Experimental results: graph coloring problemsusing the DIMACS benchmarking program dfmax, provided for this purpose. Therefore,timing comparisons are only approximate. Our machine ran the dfmax r500.5 benchmarkin 86.0 seconds; the times reported for the machines used on the other algorithms were86.9 seconds for the tabu experiments, 192.6 seconds for ig, 189.3 seconds for impasse,and 2993.6 seconds for parallel impasse. Because the dfmax benchmark runs on a singleprocessor, it is unsuitable for normalizing the times for parallel IMPASSE. We report theirunnormalized times.A variety of termination conditions were used. swo terminated after 1000 iterations.ig terminated after 1000 iterations without improvement. Distributed impasse used awide variety of di erent termination conditions to solve the di erent problems. The onlycommon element across problems was that distributed impasse stopped when the targetnumber of colors, provided as an input parameter, was reached. The times reported for365
Joslin & Clements
0
5
10
15
20
25
0 10000 20000 30000 40000 50000 60000
A vg
. p er
ce nt
o ve
r be
st in
g ro
up
Time (CPU seconds)
Iterated Greedy
Squeaky Wheel
TABU
Par IMPASSE
Dist IMPASSEFigure 6: Experimental results: quality of solution vs. timeparallel impasse are the times it took to nd the best solution that was found, not the timeit took the algorithm to terminate, which was always 3 hours. tabu ran until the algorithmdetermined that it could make no further progress, or an hour had passed, whichever came rst.The tabu numbers are for a single run on each problem. The numbers for the otheralgorithms are averages for 4 runs (parallel impasse), 5 runs (distributed impasse, parallelimpasse) or 10 runs (swo, ig, distributed impasse) on each problem.Figure 6 summarizes the performance of each technique on the set of 27 problems thatall of the algorithms solved. For each solver the graph indicates the average solution qualityand the average amount of time needed to solve the set. The ideal location on the graphis the origin, producing high quality solutions in very little time. The points shown for theother techniques are the points reported in each of the papers. The curve shown for swoshows how it performs when given varying amounts of time to solve the set. As the graphshows, swo clearly outperforms tabu, the only other general purpose technique, both interms of quality and speed. swo also outperforms ig, a graph coloring speci c algorithm,both in terms of quality and speed. The impasse solvers clearly produce the best solutionsin the group. However, impasse is a domain speci c method, and both solvers representmuch more programming e ort. The swo solver uses a general purpose search techniqueand was implemented in less than a month by a single programmer.4.3 Alternate con gurations of swoWe note that, as with the scheduling work, our rst, naive implementation of swo for graphcoloring produced respectable results. Even without color reuse, color grabbing, or the leastconstraining heuristic (the rst free color found was picked), swo matched ig on 6 problems366
\Squeaky Wheel" Optimizationand beat it on 10. However, on half of the remaining problems ig did better by 10 or morecolors.To explore the sensitivity of swo to such implementation details we tried the followingapproaches in the constructor and prioritizer, and ran swo using all combinations:Construction: With or without color grabbingAnalysis: Either blame all nodes that receive a color outside the target set, or only the rst node (in the priority sequence) that causes a new color outside the target set tobe introduced. If color grabbing is used, the determination of blame is based on the nal color assigned to the node.The di erence in solution quality from the worst combination to the best combinationwas less than 15 percent. Even when the alternative of using a standard sort instead ofthe \sticky" sort (a fairly fundamental change) was added to the mix, the spread betweenworst and best was still under 20 percent.5. Related workThe importance of prioritization in greedy algorithms is not a new idea. The \First Fit"algorithm for bin packing, for example, relies on placing items into bins in decreasing orderof size (Garey & Johnson, 1979). Another example is grasp (Greedy Randomized AdaptiveSearch Procedure) (Feo & Resende, 1995). grasp di ers from our approach in several ways.First, the prioritization and construction aspects are more closely coupled in grasp. Aftereach element is added to the solution being constructed, the remaining elements are re-evaluated by some heuristic. Thus the order in which elements are added to the solutionmay depend on previous decisions. Second, the order in which elements are selected in eachtrial is determined only by the heuristic (and randomization), so the trials are independent.There is no learning from iteration to iteration in grasp.Doubleback Optimization (dbo) (Crawford, 1996) was to some extent the inspiration forboth swo and another similar algorithm, Abstract Local Search (als) (Crawford, Dalal,& Walser, 1998). In designing swo, we began by looking at dbo, because it had beenextremely successful in solving a standard type of scheduling problem. However, dbo isonly useful when the objective is to minimize makespan, and is also limited in the typesof constraints it can handle. Because of these limitations, we began thinking about theprinciples behind dbo, looking for an e ective generalization of that approach. dbo can,in fact, be viewed as an instance of swo. dbo begins by performing a \right shift" on aschedule, shifting all tasks as far to the right as they can go, up to some boundary. In theresulting right-shifted schedule, the left-most tasks are, to some extent, those tasks that aremost critical. This corresponds to analysis in swo. Tasks are then removed from the right-shifted schedule, taking left-most tasks rst. This ordering corresponds to the prioritizationin swo. As each task is removed, it is placed in a new schedule at the earliest possible starttime, i.e., greedy construction.Like swo, als was the result of an attempt to generalize dbo. als views priority space(to use the terminology from swo) as a space of \abstract schedules," and performs a localsearch in that space. Unlike swo, if a prioritization is modi ed, and the corresponding367
Joslin & Clementsmove in solution space is downhill (away from optimal), then the modi ed prioritizationis discarded, and the old prioritization is restored. As is usual with local search, als alsosometimes makes random moves, in order to escape local minima.als, and also List Scheduling (Pinson, Prins, & Rullier, 1994), are scheduling algorithmsthat deal with domains that include precedence constraints on tasks. Both accommodateprecedence constraints by constructing schedules left-to-right temporally. A task cannotbe placed in the schedule until all of its predecessors have been placed. In order for theanalysis, prioritization and construction to be appropriately coupled, it is not su cient tosimply increase the priority of a task that is late, because the constructor may not be ableto place that task until after a lot of other decisions have been made. Consequently, someamount of blame must be propagated to the task's predecessors.The commercial scheduler optiflex (Syswerda, 1994) uses a genetic algorithm approachto modify a sequence of tasks, and a constraint-based schedule constructor that generatesschedules from those sequences. optiflex can also be viewed as an instance of swo, witha genetic algorithm replacing analysis. In e ect, the \analysis" instead emerges from therelative tness of the members of the population.Two graph coloring algorithms also bear some similarity to swo. Impasse Class Col-oration Neighborhood Search (impasse) (Morgenstern, 1993; Lewandowski & Condon,1993), like swo, maintains a target set of colors and produces only feasible colorings. Given acoloring, impasse places any nodes that are colored outside of the target set into an impasseset. On each iteration a node is selected from the impasse set, using a noisy degree-basedheuristic, and assigned a random color from the target set. Any neighbor nodes that arenow in con ict are moved to the impasse set.Iterated Greedy (ig) (Culberson & Luo, 1993), like swo, uses a sequence of nodes tocreate a new coloring on each iteration, and then uses that coloring to produce a newsequence for the next iteration. The method used to generate each new sequence di ersfrom swo. The key observation behind ig is that if all nodes with the same color in thecurrent solution are grouped together in the next sequence (i.e. adjacent to each other inthe sequence), then the next solution will be no worse than the current solution. ig achievesimprovement by manipulating the order in which the groups occur in the new sequence,using several heuristics including random based on color, descending based on color, andascending based on the cardinality of each group. ig learns groupings of nodes as it runs,but it does not learn about about the di culty of any nodes. A node's place in the sequenceindicates nothing about its expected or detected di culty.6. Analysis and future workThis section summarizes several areas of future research suggested by the results reportedin the previous sections.6.1 ScalingWhile swo uses fast, greedy algorithms for constructing solutions, and we have demon-strated its e ectiveness on problems of realistic size, the greatest threat to the scalability ofswo is that it constructs a new solution from scratch on each iteration. A partial solutionto this problem is seen in the use of a \history" mechanism for the graph coloring problems.368
\Squeaky Wheel" OptimizationUsing the same color for a node as in the previous solution means that in many cases wedo not need to check any of the other possible colors. This signi cantly speeds up theconstruction.A more fundamental solution to this problem would be to develop an incremental versionof swo. The selective reuse of colors in the graph coloring solver is a small step in thisdirection. This allows the constructor to avoid spending time evaluating other alternativeswhen the previous choice still works. More generally, it may be possible to look at thechanges made to a prioritization, and modify the corresponding solution in a way thatgenerates the same solution that would be constructed from scratch based on the newprioritization. It seems feasible that this could be done for some domains, at least for smallchanges to the prioritization, because there may be large portions of a solution that areuna ected.A more interesting possibility is based on the view of swo as performing local searchplus a certain kind of propagation. A small change in priorities may correspond to a largechange in the solution. For example, increasing the priority of one task in a schedulingproblem may change its position in the schedule, and, as a consequence, some lower prioritytasks may have to be shu ed around to accommodate that change. This is similar to whatwe might expect from moving the higher priority task, then propagating the e ects of thatchange by moving lower priority tasks as well. This single move may correspond to a largenumber of moves in a search algorithm that only looks at local changes to the schedule, andmay thus be di cult for such an algorithm to nd.Based on this view, we are investigating an algorithm we call \Priority-Limited Propa-gation" (plp). With plp, local changes are made to the solution, and then propagation isallowed to occur, subject to the current prioritization. Propagation is only allowed to occurin the direction of lower-priority elements. In e ect, a small change is made, and then theconsequences of that change are allowed to \ripple" through the plan. Because propagationcan only occur in directions of decreasing priority, these ripples of propagation decrease inmagnitude until no more propagation is possible. A new prioritization is then generated byanalyzing the resulting solution. (It should be possible to do this analysis incrementally,as well.) The resulting approach is not identical to swo, but has many of its interestingcharacteristics.6.2 Coordination of modulesFor swo to be e ective, it is obvious that analysis, prioritization and construction must allwork together to improve the quality of solutions. We have already discussed the compli-cations that can arise when constraints are placed on the order in which the constructorcan make decisions, as is the case for List Scheduling and als, where construction is donestrictly left-to-right. Without more complex analysis, the search spaces can e ectively be-come uncoupled, so that changes in priority don't cause the constructor to x problemsdiscovered by analysis.Another way the search can become uncoupled is related to the notion of \excess cost,"discussed for the scheduling implementation. The calculation of excess cost in the analyzerturned out to be a key idea for improving the performance of swo. However, problemssometimes have tasks that must be handled badly in order to achieve a good overall solu-369
Joslin & Clementstion. One of the scheduling problems described previously has two such \sacri cial" tasks.Whenever a good solution is found, the analyzer assigns high blame to these sacri cial tasks,and the constructor handles them well on the next iteration. This means that the resultingsolution is of poor overall quality, and it is not until other aws cause other tasks to moveahead of the sacri cial tasks in the priority sequence that swo can again, brie y, explorethe space of good solutions. In such cases, to some extent the analysis is actually hurtingthe ability of swo to converge on good solutions.Ideally, we would like to generalize the notion of excess cost to recognize sacri cialtasks, and allow those tasks to be handled badly without receiving proportionate blame.For problems in which a task must be sacri ced in all solutions, it may be possible to usea learning mechanism to accomplish this.However, the notion of a sacri cial task can be more subtle than this. Suppose forexample that we are scheduling the construction of two airplanes, P1 and P2, and thateach has a key task, T1 and T2, respectively, requiring all of some shared resource, R.Because of the resource con ict, we must either give R to T1 early in the schedule, startingconstruction on plane P1 before P2, or we must give R to T2 early in the schedule, withthe opposite result. Whichever of the two tasks is started early will nish on time, but theother will be late.Suppose we construct a schedule in which T1 goes rst, and T2 is late, thus receiving aheavy blame factor. swo increases the priority on T2, and as a consequence, T2 goes rstin the subsequent schedule. But then T1 is late, and on the next iteration it will again go rst. We could alternate in this manner forever, and the result would be that swo wouldfail to explore either option very e ectively, because it would be jumping back and forthbetween the option of building plane P1 rst, and the option of building plane P2 rst,without remaining in one region of the search space long enough to re ne a solution.The di culty is that neither T1 nor T2 can be identi ed as a sacri cial task. Assumingthe two planes are not identical, we cannot simply argue from symmetry that we shouldjust pick one of the two tasks to be sacri ced. If, however, we could identify a sacri cialtask by the role it plays in a solution, we could achieve what we need. Here, the task to besacri ced must be the one that belongs to whichever plane is started later. If the analyzercould reduce the blame assigned to that task in a schedule, whichever task it happens tobe, it would allow swo to explore that region of the search much more e ectively.This problem of interchangeable roles would arise even more clearly with the introduc-tion of conditional elements in a solution. Suppose, for example, we have a schedulingproblem in which the constructor may choose to include or not include task instances ofsome type, adding however many instances are needed to satisfy a resource requirement.If those tasks are all instances of the same task type, then they are interchangeable, andpenalizing one may simply cause a shu ing of those instances that does not really addressthe problem. Moreover, with conditional tasks, it is not clear how the analyzer shouldassign blame when the set of task instances in the current schedule may be very di erentfrom the set of task instances in successor schedules.To address these concerns, the notion of prioritization could be generalized to apply toadditional aspects of a problem. In scheduling this might mean not just prioritizing tasks,but also resources over various time intervals. We also propose that the these prioritizationsbe limited to the \ xed" elements of a problem. In scheduling problems, for example, these370
\Squeaky Wheel" Optimizationmay be the non-conditional tasks, resources, etc. (In our example domains, all of theelements are xed in this sense, so this was not an issue.)One intuition behind this proposal is that these are the elements that will tend to de neroles. In the earlier example with tasks T1 and T2, corresponding to the two planes beingbuilt, the critical element is not either task per se, but actually resource R, early in theschedule. If this phase of resource R receives a high priority, and the later phase of resourceR receives a lower priority, then whichever of the two tasks occurs later will be recognizedas less critical. While this does not exactly capture the notion of \role" that we would like,it comes a lot closer than the current approach. In addition, assigning priorities to the xedelements of a problem has the advantage of being applicable to problems with conditionaltasks. Research is currently under way to explore this approach.6.3 swo and local searchAlthough the ability to make large, coherent moves is a strength of the approach, it is alsoa weakness. swo is poor at making small \tuning" moves in the solution space, but thecoupled-search view of swo suggests an obvious remedy. swo could be combined with localsearch in the solution space, to look for improvements in the vicinity of good solutions.Similarly, making small changes to a prioritization would generally result in smaller movesin the solution space than result from going through the full analysis and re-prioritizationcycle.Yet another alternative is genetic algorithm techniques for \crossover" and other types ofmutation to a pool of nodes, as is done in optiflex. Many hybrid approaches are possible,and we believe that the coupled-search view of swo helps to identify some interestingstrategies for combining moves of various sizes and kinds, in both search spaces, adaptingdynamically to relative solution qualities.7. ConclusionsOur experience has been that it is fairly straightforward to implement swo in a new domain,because there are usually fairly obvious ways to construct greedy solutions, and to analyzea solution to assign \blame" to some of the elements. Naive implementations of swo tendto perform reasonably well.We have found the view of swo as performing a \coupled search" over two di erentsearch spaces to be very informative. It has been helpful to characterize the kinds of movesthat swo makes in each of the search spaces, and the e ect this has on avoiding localoptima, etc. We hope that by continuing to gain a deeper understanding of what makesswo work we will be able to say more about the e ective design of swo algorithms.As the number of directions for future research suggests, we have only begun to scratchthe surface of \Squeaky Wheel" Optimization.AcknowledgementsThe authors wish to thank Robert Stubbs of Lucent Technologies for providing the dataused for the scheduling experiments. The authors also wish to thank George L. Nemhauser,371
Joslin & ClementsMarkus E. Puttlitz and Martin W. P. Savelsbergh with whom we collaborated on using swoin a hybrid AI/OR approach. Many useful discussions came out of that collaboration, andwithout them we would not have had access to the Lucent problems. Markus also wrotethe framework for the scheduling experiments and the tabu and ip implementations.The authors also thank the members of CIRL, and James Crawford at i2 Technologies,for their helpful comments and suggestions. We would like to thank Andrew Parkes inparticular for suggestions and insights in the graph coloring domain.This e ort was sponsored by the Air Force O ce of Scienti c Research, Air Force Ma-teriel Command, USAF, under grant number F49620-96-1-0335; by the Defense AdvancedResearch Projects Agency (DARPA) and Rome Laboratory, Air Force Materiel Command,USAF, under agreements F30602-95-1-0023 and F30602-97-1-0294; and by the NationalScience Foundation under grant number CDA-9625755.The U.S. Government is authorized to reproduce and distribute reprints for Governmen-tal purposes notwithstanding any copyright annotation thereon. The views and conclusionscontained herein are those of the authors and should not be interpreted as necessarily rep-resenting the o cial policies or endorsements, either expressed or implied, of the DefenseAdvanced Research Projects Agency, Rome Laboratory, the Air Force O ce of Scienti cResearch, the National Science Foundation, or the U.S. Government.Most of the work reported in this paper was done while both authors were at CIRL.ReferencesClements, D., Crawford, J., Joslin, D., Nemhauser, G., Puttlitz, M., & Savelsbergh, M.(1997). Heuristic optimization: A hybrid AI/OR approach. In Proceedings of theWorkshop on Industrial Constraint-Directed Scheduling. In conjunction with theThird International Conference on Principles and Practice of Constraint Program-ming (CP97).Crawford, J., Dalal, M., & Walser, J. (1998). Abstract local search. In Proceedings of theAIPS-98 Workshop on Planning as Combinatorial Search. In conjunction with theFourth International Conference on Arti cial Intelligence Planning Systems (AIPS-98).Crawford, J. M. (1996). An approach to resource constrained project scheduling. In Proceed-ings of the 1996 Arti cial Intelligence and Manufacturing Research Planning Work-shop, pp. 35{39.Culberson, J. C., & Luo, F. (1993). Exploring the k{colorable landscape with iteratedgreedy. In (Johnson & Trick, 1996), pp. 245{284.Feo, T. A., & Resende, M. G. (1995). Greedy randomized adaptive search procedures.Journal of Global Optimization, 6, 109{133.Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to theTheory of NP-Completeness. W. H. Freeman.Glover, F., & Laguna, M. (1997). Tabu Search. Kluwer.372
\Squeaky Wheel" OptimizationGlover, F., Parker, M., & Ryan, J. (1993). Coloring by tabu branch and bound. In (Johnson& Trick, 1996), pp. 285{307.Gomes, C., Selman, B., & Kautz, H. (1998). Boosting combinatorial search through ran-domization. In Proceedings of AAAI-98, pp. 431{437.Johnson, D. S., & Trick, M. A. (Eds.). (1996). Cliques, Coloring, and Satis ability: SecondDIMACS Implementation Challenge, 1996, Vol. 26 of DIMACS Series in DiscreteMathematics and Theoretical Computer Science. American Mathematical Society.Joslin, D., & Clements, D. (1998). \Squeaky wheel" optimization. In Proceedings of AAAI-98, pp. 340{346.Lewandowski, G., & Condon, A. (1993). Experiments with parallel graph coloring heuristicsand applications of graph coloring. In (Johnson & Trick, 1996), pp. 309{334.Morgenstern, C. (1993). Distributed coloration neighborhood search. In (Johnson & Trick,1996), pp. 335{357.Parkes, A., & Walser, J. (1996). Tuning local search for satis ability testing. In Proceedingsof AAAI-96, pp. 356{362.Pinson, E., Prins, C., & Rullier, F. (1994). Using tabu search for solving the resource-constrained project scheduling problem. In EURO-WG PMS 4 (EURO WorkingGroup on Project Management and Scheduling), pp. 102{106 Louvain, Belgium.Selman, B., Kautz, H. A., & Cohen, B. (1993). Local search strategies for satis abilitytesting. In (Johnson & Trick, 1996), pp. 521{531.Syswerda, G. P. (1994). Generation of schedules using a genetic procedure.. U.S. Patentnumber 5,319,781.
373
A CSTP is called dynamically consistent if there exists a strategy for executing its time-points that guarantees the satisfaction of all relevant constraints no matter how the truth values of the various observations turn out (Tsamardinos, Vidal, and Pollack 2003). The strategy is dynamic in that its execution decisions can react to past observations, but not those in the future. This section defines the dynamic consistency of a CSTN in an equivalent way; however, for convenience, there are some superficial differences in notation and organization. Afterward, we provide a second characterization of the dynamic property that will be useful later on.
Definition 9 (Scenario/Interpretation Function). A scenario (or interpretation function) over a set P of propositional letters is a function, s : P → {true, false}, that assigns a truth value to each letter in P .6 As is standard practice in propositional logic, any interpretation function can be extended to provide the truth value for every possible formula involving the letters in P . Thus, any interpretation function, s, can provide the truth value of each label involving letters in P . For any label, `, the truth value of ` in the scenario, s, is denoted by s(`). Let IP (or simply I) denote the set of all interpretation functions (or complete execution scenarios) over P .
Definition 10 (Schedule). A schedule for a set of timepoints T is a mapping, ψ : T → R that assigns a real number to each time-point in T . The set of all schedules for any subset of T is denoted by ΨT (or Ψ if the context allows).
Below, the projection of a CSTN, S, onto a scenario, s, is defined to be the STN that contains all of the time-points and constraints from S whose labels are true under s (i.e., the time-points that must be executed under s, and the constraints that must be satisfied under s).
Definition 11 (Scenario Projection for a CSTN). Let S = 〈T , C, L,OT ,O, P 〉 be any CSTN, and s any interpretation function (i.e., complete scenario) for the letters in P . The projection of S onto the scenario s—denoted by scPrj (S, s)—is the STN, (T +s , C+s ), where: • T +s = {T ∈ T : s(L(T )) = true}; and • C+s = {(Y −X ≤ δ) | for some `, (Y −X ≤ δ, `) ∈ C
and s(`) = true}
6Unlike the prior work on CSTPs, we restrict attention to complete scenarios because the subsequent definition of a history requires a scenario to entail the outcome of all past observations.
Recall that condition WD1 from the definition of a CSTN stipulates that the label on any constraint must subsume the labels on the time-points it connects. Thus, for any constraint in C+s , the time-points it connects must belong to the set T +s . Definition 12 (Execution Strategy for a CSTN). Let S = 〈T , C, L,OT ,O, P 〉 be any CSTN. An execution strategy for S is a mapping, σ : I → ΨT , such that for each scenario, s ∈ I, the domain of σ(s) is T +s (cf. Defn. 11). If, in addition, for each scenario, s, the schedule σ(s) is a solution to the scenario projection, scPrj (S, s), then σ is called viable. In any case, the execution time for the time-point X in the schedule σ(s) is denoted by [σ(s)]X .
Below, the history of a time-point, X , relative to a scenario, s, and strategy, σ, is defined to be the set of observations made before the time at whichX is executed according to the schedule, σ(s) (i.e., before the time [σ(s)]X ).7
Definition 13 (Scenario history for a CSTN). Let S = 〈T , C, L,OT ,O, P 〉 be any CSTN, s any scenario, σ any execution strategy for S, and X any time-point in T +s (cf. Defn. 11). The history of X in the scenario s, for the strategy σ—denoted by scHst(X, s, σ)—is given by:
scHst(X, s, σ) = {(p, s(p)) | O(p) ∈ T +s , and [σ(s)]O(p) < [σ(s)]X}
Note that any scenario history determines a corresponding label whose (positive or negative) literals are in a one-to-one correspondence with the observations, (p, s(p)), in the history. Thus, we may sometimes (e.g., in the next definition) treat a scenario history as though it were a label.
Below, an execution strategy is called dynamic if the schedules it generates always assign the same execution time to any time-pointX in scenarios that cannot be distinguished prior to that time.8
Definition 14 (Dynamic Execution Strategy for a CSTN). An execution strategy, σ, for a CSTN is called dynamic if for all scenarios, s1 and s2, and any time-point X:
if Con(s1, scHst(X, s2, σ)), then [σ(s1)]X = [σ(s2)]X .
Definition 15 (Dynamic Consistency for a CSTN). A CSTN is called dynamically consistent if there exists an execution strategy for it that is both viable and dynamic.
The following definitions and lemma provide an equivalent, alternative characterization of a dynamic execution strategy for a CSTN. First, a scenario history relative to a numerical time—not a time-point variable—is defined.
Definition 16 (Scenario History∗ for a CSTN). Let S = 〈T , C, L,OT ,O, P 〉 be any CSTN, s any scenario, σ any execution strategy for S, and t any real number. The history∗ of t in the scenario s, for the strategy σ—denoted by
7Tsamardinos et al. (2003) define (pre)histories for arbitrary schedules, whereas here we restrict attention to schedules of the form, σ(s), where σ is an execution strategy and s is a scenario.
8Tsamardinos et al. (2003) include a disjunctive condition, Con(s1, scHst(X, s2, σ)) ∨ Con(s2, scHist(X, s1, σ)). However, since s1 and s2 play symmetric roles in the two disjuncts, and since s1 and s2 are both universally quantified (cf. Defn. 14), it suffices to include just one of the disjuncts.
scHst∗(t, s, σ)—is the set of all observations made before time t according to the schedule, σ(s). In particular: scHst∗(t, s, σ) = {(p, s(p)) | O(p) ∈ T +s and [σ(s)]O(p) < t}
Note that for all time-pointsX , scenarios s, and strategies σ, scHst(X, s, σ) = scHst∗([σ(s)]X , s, σ). Definition 17 (Dynamic∗ Execution Strategy for a CSTN). An execution strategy, σ, for an CSTN is called dynamic∗ if for any scenarios, s1 and s2, and any time-point, X:
if scHst∗([σ(s1)]X , s1, σ) = scHst∗([σ(s1)]X , s2, σ), then [σ(s1)]X = [σ(s2)]X . Notice that in this definition, the two histories, one relative to s1, the other to s2, are taken with respect to the same (numeric) time, [σ(s1)]X . If the strategy σ yields schedules for s1 and s2 that have identical histories prior to that one time, then those schedules must assign the same value to X . Lemma 5. An execution strategy σ for a CSTN is dynamic if and only if it is dynamic∗.
Natural Language Driven Image Generation (NALIG) is one of the early projects on generating static 2D scenes from natural language descriptions [1, 2]. It uses a very restricted form of input language that is basically a simple regular expression. The main focus of NALIG is to investigate the relationship between the spatial information and the prepositions in Italian phrases. The accepted form of phrases in this system is as follows:
[subject][preposition][object]
Using this regular expression, NALIG can understand inputs such as the book is on the table. It can also handle ambiguities within the phrases and infer simple implicit spatial arrangements using taxonomical rules such as Object X supports object Y that define the relations between the existing objects. These rules are defined based on state conditions, containment constraints, structural constraints, and supporting rules. For example, given an input such as a branch on the roof the system can infer that a tree near the house having a branch on the roof. In addition to spatial arrangements, NALIG also utilizes statics to infer how an object can support another object based on a physical equilibrium. All in all, NALIG is a very restricted system that does not support user interactions, flexible inputs, or 3D spatial relations.
In this section we prove the correctness of Proposition 5.16 on page 105 by using the results of Sections 6.1.2 and 6.2.2 which provide evidence for the correctness (soundness, completeness and optimality) of methods STATICHS and DYNAMICHS:
Proof of Proposition 5.16. First, we argue why Algorithm 5 must terminate. The function GETFORMULAPROBS in line 5 terminates since it applies Formulas 4.2 and 4.7 |K| times and |K| is finite by Definition 3.1. If mode = static, then STATICHS terminates due to Proposition 6.1. If mode = dynamic, then DYNAMICHS terminates due to Proposition 6.2. GETPROBDIST terminates since (1) the number of already answered queries |QA| is finite, (2) |DX| is finite since diagnoses are subsets of K and thus there is only a finite number of (minimal) diagnoses w.r.t. any DPI according to Definition 3.1 (since all sets included in the DPI are finite) and (3) reasoning (GETENTAILMENTS and ISKBVALID) is assumed to be decidable for the logic L over which the DPI is formulated as per Chapter 2. Further, GETMODE clearly terminates due to the fact that |DX| is finite and returns the mode Dmax of the diagnoses probability distribution pD() over the diagnoses in DX. Now, if the stop criterion pD(Dmax) ≥ 1 − σ is met, then GETSOLKB is called. GETSOLKB simply deletes the given diagnosis Dmax from the given KB K and adds a finite set of formulas to it, and thence terminates.
If the stop criterion is not met, then |DX| ≥ 2 must hold as otherwise the single diagnosis D ∈ DX would necessarily have fulfilled the stop criterion as its probability as per any probability measure over the sample space Ω := DX must be equal to 1 and thus greater than or equal to 1− σ where σ ≥ 0.
Due to |DX| ≥ 2, Proposition 5.15 implies that GETPOOLOFQUERIES (called within CALCQUERY) terminates and yields a non-empty query pool as output. SELECTBESTQUERY (also called within CALCQUERY) terminates as well since it simply selects one query from the pool according to the measure qsm() (cf. Section 5.3.3). Since we assume the interacting user to answer to a query or to reject it within finite time, u(Q) also terminates. It is clear that APPEND terminates. GETINVALIDDIAGS simply extracts one entry of the given q-partition and thus terminates. Finally, UPDATEQDATA also terminates by assumption (no qsm() must be used for which UPDATEQDATA might not terminate). As a consequence, all functions called in Algorithm 5 terminate. What remains to be proven is that the stop criterion must be met after a finite number of iterations, i.e. after a finite number of test cases have been added to the input DPI.
In mode = static the stop criterion must be satisfied after a finite number of iterations due to the following argumentation:
• There is a finite set of minimal diagnoses w.r.t. the input DPI 〈K,B,P ,N 〉R since each (minimal) diagnosis w.r.t. this DPI is a subset of K according to Definition 3.5 and since |K| is finite by Definition 3.1.
• In each iteration, one test case is added either to P ′ or N ′.
• Each test case added to whatever set P ′ or N ′ invalidates at least one minimal diagnosis w.r.t. the input DPI in the set DX by the definition of a query (Definition 5.2) and since each query is computed w.r.t. the leading diagnoses DX by the correctness of GETPOOLOFQUERIES (cf. Proposition 5.15).
• DX contains only minimal diagnoses w.r.t. the input DPI by Proposition 6.1.
• Also by Proposition 6.1, no invalidated minimal diagnosis w.r.t. the input DPI can be an element of some subsequent set of leading diagnoses DX.
• Therefore, unless the stop criterion is met before due to a sufficiently high probability of one of multiple leading diagnoses as per pD(), Algorithm 5 inmode = staticmust arrive at a point where
CHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 119
|DX| = 1 after a finite number of iterations. Note that |DX| = 0 is impossible due to the definition of a query (Definition 5.2) which ensures that each added test case leaves valid at least one minimal diagnosis in DX.
Algorithm 5 terminates in mode = dynamic since for any sequence QA of queries that are added to the positive or negative test cases P ′ or N ′, respectively, there is a finite number kQA such that there is no more than one minimal diagnosis w.r.t. 〈K,B,P ∪ P ′,N ∪N ′〉R for |P ′| + |N ′| = kQA wherefore the stop criterion must be met. Now, let us assume that the opposite holds. That is, there is a sequence QA∗ of queries that are added to the positive or negative test cases P ′ or N ′, respectively, and for all natural numbers k there is more than one minimal diagnosis w.r.t. 〈K,B,P ∪ P ′,N ∪N ′〉R for |P ′|+ |N ′| = k. Then we argue as follows to derive a contradiction:
• There is a finite set of (minimal) diagnoses w.r.t. any DPI 〈K,B,P ∪ P ′,N ∪N ′〉R obtained from the input DPI by the addition of test cases. This is true since |K| is finite by Definition 3.1 and since each (minimal) diagnosis w.r.t. 〈K,B,P ∪ P ′,N ∪N ′〉R is a subset of K according to Definition 3.5.
• In each iteration, one test case is added either to P ′ or N ′.
• Each test case added to whatever set P ′ or N ′ invalidates at least one minimal diagnosis w.r.t. the current DPI in the set DX by the definition of a query (Definition 5.2) and since each query is computed w.r.t. the leading diagnoses DX by the correctness of GETPOOLOFQUERIES (cf. Proposition 5.15).
• If DPI denotes the current DPI at the time DYNAMICHS is called, then the set DX returned by DYNAMICHS is a subset of or equal to mDDPI , i.e. DX contains only minimal diagnoses w.r.t. DPI by Proposition 6.2.
• Let 〈DPI0, DPI1, . . . 〉 denote the sequence of DPIs encountered in the case of adding answered queries as test cases to the input DPI DPI0 as per QA∗. Further, let 〈aD0,aD1, . . . 〉 be the sequence such that aDi := aDDPIi , i = 0, 1, . . . , i.e. aDi is the set of all diagnoses w.r.t. DPIi. Then aDi ⊃ aDi+1 for all i ≥ 0.
• As each query added as a test case to DPIi leaves valid at least one (minimal) diagnosis w.r.t. DPIi due to Definition 5.2, we have that aDk ⊃ ∅ for k = 0, 1, . . . .
• Since aDi is finite, there must be some finite number k∗ such that |aDk∗ | = 1 wherefore |mDk∗ | = 1 must also be valid. This is a contradiction.
Thence, Algorithm 5 terminates in any mode mode. Now, we show that propositions (1)-(6) of Proposition 5.16 hold for (i) mode = static and (ii) mode = dynamic.
(i): First, by the proof so far, we have that Algorithm 5 in mode = static given the input DPI 〈K,B,P ,N 〉R terminates. Since the only point where the algorithm can terminate is line 14, GETSOLKB is called with arguments 〈Dmax, 〈K,B,P ∪ P ′,N ∪N ′〉R,P ′, static〉. By the definition of GETSOLKB (see Section 5.3.2.4), we have that (K \ Dmax) ∪ UP is returned by the algorithm.
Propositions (1) and (2) follow from the specification of the GETMODE function which is called with arguments 〈DX, pD()〉. Proposition (3) is true since GETSOLKB can never be reached without pD(Dmax) ≥ 1−σ being fulfilled. DX ⊆mD〈K,B,P,N 〉R ∩mD〈K,B,P∪P ′,N∪N ′〉R is true due to Proposition 6.1, Remark 5.15 and the fact that DX is obtained as an output of STATICHS. Hence, Proposition (4) holds. Proposition (5) is implied by Remark 5.15 and by the specification of the GETFORMULAPROBS function which computes pK() from pK̃∪K() as per Formulas 4.2 and 4.7 in line 5. Finally, Proposition (6) is a consequence of the definition of the GETPROBDIST function which accounts for the computation of
CHAPTER 5. INTERACTIVE KNOWLEDGE BASE DEBUGGING 120
pD() from pK(), the input DPI, DX and the chronological sequence of all queries and associated answers QA so far. Therefore, Proposition 5.16 is true for mode = static.
(ii): First, by the proof so far, we have that Algorithm 5 in mode = dynamic given the input DPI 〈K,B,P ,N 〉R terminates. Since the only point where the algorithm can terminate is line 14, GETSOLKB is called with arguments 〈Dmax, 〈K,B,P ∪ P ′,N ∪N ′〉R,P ′, dynamic〉. By the definition of GETSOLKB (see Section 5.3.2.4), we have that (K \ Dmax) ∪ UP∪P ′ is returned by the algorithm.
Propositions (1) and (2) follow from the specification of the GETMODE function which is called with arguments 〈DX, pD()〉. Proposition (3) is true since GETSOLKB can never be reached without pD(Dmax) ≥ 1 − σ being fulfilled. DX ⊆ mD〈K,B,P∪P ′,N∪N ′〉R is true due to Proposition 6.2, Remark 5.15 and the fact that DX is obtained as an output of DYNAMICHS. Hence, Proposition (4) holds. Proposition (5) is implied by Remark 5.15 and by the specification of the GETFORMULAPROBS function which computes pK() from pK̃∪K() as per Formulas 4.2 and 4.7 in line 5. Finally, Proposition (6) is a consequence of the definition of the GETPROBDIST function which accounts for the computation of pD() from pK(), the input DPI, DX and the chronological sequence of all queries and associated answers QA so far. Therefore, Proposition 5.16 is true for mode = dynamic.
Next, we show that the solution to Interactive Static KB Debugging is found for σ = 0 in case mode = static:
(s1) DX ⊆mD〈K,B,P,N 〉R ∩mD〈K,B,P∪P ′,N∪N ′〉R holds for the output of STATICHS in each iteration by Proposition 6.1. Therefore, DX comprises only minimal diagnoses w.r.t. the input DPI that comply with all specified test cases in P ′ and N ′.
(s2) By pK̃∪K() : K̃ ∪ K → (0, 1] we derive by Formula 4.2 that each formula in K must have a probability greater than zero. Further, by Formula 4.7, no formula in K can have a probability greater than or equal to 0.5 (i.e. in particular a probability of 1 is not possible for a formula). Hence, we have that pK : K → (0, 0.5) for the measure pK() computed by GETFORMULAPROBS in line 5 in Algorithm 5. Thence, by the definition of pnodes() in STATICHS based on p() := pK() (cf. Definition 4.9 on page 64) due to the fact that pK() is given as an input argument to STATICHS in line 8, we have that no diagnosis can have an (a-priori) probability of zero. Since the function GETPROBDIST might only perform some multiplications of a diagnosis probability by 12 , also the a-posteriori probability of each diagnosis must be greater than zero.
(s3) Hence, due to σ = 0, it must be necessarily be true that |DX| = 1 before the algorithm terminates.
(s4) By Problem Definition 5.2 and the specification of the GETSOLKB function, the output solution KB must be the solution to Interactive Static KB Debugging.
That a solution found for σ > 0 in case mode = static might be an approximate solution to Interactive Static KB Debugging is a direct consequence of the definition of approximate solution given in Remark 5.12.
Finally, the proof that the solution to Interactive Dynamic KB Debugging is found for σ = 0 in case mode = dynamic is analogue to the one for mode = static, just
(d1) DX ⊆ mD〈K,B,P∪P ′,N∪N ′〉R holds for the output of DYNAMICHS in each iteration by Proposition 6.2. Therefore, DX comprises only minimal diagnoses w.r.t. the current DPI.
(d2) By (s2), (s3), Problem Definition 5.1 and the specification of the GETSOLKB function, the output solution KB must be the solution to Interactive Dynamic KB Debugging.
That a solution found for σ > 0 in case mode = dynamic might be an approximate solution to Interactive Dynamic KB Debugging is a direct consequence of the definition of approximate solution given in Remark 5.12.
This completes the proof of Proposition 5.16.
Chapter 6
Iterative Diagnosis Computation
In this chapter we will introduce and discuss two methods, STATICHS and DYNAMICHS, which are called in lines 8 and 10 of Algorithm 5, respectively. The former provides a method for solving the Interactive Static KB Debugging Problem (Problem Definition 5.2) whereas the latter aims at solving the Interactive Dynamic KB Debugging Problem (Problem Definition 5.1). Both are methods for iterative diagnosis computation that are employed to compute a set of leading diagnoses in each iteration of the presented interactive KB debugging algorithm (Algorithm 5). Each time a query has been answered by the interacting user and added to the respective set of test cases of the DPI, a subset of the leading diagnoses (and usually also a set of not-yet-computed minimal diagnoses) is invalidated. An iterative diagnosis computation method is then invoked to update the leading diagnoses set taking the new information into account that is given by the recently added test case. That is, the k ≤ nmax most probable ways of solving the Interactive Static (Dynamic) KB Debugging Problem in the light of the new evidence are extracted by STATICHS (DYNAMICHS) after the search space has been suitably pruned. In this vein, if there is only one solution left, the (exact) solution of Interactive Static (Dynamic) KB Debugging has been found.
This section describes the different network architectures and details for the experimental results.
B.1 MNIST
All the different networks were tuned on the validation dataset. This tuned network was used to evaluate the performance on the test images. The performance of our proposed architecture was evaluated over six architectures overall. Stochastic Gradient Descent (SGD) with momentum of 0.95 was used for training all the architectures. For the shallow networks (25 and 50 neurons case), we used an intial learning rate of 0.1 and dropped it by 0.75 every 10 epochs until 125 epochs, and then the learning rate was dropped by 0.998 times its previous value. For all other MLP architectures we used an initial learning rate of 0.01 and dropped it by 0.5 at every quarter of the total number of epochs. In regards to CrossNet version of the MLP network, the input layer was connected to all the subsequent hidden layers (excluding the output layer), and the hidden layers were connected to all
13
the subsequent hidden layers (including the output layer). For the 2-layer CNN architecture we used a fairly standard network. Both convolutional layers use 5× 5 filter size with 16 and 32 channels, respectively. A dropout of 0.5 is used after the second convolutional layer. All the layers used ’ReLU’ activations. For the CrossNet version of this architecture the activated outputs of both the convolutional layers were stacked together and fed to the output softmax classifier. The 2-CNN-2-fully connected architecture was designed as per [25] The first convolutional produces 32 feature maps, and the second convolutional layer produces 64 feature maps, each of using 5 × 5 filters followed by 2 × 2 pooling. The classifier comprises of 2-layer fully-connected neural network with 200 hidden units, and 10 outputs. A 10-layer CrossNet was designed for the experiment. We use 12 composite units (defined in Section.2.3). A 2× 2 average pooling layer was used after every 4 composite units. We used a step wise learning rate for training, starting from an initial learning rate of 0.1. We trained the network for 30 epochs.
B.2 CIFAR-10 and CIFAR-100 Both CIFAR-10 and CIFAR-100 contains 32× 32 RGB images belonging to 10 and 100 classes respectively. The raw input images pre-processed using global contrast normalization and ZCA whitening. ZCA whitening means that we center the data around its mean, rotate it onto its principle components, normalize individual components and then rotate them back. We held out 5,000 random training images for validation. The network was tuned to give the best performance on this validation dataset. The presented results are obtained from this tuned network. We use three different versions of CrossNets for our experiments. CrossNet v1, CrossNet v2, and CrossNet v3 consisted of 32, 64, and 48 composite units respectively. The pre-processed images were preactivated using a convolutional layer with 16 filters of 3 × 3 kernel size each. A 2 × 2 average pooling layer was used after 8, 16, and 16 composite units in each of the above mentioned CrossNets respectively. Finally, a global averaging pooling layer and a softmax classifier was used at the end. We use a step wise learning rate schedule for training, where we start with an initial learning rate of 0.1 and reduced 10 times after each 10% of the total number of epochs. We train the network for 300 epochs. No dropouts or dropconnects were used in the training. Standard data augmentation scheme in the form of translation and horizontal flips were used.
B.3 SVHN SVHN comprises of 32× 32 RGB images of digit classes. As before hyperparameters for the training were tuned on the validation set, and the tuned network was used to evaluate the performance on the test dataset. We held out 6,000 images from the training dataset for validation. The SVHN dataset is a relatively easier task as compared to CIFAR. Hence, pre-processing was only done in terms of global contrast normalization. The same three network architectures for CrossNets were used as that for CIFAR. The only difference is in terms of the total number of epochs. We train the network for 40 epochs. The learning rate schedule starts with an initial learning rate of 0.1 and is reduced 10 times after every 10 epochs. No dropouts or dropconnects were used in the training. No data augmentation was performed.
In this problem, we need to find a sequence of 12 different pitches with 12 different intervals (fig. 1). This problem can be generalized to find n different pitches with n different intervals equivalent under inversion 2. For instance, a value of n = 24 represents the all-interval series for microtones.
Therefore, a solution to this CSP is a sequence of n pairwise different variables with domain [1..n], where all modulo n intervals of the sequence are pairwise different. We give bellow a formalization of this problem
Variables: V1 ... Vn Domains: [1..n] ... [1..n] Constraints:
• C1 alldiff(V ) • C2 alldiff((Vi+1 − Vi)%n, i ≤ n) 2For instance, an interval C-E is equivalent to E-C.
There is not a constraint over the interval (Vn−V0) because that interval is always six, according to the literature. Furthermore, it is enough to calculate the series where V0 = 0 because the other ones can be obtained from that one using transposition. In addition, we know that if V1..Vn is an all-interval serie, Vn...V1 is also one. For those reasons, we include these two constraints to avoid symmetrical solutions:
• C3 V0 = 0 • C4 V0 < Vn We represent graphically this CSP (fig. 2) with a box to create n all-different variables with domain [1..n], an x→ dx box for C2 with an all-different parameter , an equality box for C3, and an inequality box for C4.
In this section, we will consider the version in which the agents are fully rational, in the sense that their response function is HardMax. We show that principals are not incentivized to explore— i.e., to deviate from DynamicGreedy. The core technical result is that if one principal adopts DynamicGreedy, then the other principal loses all agents as soon as he deviates.
To make this more precise, let us say that two MAB algorithms deviate at (local) step n if there is an action a ∈ A and a realization h of step-n history such that h is feasible for both algorithms, and under this history the two algorithms choose action a with different probability.
Theorem4.1. Assume HardMax response function with fair tie-breaking. Assume that alg1 is DynamicGreedy, and alg2 deviates from DynamicGreedy starting from some (local) step n0 < T . Then all agents in global rounds t ≥ n0 select principal 1.
Corollary 4.2. The competition game between principals has a unique Nash equilibirium: both principals choose DynamicGreedy.
Remark 4.3. This corollary holds under a more general model which allows time-discounting: namely, the utility of each principal i in each global round t is Ui,t(rt) if this principal is chosen, and 0 otherwise, where Ui,t(·) is an arbitrary non-decreasing function with Ui,t(0) > 0.
String kernel assumes training and testing samples are drawn from the same probability distribution. To consider the variation between source and target samples in our application, domain adaptation [10], [13] serves as a natural candidate to tackle this computational challenge. In machine learning, domain adaptation aims to use data or a model of a well-analyzed source domain to obtain or refine a model for a less analyzed target domain. The specific “domain transfer” setting being focused in this paper assumes that historical labels only exist in the source context and are not available in the target context (reasons explained in Section 1). Accordingly, we propose “Transfer String Kernel (TSK)” approach to achieve better cross-context TFBS predictions by transferring knowledge from an annotated context to an unannotated context.
4 TSK revises the (k,m)-mismatch string kernel framework using a “Kernel Mean Matching” (KMM) strategy [9] in order to perform knowledge transfer. To our knowledge, TSK has not been proposed in the literature for cross-context TFBS prediction tasks. Specifically, TSK adapts string kernel under the “covariate shift” assumption [9]. It assumes that the conditional probability distribution of the output variable, given the input variable, remains fixed in both the source and target set. In other words, the data shift happens for the marginal probability distribution of feature variables (from Psd(x) to Ptd(x)) and not for the conditional distribution P (y|x).
The key to correcting this type of sampling bias is to estimate the “importance weight” for each source sample [8]:
β(x, y) := Ptd(x, y)
Psd(x, y) =
P (y|x)Ptd(x) P (y|x)Psd(x) = Ptd(x) Psd(x) . (8)
The KMM estimator accounts for the difference between Ptd(x, y) and Psd(x, y) by re-weighting the source points so that the means of the source and target points in a Reproducing Kernel Hilbert Space (RKHS) are close. This reweighing process is therefore called “Kernel Mean Matching” (KMM). When the RKHS is universal, the population solution of weight vector β̂ (in the following Equation 9) to the KMM optimization (of matching means) is exactly the vector form of ratio Ptd(x)
Psd(x) (in Equation 8) including all x from the source
domain. Let βi represent the “importance weight” of a source instance xi. More specifically, KMM uses a “maximum mean discrepancy” measure to minimize the difference between the empirical mean of the source and the empirical mean of the target distribution. Formally, KMM attempts to match the mean elements in a feature space induced by a kernel function K(·, ·):
β̂ = argminβ L̂(β), (9)
where,
L̂(β) = ‖ 1
nsd
nsd∑
i=1
βi ∗ φ(x sd i )−
1
ntd
ntd∑
i=1
φ(xtdi )‖ 2 (10)
= 1
n2sd β
T Kβ −
2
n2sd κTβ + constant, (11)
s.t. βi ∈ [0, B] and | nsd∑
i=1
βi − nsd| ≤ nsdǫ.
Here, K is the kernel matrix among all source examples, and κi is the weighted sum of kernel values among a source example and all target examples:
Kij :=K(x sd i , x sd j ), (12)
κi := nsd
ntd
ntd∑
j=1
K(xsdi , x td j ). (13)
Large values of κi correspond to more important observations xsdi and are more likely to lead to larger βi. Equation 9 involves a quadratic program which can be efficiently solved using the interior point methods or any other subsequent procedures such as projected gradient optimization method [9].
Algorithm 1 Algorithm for Transfer String Kernel through matching means in RKHS
1: procedure TSK 2: Implement (k,m)-mismatch string kernel to calculate the
kernel matrix K and vector κ; 3: Use Kernel Mean Matching estimator on K and κ to obtain
the importance weight β̂i for each source sample i; 4: Use β̂i to perform instance re-weighted SVM training step; 5: Use the trained SVM model to perform sequence classifica-
tion of target samples.
The derived importance weights β̂ are then used to re-weigh source samples in a revised support vector machine training algorithm, i.e., an instance weighted SVM that aims to optimize:
nsd∑
i=1
αi − 0.5 ∑
i,j∈sd
αiαjyiyjK(x sd i , x sd j ), (14)
s.t.
nsd∑
i=1
αiyi = 0, and β̂iC ≥ αi ≥ 0. (15)
Here αi are the slack variables in the SVM, the same as αi in Eq 3. In summary, we apply KMM in conjunction with the (k,m)mismatch string kernel using the algorithm summarized in Algorithm 1, thus enabling us to perform knowledge transfer across domains when classifying strings.
This section is meant to offer a brief and gentle introduction to our formal language and logic, and to discuss three different types of permission and their relation with the concept of normative defeasibility. Moreover, we illustrate the idea of preference over permissions that explicitly derogate to prohibitions.
These aspects will be formally handled in Section 3. The whole discussion of the computational aspects of permission in Defeasible Logic is postponed to Sections 4 and 5.
2.1. Informal presentation of the logic
Let us summarise the basic logical intuitions behind our framework.
1. Permissive and prescriptive norms are represented by means of defeasible rules, whose conclusions normally follow unless they are defeated by contrary evidence. For example, the rule
Order ⇒O Pay
says that, if we send a purchase order, then we will be defeasibly obliged to pay; the rule
Order,Creditor ⇒P ¬Pay
states that if we send an order, in general we are not obliged to pay if we are creditors towards the vendor for the same amount.
2. Rules introduce modalities: if we have the rule a ⇒O b and a holds, then we obtain Ob. That is to say, in the scenario where conditions described by a hold, the obligation of doing b is active as well. The advantage is that explicitly deriving modal literals such as Ob adds expressive power to the language, since Ob may appear in the antecedent of other rules, which can then be triggered.
3. For the sake of simplicity, modal literals can only occur in the antecedent of rules. In other words, we do not admit nested modalities, i.e., rules such as a ⇒O Pb. This is in line with our idea that the applicability of rules labeled with modality ✷ (where ✷ can be O for obligation or P for permission) is the condition for deriving literals modalised with ✷. 4. The symbols O and P are not simple labels: they are modalities. O is non-reflexive1: consequently, we do not have a conflict within the theory when ¬a is the case and we derive that a is mandatory (Oa); this amounts to having a violation. The modality P works in such a way that two rules for P supporting a and ¬a do not clash, but a rule like ⇒P b attacks a rule such as ⇒O ¬b (and vice versa). 5. Like standard Defeasible Logic, our extension is able to establish the relative strength of any rule (thus to solve rule conflicts) and has two types of attackable rules: defeasible rules and defeaters. Defeaters in Defeasible Logic are a special kind of rules: they are used to prevent conclusions but not to support them. For example, the defeater
SpecialOrder,PremiumCustomer ❀O ¬PayBy7Days
can prevent the derivation of the obligation for premium customers placing special orders to pay within the deadline of 7 days, but cannot be used to directly derive any conclusion.
2.2. Permissions and defeasibility
The above framework, though simple, allows us to express three basic types of permissions as well as illustrate interesting connections with the idea of defeasibility.
1As it is well-known, in a non-reflexive modal logic ✷a does not imply a, where ✷ is a modal operator.
Weak permission. A first way to define permissions in Defeasible Logic is by simply considering weak permissions and stating that the opposite of what is permitted is not provable as obligatory. Let us consider a normative system consisting of the following two rules:
r1 : Park,Vehicle ⇒O ¬Enter r2 : Park,Emergency ⇒O Enter.
Here the normative system does not contain any permissive norm. However, since Defeasible Logic is a sceptical non-monotonic logic, in case both r1 and r2 fire we neither conclude that it is prohibited nor that it is obligatory to enter, because we do not know which rule is stronger. Hence, in this context, both ¬Enter and Enter are weakly permitted.
As already argued, this is the most direct way to define the idea of weak permission: some q is permitted by a code iff q is not prohibited by that code. Accordingly, saying that any literal q is weakly permitted corresponds to the failure of deriving¬q using rules for O. Notice that, in Defeasible Logic, this does not amount to obtain ¬O¬q.
Explicit permissions are defeaters. In Defeasible Logic any rule can be used to prevent the derivation of a conclusion. For instance, suppose there exists a norm that prohibits to U-turn at traffic lights unless there is a “U-turn permitted” sign:
r1 : AtTrafficLight ⇒O ¬Uturn r2 : AtTrafficLight,UturnSign⇒O Uturn.
In this example we use a defeasible rule for obligation to block the prohibition to U-turn. However, this is not satisfactory: if we do not know whether r2 is stronger than r1, then the best we can say is that U-turn is weakly permitted. Furthermore, if r2 prevails over r1, we derive that U-turn is obligatory.
Thus, there are good reasons to argue that defeaters for O are suitable to express an idea of strong permission2. Explicit rules such as r : a ❀O q state that a is a specific reason for blocking the derivation of O¬q (but not for proving Oq). In other words, this rule does not support any conclusion, but states that ¬q is deontically undesirable. Consider this example:
r1 : Weekend,AirPollution ⇒O ¬UseCar r2 : Weekend,Emergency ❀O UseCar.
Rule r1 states that on weekends it is forbidden to use private cars if a certain air pollution level is exceeded. Defeater r2 is in fact an exception to r1, and so it seems to capture the above idea that explicit permissive norms (especially in the law) provide exceptions to obligations.
Explicit permissions using permissive rules. Another approach is based on introducing specific rules for deriving permissions [25,9]. Let us consider the following situation:
r1 : Weekend,AirPollution ⇒O ¬UseCar r′2 : Emergency ⇒P UseCar.
As r2 in the previous scenario, r′2 looks like an exception to r1. The apparent difference between r2 and r′2 is that the latter is directly used to prove that the use of the car is permitted (PUseCar) in case of emergencies. The question is: does it amount to a real difference?
2The idea of using defeaters to introduce permissions was introduced in [19].
Although r2 is a defeater, it is specifically used to derive the strong permission to use the car, like r′2. In addition, rules such as r ′ 2 do not attack other permissive rules, but are in conflict only with rules for obligation intended to prove the opposite conclusion. This precisely holds for defeaters.
Moreover, let us suppose to have the defeater s : a ❀P b. Does s attack a rule like ⇒P ¬b?
If this is the case, s would be close to an obligation. The fact that Pb does not attack P¬b makes it pointless for s to introduce defeaters for P. But, if this is not the case, s could only attack ⇒O ¬b, thus being equivalent to s′ : a ❀O b.
Therefore, although it is admissible to have defeaters, we do not need to distinguish defeaters for O from those for P. One way to mark the difference between ❀ and ⇒P is by stating that only the latter rule type admits ordered sequences of strong permissions in the head of a rule, which are supposed to derogate or make exceptions to prohibitions. This matter will be discussed in the next subsection.
2.3. Permissions, obligations, and preferences
The introduction of ordered sequences of strong permissions in the head of a rule, which derogate or make exceptions to prohibitions, can be logically modelled by enriching the formal language and following these guidelines:
1. In many domains, such as the law, norms often specify mandatory actions to be taken in case of their violation. In general, obligations in force after the violation of some other obligations correspond to contrary-to-duty (CTD) obligations. These constructions affect the formal characterisation of compliance since they identify situations that are not ideal, but still acceptable. A compact representation of CTDs may resort to the non-boolean connective ⊗ [14]: a formula like x ⇒O a⊗ b means that, if x is the case, then a is obligatory, but if the obligation a is not fulfilled, then the obligation b is activated and becomes in force until it is satisfied, or violated. 2. Concepts introduced at point 1 can be extended to permissive rules with the subscripted arrow ⇒P by introducing the non-boolean connective ⊙ for sequences of permissions. As in the case of ⊗, given a rule like ⇒P a⊙b, we can proceed through the ⊙-chain to obtain the derivation of Pb. However, permissions cannot be violated, and consequently it does not make sense to obtain Pb from ⇒P a⊙b and ¬a. In this case, the reason to proceed in the chain is rather that the normative system allows us to prove O¬a. Hence, ⊙ still establishes a preference order among strong permissions and, in case the opposite obligation is in force, another permission holds. This is significant especially when strong permissions are exceptions to obligations.
In this paper we take a neutral approach as to whether ordered sequences of obligations or permissions are either given explicitly, or inferred from other rules. However, we point out that normative documents often explicitly contains provision with such structures. A clear example of this is provided by the Australian “National Consumer Credit Protection Act 2009” (Act No. 134 of 2009) which is structured in such a way that for every section establishing an obligation or a prohibition, the penalties for violating the provision are given in the section itself.
Example 1 (National Consumer Credit Protection Act 2009). Section 29 (Prohibition on engaging in credit activities without a licence) of the act recites:
(1) A person must not engage in a credit activity if the person does not hold a licence authorising the person to engage in the credit activity. Civil penalty: 2,000 penalty units. [. . . ] Criminal penalty: 200 penalty units, or 2 years imprisonment, or both.
This norm can be represented as
r1 :⇒O ¬CreditActivity⊗ 2000CivilPenaltyUnits
r2 : CreditLicence ⇒P CreditActivity
where r2 > r1. The first rules state that in absence of other information a person is forbidden to engage in credit activities (O¬CreditActivity), and then the second rule establish an exception to the prohibition, or in other terms it recites a condition under which such activities are permitted. The section then continues by giving explicit exceptions (permissions) to the prohibition to engage in credit activity, even without a valid licence.
Sequences of permissions are a natural fit for expressions like “the subject is authorised, in order of preference, to do the following: (list)” or “the subject is entitled, in order of preference, to one of the following: (list)”. This is illustrated in the next example.
Example 2 (U.S. Copyright Act). A concrete instance of sequences of permissions is given by Section 504(c)(1) (Remedies for infringement: Damages and profits) of the U.S. Copyright Act (17 USC §504).
Except as provided by clause (2) of this subsection, the copyright owner may elect, at any time before final judgment is rendered, to recover, instead of actual damages and profits, an award of statutory damages for all infringements involved in the action, with respect to any one work, for which any one infringer is liable individually, or for which any two or more infringers are liable jointly and severally, in a sum of not less than $750 or more than $30,000 as the court considers just. [. . . ]
The above provision can be modelled as
infringment,beforeJudgment ⇒P ActualDamages⊙ StatutoryDamages
The above rendering of the textual provision is based on the interpretation of the term ‘instead’, which suggests that the copyright owners are entitled by default the award of the actual damages and profits, but they may elect to recover statutory damages, which is then the second option if exercised by the relevant party.3
As we have just seen, chains of obligations are appropriate to capture the obligations and the penalties related to them. Furthermore, this kind of structure has been successfully used for applications in the area of business process compliance [21]. In a situation governed by the rule ⇒O a⊗ b and where ¬a and b hold, the norm has been complied with (even if to a lower degree than if we had a). On the contrary, if we had two rules ⇒O a and ¬a ⇒O b, then the first norm would have been violated, while the second would have been complied with. But in overall, the whole case would be not compliant [20].
3Here we speak of entitlements or rights. A right is a permission on one party (in this case the copyright owner) generating an obligation on another party (in this case the infringer). For a more detailed discussion on the concept of right see [30].
Consider the following example:
r1 : Invoice ⇒O PayWithin7days r2 : OPayWithin7days,¬PayWithin7days ⇒O Pay5%Interest r3 : OPay5%Interest,¬Pay5%Interest ⇒O Pay10%Interest.
What happens if a customer violates both the obligation to pay within 7 days after the invoice and the obligation to pay the 5% of interest, but she pays the total amount plus the 10% of interest? In the legal perspective the customer should be still compliant, but in this representation contract clauses r1 and r2 have been violated. However, if we represent the whole scenario with the single rule
Invoice ⇒O PayBy7days⊗Pay5%Interest⊗Pay10%Interest,
then the rule is not violated, and the customer is compliant with the contract. Even when the text of legal provisions does not explicitly have this form, there are cases where the joint interpretation of several legal provisions still leads to formulate applicable norms with orders among derogations.
Example 3 (Formal equality and affirmative action). Art. 3, 1st paragraph, of the Italian constitution ensures formal equality of citizens (in fact, all individuals) before the law, namely, an equal legal treatment for everybody:
All citizens have equal social dignity and are equal before the law, without distinction of sex, race, language, religion, political opinion, personal and social conditions. [. . . ]
This general principle can be sometimes derogated, for example, when derogations are meant “to remove those obstacles of an economic or social nature which constrain the freedom and equality of citizens” (art. 3, 2nd par.). In fact, one may argue that permitting (which is different from imposing as mandatory) the adoption of affirmative action policies in favour of women (e.g., introducing quotas for women in politics and the job market) is a flexible legal measure to remove some of those obstacles. Now, suppose a quota for women is guaranteed in public institutions in hiring and promoting employees, but another similar derogation can be applied to disabled people. Imagine that, in a specific case, it is not possible to apply both derogations (for example, this would lead to exceeding the number of jobs available) and so we have to choose to hire a woman or a disabled man. In absence of any further legal provision, one possible solution is to balance both options with respect to the specific facts X of the case, thus ranking, in a rule r, these options in order of preference, given the facts X (on balancing, see [4,31]). For instance, if disabled men should be favoured over non-disabled women (because disability in this case reinforces a more serious discrimination or disadvantage) then r is the following:
r : X ⇒P Hire Disabled Men⊙Hire NonDisabled Women
The reason why we should still keep as a second option Hire NonDisabled Women depends on the fact that we can draw only defeasibly the permission of Hire Disabled Men. Indeed, we have only considered art. 3 of the Italian constitution but other legal provisions or factual reasons could block this conclusion. For example, suppose that the disabled person applying for the job was some years earlier convicted of the crime of belonging to a mafia organisation, while the law prohibits in general and without exceptions for public institutions to hire people who committed that crime. Or imagine that,
in the meantime, the disabled man has withdrawn his request for a job. In both cases, despite X occurs, the first option does not hold and, all things considered, the second one can be applied in order to derogate to art. 3, 1st par., of the Italian constitution.
A model of the undersea environment is provided to evaluate the performance of the proposed framework. To model a realistic marine environment, a three-dimensional terrain in scale of {10×10 km (x-y), 1000 m(z)} is considered based on a realistic example map presented by Figures 5.4 and 5.5, in which the operating field is covered by uncertain static-moving objects, several fixed waypoints and variable ocean current. A k-means clustering method is employed to classify the coast, water and uncertain area of the map. To this purpose, a large map with the size of 1000×1000 pixels is conducted that presents 10×10 km square area for the TAR and a smaller part of the map in size of 350×350 pixels that corresponds to 3.5×3.5 km square is selected to implement and test the ORPP performance. In this map each pixel corresponds to 10×10 m square space.
The clustered map is converted to a matrix format, in which the matrix size is same to map’s pixel density. The corresponding matrix is filled with value of zero for coastal sections (forbidden area in black colour), value of (0,0.35] for uncertain sections (risky area in grey colour), and value of one for water covered area as valid zone for vehicles motion that is presented by white colour on the clustered map. The utilized clustering method is capable of clustering any alternative map very efficiently in a way that the blue sections sensed as water covered zones and other sections depending on their colour range categorized as forbidden and uncertain zones.
CHAPTER 5. ARMSP ARCHITECTURE
136
CHAPTER 5. ARMSP ARCHITECTURE
137
The operation environment is covered by waypoints, so it is modelled as a geometrical network, where nodes corresponds to waypoints. Various tasks are assigned to passible
CHAPTER 5. ARMSP ARCHITECTURE
138
distances between connected nodes in advance. Hence, every edge has weight and cost that is combination of tasks priority, tasks completion time, length of the edges and the time required for traversing the edges. Number of waypoint is set with a random number between 30 to 50 nodes that is calculated with a uniform distribution. The network topology also transforms randomly with a Gaussian distribution on the problem search space. A fixed set of 15 tasks, which are characterized with risk percentage, priority value, and completion time, are specified and are randomly assigned to some of the edges of the graph. The edges that are not assigned with a task are weighted with 1. The location of nodes are randomized according to {Pix,y~U(0,10000) and Piz~U(0,100)} in the water-covered area.
In addition to offline map, different types of static-moving obstacles are considered in this study in order to cover different possibilities of the real world situations (modelled in Chapter 4). Obstacle’s velocity vector and its coordinates can be measured by the sonar sensors with a specific uncertainty that is modelled with a Gaussian distribution; hence, each obstacle is presentable by its position (Θp), dimension (diameter Θr) and uncertainty ratio. Position of the obstacles are initialized using normal distribution of N~(0,σ2) bounded to position of two targeted waypoints by the path planner (e.g. Pax,y,z<Θp<Pbx,y,z), where σ2≈Θr. Further details on modelling of different obstacles is given by Chapter 4.
Beside the uncertainty of operating field, which is taken into account in the modelling of the operation terrain, water current needs to be addressed thoroughly in accordance with the type and the range of the mission. For the purpose of this study, a 3D turbulent time-varying current is considered that is generated using a multiple layered 2D current maps, in which the current circulation patterns gradually change with depth. It is assumed the AUV is traveling with constant velocity |v| between two waypoints. The vehicular constraints and boundary conditions on AUV actuators and state also are taken into account for realistic modelling of the AUV operation. As discussed earlier the violation function for the path planners is defined as a combination of the vehicle’s depth, surge, sway, theta, yaw and collision violations. According to (5-1), the boundaries of the constraints are defined as follows: the zmin=0(m); zmax=100(m);
CHAPTER 5. ARMSP ARCHITECTURE
139
umax=2.7(m/s); vmin=-0.5(m/s); vmax=0.5(m/s); θmax=20 (deg); ψmin=-17 (deg) and ψmax=17 (deg). The proposed autonomous/reactive system is implemented in MATLAB®2016 on a desktop PC with an Intel i7 3.40 GHz quad-core processor and its performance is statistically analysed.
To achieve a contextualized semantic representation of a term, both the term and its local context, i.e., all accompany terms in the document containing it, must be required as described in our problem formulation in Section 1. In real applications, a test document could mismatch training data. For instance, it could be a subset of a training document by using fewer yet more informative terms or an enhanced version of a training document by adding more coherent terms. In this setting, we would design experiments to test mismatched documents. As argued in Section 6.1.1, it does not seem possible to attain all the different concepts and their similarities in terms of all possible contexts. Thus, it is impossible for us to simulate on one mismatch situation that more coherent terms are added to existing documents. Fortunately, we can simulate the other mismatch situation, incomplete local context, by removing a few terms from existing documents.
To simulate the incomplete local context situation, a training example of complete local context, ~ = , | , is altered into a corrupted version, ~Ý = ? , Þ H where ß is a subset of the original document achieved by removing a number of terms randomly from . The incomplete context, Þ , corresponds to the topics distribution obtained from the incomplete document Þ. As a result, the use of fewer accompany terms in Þ results in larger uncertainty in semantic priming and hence causes a bigger difficulty in priming all the accompany terms in the
original document . Here, we emphasize that the ground-truth is the original document but the local context is derived from a subset of this document in semantic priming under this setting. In our incomplete local context experiments, we used the missing rate defined by ?1 − 7ß |7|H ∗ 100% to control the number of terms removed randomly from a complete document. In this paper, we report results based on the missing rate in different ranges: up to 10%, between 10% and 30% and between 30% and 50% due to the variable length of different documents.
Our work shares three overall objectives with prior work: estimating 3D structure from 2D images, determining when there is occlusion, and active vision. However, our work explores each of these issues from a novel perspective.
Prior work on structure estimation (e.g. (Changsoo et al., 2009; Gupta et al., 2010; Saxena et al., 2007)) focuses on surface estimation, recovering a 3D surface from 2D images. In contrast, our work focuses on recovering the constituent structure of an assembly: what parts are used to make the assembly and how such parts are combined. Existing state-of-the-art surface reconstruction methods (e.g. Make3D (Saxena et al., 2008)) are unable to determine surface structure of the kinds of LINCOLN LOG assemblies considered here. Ever if such surface estimates were successful, such estimates alone are insufficient to determine the constituent structure.
Prior work on occlusion determination (e.g. (Gupta et al., 2010; Hoiem et al., 2011)) focuses on finding occlusion boundaries: the 2D image boundaries of occluded regions. In contrast, our work focuses on determining occluded parts in the constituent structure. We see no easy way to determine occluded parts from occlusion boundaries because such boundaries alone are insufficient to determine
even the number of occluded parts, let alone their types and positions in a 3D structure.
Prior work on active vision (e.g. (Maver and Bajcsy, 1993)) focuses on integrating multiple views into surface estimation and selecting new viewpoints to facilitate such in the presence of occlusion. In contrast, our work focuses on determining the confidence of constituent structure estimates and choosing an action with maximal anticipated increase in confidence. We consider not only viewpoint changes but also robotic disassembly to view object interiors. Also note that the confidence estimates used in our approach to active vision are mediated by the visual language model. We might not need to perform active vision to observe all occluded structure as it might be possible to infer part of the occluded structure. Prior work selects a new viewpoint to render occluded structure visible. We instead select an action to maximally increase confidence. Such an action might actually not attempt to view an occluded portion of the structure but rather increase confidence in a visible portion of the structure in a way that when mediated by the language model ultimately yields a maximal increase in the confidence assessment of a portion of the structure that remains occluded even with the action taken.
Most probabilistic programming languages, including PRISM [3], ICL [3], ProbLog [4] and LPAD [11], are
based on Sato’s distribution semantics [3]. In this paper we use ProbLog as it is the simplest of these languages. However, our approach can easily be used for the other languages as well.
Syntax. A ProbLog program consists of a set of probabilistic facts and a logic program, i.e. a set of rules. A probabilistic fact, written p::f, is a fact f annotated with a probability p. An atom that unifies with a probabilistic fact is called a probabilistic atom, while an atom that unifies with the head of some rule is called a derived atom. The set of probabilistic atoms must be disjoint from the set of derived atoms. Below we use as an example a ProbLog program with probabilistic facts 0.3::rain and 0.2::sprinkler and rules wet :- rain and wet :- sprinkler. Intuitively, this program states that it rains with probability 0.3, the sprinkler is on with probability 0.2, and the grass is wet if and only if it rains or the sprinkler is on. Compared to PLP languages like PRISM and ICL, ProbLog is less restricted with respect to the rules that are allowed in a program. PRISM and ICL require the rules to be acyclic (or contingently acyclic) [3]. In addition, PRISM requires rules with unifiable heads to have mutually exclusive bodies (such that at most one of these bodies is true at once; this is the mutual exclusiveness assumption). ProbLog does not have these restrictions, for instance, we can have cyclic programs with rules such as smokes(X) :- friends(X,Y), smokes(Y). This type of cyclic rules are often needed for tasks such as collective classification or social network analysis.
Semantics. A ProbLog program specifies a probability distribution over possible worlds. To define this distribution, it is easiest to consider the grounding of the program with respect to the Herbrand base. Each ground probabilistic fact p::f gives an atomic choice, i.e. we can choose to include f as a fact (with probability p) or discard it (with probability 1 − p). A total choice is obtained by making an atomic choice for each ground probabilistic fact. To be precise, a total choice is any subset of the set of all ground probabilistic atoms. Hence, if there are n ground probabilistic atoms then there are 2n total choices. Moreover, we have a probability distribution over these total choices: the probability of a total choice is defined to be the product of the probabilities of the atomic choices that it is composed of (atomic choices are seen as independent events). In our above example, there are 4 total choices: {}, {rain}, {sprinkler}, and {rain, sprinkler}. The probability of the total choice {rain}, for instance, is 0.3× (1− 0.2).
Given a particular total choice C, we obtain a logic program C ∪ R, where R denotes the rules in the ProbLog program. This logic program has exactly one
well-founded model2 WFM(C ∪ R). We call a given world ω a model of the ProbLog program if there indeed exists a total choice C such that WFM(C∪R) = ω. We use MOD(L) to denote the set of all models of a ProbLog program L. In our example, the total choice {rain} yields the logic program {rain, wet :- rain, wet :- sprinkler}. The WFM of this program is the world {rain,¬sprinkler, wet}. Hence this world is a model. There are three more models corresponding to each of the three other total choices. An example of a world that is not a model of the ProbLog program is {rain,¬sprinkler,¬wet} (it is impossible that wet is false while rain is true).
Everything is now in place to define the distribution over possible worlds: the probability of a world that is a model of the ProbLog program is equal to the probability of its total choice; the probability of a world that is not a model is 0. For example, the probability of the world {rain,¬sprinkler, wet} is 0.3× (1− 0.2), while the probability of {rain,¬sprinkler,¬wet} is 0.
Most research in the area of machine translation evaluation focuses on the translation quality of the observed translation systems. The research presented in this manuscript focuses entirely on the throughput of translation system and proposes a method to increase the throughput with no effect on the quality of the translation.
There are quite a few cases where the machine translation throughput is a crucial aspect such as translation of large quantities of text, e. g. translating all the texts in the Project
2 Gutenberg1 or translating huge amounts of manuals in order to enter a new market, etc. Some of these cases can be solved using publicly available services such as Google translate2 or Microsoft Bing Translator3 although the speed of translation (actually the amount of text that can be translated) is limited. However, there are cases where such approach is not viable, such as translating sensitive information ranging from local correspondence to proprietary literature or translating domain-specific texts where a proprietary translation system must be used. The obvious solution is using faster machines, but this solution requires new investment. Using public clouds like Amazon EC3 would reduce the investment costs, but for many applications the cost would still be too high. This approach would also involve an architecture change [1]. Autodesk Brasil ventured in a one-time job of translating most of their manuals into Brazilian Portuguese, the job was done using the Apertium translation system as described in [2].
As said, the presented research focuses mainly on machine translation of large amounts of text on commodity machines (cost-effective). MapReduce is a programming model for processing big data sets in a distributed fashion. The basic question this research focuses on is how efficiently can a Machine Translation (MT) system be implemented in a MapReduce model? The translation task of large amounts of text can be divided into smaller units with no effect on the translation quality as all the machine translation systems base translations on independent translation units. Usually the translation of sentences is done independently although some research has been done on extending the boundaries for translation units over the sentence boundaries [3]. The natural way of increasing the translation throughput (the number of translated words in a defined amount of time) is to translate parts of the text on separate translation systems as every sentence is translated independently.
The rest of the manuscript is organized as follows: The domain description is presented in sections 2 through 6. The methodology is presented in section 7. The evaluation methodology with results is presented in section 8. The manuscript concludes with the discussion and description of further work in section 9.
1 Project Gutenberg Literary Archive Foundation: http://www.gutenberg.org/ 2 Google translate: http://translate.google.com/ 3 Microsoft Bing Translator: http://www.bing.com/translator
3
We have calculated the Infobox size as the base 10 log of the bytes of data contained within the mediawiki tags that wrap an infobox, and we have normalized it with respect to the ArticleLength, introduced in Section 3.
The main parameters required for the shapelet discovery process are minLen and maxLen, which define the range of possible shapelet candidate lengths. The shapelet discovery process searches for candidates in all possible window sizes between the provided minimum and maximum length sizes. For example, if minLen = 10 and maxLen = 20, then the shapelet discovery process will search for shapelets in all window sizes starting from 10 and ending at 20. Therefore, setting these values to the extreme cases, where minLen = 1 and maxLen = m, where m is the time series instance length, makes the algorithm search over the entire candidate set. Another approach is to set these parameters based on some assumptions about the possible shapelet lengths. However, setting these parameters incorrectly can be detrimental to the shapelet discovery process. Setting the parameters to a very small window can cause the shapelet discovery process to miss important features because they are not covered in the search window while setting the window to a very large size can cause suboptimal feature selection. A third approach is to use a parameter optimization phase before creating the complete classification model.
The experiments were executed using two main approaches. For the first approach, instead of setting the parameter values to the extreme cases, or making any assumptions about the possible shapelet lengths, we take a cautious approach and set the parameters to a constant fraction of the time series length for all datasets. For all experiments, we used minLen = d0.25×me and maxLen = b0.67×mc, where m is the length of time series. This allows to cover more than 66% of the time series length at the start of the discovery
5 http://www.cs.ucr.edu/˜eamonn/time series data/
process and narrows the search up to just a quarter of the time series length. For our second approach, we used a parameter optimization phase to search for the best shapelet candidate lengths for each dataset and then performed the experiments using these learned parameters. The parameter optimization was performed with only the training split of the datasets.
The Random-Shapelets algorithm evaluates a small fraction of all the possible candidates in the specified minLen and maxLen range. This fraction of candidates is determined by the sampling ratio. All the experiments involving the Random-Shapelets algorithm have been performed with a 1% sampling ratio. This includes the experiments for evaluating the Random-Shapelets algorithm itself and the variants of ensembles of Random-Shapelets.
Intrinsic evaluation in nlg is dominated by two methodologies, one relying on human judgements (and hence subjective), the other on corpora.
The convolutional arithmetic circuit (ConvACs) model employs the standard outer (tensor) products, which for two tensors, A P RI1ˆ¨¨¨ˆIN and B P RJ1ˆ¨¨¨ˆJM , are defined as
(A ˝ B)i1,...,iN ,j1,...,jM = ai1,...,iN ¨ bj1,...,jM .
In order to convert ConvAC tensor models to widely used convolutional rectifier networks, we need to employ the generalized (nonlinear) outer products, defined as [Cohen and Shashua, 2016]
(A ˝ρ B)i1,...,iN ,j1,...,jM = ρ(ai1,...,iN , bj1,...,jM), (4.27)
191
where the ρ operator can take various forms, e.g.,
ρ(a, b) = ρσ,P(a, b) = P[σ(a), σ(b)], (4.28)
and is referred to as the activation-pooling function11, which meets the associativity and the commutativity requirements: ρ(ρ(a, b), c) = ρ(a, ρ(b, c)) and ρ(a, b) = ρ(b, a), @a, b, c P R). For two vectors, a P RJ and b P RJ , it is defined as a matrix C = a ˝ρ b P RIˆJ , with entries cij = ρ(ai, bj). Note that the nonlinear function ρ can also take special form: cij = maxtaibj, 0u or cij = maxtai, bj, 0u.
For a particular case of the convolutional rectifier network with max pooling, we can use the following activation-pooling operator
ρσ,P(a, b) = maxt[a]+, [b]+u = maxta, b, 0u. (4.29)
In an analogous way, we can define the generalized Kronecker and the Khatri-Rao products.
Example 11 Consider a generalized CP decomposition, which corresponds to a shallow rectifier network in the form [Cohen and Shashua, 2016]
W c = R ÿ
r=1
λ (c) r ¨ (w (1) r ˝ρ w (2) r ˝ρ ¨ ¨ ¨ ˝ρ w (N) r ), (4.30)
where the coefficients λ(c)r represent weights of the output layer, vectors w(n)r P RIn are weights in the hidden layer and the operator ˝ρ denotes the nonlinear outer products defined above.
The generalized CP decomposition can be expressed in an equivalent vector form, as follows
wc = vec(Wc) = [W (N) dρ W(N´1) dρ ¨ ¨ ¨ dρ W(1)]λ(c), (4.31)
where dρ is the generalized Khatri-Rao product of two matrices. It should be noted that if we employ weight sharing, then all vectors w(n)r = wr, @n, and consequently the coefficient tensor, W c, must be a symmetric tensor which further limits the ability of this model to
11The symbols σ(¨) and P(¨) are respectively the activation and pooling functions of the network.
192
approximate a desired function.
Example 12 Consider the simplified HT model as shown Figure 4.7, but with the generalized outer product defined above. Such nonlinear tensor networks can be rigorously described for I1 = I2 = ¨ ¨ ¨ = IN = I, as follows
w(ď0,j) r(0) = w(0,j) r(0)
W(ďl,j) r(l)
= R(l´1) ÿ
r(l´1)=1
w(l,j) r(l´1),r(l)
¨ (
W(ďl´1,2j´1) r(l´1) ˝ρ W (ďl´1,2j) r(l´1)
) P RIˆ¨¨¨ˆI
Wc = W (ďL,1),
for l = 1, . . . , L and j = 1, . . . , 2L´l , or in an equivalent matrix form as
W(ď0,j) = W(0,j) P RIˆR(0) W(ďl,j) = ( W(ďl´1,2j´1) dρ W(ďl´1,2j) ) W(l,j) P RI2 lˆR (l)
vec(Wc) = ( W(ďL´1,1) dρ W(ďL´1,2) ) λ(c) P RIN ,
(4.32)
for l = 1, . . . , L´ 1, where the core tensors are reduced to matrices W(l,j) P RR
(l´1)ˆR(l) with entries w(l,j) r(l´1),r(l´1),r(l)
. We should emphasize that the HT/TTNS architectures are not the only suitable TN architectures which can be used to model DCNNs, and the whole family of powerful tensor networks can be employed for this purpose. Particularly attractive and simple are the TT/MPS, TT/MPO and TC models for DNNs, for which efficient decomposition and tensor completion algorithms already exist. The TT/MPS, TT/MPO and TC networks provide not only simplicity in comparison to HT, but also very deep TN structures, that is, with N hidden layers. Note that the standard HT model generates architectures of DCNNs with L = log2(N) hidden layers, while TT/TC networks may employ N hidden layers. Taking into account the current trend in deep leaning to use a large number of hidden layers, it would be quite attractive to employ QTT tensor networks with a relatively large number of hidden layers, L = N ¨ log2(I).
To summarize, deep convolutional neural networks may be considered as a special case of hierarchical architectures, which can be indirectly simulated and optimized via relatively simple and well understood tensor networks, especially HT, TTNS, TT/MPS, TT/MPO, and TC (i.e., using
193
unbalanced or balanced binary trees and graphical models). However, more sophisticated tensor network diagrams with loops, discussed in the next section may provide potentially better performance and the ability to generate novel architectures of DCNNs.
The use of directed acyclic graphs (DAGs) to represent conditional independence relations among random variables has proved fruitful in a variety of ways. Recursive structural equation models are one kind of DAG model. However, non-recursive structural equation models of the kinds used to model economic processes are naturally represented by directeed cyclic graphs (DCG). For linear systems associated with DCGs with independent errors, a characterisation of conditional independence constraints is obtained, and it is shown that the result generalizes in a natural way to systems in which the error variables or noises are statistically dependent. For non-linear systems with independent errors a sufficient condition for conditional independence of variables in associated distributions is obtained.
1. INTRODUCTION
The introduction of statistical models represented by directed acyclic graphs (DAGs) has proved fruitful in the construction of expert systems, in allowing efficient updating algorithms that take advantage of conditional independence relations (Pearl, 1988, Lauritzen et. al., 1993 ), and in inferring causal structure from conditional independence relations (Spirtes and Glymour, 1991, Spirtes, Glymour and Scheines, 1993, Pearl and Verma, 1991, Cooper, 1992). As a framework for representing the combination of causal and statistical hypotheses, DAG models have shed light on a number of issues in statistics ranging from Simpson's Paradox to experimental design (Spirtes, Glymour and Scheines, 1993). The relations of DAGs with statistical constraints, and the equivalence and distinguishability properties of DAG models, are now well understood, and their characterization and computation involves three properties connecting graphical structure and probability distributions: (i) a local directed Markov property, (ii) a global directed Markov property, (iii) and factorizations of joint densities according to the structure of a graph (Lauritizen et al., 1990).
Recursive structural equation models are one kind of DAG model. However, non-recursive structural equation
models are not DAG models, and are instead naturally represented by directed cyclic graphs in which a finite series of edges representing influence leads from a vertex representing a variable back to that same vertex. Such graphs have been used to model feedback systems in electrical engineering (Mason, 1953, 1956), and to represent economic processes (Haavelmo, 1943, Goldberger, 1973). In contrast to the acyclic case, almost nothing general is known about how directed cyclic graphs (DCGs) represent conditional independence constraints, or about their equivalence or identifiability properties, or about characterizing classes of DCGs from conditional independence relations or other statistical constraints. This paper addresses the first of these problems, which is a prerequisite for the others. The issues turn on how the relations among properties (i), (ii) and (iii) essential to the acyclic case generali�r more typically fail to generalize--to directed cyclic graphs and associated families of distributions. It will be shown that when DCGs are interpreted by analogy with DAGs as representing functional dependencies with independently distributed noises or 11 error terms, 11 the equivalence of the fundamental global and local Markov conditions characteristc of DAGs no longer holds, even in linear systems, and in non-linear systems both Markov properties may fail. For linear systems associated with DCGs with independent errors or noises, a characterisation of conditional independence constraints is obtained, and it is shown that the result generalizes in a natural way to systems in which the error variables or noises are statistically dependent. For non-linear systems with independent errors a sufficient condition for conditional independence of variables in associated distributions is obtained.
The remainder of this paper is organized as follows: Section 2 defmes relevant mathematical ideas and gives some necessary technical results on DAGs and DCGs. Section 3 obtains results for non-recursive linear structural equations models. Section 4 treats non-linear models of the same kind.
2. D IRECTED GRAPHS
I place sets of variables and defined terms in boldface, and individual variables in italics. A directed graph is an ordered pair of a fmite set of vertices V, and a set of
492 Spirtes
directed edges E. A directed edge from A to B is an ordered pair of distinct vertices <A,B> in V in which A is the tail of the edge and B is the head; the edge is out of A and into B, and A is parent of B and B is a child of A. A sequence of distinct edges <E1, ... ,E n > in G is an undirected path if and only if there exists a sequence of vertices <V1. ... , Vn+l> such that for 1 � i �n either <V;, Vi+l> = Ei or <Vi+!. Vi>= Ei. A path U is acyclic if no vertex occurring on an edge in the path occurs more than once. A sequence of distinct edges <Et. .... En> in G is a directed path if and only if there exists a sequence of vertices <Vt. .... Vn+l> such that for 1 � i� n, < V;,V;+l> = E;. If there is an acyclic directed path from A to B or B = A then A is an ancestor ofB, and B is a descendant of A. A directed graph is acyclic if and only if it contains no directed cyclic paths. 1
A directed acyclic graph (DAG) G with a set of vertices V can be given two distinct interpretations. On the one hand, such graphs can be used to represent causal relations between variables, where an edge from A to B in G means that A is a direct cause of B relative to V. A causal graph is a DAG given such an interpretation. On the other hand, a DAG with a set of vertices V can also represent a set of probability measures over V. Following the terminology of Lauritzen et. al. (1990) say that a probability measure over a set of variables V satisfies the local directed Markov property for a DAG G with vertices V if and only if for every W in V , W is independent ofV\(Descendants(W,G) u Parents(W,G)) given Parents(W,G), where Parents(W,G) is the set of parents of W in G, and Descendants(W,G) is the set of descendants of Win G. A DAG G represents the set of probability measures which satisfy the local directed Markov property for G . The use of DAGs to simultaneously represent a set of causal hypotheses and a family of probability distributions extends back to the path diagrams introduced by Sewell Wright (1934). Variants ofDAG models were introduced in the 1980's in Wermuth (1980), Wermuth and Lauritzen (1983), Kiiveri, Speed, and Carlin (1984), Kiiveri and Speed (1982), and Pearl (1988). 2
Lauritzen et. al. also defme a global directed Markov property that is equivalent to the local directed Markov property for DAGs. Several preliminary notions are required. Let An(X, G) be the set of ancestors of members of X in G. Let G(X) be the subgraph ofG that contains
1 An undirected path is often defmed as a sequence of vertices rather than a sequence of edges. The two definitions are essentially equivalent for acyclic directed graphs, because a pair of vertices can be identified with a unique edge in the graph. However, a cyclic graph may contain more than one edge between a pair of vertices. In that case it is no longer possible to identifY a pair of vertices with a unique edge. 2It is often the case that some further restrictions are placed on the set of distributions represented by a DAG. For example, one could also require the Minimality Condition, i.e. that for any distribution P represented by G, P does not satisfY the local directed Markov Condition for any proper subgraph of G. This condition, and others are discussed in Pearl(l988) and Spirtes, Glymour, and Scheines(1993). We will not consider such further restrictions here.
only vertices in X, with an edge from A to B in X if and only if there is an edge from A to B in G. (JM moralizes a directed graph G if and only if (JM is an undirected graph with the same vertices as G, and a pair of vertices X and Y are adjacent in aM if and only if either X and Yare adjacent in G, or they have a common child in G. In an undirected graph G, if X, Y, and Z are disjoint then X is separated from Y given Z if and only if every undirected path between a member of X and a member ofY contains a member of Z. If X , Y and Z are disjoint sets of variables, X andY are d-separated given Z in a directed gr�ph G just when X and Y are separated given Z in GM(An(X u Y u Z,G)). The relation defmed here was defined in Lauritzen et al. (1990). "d-separation" is a graphical relation introduced by Pearl (1986). Since Lauritzen et. al. (1990) proved that their graphical relation is equivalent to Pearl's for acyclic graphs, and the proof is readily extended to the cyclic case, I will also use "d separation" to refer to the graphical relation just described. Now the defmition: A probability measure over V satisfies the global directed Markov property for DAG G if and only if for any three disjoint sets of variables X, Y, and Z included in V, if X is d-separated from Y given Z, then X is independent of Y given Z. Lauritzen et. al. (1990) shows that the global and local directed Markov properties are equivalent in DAGs, even when the probability distributions represented have no density function. In section 2, I show that the local and global directed Markov properties are not equivalent for cyclic directed graphs.
The following lemmas relate the global directed Markov property to factorizations of a density function. Denote a density function over V by j(V), where for any subset X of V , .f{X) denotes the marginal of j(V). If.f{V) is the density function for a probability measure P over a set of variables V, say that P factors according to directed graph G with vertices V if and only if for every subset X ofV,
P=f•J.L
/(An(X,G)) = IlKv(V,Parents(V,G)) VeAn(X,G)
where gv is a non-negative function, and J.L is a product measure. The following result was proved in Lauritzen et. al. (1990).
Lemma 1: If V is a set of random variables with a probability measure P that is absolutely continuous with respect to a product measure JJ, then P factors according to DAG G if and only if P satisfies the global directed Markov property for G.
As in the case of acyclic graphs, the existence of a factorization according to a cyclic directed graph G does entail that a measure satisfies the global directed Markov property for G. The proof given in Lauritzen et. al. (1990) for the acyclic case carries over essentially unchanged for the cyclic case.
Directed Cyclic Graphical Representations of Feedback Models 493
Lemma 2: If V is a set of random variables with a probability measure P that factors according to directed (cyclic or acyclic) graph G, then P satisfies the global directed Markov property for G.
However, unlike the case of acyclic graphs, if a probability measure over a set of variable V satisfies the global directed Markov property for cyclic graph G and has a density function f(V), it does not follow that f(V) factors according to G.
The following weaker result relating factorization of densities and the global directed Markov property does hold for both cyclic and acyclic directed graphs.
Lemma 3: If V is a set of random variables with a probability measure P that that is absolutely continuous with respect to a product measure J.l, has a positive density function f(V), and satisfies the global directed Markov property for directed (cyclic or acyclic) graph G, thenf(V) factors according to G.
3. NO N-RECU R S IVE LINEAR S TRUCTURAL EQUATION MODELS
The problem considered in this section is to investigate the generalization of the Markov properties to linear, non recursive structural equation models. First we must relate the social scientific terminology to graphical representations, and clarify the questions.
Linear structural equation models (which, following the terminology of Bollen (1989), will be referred to as linear SEMs) can also be represented as directed graph models. In a linear SEM the random variables are divided into two disjoint sets, the error terms and the non-error terms. Corresponding to each non-error random variable V is a unique error term ey. A linear SEM contains a set of linear equations in which each non-error random variable Vis written as a linear function of other non-error random variables and ey. A linear SEM also specifies a joint distribution over the error terms. So, for example, the following is a linear SEM, where a and b are real constant linear coefficients, ex, ey, and ez are jointly independent "error terms", and X, Y, Z, are random variables:
X= a Y+ex Y= b Z+ ey
Z=ez ex, ey , ez are jointly independent and normally
distributed
The directed graph of a linear SEM with uncorrelated errors is written with the convention that an edge does not appear if and only if the corresponding entry in the coefficient matrix is zero; the graph does not contain the error terms. Figure 1 is the DAG that represents the SEM shown above. A linear SEM is recursive if and only if its directed graph is acyclic.
z
Figure 1: Example ofRecursive SEM
Initially I will consider only linear SEMs in which the error terms are jointly independent, but we will see that in the linear case in an important sense nothing is lost by this restriction: a linear SEM with dependent errors generates the same restrictions on the covariance matrix as does some linear SEM with extra variables and independent errors. Further, such an SEM with extra variables can always be found such that the sub graph of that SEM on the original variables is the same as the original graph.
A linear SEM containing disjoint sets of variables X, Y, and Z linearly entails that X is independent ofY given Z if and only if X is independent ofY given Z for all values of the non-zero linear coefficients and all distributions of the exogenous variables in which they are jointly independent and have positive variances. Let PXY.Z be the partial correlation of X and Y given Z. A linear SEM containing X, Y, and Z, where X :1:- Y and X and Yare not in Z, linearly entails that PXY.Z = 0 if and only PXY.Z = 0 for all values of the non-zero linear coefficients and all distributions of the exogenous variables in which each pair of exogenous variables has zero correlation, each exogenous variable has positive variance, and in which PXY.Z is defmed. It follows from Kiiveri and Speed (1982) that if the error terms are jointly independent, then any distribution that forms a linear, recursive SEM with a directed graph G satisfies the local directed Markov property for G. One can therefore apply d-separation to the DAG in a linear, recursive SEM to compute the conditional independencies and zero partial correlations it linearly entails. The d-separation relation provides a polynomial (in the number of vertices) time algorithm for deciding whether a given vanishing partial correlation is linearly entailed by a DAG.
Linear non-recursive structural equation models (linear SEMs) are commonly used in the econometrics literature to represent feedback processes that have reached equilibrium.3 Corresponding to a set of non-recursive linear equations is a cyclic graph, as the following example from Whittaker (1990) illustrates.
X1 = Exl X2 = e:x2
X3 = f331x1 + /3:3¥4 + eXJ X4 = f34zXz + /34�3 + ex4
Ex:J, exz, eXJ, Ex4 are jointly independent and normally distributed
3cox and Wermuth (1993), Wermuth and Lauritzen(1990) and (indirectly) Frydenberg(1990) consider a class of non-recursive linear models they call block recursive. The block recursive models overlap the class of SEMs, but they are neither properly included in that class, nor properly include it. Frydenberg (1990) presents necessary and sufficient conditions for the equivalence of two block recursive models.
494 Spirtes
X �-��rr
X
2---.... •�x4
Figure 2: Example ofNon-recursive SEM
In DAGs the global directed Markov property entails the local directed Markov property, because a variable V is d separated from its non-parental non-descendants given its parents. This is not always the case in cyclic graphs. For example, in figure 4, X4 is not d-separated from its non parental non-descendant XI given its parents X2 andX3, so the local directed Markov property does not hold.
(Note that this use of cyclic directed graphs to represent feedback processes represents an extension of the causal interpretation of directed graphs. The causal structure corresponding to Figure 2 is described by an infmite acyclic directed graph containing each variable indexed by time. The cyclic graph can be viewed as a compact representation of such a causal graph. I am indebted to C. Glymour for pointing out that the local Markov condition fails in Whittaker's model. Indeed, there is no acyclic graph (even with additional variables) that linearly entails all and conditional independence relations linearly entailed by Figure 2, although Thomas Richardson has pointed out that the directed cyclic graph of Figure 2 is equivalent to one in which the edges from Xt to X3 and Xz to X4 are replaced, respectively, by edges from Xt to X4 and from Xz to X3.)
Theorem 1: The probability measure P of a linear SEM L (recursive or non-recursive) with jointly independent error terms satisfies the global directed Markov property for the directed (cyclic or acyclic) graph G of L, i.e. if X, Y, and Z are disjoint sets of variables in G and X is d separated from Y given Z in G , then X and Y are independent given Z in P. 4
Theorem 2: In a linear SEM L with jointly independent error terms and directed (cyclic or acyclic) graph G containing disjoint sets of variables X, Y and Z, if X is not d-separated from Y given Z then L does not linearly entail that X is independent of Y given Z.
Applying Theorems 1 and 2 to the directed graph in Figure 2, only two conditional independence relations (and their consequences) are entailed: Xt is independent of Xz, and Xt is independent ofXz givenX3 andX4.
Theorem 3: In a linear SEM L with jointly independent error terms and (cyclic or acyclic) directed graph G containing X , Y and Z, where X"# Y and Z does not
4 This theorem has been independently proved by Jan Koster of the Erasmus University Rotterdam, in a paper which has not yet been published but has been submitted to Statistical Science.
contain X or Y, X is d-separated from Y given Z if and only if L linearly entails that Pxr.z = 0. As in the acyclic case, d-separation provides a polynomial time procedure for deciding whether cyclic graphs entail a conditonal independence or vanishing partial correlation
Theorem 3 can be used to relax the restriction that the error terms in an a linear SEM L be jointly independent. We will slightly extend the defmition of linear entailment. Assume that some pairs of exogenous variables may have correlations fixed at zero. A linear SEM containing X, Y, and Z, where X "# Y and X and Yare not in Z, linearly entails that PXY.Z = 0 if and only PXY.Z = 0 for all values of the non-zero linear coefficients and for each distribution over the exogenous variables in which pairs of exogenous variables fixed at zero correlation keep a zero correlation, each exogenous variable has positive variance, and in which PXY.Z is defmed. If exand e y are not independent in linear SEM L with a non-singular covariance matrix, there is a linear SEM L' with independent error terms such that the marginal distribution of L' over the variables in L has the same covariance matrix as L. Form the graph G ' of L' from the graph G of L in the following way. Add a latent variable T to G , and add edges from T to X and Y. In L ', modify the equation for X by making it a linear functions of the parents of X (including T) in G ', and replace ex bY ex, modify the equation for Y in an analogous way. There always exist linear coefficients and distributions over T and the new error terms such that the marginal covariance matrix for L' is equal to the covariance matrix of L, and ex and e'y are independent. The process can be repeated for each pair of variables with correlated errors in L. Hence the zero partial correlations entailed by L can be derived by applying Theorem 3 to the graph of L'. Figure 3 illustrates this process. The set of variables V in the graph on the left is {Xt.X2,X3,X4}. The graph on the left correlates the errors between Xt and Xz (indicated by the undirected edges between them.) The graph on the right has no correlated errors, but does have a latent variable T that is a parent of Xt and Xz. The two graphs linearly entail the same zero partial correlations involving only variables in V (in this case they both entail no non-trivial zero partial correlations).
x3 = a X Xz + b X x4 + EJ x3 =a X Xz + b X x4 + E3 X4=c XXt + d xX3 + e4 X4=c XXt + d XX3+E4 Xt = Et X t = e x T + e'1 Xz = ez Xz = f x T + e'z Et and ez correlated e't and ez uncorrelated
4. NO N-LINEAR EQUATION MODELS
S TRUC TUR A L
A linear SEM is a special case of a more general kind of SEM in which the equations relating a given variable to other variables and a unique error term need not be linear. In a SEM the random variables are divided into two
disjoint sets, the error terms and the non-error terms. Corresponding to each non-error random variable V is a unique error term ey. A SEM contains a set of equations in which each non-error random variable Vis written as a measureable function of other non-error random variables and ev. The convention is that in the directed graph of a SEM there is an edge from A to B if and only if B is an argument in the function for A. As in the linear case, I will still assume that density functions exist for both the probabilty measure over the error terms and the non-error terms, each non-error term V is a function of the error terms of its ancestors in G, each ev is a function of V and its parents in G (which will be the case if the errors are additive or multiplicative), the Jacobian of the transformation between the error terms and the non-error terms is well-defmed, and the error terms are jointly independent. Call such a set of equations and its associated graph a pseudo-indeterministic SEM. A directed graph G pseudo-indeterministically entails that X is independent of Y given Z if and only if in every pseudo-indeterministic SEM with graph G , X is independent ofY given Z.
This section establishes that d-separation is a necessary condition for a DAG to pseudo-indeterministically entail a conditional independence relation, but in a cyclic directed graph d-separation may not be a sufficient condition for a DAG to pseudo-indeterministically entail a conditional independence relation. Instead, a different condition, yielding a polynomial time algorithm, is found to suffice for a cyclic direected graph to pseudo indeterministically entail a conditional independence relation.
By Theorem 2, d-separation is a necessary condition for a conditional independence claim to be entailed by an SEM. The following remarks show d-separation is also sufficient for acyclic SEMs, but not for cyclic SEMS.
Theorem 4: If G is a DAG containing disjoint sets of variables X, Y and Z, X is d-separated from Y given Z if and only L pseudo-indeterministically entails that X is independent ofY given Z.
The following example gives a concrete illustration that there is a cyclic graph G in which X is d-separated from Y given { Z, W }, but G does not pseudo-indeterminstically entail that X is independent of Y given { Z, W}.
X
y
•w
!i • z
The Jacobean of the transformation from the es is 11(1 -X x f). Hence, transforming the joint normal density of the es yields
f(X,Y,Z, W) = 1 -x2 -y2 -2- x exp(--) x exp(--) x 4n 2 2 -(z- w x y)2 -(w- z x x)2
exp( ) x exp( ) x 2 2
11- (;X Y)l
496 Spirtes
(a) (b)
Figure 6: Collapsed Graph
Two of the most primary features of speech prosody have to do with chunking speech into linguistically relevant units above the segment and the relative salience of the given units; that is, boundaries and prominences, respectively. These two aspects are present in every utterance and are central to any representation of speech prosody; moreover, they give rise to a hierarchy. Ideally they would be represented with a uniform methodology that would take into account both the production and the perceptual aspects of the speech signals. Such a system would be beneficial to both basic speech research and speech technology, especially speech synthesis. On the other hand, to be useful for data oriented
∗Corresponding author
ar X
iv :1
51 0.
01 94
9v 1
[ cs
.C L
] 7
research and technology, the system should strive towards being unsupervised as opposed to annotation systems that rely on humans. Ideally the system would still behave in a human-like fashion, while avoiding the subjectiveness and variability caused by the blend of top-down and bottom-up influences involved in the interpretation of linguistic speech signals.
In this paper we present a hierarchical, time-frequency scale-space analysis of prosodic signals (e.g., fundamental frequency, energy, duration) based on the continuous wavelet transform (CWT). The presented algorithms can be used to analyse and annotate speech signals in an entirely unsupervised fashion. The work stems from a need to annotate speech corpora automatically for text-tospeech synthesis (TTS) [1] and the subject matter is mainly examined from that point of view. However, the presented representations should be of interest to anyone working on speech prosody.
Wavelets extend the classical Fourier theory by replacing a fixed window with a family of scaled windows resulting in scalograms, resembling the spectrogram commonly used for analysing speech signals. The most interesting aspect of wavelet analysis with respect to speech is that it resembles the perceptual hierarchical structures related to prosody. In scalograms speech sounds, syllables, (phonological) words, and phrases can be localised precisely in both time and frequency (scale). This would be considerably more difficult to achieve with traditional spectrograms. Furthermore, the wavelets give natural means to discretise and operationalise the continuous prosodic signals.
Figure 1 depicts the hierarchical nature of speech as captured in a timefrequency scale-space by CWT of the signal envelope of a typical English utterance. The upper part contains the formant structure (which is not visible due to the rectification of the signal) as well as the fundamental frequency in terms of separate glottal pulses. Underneath the f0 scale are the separate speech segments followed by (prominent) syllables, as well as prosodic words. The lower part including the syllables and prosodic words depicts the suprasegmental and prosodic structure which has typically not been represented in e.g., the melfrequency cepstral coefficient (MFCC) based features in both ASR and TTS.
Spoken language is organised hierarchically both structurally and phonetically: words belong to phrases and are built up from syllables which are further divisible into phonemes which stand for the actual speech sounds when the structures are realised as speech. This has many non-obvious effects on the speech signal that need to be modelled. The assumption of hierarchical structure combined with new deep learning algorithms has lead to recent breakthroughs in automatic speech recognition [2]. In synthesis the assumption has played a key role for considerably longer. The prosodic hierarchy has been central in TTS since 1970’s [3, 4] and most current systems are based on some kind of a hierarchical utterance structure. Few systems go above single utterances (which typically represent sentence in written form), but some take the paragraph sized units as a basis of production [5].
The hierarchical utterance structure serves as a basis for modelling the prosody, e.g., speech melody, timing, and stress structure of the synthetic speech. Controlling prosody in synthesis has been based on a number of different
theoretical approaches stemming from both phonological considerations as well as phonetic ones. The phonologically based ones stem from the so called Autosegmental Metrical theory [6] which is based on the three-dimensional phonology developed in [7, 8] as noted in [9]. These models are sequential in nature and the hierarchical structure is only implicated in certain features of the models. The more phonetically oriented hierarchical models are based on the assumption that prosody – especially intonation – is truly hierarchical in a super-positional and parallel fashion.
Actual models capturing the superpositional nature of intonation were first proposed in [10] by Öhman, whose model was further developed by Fujisaki [11, 12] as a so called command-response model which assumes two separate types of articulatory commands; accents associated with stressed syllables superposed on phrases with their own commands. The accent commands produce faster changes which are superposed on a slowly varying phrase contours. Several superpositional models with a varying degree of levels have been proposed since Fujisaki [13, 14, 15, 16]. Superpositional models attempt to capture both the chunking of speech into phrases as well the highlighting of words within an utterance. Typically smaller scale changes, caused by e.g., the modulation of the airflow (and consequently the f0) by the closing of the vocal tract during certain consonants, are not modelled.
Prominence is a functional phonological phenomenon that signals syntagmatic relations of units within an utterance by highlighting some parts of the speech signal while attenuating others. Thus, for instance, some of syllables
within a word stand out as stressed [17]. At the level of words prominence relations can signal how important the speaker considers each word in relation to others in the same utterance. These often information based relations range from simple phrasal structures (e.g., prime minister, yellow car) to relating utterances to each other in discourse as in the case of contrastive focus (e.g., ”Where did you leave your car? No, we WALKED here.”). Although prominence probably functions in a continuous fashion, it is relatively easily categorised in e.g, four levels where the first level stands for words that are not stressed in any fashion prosodically to moderately stressed and stressed and finally words that are emphasised (as the word WALKED in the example above). These four categories are fairly easily and consistently labeled even by non-expert listeners [18, 19, 20]. In sum, prominence functions to structure utterances in a hierarchical fashion that directs the listener’s attention in a way which enables the understanding of the message in an optimal manner. However, prominent units – be they words or syllables – do not by themselves demarcate the speech signal but are accompanied by boundaries that chunk the prominent and non-prominent units into larger ones: syllables to (phonological) words, words to phrases, and so forth. Prominence and boundary estimation have been treated as separate problems stemming from different sources in the speech signals.
As functional – rather than a formal – prosodic phenomena prominences and boundaries lend themselves optimally to statistical modelling. The actual signalling of prosody in terms of speech parameters is extremely complex and context sensitive – the form follows function in a complex fashion. As onedimensional features, prominence and boundary values provide for a means to reduce the representational complexity of speech annotations in an advantageous way. In a synthesis system it occurs at a juncture that is relevant in terms of both representations and data scarcity. The complex feature set that is known to effect the prosody of speech can be reduced to a few categories or a single continuum from dozens of context sensitive features, such as e.g, part-of-speech and whatever can be computed from the input text. Taken this way, both word prominence and boundaries can be viewed as abstract phonological functions that impact the phonetic realisation of the speech signal predictably and that can show considerable phonetic variation in its manifestation. They are essential constituents of the utterance structure, whereas features like part-of-speech or information content (which are typically used for predicting prosody) are not.
Word prominence has been shown to work well in TTS for a number of languages, even for English which has been characterised as a so called intonation language [21, 22, 23, 24, 25]. In principle English should require a more detailed modelling scheme with explicit knowledge about the intonational forms. The perceived prominence of a given word in an utterance is a product of many separate sources of information; mostly signal based although other linguistic factors can modulate the perception [26, 19]. Typically a prominent word is accompanied with a f0 movement, the stressed syllable is longer in duration, and its intensity is higher. However, estimating prominences automatically is not straight-forward and a multitude of differenct estimation algorithms have
been suggested (see Section 3 for more detail). Statistical speech synthesis requires relatively little data as opposed to unitselection based synthesis. However, labelling even small amounts of speech – especially by experts – is prohibitively time consuming. In order to be practicable the labelling of any feature in the synthesis training data should be preferably attainable with automatic and unsupervised means.
In what follows we present recently developed methods for automatic prominence estimation based on CWT (Section 2) which allow for fully automatic and unsupervised means to estimate both (word) prominences and boundary values from a hierarchical representation of speech (see [27, 28, 29] for earlier work). The main insight in this methodology is that both prominences and boundaries can be treated as arising from the same sources in the (prosodic) speech signals and estimated with exactly the same methods. These methods, then, provide for a uniform representation for prosody that is useful in both speech synthesis and basic phonetic research. These representations are purely computational and thus objective. It is – however – interesting to see how the proposed methods relate to annotations provided by humans as well as earlier attempts at the problem (Section 3).
In this paper, we have proposed a new probabilistic approach for the multi-label classification problem. Our approach models different input-output relations using
conditional tree-structured Bayesian networks, while the mixtures-of-experts architecture aims to compensate for the tree-structured restrictions and as a result achieves a more accurate model. We have formulated and developed the algorithms for learning the model from data and for performing multi-label predictions on future data instances. Our experiments on a broad range of datasets showed that our approach outperforms several state-of-the-art methods and produces more reliable probabilistic estimates.
Referencess
Alessandro Antonucci, Giorgio Corani, Denis Deratani Mauá, and Sandra Gabaglio. An ensemble of bayesian networks for multilabel classification. In IJCAI, pages 1220–1225, 2013.
Iyad Batal, Charmgil Hong, and Milos Hauskrecht. An efficient probabilistic framework for multi-dimensional classification. In Proceedings of the 22nd ACM International Conference on Information and Knowledge Management, CIKM ’13, pages 2417–2422. ACM, 2013.
C. Bielza, G. Li, and P. Larrañaga. Multi-dimensional classification with bayesian networks. International Journal of Approximate Reasoning, 52(6):705 – 727, 2011.
Matthew R. Boutell, Jiebo Luo, Xipeng Shen, and Christopher M. Brown. Learning multi-label scene classification. Pattern Recognition, 37(9):1757 – 1771, 2004.
Kim-Anh L Cao, Emmanuelle Meugnier, and Geoffrey J. McLachlan. Integrative mixture of experts to combine
clinical factors and gene markers. Bioinformatics, 26(9): 1192–1198, 2010.
Weiwei Cheng and Eyke Hüllermeier. Combining instancebased learning and logistic regression for multilabel classification. Machine Learning, 76(2-3):211–225, 2009.
Amanda Clare and Ross D. King. Knowledge discovery in multi-label phenotype data. In In: Lecture Notes in Computer Science, pages 42–53. Springer, 2001.
Krzysztof Dembczynski, Weiwei Cheng, and Eyke Hüllermeier. Bayes optimal multilabel classification via probabilistic classifier chains. In Proceedings of the 27th International Conference on Machine Learning (ICML10), pages 279–286. Omnipress, 2010.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B, 39: 1–38, 1977.
Reza Ebrahimpour, Mohammad R. Moradian, Alireza Esmkhani, and Farzad M. Jafarlou. Recognition of persian handwritten digits using characterization loci and mixture of experts. JDCTA, 3(3):42–46, 2009.
Jack Edmonds. Optimum branchings. Research of the National Bureau of Standards, 71B:233–240, 1967.
Andrew Estabrooks and Nathalie Japkowicz. A mixture-ofexperts framework for text classification. In Proceedings of the 2001 Workshop on Computational Natural Language Learning - Volume 7, ConLL ’01, pages 9:1–9:8, Stroudsburg, PA, USA, 2001. Association for Computational Linguistics.
Shantanu Godbole and Sunita Sarawagi. Discriminative
methods for multi-labeled classification. In PAKDD’04, pages 22–30, 2004.
Isobel Claire Gormley and Thomas Brendan Murphy. Mixture of Experts Modelling with Social Science Applications, pages 101–121. John Wiley & Sons, Ltd, 2011.
Daniel Hsu, Sham Kakade, John Langford, and Tong Zhang. Multi-label prediction via compressed sensing. In NIPS, pages 772–780, 2009.
Robert A. Jacobs and Michael I. Jordan. Learning piecewise control strategies in a modular neural network architecture. IEEE Transactions on Systems, Man, and Cybernetics, 23(2):337–345, 1993.
Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures of local experts. Neural Comput., 3(1):79–87, March 1991. ISSN 0899-7667.
Michael I. Jordan and Lei Xu. Convergence results for the em approach to mixtures of experts architectures. Neural Networks, 8(9):1409–1431, 1995.
Hideto Kazawa, Tomonori Izumitani, Hirotoshi Taira, and Eisaku Maeda. Maximal margin labeling for multi-topic text categorization. In Advances in Neural Information Processing Systems 17, pages 649–656. MIT Press, 2005.
D. C. Liu and J. Nocedal. On the limited memory bfgs method for large scale optimization. Math. Program., 45(3):503–528, December 1989.
Zhiwu Lu. A regularized minimum cross-entropy algorithm on mixtures of experts for time series prediction and curve detection. Pattern Recogn. Lett., 27(9):947–955, July 2006. ISSN 0167-8655.
S. I. Mossavat, O. Amft, B. De Vries, Petko Petkov, and W. Bastiaan Kleijn. A bayesian hierarchical mixture of experts approach to estimate speech quality. In 2010 2nd International Workshop on Quality of Multimedia Experience, volume QoMEX 2010 - Proceedings, pages 200–205, 2010.
Guo-Jun Qi, Xian-Sheng Hua, Yong Rui, Jinhui Tang, Tao Mei, and Hong-Jiang Zhang. Correlative multilabel video annotation. In Proceedings of the 15th international conference on Multimedia, MULTIMEDIA ’07, pages 17–26. ACM, 2007a.
Y. Qi, J. Klein-Seetharaman, and Z. Bar-Joseph. A mixture of feature experts approach for protein-protein interaction prediction. BMC bioinformatics, 8(Suppl 10): S6, 2007b. ISSN 1471-2105.
Jesse Read, Bernhard Pfahringer, Geoff Holmes, and Eibe Frank. Classifier chains for multi-label classification. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II, ECML PKDD ’09, pages 254–269. Springer-Verlag, 2009.
David Sontag. Approximate Inference in Graphical Models using LP Relaxations. PhD thesis, Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2010.
Farbound Tai and Hsuan-Tien Lin. Multi-label classification with principle label space transformation. In Proceedings of the 2nd International Workshop on MultiLabel Learning, 2010.
Robert Endre Tarjan. Finding optimum branchings. Networks, 7(1):25–35, 1977.
Grigorios Tsoumakas, Min-Ling Zhang, and Zhi-Hua Zhou. Learning from multi-label data. ECML PKDD Tutorial, 2009.
Linda C. van der Gaag and Peter R. de Waal. Multidimensional bayesian network classifiers. In Probabilistic Graphical Models, pages 107–114, 2006.
Steven Richard Waterhouse. Classification and Regression Using Mixtures of Experts. PhD thesis, University of Cambridge, Department of Engineering, 1997.
A. S. Weigend, M. Mangeas, and A. N. Srivastava. Nonlinear gated experts for time series: Discovering regimes and avoiding overfitting. International Journal of Neural Systems, 6:373–399, 1995.
Andreas S. Weigend and Shanming Shi. Predicting daily probability distributions of S&P500 returns. Journal of Forecasting, 19(4), July 2000.
Changhe Yuan, Tsai-Ching Lu, and Marek J. Druzdzel. Annealed map. In UAI, pages 628–635. AUAI Press, 2004.
Seniha Esen Yuksel, Joseph N. Wilson, and Paul D. Gader. Twenty years of mixture of experts. IEEE Trans. Neural Netw. Learning Syst., 23(8):1177–1193, 2012.
Min-Ling Zhang and Kun Zhang. Multi-label learning by exploiting label dependency. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’10, pages 999–1008. ACM, 2010.
Min-Ling Zhang and Zhi-Hua Zhou. Multilabel neural networks with applications to functional genomics and text categorization. IEEE Transactions on Knowledge and Data Engineering, 18(10):1338–1351, 2006.
Min-Ling Zhang and Zhi-Hua Zhou. Ml-knn: A lazy learning approach to multi-label learning. Pattern Recogn., 40(7):2038–2048, July 2007.
Yi Zhang and Jeff Schneider. Multi-label output codes using canonical correlation analysis. In AISTATS 2011, 2011.
Yi Zhang and Jeff Schneider. Maximum margin output coding. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), ICML ’12, pages 1575–1582. Omnipress, 2012.
LogAnswer [2] is an open domain question answering system. It is accessible by a web interface (http://www.loganswer.de) similar to that of a search engine, see Fig. 1. The user enters a question into the text box and LogAnswer presents the three best answers, which are highlighted in the relevant textual sources to provide a context. While many systems for natural language question answering focus on shallow linguistic methods, LogAnswer uses an automated theorem prover (ATP) to compute the replies.
The system was developed in the LogAnswer project which was a cooperation between the IICS (Intelligent Information and Communication Systems) at the Fernuniversität in Hagen and the Artificial Intelligence Research Group (AGKI) at the University Koblenz-Landau. The project was funded by the German Research Foundation DFG (Deutsche Forschungsgemeinschaft) and aimed at the development of efficient and robust methods for logic-based question answering. The IICS is experienced in computational linguistics and knowledge engineering. Within the LogAnswer project the IICS handled the natural language aspects and provided the knowledge base. As an expert in automated theorem proving, the AGKI was responsible for the deductive aspects of the LogAnswer project.
As indicated in (c) in the list of properties of cognitive computing systems, it is important to take care that the different modules interact and cooperate closely. When combining NLP and automated reasoning as in the LogAnswer system, paying attention
Freitag, 17. Oktober 14
to the conflicting aims of the two fields is important. Since NLP methods are often confronted with flawed textual data, they strive toward robustness and speed. Nevertheless, they lack the ability to perform complex inferences. In contrast to that, a theorem prover uses a sound calculus to derive precise complex proofs. However, even minor flaws or omissions in the data can lead to a failure of the derivation process. Furthermore, refutationally complete theorem provers can have problems when dealing with large amounts of data due to the fact that they can easily get stuck performing redundant inferences. In the LogAnswer system NLP is used to filter the input for the theorem prover to a fraction of the knowledge available to LogAnswer, and the prover is embedded into a relaxation mechanism which can lessen the proof requirements for imperfect input data [3].
As claimed in (a) in the list of properties, the LogAnswer system uses multiple knowledge formats. One part of the knowledge is provided by a snapshot of the German Wikipedia, which has been translated into a semantic network representation in the MultiNet (Multilayered Extended Semantic Networks) formalism [5]. To make the semantic networks accessible to modern theorem provers, LogAnswer is also equipped with a representation of the MultiNet knowledge base in first-order logic (FOL). See [3] for details on the translation of the MultiNet knowledge base into a first-order logic knowledge base. All in all, 29.1 million natural language sentences have been translated. In addition to that, a background knowledge consisting of 12,000 logical rules and facts is used. This background knowledge provides general knowledge which is advantageous for the setting of question answering. Automated reasoning enables the integration of this background knowledge.
In Figure 2 it is depicted how LogAnswer processes a question. Since it is a webbased question answering system, users expect the system to respond quickly. This aspect of time criticality corresponds to (d) in the list of properties and is a serious restriction of the time available for the LogAnswer system to process a question. In such a restricted time, a question cannot be answered directly using the the whole knowl-
6 edge base. Therefore, several different techniques such as natural language processing, information retrieval, machine learning and automated deduction come to use. This corresponds to claim (b) in the list of properties. After translating the question into the MultiNet and FOL representation, the Wikipedia content is matched against the given query using retrieval and shallow linguistic criteria. By this, lists of features like the number of matching lexemes between passages and the question or the occurrences of proper names in the passage are computed. Afterwards an ML algorithm ranks text passages using these features. Then up to 200 text passages are extracted from the knowledge base according to this ranking. These so-called answer candidates have a high probability to contain the answer and can be computed rapidly. The computation of feature lists is implemented robustly, which allows to handle documents containing syntactic errors and thus to extract answers from text passages which cannot be parsed completely. In the next step the theorem prover Hyper [9] is used. The Hyper theorem prover is an implementation of the hypertableaux calculus [1] extended with equality. It has been shown to be very suitable for the type of reasoning problems occurring in the question answering setting, which are characterized by their large number of irrelevant axioms.
With the help of Hyper the answer candidates are tested consecutively. For each of these tests, the logical representation of both the query and an answer candidate together with the background knowledge are fed into Hyper. A successful proof provides an answer by giving an instantiation of the variables of the logical representation of the query. If no proof can be found in time, query relaxation techniques come to pass. These techniques allow certain subgoals of the query to be weakened or dropped in order to enable the prover to find a proof in short time. Query relaxation increases the likelihood of finding an answer even if the knowledge at hand is incomplete. However, the drawback of this technique is that it decreases the probability that the answer found is relevant to the query. As claimed in (e) in the list of properties, the LogAnswer system is aware of its own accuracy, because all proofs are ranked by machine learning algorithms. The three proofs with the highest rank are translated back into natural language answers and are presented to the user.
RaQueL provides functions for special tasks such as semantic labeling of an image, ranking a trajectory based on human preference, finding a set of affordances for a given object, grounding natural language expressions, etc. RaQueL also provides functionalities to work with media files of RoboBrain, such as functions to generate and work with heatmaps, trajectories, videos, etc. For full list of functions supported by RaQueL and more examples, please refer to the RoboBrain website.
A big shortcoming of SMT-based methods is that they need another model to generate the first line. For example, He (2012) expands user keywords, then use constraint templates and a language model to search for a line. For RNN Encoder-Decoder, words and sentences will be mapped into the same vector space. Since our system is based on characters, words can be considered as short sequences. Ideally, SPB will generate a relevant line taking a word as input. But the training pairs are all long sequences, it won't work well when the input is a short word. Therefore, we train the third EncoderDecoder, called Word Poem Block (WPB). Based on the model parameters of trained SPB, we use some <word, line> pairs to train it more to improve WPB's ability of generating long sequences with short sequences.
We use the notation g1:t as a shorthand for ∑t
τ=1 gτ . Similarly we write Q1:t for a sum of matrices Qt, and f1:t to denote the function f1:t(x) = ∑t τ=1 fτ (x). We write x
Ty or x · y for the inner product between x, y ∈ Rn. The ith entry in a vector x is denoted xi ∈ R; when we have a sequence of vectors xt ∈ Rn indexed by time, the ith entry is xt,i ∈ R. We use ∂f(x) to denote the set of subgradients of f evaluated at x.
Recall A ∈ Sn++ means ∀x 6= 0, xTAx > 0. We use the generalized inequality A ≻ 0 when A ∈ Sn++, and similarly A ≺ B when B − A ≻ 0, implying xTAx < xTBx. We define A B analogously for symmetric positive semidefinite matrices Sn+. For B ∈ Sn+, we write B1/2 for the square root of B, the unique X ∈ Sn+ such that XX = B (see, for example, Boyd and Vandenberghe [2004, A.5.2]). We also make use of the fact that any A ∈ Sn+ can be factored as A = PDPT where PTP = I and D = diag(λ1, . . . , λn) where λi are the eigenvalues of A.
Following the arguments of Zinkevich [2003], for the remainder we restrict our attention to linear functions. Briefly, the convexity of ft implies ft(x) ≥ gTt (x − xt) + ft(xt), where gt ∈ ∂f(xt). Because this inequality is tight for x = xt, it follows that regret measured against the affine functions on the right hand side is an upper bound on true regret. Furthermore, regret is unchanged if we replace this affine function with the linear function gTt x. Thus, so long as our algorithm only makes use of the subgradients gt, we may assume without loss of generality that the loss functions are linear.
Taking into account this reduction and the functional form of the rt, the update of FTPRL is
xt+1 = argmin x∈F
(
1
2
t ∑
τ=1
(x − xτ )TQτ (x− xτ ) + g1:t · x ) . (5)
In this section, we evaluate the quality of selected features by their clustering performance. We use the the popular coregularized spectral clustering [7] for clustering multi-view data 5. We set their σ as the median of pairwise Euclidean distances between data points and λ = 0.1 as suggested in the paper. KMeans is then used on these latent factors. We repeat the KMeans experiment for 20 times (since it is initilization) and report the average performance. We vary the number of features d in the range of {100, 200, 300, 400}. For each feature size d, we choose appropriate λ in our method via binary search to let the number of selected features (with score sp = 1) within d± 10.
Following the typical experimental setting for unsupervised feature selection [26] [8] [25], we use Accuracy and Normalized Mutual Information (NMI) to evaluate the result of clustering. Accuracy is defined as follows.
Accuracy = 1
n n∑ i=1 I(ci = map(pi)) (20)
where pi is the clustering result of instance i and ci is its real class label. map(·) is a mapping function that maps each cluster label to a ground-truth label using Kuhn-Munkres Algorithm [10].
Normalized Mutual Information (NMI) is another popular metric for evaluating clustering performance. Let C be the set of clusters from the ground truth and C ′ obtained from a clustering algorithm. Their mutual information MI(C,C ′) can be defined as follows:
MI(C,C ′) = ∑
ci∈C,c′j∈C′ p(ci, c
′ j) log
p(ci, c ′ j)
p(ci)p(c′j) (21)
where p(ci) and p(c′j) are the probabilities that a random instance from the data set belongs to ci and c′j , respectively, and p(ci, c′j) is the joint probability that the instance belongs to the cluster ci and c′j at the same time. In our experiments, we use the normalized mutual information as in previous work [8] [16].
NMI(C,C ′) = MI(C,C ′)
max(H(C), H(C ′)) (22)
where H(C) and H(C ′) are the entropy of C and C ′. Higher value of and Accuracy and NMI indicates better quality of clustering.
5We use the code at http://www.umiacs.umd.edu/∼abhishek/code coregspectral.zip
We set k = 5 for the kNN neighbor size in the baseline methods and our approach following previous convention [8]. For the number of pseudo-classes in UDFS, RSFS, MVFS and MVUFS, we use the ground-truth number of classes. Also, we perform grid search in the range of {0.1, 1, 10} for the regularization parameters in these baseline methods. Besides their best performance, we also report the median performance for them. For CDMA-FS proposed in this paper, we use ‘0/1’ weighting in the W and we fix σ2 = 1 and α = 0.01 for all the datasets after normalizing each data point to unit length. We set the maximum number of iterations for the cross-diffusion process as 20.
Proof of Lemma 101. The proof is essentially a vdery simple modification of the one provided by Nemirovski
and Yudin in Section 4.4.2 of [1]. We provide an abridged version here with the appropriate modifications
needed to deal with the non-dual case with a few minor alterations to relate to fat-shattering dimension. To prove the lower bound, we first start by picking x1, . . . ,xm ∈ X add s1, . . . , sm ∈ R. Now the functions we shall consider are of form
z (h; (x1, s1), . . . , (xm, sm)) = max i∈[m]
i(〈h,−xi〉+ si)
183
where ∈ {±1}m. Notice that each z ∈ ZLip(X ). Note also that for any ∈ {±1}m,
− inf h∈H z (h; (x1, s1), . . . , (xm, sm)) = − inf h∈H max i∈[m] i(〈h,−xi〉+ si) = sup h∈H min i∈[m] i(〈h,xi〉 − si) .
(9.5.1)
Remember that we want to show that for any AO there exists a function that requires at least m calls to some Oracle O to ensure sub-optimality less than > 0. The first thing we notice is that the family of functions we consider are piece wise linear and so any local oracle can give no more information that function value and gradient at point of query. Now given an Optimization algorithm AO the exact function we shall use for the lower bound will be constructed in m steps based on the algorithm and the choosen x1, . . . ,xm ∈ X add s1, . . . , sm ∈ R. The procedure for constructing the function is given below :
Initialize I1 = [m] for t = 1 to m
AO picks ht ∈ H for query i(t) = argmax
i∈It {|〈ht,−xi〉+ si|}
t =
{ +1 if ( 〈 ht,−xi(t) 〉 + si(t)) ≥ 0
−1 otherwise It+1 = It \ {i(t)} zt(h) = maxj∈[t] { j( 〈 h,−xi(j) 〉 + si(j))
} Return answer to query as It = O(ht, zt).
end for
The first thing we notice about zm is that it is of the form :
zm(·) = z (·; (xi(1), si(1)), . . . , (xi(m), si(m))) (9.5.2)
where 1, . . . , m are given by the procedure above. Next, zm is such that, for any i ∈ [m] and any local oracle O,
O(hi, zm) = O(hi, zi)
hence the h1, . . . ,hm returned by the algorithm when it is presented with function fm is the same as the corresponding ones in the above procedure. Finally, by the way the functions are constructed (specifically
the way t is picked),
zm(hm) ≥ 0
184
Hence we conclude that
zm(hm)− inf h∈H zm(h) ≥ − inf h∈H zm(h)
= − inf h∈H z (h; (xi(1), si(1)), . . . , (xi(m), si(m))) (by Eq. 9.5.2)
= sup h∈H min i∈[m]
i(〈h,xi〉 − si) (by Eq. 9.5.1)
≥ inf ∈{±1}m sup h∈H min i∈[m] i(〈h,xi〉 − si) .
Furthermore note that the choice of x1, . . . ,xm ∈ X and s1, . . . , sm are arbitrary. Hence we can conclude that for any β > 0, if for any m there exists x1, . . . ,xm ∈ X and s1, . . . , sm ∈ R such that
inf ∈{±1}m sup h∈H min i∈[m]
i(〈h,xi〉 − si) > β ,
then no oracle based algorithm can achieve sub-optimality smaller than β in m or less steps. However note
that this is exactly the definition of fat-shattering dimension at scale 2β (Definition 13) for the linear class
given by
F = {x 7→ 〈h,x〉 : h ∈ H} .
Hence we conclude the lemma statement.
Proof of Theorem 102. The first inequality is a direct consequence of Lemma 101. For the upper bound on the Rademacher complexity note that, for any n ∈ N, by the refined Dudley integral bound we have that :
Riidn (Flin(H,X )) ≤ inf α>0 4α+ 10 ∫ 1 α √ fatiidβ (Flin(H,X )) log(n) n dβ  We now divide the analysis into two cases, first where q ∈ [2,∞) and next where q ∈ (0, 2). We start for the case when q ∈ [2,∞) and see that using the assumption of this theorem and Lemma 101 we see that for any
185
q ∈ [2,∞):
Riidn (Flin(H,X )) ≤ inf α>0
{ 4α+ √ V q log(n)
n 10 ∫ 1 α 1 β q 2 dβ }
≤ inf α>0
{ 4α+ √ V q log(n)
n 10 ∫ 1 α ( q 2 − 1 ) log(1/β) + 1 β q 2 dβ }
≤ inf α>0
{ 4α+ 10 √ V q log(n)
n
[ log(β)
β q 2−1 ]1 α }
≤ inf α>0
{ 4α+ 10 √ V q log(n)
n
log(1/α)
α q 2−1
}
≤ 9V log 1+ 1q (n)
n1/q
where in the last step above we used the value α = V log 1/q(n)
n1/q . Now we turn our attention to case when q ∈ (0, 2). As for this case we simply note that ( V )q ≤ (V )2 and so using the case when q = 2 we conclude that
Riidn (Flin(H,X )) ≤ 9V log3/2(n)
n1/2
This concludes the proof.
Contextual information can be static i.e. those aspects of a pervasive system that are invariant, such as a person’s date of birth. However, the majority of contextual information is dynamic, such as location, with its persistence being highly variable. Every element of the RoCoM ontology, beginning with the top level classes like ‘entity’, ‘activity’ and ‘event’, have their contextual information separated into two hierarchies - static and dynamic. This enables ease of distinction between contextual information that is persistent over a long period of time (static) and that which needs to be updated frequently based on its freshness (dynamic).

FB15k is a commonly used dataset in knowledge base completion. Table 1 shows statistics of FB15k. FB15k dataset contains factual information in our world, e.g., location/country/language spoken. As FB15k has various kinds of relation, it is suitable for the evaluation of PTransR. Therefore, we choose FB15k as our experimental dataset.
For clarity of presentation, we focus on a restricted matching setup with simplifying assumptions. Nevertheless, this setup has enough complexity to make the point on nonconvexity and diffusion.
The 196 − 100 − 50 − 10 networks were trained on the MNIST and TIDIGITS datasets using the ADAM optimizer [27] and the mean squared error as loss function. The low-precision training (three signed bits per synapse) was done using a high-precision store and low-precision activations in the manner of the method simultaneously described in [48, 10]. An L1 regularization scheme was applied to negative weights only to reduce the number of negative inputs to neurons, as they would slow down the circuits. The Keras software toolkit [9] was used to perform the training. A custom layer consisting of the parameterized activation function f(x) = max{0, a ·Wx} , using the extracted parameter a was added and used to model the neuron activation function.
Different sets of empirically found hyperparameters were used during training for the MNIST
and TIDIGITS datasets. A reduced resolution version (14× 14 pixels) of the MNIST dataset was generated by identifying the 196 most active pixels (highest average value) in the dataset and only using those as input to the network. The single images were normalized to a mean pixel value of 0.04. The learning rate was set to 0.0065, the L1 penalty for negative weights was set to 10−6, and the networks were trained for 50 epochs with batch sizes of 200.
Each spoken digit of the TIDIGITS dataset was converted to 12 mel-spectrum cepstral coefficients (MFCCs) per time slice, with a maximum frequency of 8 kHz and a minimum frequency of 0 kHz, using 2048 FFT points and a skip duration of 1536 samples. To convert the variablelength TIDIGITS data to a fixed-size input, the input was padded to a maximum length of 11 time slices, forming a 12x11 input for each digit. First derivative and second derivatives of the MFCCs were not used. To increase robustness, a stretch factor was applied, changing the skip duration of the MFCCs by a factor of 0.8, 0.9, 1.0, 1.1, and 1.3, allowing fewer or more columns of data per example, as this was found to increase accuracy and model robustness. A selection of hyperparameters for the MFCCs were evaluated, with these as the most successful. The resulting dataset was scaled pixel-wise to values between 0 and 1. Individual samples were then scaled to yield a mean value of 0.03. The networks were trained for 512 epochs on batches of size 200 with the learning rate set to 0.0073, and the L1 penalty to 10−6.
La definición antonímica es una noción de Bosque (1982) que abarca la definición por exclusión y la definición por inclusión negativa de ReyDebove (1967). El primer tipo consiste en definir una palabra negando su opuesto mediante un hiperónimo de significado negativo (ej. 17) y el segundo, mediante una negación sintáctica (ej. 18) (Rey-Debove 1967: 153-155).
ej. 17 ASIMETRÍA. Falta de simetría. [DLE37]
ej. 18 INCONFUNDIBLE. No confundible. [DLE]
Dentro de la definición antonímica, se puede concebir un tercer tipo: la definición mesonómica, consistente en una definición por exclusión doble (Borsodi 1966 apud Bosque 1982: 110). En una definición mesonómica, el definiéndum se identifica por medio de la exclusión de otras dos unidades:
ej. 19 INDIFERENCIA. Estado de ánimo en que no se siente inclinación ni repugnancia hacia una persona, objeto o negocio determinado. [DLE]
37 Diccionario de la lengua española (Real Academia Española 2001)
3. La definición
151
La definición antonímica en terminología presenta los mismos problemas que la definición sinonómica en cuanto que confía en exceso en los conocimientos previos del usuario. Además, en el caso de la definición por excluyente negativo el genus es incorrecto, ya que, tomando el ej. 17, no se puede afirmar que lentitud sea un tipo de falta. No obstante, es posible incluir relaciones de oposición en una definición, pero no debe ser lo que determine el genus.
Aaron Li-Feng Han
University of Macau
2014
Let X be the input space consisting of an arbitrary subset of Rd and Y = {−1,+1} be the output space. An example is an input-output pair (x, y) where x ∈ X and y ∈ Y. In a PAC-Bayes setting [13], each example (x, y) is drawn from a fixed, but unknown, probability distribution D on X × Y. Let f (x, v) : X → Y be any classifier with a set of variables v. The learning task is to choose a posterior distribution Q over a space F of classifiers and a space V of variables such that the Q-weight majority classifier BQ = sign[E( f ,v)∼Q f (x, v)] will have the smallest possible risk on the training example set S = {(x1, y1), · · · , (xm, ym)}. The output of BQ is closely related to the output of the Gibbs classifier GQ which first chooses a classifier f and a vector v according to Q, and then classifies an example x. The true risk R(GQ) and the empirical risk RS (GQ) of this Gibbs classifier are given by:
R(GQ) = E ( f ,v)∼Q E (x,y)∼D I( f (x, v) , y) (6)
RS (GQ) = E ( f ,v)∼Q 1 m
m ∑
i=1
I( f (xi, v) , yi) (7)
This setting is naturally accommodated by PAC-Bayes theory since v can be considered as a part of f . Among several PACBayes bounds [13], [5], [2], [12], the bound derived in [2] is quite tight and gives an explicit bound for the true risk R(GQ), which allows the derivation of the posterior Q, in contrast to most of the other implicit bounds over KL(Rs(GQ)‖R(GQ)). Theorem 1. For any distribution D over X×Y, any space F of classifiers, any space V of random variables, any distribution P over F×V, any δ ∈ (0, 1], and any real number C > 0, ∀ Q over F ×V, we have:
Pr
(
R(GQ) ≤ 1
1 − e−C
[ 1 − exp { − CRS (GQ)
− 1 m [KL(Q‖P) − ln δ] }] ) ≥ 1 − δ
This is a slight extension of Corollary 2.2 of [2], and can be proved by replacing f with ( f , v) and reapplying its proof [2].
The above risk bound is derived for labeled data. Here we have extended the bound to accommodate both labeled and unlabeled data for semi-supervised learning in the following theorem. The semi-supervised bound is different with [12] whose bound is implicit and has no explicit solution of Q.
3 Theorem 2. For any distribution D over X×Y, any space F of classifiers, any space V of random variables, distribution P over F×V, any δ ∈ (0, 1], and any real number C > 0, ∀ Q over F ×V, we have:
Pr
(
R(GQ) ≤ 1
1 − e−C
[ 1 − exp { − C [ eS (GQ) + 1 2 dS (GQ) ]
− 1 m [KL(Q‖P) − ln δ] }] ) ≥ 1 − δ
where the risks for labeled and unlabeled data are eS (GQ)= E( f1,v1)∼QE( f2,v2)∼Q 1 m ∑m i=1 I( f1(xi, v1) , yi))I( f2(xi, v2), yi)) and dS (GQ) = E( f1,v1)∼QE( f2,v2)∼Q 1 m ∑m i=1 I( f1(xi, v1) , f2(xi, v2)).
Proof: Let E f be the abbreviation of E( f ,v)∼Q. Note that E f1, f2I( f1 , f2) = E f1, f22I( f1 , y)I( f2 = y) = E f1, f2 2I( f1 , y)(1− I( f2 , y)) = E f1, f22(I( f1 , y)−I( f1 , y)I( f2 , y)) and RS (GQ)= 1 m ∑ i E f1 I( f1 , yi) (Eq. (7)). Therefore RS (GQ)= 1 m ∑ i E f1I( f i 1 , yi) = 1m ∑ i E f1, f2I( f i 1 , yi)I( f i 2 , yi) + 1 2m ∑ i E f1, f2 I( f i 1 , f i 2) = eS (GQ)+ 12 dS (GQ), ( f i 1 = f1(xi)). Substituting RS (GQ) = eS (GQ) + 12 dS (GQ) into Theorem 1, then we obtain Theorem 2. Since dS (GQ) is independent of labels, it allows classifiers using the above bound to exploit unlabel data. Minimizing this risk dS (GQ) would contract the posteriors over the stochastic classifier and the stochastic feature space, making classification and feature mapping less uncertain.
Taking the VQA problem as a classification task is simple to be implemented and evaluated, and it is easy to be extended to generation or multiple choice tasks through a network surgery using the fused feature in the previous step. We use a linear layer and a softmax layer to map from the fused feature to the answer candidates, of which the entries are the top-1000 answers from the training data.
Considering multiple choice VQA problems, e.g. Visual7W [36] telling questions and COCO-VQA [3] multiple choice tasks, our model is adaptive to be extended by concatenating the question and answer vectors before fusion with visual features or by using bilinear model between the final fused feature and answer feature [26, 11], which is a possible future work. Meanwhile, in view of generation VQA problem, we can train an LSTM taking the fused feature as input to obtain answer word lists, phrases or sentences [22, 6].
Based on the discussion in the previous section, Fk is set of (~xi,~xj) which are infeasible based on the optimization constraint and we see them as the singular conditions for the optimization algorithm. One way to prevent the infeasible target set Fk from influencing the optimization problem (4) is to simply rule them out prior to the start of the optimization algorithm as a pre-processing step. To do so, we check the relevant nearby imposters ~xl for each pair of (~xi,~xj) and in case of existing any infeasible triples in combination with (~xi,~xj) will be eliminated from N ki by making ηij = 0 in (3) otherwise Wij = 1. Then we search for the next closest neighbor to ~xi in order to substitute ~xj with that neighbor.
We have proposed a novel hierarchical model over linguistic trees which exploits global context by conditioning the generation of a rule in a tree on an unbounded tree context consisting of the vertical chain of its ancestors. To facilitate learning of such a large and unbounded model, the predictive distributions associated with tree contexts are smoothed in a recursive manner using a hierarchical Pitman-Yor process. We have shown how to perform prediction based on our model to predict the parse tree of a given utterance using various search algorithms, e.g. A* and Markov Chain Monte Carlo.
This consistently improved over baseline methods in two tasks, and produced state-of-the-art results for Danish part-of-speech tagging.
In future, we would like to consider sampling the seating arrangements and model hyperparameters, and seek to incorporate several different notions of context besides the chain of ancestors.
We characterized the domain-independent challenges posed by an execution aid that interactively supports humans monitoring the activity of distributed teams of cooperating agents, both human and machine. The most important issues for interactive monitoring are adaptivity, plan- and situationspecific monitoring, reactivity, and high-value, user-appropriate alerts. We showed how properties of various domains influence these challenges and their solutions. We then presented a top-level domain-independent categorization of the types of alerts a plan-based monitoring system might issue to a user. The different monitoring techniques generally required for each category are often domain specific and task specific.
Our monitoring framework integrates these various techniques and then uses the concept of value of an alert to control interaction with the user. This conceptual framework facilitates integration of new monitoring techniques and provides a domain-independent context for future discussions of monitoring systems. We discussed various design tradeoffs that must be made during the application of our monitoring framework to a domain (Sections 6.4 and 6.6).
We use this framework to describe a monitoring approach we developed and have used to implement Execution Assistants (EAs) in two different dynamic, data-rich, real-world domains. Our approach is based on rich plan representations, which allow the execution aid to filter, interpret, and react to the large volume of incoming information, and alert the user appropriately. An expressive plan representation is necessary for representing SUO plans, which must coordinate distributed units, trigger contingencies, and enforce a variety of constraints. It is equally important that this representation be monitorable by machines and meaningful to humans. Our plan representation and mission model were able to model a representative SUO scenario with enough fidelity to provide value (as judged by our domain experts) and was also sufficient for plans in the UV-Robotics domain.
We developed a sufficiently rich plan representation by extending an existing plan representation with a hierarchical, object-oriented mission model that encodes knowledge about primitive actions and mission-specific monitoring methods. The SUO EA implements a novel integration of these hierarchical monitoring methods with a reactive control system. The EA invokes the most specific methods defined in the hierarchy at appropriate points during monitoring.
One central challenge, in our domains as well as medical monitoring, is to avoid overwhelming the user with unwanted alerts and/or false alarms. We define the concepts of value of information and value of giving an alert as the principles for determining when to give an alert. We describe the properties of VOI and VOA, criteria for computing them, the advantages of qualitative reasoning in
our domains, and the successful use of these concepts in our applications. VOI and VOA algorithms must be customizable to the user, plan, and situation.
By using an asynchronous multiagent architecture and an extended version of the PRS reactive control system, we monitored the execution of both SUO and UV-Robotics plans with acceptable latency, given a dozen or more incoming events per second. PRS extensions include temporal monitors and efficiency improvements. Methods from the mission model are used throughout the SUO monitoring process for action-specific monitoring. Our evaluation showed that our plan-aware EAs generated appropriate alerts in a timely manner without overwhelming the user with too many alerts, although a small percentage of the alerts were unwanted. We have shown the utility of using advanced AI planning and execution technologies in small unit operations.
The application to UV-Robotics showed the generality of our SUO framework and monitoring concepts. We implemented a complex execution assistant in about one person-week, using code from the SUO EA. The UV EA uses the same plan representation and basic architecture as the SUO EA, but the inputs are different as are the tasks and the algorithms that respond to the inputs and generate alerts.
Future work. The most obvious area for future work in the SUO domain is incorporation of a planning assistant to complete the loop of continuous planning and execution. This integration has already been accomplished in the UV-Robotics domain, but the difficulty in the SUO domain is an interface that allows a soldier to interact effectively with the planning tool, using a wearable computer in a battlefield situation. Several research programs are addressing this problem, some of which are mentioned in Section 2.
Within the scope of execution monitoring, future work on our EAs could model and detect other types of plan deviations (such as loss of surprise or additional types of fratricide risks), project future failures, and provide higher-fidelity specialized reasoners, particularly for terrain reasoning. Additional theoretical work on VOI and VOA would support better quantitative estimates of VOI and VOA. The SUO mission model already has a method for projecting failures and a low-fidelity projection capability could be easily added. In the UV-Robotics domain, we plan to implement additional types of alerts in the near future, and extend the UV EA to serve the higher layers of the architecture that have more in common with the SUO EA alert types and triggers. The fragility of the UV communication network in hostile domains provides a set of interesting monitoring challenges that may result in the incorporation of specific monitoring-related tasks within cooperative team missions. Monitoring strategies for uncertain communication environments is an important research challenge for the UV-Robotics domain. Additional alerts being considered for future implementation include monitoring movement of entities in and out of geographical sectors mentioned in the plan, monitoring the deterioration or improvement of communication conditions, and monitoring the actions and intentions of coordinating team members to facilitate cooperative behavior.
DBpedia is a crowd-sourced comminity effort to extract structured information from Wikipedia(Lehmann et al., 2014). The English version of the DBpedia knowledge base provides a consistent ontology, which is shallow and cross-domain. It has been manually created based on the most commonly used infoboxes within Wikipedia. Some ontology classes in DBpedia contain hudrends of thousands of samples, which are ideal candidates to construct an ontology classification dataset.
The DBpedia ontology classification dataset is constructed by picking 14 non-overlapping classes from DBpedia 2014. They are listed in table 3. From each of thse 14 ontology classes, we randomly choose 40,000 training samples and 5,000 testing samples. Therefore, the total size of the training dataset is 560,000 and testing dataset 70,000.
Before feeding the data to the models, we concatenate the title and short abstract together to form a single input for each sample. The length of input used was l0 = 1014, therefore the frame length after last convolutional layer is l6 = 34. Using an NVIDIA Tesla K40, Training takes about 5 hours per epoch for the large model, and 2 hours for the small model. Table 4 shows the classification results.
The results from table 4 indicate both good training and testing errors from our models, with some improvement from thesaurus augmentation. We believe this is a first ev-
idence that a learning machine does not require knowledge about words, phrases, sentences, paragraphs or any other syntactical or semantic structures to understand text. That being said, we want to point out that ConvNets by their design have the capacity to learn such structured knowledge.
Figure 3 is a visualization of some kernel weights in the first layer of the large model trained without thesaurus augmentation. Each block represents a randomly chosen kernel, with its horizontal direction iterates over input frames and verticle direction over kernel size. In the visualization, black (or white) indicates large negative (or positive) values, and gray indicates values near zero. It seems very interesting that the network has learnt to care more about the variations in letters than other characters. This phenomenon is observed in models for all of the datasets.
ing and experiments
Focusing on the third step above, where the mixing matrix already has orthogonal columns, ICA algorithms already suffer dramatically from the presence of heavytailed data. As proposed in [10], Gaussian damping is a preprocessing technique that converts data from an ICA model X = AS, where A is unitary (columns are orthogonal with unit l2-norm) to data from a related ICA model XR = ASR, where R > 0 is a parameter to be chosen. The independent components of SR have finite moments of all orders and so the existing algorithms can estimate A.
Using samples of X, we construct the damped random variable XR, with pdf
ρXR(x) ∝ ρX(x) exp(−‖x‖ 2/R2). To normalize the right hand side, we can estimate
KXR = E exp(−‖X‖ 2/R2)
so that
ρXR(x) = ρX(x) exp(−‖x‖ 2/R2)/KXR .
If x is a realization of XR, then s = A −1x is a realization of the random variable SR and we have that SR has pdf ρSR(s) = ρXR(x). To generate samples from this distribution, we use rejection sampling on samples from ρX . When performing the damping, we binary search over R so that about 25% of the samples are rejected. For more details about the technical requirements for choosing R, see [10].
68
Figure 2.1 shows that, when A is already a perfectly orthogonal matrix, but where S may have heavy-tailed coordinates, several standard ICA algorithms perform better after damping the data. In fact, without damping, some do not appear to converge to a correct solution. We compare ICA with and without damping in this case: (1) FastICA using the fourth cumulant (“FastICA - pow3”), (2) FastICA using log cosh (“FastICA - tanh”), (3) JADE, and (4) Second Order Joint Diagonalization as in, e.g., [26] .
We define the optimization problem of maximizing a monotone submodular function f subject to a dknapsack constraint ξ (SMDK) as follows:
max S⊆V
f(S) s.t. S ∈ ξ (1)
we use S∗ to denote the optimal solution of SMDK, i.e., S∗ = argmaxS∈ξ f(S). According to the definition of the d-knapsack constraint, a cardinality constraint with a budget k is a special case of a 1-knapsack constraint with c(v) = 1/k, ∀v ∈ V . As SM with a cardinality constraint has been proved to be NP-hard [19], SMDK is also NP-hard.
The cost-effective greedy algorithm CEGreedy proposed in [15] for SM with 1-knapsack constraints can be adopted for SMDK2. As shown in Algorithm 1, CEGreedy starts from an empty set S = ∅ and iteratively adds an element v∗ into S until S∪{v} /∈ ξ for any remaining element v. The criterion for picking v∗ is based on its cost-effectiveness, i.e., v∗ maximizes ∆f (v|S)maxj∈[1,d] cj(v) at the i-th iteration among all elements v ∈ V \ S. Finally, CEGreedy compares S with the singleton element vmax with the largest utility value and returns the better one as the final solution.
In a research by Lazarevic and Obradovic [16] where clustering was also used to prune neural network ensembles,
the researchers used diabetes and glass datasets, which we also used in our experiments. Table II depicts the accuracy of their entire and pruned ensembles with RF and our CLUBDRF. For both datasets, our CLUB-DRF was superior to their pruned ensemble. Notably with the glass dataset, CLUB-DRF has made a clear progression in predictive accuracy with a healthy 7.06% increase over the pruned neural network.
Proof If t ∈ src(G), from Definition 5, we have P0(αG(t)) = P0(ξ(t)) = (P0 ◦ ξ) (t) = αG′(t).
Consider the case where t is an element of V such that t /∈ src(G). Assume the induction hypothesis that (19) holds for every element of E−G(v). If op(v) = “+” then, by Definition 5, Lemma 7, and the induction hypothesis, we have
P0(αG(t)) = P0  ∑ e∈E−G(t) αG(e)  = ∑ e∈E−G(t) P0(αG(e)) = ∑ e∈E−G(t) αG′(t) .
Otherwise (i.e., op(t) = “·”), by using Definition 5, Lemma 7, the linear combination of elements of a basis {u⊗êi}u∈U, i∈{0,1} of A⊗SBC1S for αG(t), say, αG(t) = ∑ u∈U, i∈{0,1} ηt,u,i(u⊗ êi) where ηt,u,i ∈ S, and the induction hypothesis, and by noting that x = (P0(x)) ⊗ ê0 + (P1(x))⊗ ê1, P0((P0(x))⊗ ê0) = P0(x), and P0((P1(x))⊗ ê1) = 0A for every x ∈ A⊗S BC1S , and êi1 · · · êin = ê0 if and only if ij = 0 for all j, we have
P0(αG(t)) = P0  ∏ e∈E−G(t) αG(e)  = P0  ∏ e∈E−G(t) ∑ u∈U, i∈{0,1} ηe,u,i (u⊗ êi)  =
∏ e∈E−G(t) (∑ u∈U ηe,u,0 (u⊗ ê0) ) = ∏ e∈E−G(t) P0(αG(e)) = ∏ e∈E−G(t) αG′(e) = αG′(t) .
Consider the case where t is an element of E. Assume the induction hypothesis that (19) holds for tail(t). Then, by using Definition 5 and the induction hypothesis, we have P0(αG(t)) = P0(αG(tail(t))) = αG′(tail(t)) = αG′(t).
Therefore, we have proven that the equation (19) holds for every t ∈ V ∪E by induction on the finite dag G.
Toward general match. While the previous section mainly focuses on clothes matching, we also train classifiers on the other twenty top-level categories from the Amazon dataset and present the results in Table 2. As can be seen, we obtain good accuracy in predicting compatibility relationships in a variety of categories. What’s more, we have also tried to train a single model to predict compatibility relationships for all categories. There appears to be no “silver bullet” and the result is dissatisfactory: the AUC score of that single model is only 0.694.
The comparison across categories is particularly interesting. Our approach performs relatively poor on the categories “CDs & Vinyl” and “Digital Music” since the content of music is too rich to be described very clearly in a short title description. In contrast, the title description is long enough to describe an item from the category ‘Musical Instruments’ clearly and thus our approach performs very well on that. In a word, the better titles can describe the attributes of items in a category, the higher performance can be achieved on that category by our approach.
In this section, we will formally describe the problem of limiting the generative network to learn a single type of input distribution. Consider two random variables X and Y representing instances of two input distributions in same signal space (e.g. image space). Lets assume we have K and J number of samples from each distribution, X = {x1, ..., xK} and Y = {y
1 , ..., yJ}. X is the input distribution we want the
network to reconstruct as well as possible, let the reconstruction be called X̂. On the other hand, Y is the distribution that we do not want to the network to reconstruct. Let its recon-
structed space be represented by Ŷ. In order to achieve this objective, we need to maximize
pθ(X̂|X)− pθ(Ŷ|Y) =
K ∑
i=1
log pθ(x̂i|xi)−
J ∑
i=1
log pθ(ŷi|yi)
(1)
By maximizing the probability of reconstruction for X and minimizing it for Y, the generative properties of the model can be controlled in the desired way. It is important to note that usually the data for the distribution X is available in plenty while the data for Y is available scarcely (K ≫ J).
Now a day’s massive volume of online text is available through different websites, internet news feed, emails, cooperate databases and digital library. The main problem is to classify text documents from such massive databases. Using set of training labelled examples statistical text learning algorithms can be trained to approximately classify documents. The news articles and web pages were automatically catalogued by these text classification algorithms.
Naïve Bayes Classifier: The Naïve Bayes—well known probabilistic classifier—and describes its application to text. In order to incorporate unlabelled data, the foundation Naïve Bayes was build. The task of learning of a generative model is to estimate the parameters using labelled training data only. The estimated parameters are used by the algorithm to classify new documents by calculating which class the generated the given document belongs to. The naive Bayesian classifier works as follows:
• Consider a training set of samples, each with the class labels T. There are k classes, C1,C2, . . . ,Ck. Every sample consists of an n-dimensional vector, X = { x1, x2, . . . , xn}, representing n measured values of the n attributes, A1,A2, . . . ,An, respectively. • The classifier will classify the given sample X such that it belongs to the class having the highest posterior probability. That is X is predicted to belong to the class Ci if and only
P(Ci |X) > P(Cj |X) for 1≤ j ≤ m, j≠ i.
Thus we find the class that maximizes P(Ci |X). The maximized value of P(Ci |X) for class Ci is called the maximum posterior hypothesis. By Bayes’ theorem
• Only P (X| Ci) P (Ci) value needs to be maximized as for all classes value of P(X) is same. If the priori probabilities, P (Ci) of the class are not known, then it is assumed that the classes are likely to be equal, that is, P (C1) = P(C2) = . . . = P(Ck), and we would therefore maximize P(X| Ci). Otherwise the value of P(X| Ci) P(Ci) is maximized. The priori probabilities of a class are estimated by
P(Ci) = freq(Ci, T)/|T|.
• To compute P (X| Ci) would be computationally expensive as given data sets consist of many attributes. To reduce computation in evaluation of P (X| Ci) P (Ci), the conditional class independence of naive assumption is made. The values of the attributes of the class label of the given sample presume to be conditionally independent of one another. Mathematically this means that
Estimation of the probabilities P (x1| Ci), P (x2| Ci). . . P (xn| Ci) can easily be done from the training set. For sample X, xk refers to the value of attribute Ak.
(a) If Ak is categorical, then the number of samples P (xk| Ci) of class Ci in T have the value xk for attribute Ak, divided by the number of sample of class Ci, freq (Ci, T), in T. (b) We typically assume that if Ak is continuous-valued then the values have a Gaussian distribution with a mean μ and standard deviation defined by
so that p(xk| Ci) = g(xk, μ Ci , σ Ci ).
We need to compute μ Ci and σ Ci, which are the mean and standard deviation of values of attribute xk for training samples of class Ci.
• To predict the class label X, the evaluation of P (X| Ci) P (Ci) is done for each class Ci. The class that maximizes the value of P (X| Ci) P (Ci) is the class label of X is Ci predicted by the classifier. [7] • Expectation Maximization: Due to high variance in the parameter estimates of the generative model, the Naive Bayes method’s accuracy will suffer because it has small set of labelled training data. This problem can be overcome by augmenting this labelled training data with a large set of unlabelled data and combining the two sets with EM, we can improve the parameter estimates. EM is used for maximum likelihood or maximum posterior estimation in problems with incomplete data and is class of iterative algorithms.
Biological data is modelled using probabilistic models, such as HMM (Hidden Markov Model) or Bayesian networks which are efficient and robust procedures for learning parameters from observations. There are various sources for missing values such as in medical diagnosis, missing data for certain tests or gene expression clustering due to intentional omission of gene-to-cluster assignments in the probabilistic model. Such error does not occur in EM algorithm.
Consider an example of a simple coin-flip-ping experiment in which we are given a pair of unknown biases θA and θB of coins A and B respectively that is, on any given flip, coin A will land on tails with probability 1– θA and heads with probability θA and similarly for coin B. The goal is to estimate θ = (θA, θB) by repeating the following procedure five times: with equal probability, choose one of the two coins randomly and perform with the selected coin ten independent tosses. Thus, a total of 50 coin tosses are performed. During the experiment, suppose the track of two vectors x = (x1, x2, …, x5) and z = (z1, z2,…, z5) is kept, during the ith set of tosses where the identity of the coin used is zi ∈ {A,B},where xi ∈ {0,1,…,10} is the number of heads
observed. In the model the parameter estimation setting also known as complete data case in which the values of all relevant random variables which consists of each coin flip and type of coin used for flip are known. Here, a simple way to estimate θA and θB is to return the observed proportions of heads for each coin:
and
It is known as maximum likelihood estimation which assesses the quality based on the probability assigned to the observed data of statistical model. The formulae are solved for the parameters ( , ) ,that maximize If logP(x, z; θ) is the logarithm of the joint probability obtained for any particular vector of observed head counts x and coin types z.
Now consider the parameter estimation problem in which the recorded head counts x are given and z as hidden variables or latent factors is not given. Such parameter estimation setting is known as the incomplete data case. The coin used for each set of tosses is unknown, thus the computation of heads for each coin is no longer possible.This problem, the parameter estimation can be reduced with incomplete data to maximum likelihood estimation with complete data, if there was some way of guessing the correctly used coin from each of five sets. One of the iterative schemes used to obtain completions could work as follows: start from some initial
parameters, , determine whether coin A or coin B from each five sets was
more likely to have generated the observed flips . Then, assume the guessed coin to be correct, and apply the estimation procedure of regular maximum likelihood to get . Finally, repeat these two steps until convergence. As the quality of the resulting completions improves so the estimated model also improves.
The expectation maximization algorithm oscillates between the steps of guessing the model depending on probability distribution over completions of missing data (known as the E-step) and then re-estimating the model parameters using these completions (known as the M-step). The name ‘E-step’ needs only to compute expected statistics over completions rather than explicitly forming probability distribution over completions. Similarly, the name ‘M-step’ consists of model re-estimation which can be thought of as ‘maximization’ of the expected log-likelihood of the data. [8, 9]
After training the deep neural networks as described in Section 4, we evaluate them using three measures, each more conclusive and computationally expensive than the last.
The first metric, presented in Section 5.1, checks that a trained model can accurately predict whether a clause was used in the final proof. This accuracy test is done on a holdout set of proof tasks from the training data.
Next, in Section 5.2, we run E prover over the same holdout set of 9,159 FOL proof tasks from Kaliszyk & Urban (2015a) using the trained networks to help guide the clause selection. These theorems all have at least one ATP proof using classical search heuristics, and 96.6% of these theorems can be proved using one of E prover’s built-in automated heuristics, so there is little room for improvement on this dataset. However, it allows us to perform a sanity check that the neural nets aren’t doing more harm than good when added to the auto heuristic.
The final test is the most interesting but computationally expensive: do these deep network guided selection heuristics allow us to prove Mizar statements that do not yet have any ATP proof? For this, we use a corpus of 25,361 theorems from Mizar for which no proof was found in Kaliszyk & Urban (2015a). Because this is the most computationally expensive task, we only run it using the networks that performed best on the previous tasks. In Section 5.3, we present the models that, in aggregate, allow us to find proofs for 7.36% of these “hard statements”.
An important class of sequential value of information problems is the class of Bandit problems. In the classical k-armed bandit problem, as formalized by Robbins (1952), a slot machine is given
with k arms. A draw from arm i results in a reward with success probability pi that is fixed for each arm, but different (and independent) across each arm. When selecting arms to pull, an important problem is to trade off exploration (i.e., estimation of the success probabilities of the arms) and exploitation (i.e., repeatedly pulling the best arm known so far). A celebrated result by Gittins and Jones (1979) shows that for a fixed number of draws, an optimal strategy can be computed in polynomial time, using a dynamic programming based algorithm. While similar in the sense that an optimal sequential strategy can be computed in polynomial time, Gittins algorithm however has different structure from the dynamic programming algorithms presented in this paper.
Note that using the “function optimization” objective function described in Section 2.1, our approach can be used to solve a particular instance of bandit problems, where the arms are not required to be independent, but, in contrary to the classical notion of bandit problems, can not be chosen repeatedly.
In this work, we initiate the study of clustering under local stability. We define local perturbation resilience, a property of a single optimal cluster rather than the instance as a whole. We give algorithms that simultaneously achieve guarantees in the worst case, as well as guarantees when the data is stable.
Specifically, we show that local search outputs the optimal (3 + )-LPR clusters for k-median and the (9 + )-LPR clusters for k-means. For k-center, we show that any 2-approximation outputs the optimal 2-LPR clusters, as well as the optimal (3, )-LPR clusters when assuming the optimal clusters are not too small. We provide a natural modification to the asymmetric k-center approximation algorithm of Vishwanathan [38] to prove it outputs all 2-SLPR clusters. Finally, we show APX-hardness of clustering under (α, )-approximation stability for any α ≥ 1, > 0. It would be interesting to find other approximation algorithms satisfying the condition in Lemma 7, and in general to further study local stability for other objectives and conditions.
Algorithms that build lexical chains consider one by one words for inclusion in the chains
constructed so far. Important parameters to consider are the lexical resource used, which
determines the lexicon and the possible relations between the words, called thesaural relations
by Morris and Hirst (1991), the thesaural relations themselves, the transitivity of word relations
and the distance — measured in sentences — allowed between words in a chain (Morris and
Hirst, ibid.).
Our lexical chain building process builds proto-chains, a set of words linked via thesaural
relations. Our implementation refines the proto-chains to obtain the final lexical chains. We
summarize the lexical chain building process with these five high levels steps:
1. Choose a set of thesaural relations;
2. Select a set of candidate words;
3. Build all proto-chains for each candidate word;
4. Select the best proto-chains for each candidate word;
5. Select the lexical chains.
Statistical Machine Translation (SMT) has been the dominant translation paradigm for decades [3, 4, 5]. Practical implementations of SMT are generally phrase-based systems (PBMT) which translate sequences of words or phrases where the lengths may differ [25].
Even prior to the advent of direct Neural Machine Translation, neural networks have been used as a component within SMT systems with some success. Perhaps one of the most notable attempts involved the use of a joint language model to learn phrase representations [13] which yielded an impressive improvement when combined with phrase-based translation. This approach, however, still makes use of phrase-based translation systems at its core, and therefore inherits their shortcomings. Other proposed approaches for learning phrase representations [7] or learning end-to-end translation with neural networks [23] offered encouraging hints, but ultimately delivered worse overall accuracy compared to standard phrase-based systems.
The concept of end-to-end learning for machine translation has been attempted in the past (e.g., [8]) with
limited success. Following seminal papers in the area [39, 2], NMT translation quality has crept closer to the level of phrase-based translation systems for common research benchmarks. Perhaps the first successful attempt at surpassing phrase-based translation was described in [30]. On WMT’14 English-to-French, this system achieved a 0.5 BLEU improvement compared to a state-of-the-art phrase-based system.
Since then, many novel techniques have been proposed to further improve NMT: using an attention mechanism to deal with rare words [36], a mechanism to model translation coverage [40], multi-task and semi-supervised training to incorporate more data [14, 28], a character decoder [9], a character encoder [11], subword units [37] also to deal with rare word outputs, different kinds of attention mechanisms [29], and sentence-level loss minimization [38, 33]. While the translation accuracy of these systems has been encouraging, systematic comparison with large scale, production quality phrase-based translation systems has been lacking.
In this paper we have presented a simple and effective LSTM transition-based dependency parser. Its performance rivals that of far more complicated approaches, while still being capable of integrating with minimal changes to their architecture.
Additionally, we showed that the application of dropout to the input layer can improve the performance of a network. Like our other contributions here this is simple to apply to other models and is not only limited to the architectures presented in this work.
Finally, we proposed a method of using pretrained FFNs as initializations for an RNN-based model. We showed that this approach can produce gains in accuracy for both LSTMs and Elman networks, with the final LSTM model surpassing or matching most state-of-the-art LSTM-based models.
This initialization method can potentially be applied to any LSTM-based task, where a 1-to-1 relation between inputs can first be modelled using an FFN. Exploring the effects of this method on other tasks is left for future work.
In this section, we evaluate the proposed SMLM for the optimization of complex loss function. Three different applications are considered, which are aircraft event recognition, intrusion detection in wireless mesh networks, and image classification.
3.1 Aircraft event recognition
Recognizing aircraft event of aircraft landing is an important problem in the area of civil aviation safety research. This procedure provides important information for fault diagnosis and structure maintenance of aircraft [36]. Given a landing condition, we want to predict if it is normal and abnormal. To this end, we extract some features, and use them to predict the aircraft event of normal or abnormal. In this experiment, we evaluate the proposed algorithm in this application, and use it as a model for the prediction of aircraft event recognition.
Before presenting the actual patterns for date expressions (<DATE-EXPR>), we first present patterns for auxiliary constructs of <DAY-EXPR>, <MON-EXPR>, and <YEAR-EXPR> which are in turn used within the <DATE-EXPR> patterns. The patterns are presented as regular expressions where ? denotes zero or one, * denotes zero or more, | denotes the OR operator and parentheses are for grouping purposes. The patterns may include both the classes of lexical entries, described in the previous section, and rarely individual entries themselves, like yıl (‘year’), sene (‘year’), ay (‘month’), gün (‘day’), and saat (‘hour’).
Though not denoted in the patterns, there are also constraints, regarding the lexical entries, that should be enforced during the utilization of the patterns. For instance, the <NUM> values within the <DAY-EXPR> should be within the range of [1..31] while the <NUM> values within the <MON-EXPR> should be within the range of [1..12].
<DAY-EXPR> → (<NUM><APST> | (<ORD> | <DAY>) gün)<SUF>*
<MON-EXPR> → <MON><APST><SUF>* | (<ORD> | <MON>) ay)<SUF>*
<YEAR-EXPR> → <NUM> ((yıl | sene)<SUF>*)?
Below provided are some wide-coverage patterns for extracting date expressions in Turkish.
<DATE-EXPR> → (<NUM>.<NUM>.<NUM> | <NUM>/<NUM>/<NUM>) (1)
<DATE-EXPR> → <NUM>? <MON> <NUM>? <DAY>? (2)
<DATE-EXPR> → <YEAR-EXPR> <MON-EXPR>? <DAY-EXPR>? (3)
<DATE-EXPR> → <YEAR-EXPR> <NUM> (<MON><SUF>* | <MON> <DAY>?) (4)
<DATE-EXPR> → <MON-EXPR> <DAY-EXPR>? (5)
<DATE-EXPR> → <MOD>? (<T-UNIT> | <DAY> | <MON> | <SEAS>) (6)
<DATE-EXPR> → <DEIC> (7)
Sample date instances conforming to some of these patterns are given in Table 1. In this table and the other tables in the current paper, the first column shows the Turkish samples, the second column shows their meanings in English, the third column shows the TIMEX3 annotation of the sample, and the fourth column shows the number of the pattern that the sample conforms to. For the sample in the second to last row of Table 1, the normalized value is given with respect to a reference date in the year 2015.
The authors would like to thank to professors Dan Cristea, Grigor Moldovan and Ioan Andone for their advices and observations during the research activities.

3D movies and videos have now gained increased popularity. An extra depth channel in addition to the RGB channels may be handily available sooner or later. Since our OSSVM framework can be easily extended to handling the extra depth dimension, we also apply our method to RGB-D data for evaluation. The segmentation rectification model for RGB-D data is the same as Eq. (11), except that the potential is defined as
ΨRGBD =  ∑ pp′ w eRGB pp′ ∣∣f tp − f tp′ ∣∣, {p, p′} ∈ N∑ pp′ w eD pp′
∣∣f tp − f tp′ ∣∣, {p, p′} ∈ N∑ pq(1− htq)f tp, {p, q} ∈ Nfh∑ pq h t q(1− f tp), {p, q} ∈ Nfh
 , (19)
9 FB C la ss ifi er [6 ]+ M at ti ng
FB C
la ss
ifi er
+ U
W G C A do be R ot ob ru
sh O ur m et ho d (2 C SS V M
) O ur m et ho d (O SS V M )
Fig. 13: Results of fully automatic segmentation propagation for the “Catwalk” sequence on frames 1 (left), 6 (middle) and 19 (right), given keyframe segmentation. The background is darkened for visualization.
weDpp′ = { exp(−5IeD(p, p′)), IeD(p, p′) 6= 0 20, Otherwise , (20)
where IeD is the edge map from depth channel, weRGBpp′ = wepp′ has been defined in Eq. (10).
Similar to the RGB case, the OSSVM model for RGB-D is
min w,~ε
1 2 ‖w‖2 + C N ( N∑ k=1 εk ) − wedgeRGB
s.t.: ∀k, w ·ΨRGBD(xk, f∗k ) ≤ −1 + εk, wedgeRGB ≤ w
edge D ,∑
i
wi = 1,w ≥ 0,
(21)
in which we added one additional constraint to the model to represent our prior that the edge term from depth is more reliable than that from RGB values and this reformulation does not require additional free parameters.
The dataset we used is from the INRIA 3D movie dataset [33]. Since a number of sequences in the original dataset contain very dark or motion-blurred objects, we select a subset containing 22 sequences with identifiable object boundaries in RGB domain for this experiment. Note that visually identifiable boundaries are required in the context of video cutout and for ground truth delineation. There are a total of 835 frames in the selected subset. Besides, the original dataset only provides the keyframe segmentations. Therefore, we manually cutout each frame.
10
The visual comparison of results from the related methods are shown in Figure 16. From the visual results, we can observe that the results are comparable and our OSSVM on RGB-D outperforms others in general. The quantitative results are shown in Figure 17 and Table 3, which further validated our observation. The first impression is that the overall errors are only around 3 pixels small for many of the methods up to 10 subsequent frames. Besides, we see that our OSSVM on RGB-D and 2CSSVM on RGB-D are very comparable and OSSVM is still better than the other methods.
The comparable performance is due to that the object/background motion in the videos in this dataset are often either very small or abrupt, and the method may perform equally good or bad on most of them. The RGB-D
video cutout system would further benefit from a dedicated RGB-D object classifier which is beyound the scope of this paper.
We compared the proposed confidence measure re-estimation algorithm with the baseline system for the three STD tasks. The baseline system directly adopted the ASR posterior score as the confidence measure for each query term. Keywordspecific threshold was applied for all systems as the final decision recall method [16]. Experimental results are listed in Table 2. We can see that the proposed confidence reestimation approach achieves consistent improvements for all the three typical speech retrieval tasks. Considering the amount of training data available in these three tasks, the re-
sults in Table 2 also indicate that the proposed confidence reestimation method is neither language-dependent, nor sensitive to the amounts of training resources.

ar X
iv :1
01 2.
30 18
v1 [
cs .A
I] 1
4 D
Temporal Logic Model Checking is a verification method in which we describe a system, the model, and then we verify whether some properties, expressed in a temporal logic formula, hold in the system. It has many industrial applications. In order to improve performance, some tools allow preprocessing of the model, verifying on-line a set of properties reusing the same compiled model; we prove that the complexity of the Model Checking problem, without any preprocessing or preprocessing the model or the formula in a polynomial data structure, is the same. As a result preprocessing does not always exponentially improve performance.
Symbolic Model Checking algorithms work by manipulating sets of states, and these sets are often represented by BDDs. It has been observed that the size of BDDs may grow exponentially as the model and formula increase in size. As a side result, we formally prove that a superpolynomial increase of the size of these BDDs is unavoidable in the worst case. While this exponential growth has been empirically observed, to the best of our knowledge it has never been proved so far in general terms. This result not only holds for all types of BDDs regardless of the variable ordering, but also for more powerful data structures, such as BEDs, RBCs, MTBDDs, and ADDs.
In this section, we propose our algorithm Extended-Learn, discuss its technical steps, and prove its regret bounds. The main idea of our algorithm is to maintain a distribution over all instances of the objects by keeping track of an evolving point wt = (vt,xt,λt) in the augmented formulation space W through trials t = 1, . . . , T . The main structure of Extended-Learn is show in Algorithm 1. Similar to [13], our algorithm consists of three main technical parts:
1. Prediction: Predict with an instance γt−1 ∈ H such that E [ γt−1 ] = vt−1
2. Update: Update the mixture wt−1 to ŵt−1 according to the incurred loss multiplicatively.
3. Projection: Project the updated mixture ŵt−1 back to the polytope W and obtain wt.
In the prediction step, which is discussed in 4.1, we draw an instance probabilistically from the distribution latent in wt−1 ∈ W such that it has the same expected value as vt−1 ∈ V . For the update step, having defined Lt = (ℓt,0,0), the updated ŵt−1 is obtained from a trade-off between the linear loss and the unnormalized relative entropy [13]. :
ŵ t−1 = argmin
w∈Rn+2m
∆(w||wt−1) + ηw · Lt
2The reader will see having affine subspaces allows us to take an efficient projection approach, namely iterative Bregman projections, in Section 4.2
It is fairly straight-forward to see:
∀i ∈ [n+ 2m], ŵt−1i = wt−1i e−η L t i −→

 
 
∀i ∈ [n], v̂t−1i = vt−1i e−η ℓ t i ∀i ∈ [m], x̂t−1i = xt−1i ∀i ∈ [m], λ̂t−1i = λ̂t−1i
In the projection step, for which we propose an iterative approach in 4.2, we obtain wt, the Bregman projection of ŵt−1 back to the augmented formulation space W ,
wt = argmin w∈W ∆(w||ŵt−1)
Algorithm 1 Extended-Learn
1: w0 ← q ∈ W – a proper prior distribution discussed in 4.3 2: For t = 1, . . . , T 3: wt−1 = (vt−1,xt−1,λt−1) ∈ W 4: Execute Prediction(wt−1) and get a random instance γt−1 ∈ H s.t. E [ γt−1 ] = vt−1 5: Incur a loss γt−1 · ℓt 6: Update ŵt−1 = (v̂t−1, x̂t−1, λ̂ t−1 ): 7: Keep unchanged x̂t−1 ← xt−1, λ̂t−1 ← λt−1 8: Update v̂t−1 as v̂t−1i ← vt−1i e−η ℓ t i 9: Execute Projection(ŵt−1) and obtain wt which is wt = argmin w∈W ∆(w||ŵt−1)
We minimize the higher order CRF defined earlier in Eq.(11) by decomposing the problem into sub-modular terms. Kohli et al. [21] showed that ro-
bust higher order functions can be found by expanding or replacing the clique function using auxiliary binary terms. Optimal displacements are calculated for these algorithms in order to minimize the clique potentials. Therefore, the high-order displacement terms are transformed into sub-modular quadratic functions, and are optimized based on graph cuts. The minima of the energy function represents the solution for the tumor segmentation based on a labelling process which assigns a class at voxel of the image, generating tumor and normal tissue segmentations.
Finally we apply a Primal-Dual algorithm called FastPD [27] which can efficiently solve the problem in a discrete domain by formulating the duality theory in linear programming. Compared to other methods which do not provide optimal solutions or require long computational times to approximate the global minimum, the advantage of FastPD lies in its generality and efficient computational speed. It also guarantees that the generated solution will be the best approximation of the true global optimum without the condition of linearity.
We conduct the performance evaluations on several diverse data collections, i.e., two text corpora, three image databases and three gene expression data. Their important statistics are summarized in Table I and the brief descriptions about them are shown below.
Text corpora. NGroups5 is selected from the popular newsgroup data collection 20Newsgroups1. We use a subset of the 20news-bydate version, which removed the duplicates and some headers. This subset contains five different topics, which refer to 4,052 documents. RCV1-5 is a subset chosen from the Reuters Corpus Volume I2 (RCV1) collection [27]. We choose five topics contained in a smaller RCV1 database [13], and this subset is associated with the ‘M141’,‘GCAT’,‘G151’,‘G158’, ‘G159’ topics. For the two data sets, we preprocess them using a standard feature selection mechanism in [35] for text data, i.e., we select 1,200 words with the highest contribution to the mutual information between the words and the documents [14].
Image databases. The COIL203 image library contains 20 different objects viewed from varying angles. Each object has 72 gray scale images with the size of 32×32. The AlphaDigit4 is a handwritten image database that contains 1,404 binary images, covering 20×16 digits of “0” through “9” and capitals “A” through “Z”. UMIST5 is a face image database that contains 575 multi-view face images of 20 people, referring to a range of poses from profile to frontal views. Each image is rescaled to 28×23 pixels.
1http://people.csail.mit.edu/jrennie/20Newsgroups/ 2http://www.daviddlewis.com/resources/testcollections/rcv1/ 3http://www1.cs.columbia.edu/CAVE/software/softlib/coil-20.php 4http://cs.nyu.edu/∼roweis/data.html 5http://images.ee.umist.ac.uk/danny/database.html
Gene expression data. The three microarray gene expression data6 Leukemia2, LungCancer and SRBCT are often used to do multicategory cancer diagnosis in bioinformatics [36]. Leukemia2 and LungCancer are produced by oligonucleotide-based technology. SRBCT was obtained by using two-color cDNA platform with consecutive image analysis. Similar to [28], we removed the genes which vary little across samples so as to reduce the computational complexity. Table II shows the exact preprocessing strategy of the gene expression data. Details about each term can be referred to [28]. No operation is conducted on SRBCT since it has been originally preprocessed in [25].
Notice that all data sets used here are normalized to unit Euclidean length in prior. Besides, the entries of each input data matrix are nonnegative, thereby all compared algorithms can be applied to yield the clustering results.
Cette variante consiste à retourner le nombre de mots communs entre les unités lexicales (mots pleins) du contexte du mot à désambiguïser et les traits sémantiques des définitions de chaque sens candidat. Navigli (2009) décrit cette variante. Dans nos expériences, le contexte représente le paragraphe. La fonction utilisée pour mesurer la similarité sémantique se
présente par : 𝐿𝑒𝑠𝑘𝑉𝑎𝑟𝑖𝑎𝑛𝑡𝑒 = │𝑐𝑜𝑛𝑡𝑒𝑥𝑡𝑒(𝑤) ∩ 𝐷(𝑆𝑖 (w))│où 𝑤 est le mot à désambiguïser et 𝑆𝑖 est l’IIème sens du mot 𝑤. Un problème important dans la mesure de Lesk est qu’elle est très sensible aux mots présents dans les définitions. Une absence des mots importants dans les définitions retourne des résultats qui ne sont pas de très bonne qualité. L’une des améliorations proposées pour ce problème est l’algorithme de Lesk étendu.
In this paper, we propose an end-to-end query-by-example STD model based on an attention-based multi-hop network which has been successfully used in QA. The model can be trained in either an supervised or unsupervised fashion. In the supervised scenario, we show that attention and multiple hops are both very helpful, and that the attention weights of the proposed model reveal the time span of the input keyterm. In the unsupervised setting, the neural network mimics DTW behavior, and achieves performance comparable to DTW with shorter runtimes. In the future, we will explore more new attention-based models, and investigate new models which directly output time spans instead of a confidence score. We will also apply the attention-based multi-hop model on STD with text query.
8We implemented DTW in C++ and the network using Tensorflow. On average, without using a GPU, the proposed approach was 7 times faster than DTW.
Fig. 3 shows the framework of a convolutional gated recurrent neural network for audio tagging. The CNN is regarded as the feature extractor along the short window (e.g., 32ms) from the basic features, e.g., MFBs, spectrograms or raw waveforms. Then the robust features extracted are fed into the GRU-RNN to learn the long-term audio patterns. For the audio tagging task, there is a lot of background noise, and acoustic events may occur repeatedly and randomly along the whole chunk (without knowing the specific frame locations). The CNN can help to extract robust features against the background noise by the max-pooling operation, especially for the raw waveforms. Since the label of the audio tagging task is at the chunk-level rather than the frame-level, a large number of frames of the context were fed into the whole framework. The GRU-RNN can select related information from the longterm context for each audio event. To also utilize the future information, a bi-directional GRU-RNN is designed in this work. Finally the output of GRU-RNN is mapped to the posterior of the target audio events through one feed-forward neural layer, with sigmoid output activation function. This framework is flexible enough to be applied to any kinds of features, especially for raw waveforms. Raw waveforms have lots of values, which leads to a high dimension problem. However the proposed CNN can learn on short windows like the short-time Fourier transform (STFT) process, and the FFTlike basis sets or even mel-like filters can be learned for raw waveforms. Finally, one-layer feed-forward DNN gets the final sequence of GRUs to predict the posterior of tags.
Binary cross-entropy is used as the loss function in our work, since it was demonstrated to be better than the mean squared error in [8] for labels with zero or one values. The loss can be defined as:
E = − N∑
n=1
‖TnlogT̂n + (1− Tn)log(1− T̂n)‖ (6)
T̂n = (1 + exp(−O))−1 (7)
where E is the binary cross-entropy, T̂n and Tn denote the estimated and reference tag vector at sample index n, respectively. The DNN linear output is defined as O before the sigmoid activation function is applied. Adam [30] is used as the stochastic optimization method.
Emojis are used extensively in social media, however little is known about their use and semantics, especially because emojis are used differently over different communities (Barbieri et al., 2016a; Barbieri et al., 2016b). In this paper, we provide a neural architecture to model the semantics of emojis, exploring the relation between words and emojis. We proposed for the first time an automatic method to, given a tweet, predict the most probable emoji associated with it. We showed that the LSTMs outperform humans on the same emoji prediction task, suggesting that automatic systems are better at generalizing the usage of emojis than humans. Moreover, the good accuracy of the LSTMs suggests that there is an important and unique relation between sequences of words and emojis.
As future work, we plan to make the model able to predict more than one emoji per tweet, and explore the position of the emoji in the tweet, as close words can be an important clue for the emoji prediction task.

In this section we describe a method for scaling inference to large datasets. In particular, we use a local search method based on the Metropolis-Hastings (MH) algorithm with a canopy-based proposer. We briefly describe MH and canopies, then present our jump function.
MH is a Markov-chain Monte Carlo method that stochastically performs local changes by probabilistically accepting jumps from a proposal distribution Q conditioned on the current state y producing a new configuration y′. The proposed configuration y′ is accepted with probability α = π(y′|x)/π(y|x)× q(y|y′)/q(y′|y), where π is the distribution encoded by the model (see Eq 1), and q is the probability of proposing the jump to y′. Since we are performing maximum a posteriori (MAP) with no latent variables, we can safely ignore the ratio containing q [21].
In order to avoid inefficiencies arising from unnecessary exploration, we inject the following domain-specific knowledge into the proposal distribution. First, the proposal distribution is designed so that inference explores only the space of valid configurations (settings to the hidden variables that result in an invalid clustering are not considered). Second, we use the idea of canopies to propose jumps that are more likely to be accepted by the model, and that lead to high-scoring configurations.
A canopy is a relaxation of a clustering where mentions can refer to more that one entity (in other words the transitivity assumption is not enforced and mentions can be in more than one cluster) [20]. Formally, we define a canopy C as a set of mention sets. Typically, canopies are constructed so that mentions occurring in the same set are highly likely to be coreference, for example, they can be constructed such that all mentions in the same set are within a certain cosine threshold of each other.
Let Γ = {Ci} be a set of canopies. Also, let the notation t ∼ρ T mean to draw an element t from a set T with probability distribution ρ : T → [0, 1] s.t. ∑ t∈T ρ(s) = 1. The class of proposal distributions based on canopies is defined as follows:
1: Input: current configuration y 2: Output: proposed configuration y′ 3: C ∼ρΓ //pick a random canopy 4: S ∼ρC //pick a set of mentions from canopy 5: //pick a new entity for mention ma ∼ρ S mb ∼ρ S ma.e← mb.e // move ma to mb’s entity 6: return y′
In practice we take ρ to be uniform in distribution.In our implementation we use the last name and canonical name clusterings from Section 3.4 as the set of canopies Γ . We further enrich the above proposal distribution to create new entities and explore random configurations. Specifically, with 20% probability, we pick a random mention and move it to a random entity (this entity may be an empty one), and with 80% probability we run the canopy proposer.
Note that for each step, the jump function moves a single mention from one entity to another; each jump changes the settings of only three hidden variables. Therefore, even with factors over entire entities, evaluating the MH acceptance score requires computing as many factors as the number of total mentions in the two entities.
G an
es an
et al
. (2
01 2)
S im
R D
A G
x x
M o d
ifi ca
ti o n
o f
W u
a n
d P
a lm
er (1
9 9 4 )
m ea
su re
to av
o id
ca se
s in
w h
ic h
n ei
g h b
o u rs o f a co n ce p t m ig h t h av e h ig h er si m il a rit y va lu es th a n co n ce p ts w h ic h a re o rd er ed w it h it
. T ab le 3. 2 : S em a n ti c si m il a ri ty m ea su re s o r ta x o n o m ic d
is ta
n ce s d efi n ed u si n g a st ru ct u ra l a p p ro a ch . T h es e m ea su re s ca n b e u se d to co m p ar e a p a ir o f co n ce p ts d efi n ed in a ta x o n o m y o r a n y p a ir o f el em en ts d efi n ed in a p a rt ia ll y o rd er ed se t
Stochastic optimization has received lots of attention recently in data mining and machine learning communities (Hu et al., 2009; Shalev-Shwartz et al., 2009; Zhang et al., 2013). It is generally formulated as a minimization problem where the objective is an expectation over unknown distributions (Nemirovski et al., 2009), or defined in terms of the access model, which assumes there is a stochastic oracle that generates unbiased estimate of the objective or its gradient (Hazan and Kale, 2011). Algorithms for stochastic optimization, such as the stochastic gradient descent (SGD), utilize stochastic gradients of the objective for updating. Those methods have lightweight computation per iteration, and are widely used to reduce the time complexity of optimization problems.
In this paper, we take a different perspective and exploit stochastic optimization as a tool to reduce the space complexity for certain optimization problem over matrices. Specifically, we study the nuclear norm regularized convex optimization problem
min W∈W⊆Rm×n
F (W ) = f(W ) + λ‖W‖∗ (1)
Algorithm 1 SPGD for nuclear norm regularization Input: The number of trials T , and the regularization parameter λ
1: Initialize W1 = 0 2: for t = 1, 2, . . . , T do 3: Construct a low-rank stochastic gradient Ĝt = AtB ⊤ t of f(·) at Wt 4: Calculate Wt+1 according to (2) 5: end for 6: return WT+1
where W is the domain of the solution, f(·) is a convex function, and ‖ · ‖∗ is the nuclear norm of matrices. Note that the above problem is a special case of convex composite optimization (Nesterov, 2013). Due to the nuclear norm regularizer, the optimal solution W∗ to (1) is a low-rank matrix, provided λ is large enough. Here, we consider the large-scale setting: “both m and n are very large such that directly storing an m×n matrix in memory is impossible”. This setting excludes deterministic methods for solving (1) (Nesterov, 2013), as they need to evaluate the gradient of f(·), which needs O(mn) memory. Although various stochastic algorithms has been proposed for convex composite optimization and can also be applied to (1), their primal goal is to reduce the time complexity of evaluating the gradient of f(·) instead of the space complexity (Lin et al., 2011; Lan, 2012). As a result, most of them still have an O(mn) space requirement.
The only memory-efficient algorithm for solving (1) that we found in the literature is a heuristic algorithm developed by Avron et al. (2012). Under the condition that it is possible to generate a stochastic gradient of f(·) which is also low-rank, they propose to combine SGD and truncated SVD to solve (1). By representing all the intermediate solutions and stochastic gradients in low-rank factorization forms, the space complexity can be reduced to O(m+n). The major limitation of this approach is that there is no theoretical guarantee about its convergence, due to the fact that the truncated SVD operation introduces a constant error in each iteration. Furthermore, when the objective value is difficult to calculate, it is also unclear which iterate should be used as the final solution.
Inspired by the previous studies, we develop a novel stochastic algorithm that optimizes (1) in a memory-efficient way and is supported by formal theoretical guarantees. Specifically, we propose to use stochastic proximal gradient descent (SPGD) instead of SGD to solve (1), and take the last iterate of SPGD as the final solution. Similar to the heuristic algorithm of Avron et al. (2012), the proposed algorithm always maintains a low-rank factorization of iterates that can be conveniently held in memory, and both the time and space complexities in each iteration are O(m+n). Built upon recent progresses in stochastic optimization (Rakhlin et al., 2012; Shamir and Zhang, 2012), we further analyze the convergence property of the proposed algorithm, and show that the last iterate has an O(log T/ √ T ) convergence rate for general convex functions, and an O(log T/T ) rate for strongly convex functions.
2. The Algorithm
Our algorithm is based on the following observation. For any W ∈ W, it is always possible to generate a low-rank matrix Ĝ represented as AB⊤ such that
E[Ĝ] = E[AB⊤] ∈ ∂f(W )
and A and B occupy O(m) and O(n) memory, respectively. The existence and construction of such a low-rank matrix can be found in previous studies (Avron et al., 2012; Chen et al., 2014), and will be discussed later.
Denote by Wt the solution at the t-th iteration, and let Ĝt = AtB ⊤ t be the low-rank stochastic gradient of f(·) at Wt. Then, we update the current solution by the SPGD, which is a stochastic variant of composite gradient mapping Nesterov (2013)
Wt+1
=argmin W∈W
1 2 ‖W −Wt‖2F + ηt〈W −Wt, Ĝt〉+ ηtλ‖W‖∗
=argmin W∈W
1
2
∥∥∥W − ( Wt − ηtĜt )∥∥∥ 2
F + ηtλ‖W‖∗
(2)
where ηt > 0 is the step size. Due to the presence of the nuclear norm regularizer, all the iterates Wt’s tend to be low-rank matrices. The fact that both Wt and Ĝt are low-rank matrices can in turn be exploited to accelerate the updating.
In the beginning, we simply set W1 be the zero matrix, and take the last iterate WT+1 as the final solution. The complete procedure is summarized in Algorithm 1.
Our setup aims at directly transferring knowledge between image and text representations and the main goal is to find reusable sentence embeddings. We make direct use of paired data, consisting of pictures and text describing them. We propose a model consisting of two separate encoders - one for images and another one for text. An overview of the archiecture is presented in Figure 2.
For the text encoder, we consider three families of models which combine words into text representations. For the bag-of-words (BOW) model, the sentence embedding is simply a normalized sum of vectors corresponding to the individual words. For the RNN model, we create a stacked recurrent neural network encoder (LSTM or GRU based). Finally, for the CNN model, the encoder includes a convolutional layer followed by a fully connected layer, as described in [9].
For encoding images, we use a pre-trained InceptionV3 network [25] which provides a 2048- dimensional feature vector for each image in the dataset (images are rescaled and cropped to 300 x 300 px).
Let Eimg(I) denote the 2048-dimensional embedding vector for an image I and Etxt(S) the N - dimensional embedding for sentence S produced by txt ∈ {BOW,RNN,CNN}. Throughout this paper, we will refer to the cosine similarity of two vectors v1, v2 as sim(v1, v2) = v1·v2‖v1‖‖v2‖ . Informally speaking, our training goal is to maximize sim(Eimg(I), Etxt(S)), when sentence S is paired with (i.e. describes) image I , and minimize this value otherwise.
Formally, let V be the vocabulary, N the embedding dimensionality and txt ∈ {BOW,RNN,CNN} the text encoding model. Let’s define an affine transformation W ∈ R2048×N that transforms the 2048-dimensional image embeddings to N dimensions. Our learnable parameters consist of a word embedding matrix ∈ R|V |×N , the internal parameters of the txt model (when txt is an RNN or CNN encoder), as well as the transformation matrix W . For each batch of image-sentence pairs of the form (I1, S1), ..., (IB , SB), we randomly shuffle the sentences S1, ..., SB , and add the following "incorrect" pairs to the batch (I1, Sσ(1)), ..., (IB , Sσ(B)), with {σ(1), .., σ(B)} a random permutation of {1, ..., B}. If we denote
sim(Ii, Sj) := sim(WEimg(Ii), Etxt(Sj))
then our training goal is to maximize the Pearson correlation ρ(x, y) := Cov(x,y)Std(x)Std(y) between the vectors
[sim(I1, S1), ..., sim(IB , SB)︸ ︷︷ ︸ B correct pairs , sim(I1, Sσ(1)), ..., sim(IB , Sσ(B))︸ ︷︷ ︸ B wrong pairs ]
and [ 1, 1, ..., 1︸ ︷︷ ︸ B positive labels , −1,−1, ...,−1︸ ︷︷ ︸ B negative labels ].
We will denote models trained this way VETE-BOW, VETE-RNN and VETE-CNN, respectively.
In the present study we use the STULONG dataset [42] which is a longitudinal primary preventive study of middleaged men lasting twenty years for accessing the risk of atherosclerosis and cardiovascular health depending on personal and family history collected at Institute of Clinical and Experimental Medicine (IKEM) in Praha and the Medicine Faculty at Charles University in Plzen (Pilsen). The STULONG dataset is divided into four sub-groups namely Entry, Letter, Control and Death. The Entry dataset consists of 1417 patient records with 64 attributes having either codes or results of size measurements of different variables or results of transformations of the rest of the attributes during the first level examination. We utilize the Entry, Control and Death datasets for our predictive modeling. The Entry level dataset is divided into three groups (a) normal group (NG), (b) pathological group (PG), (c) risk group (RG), and (d) not allotted (NA) group, based on the studied group of patients
(KONSKUP) in (1, 2), (5), (3, 4), (6) respectively. We form a new dataset by joining Entry, Control and Death datasets as follows: (i) we write the identification number of a patient (ICO) based on the selection criteria suggested in [43] and determine the susceptibility of a patient to atherosclerosis based on the attributes recorded in Control and Death tables. An individual is considered to have cardiovascular disease if he or she has history of heart disease (i.e., he or she has at least one positive value on attributes such as myocardial infarction (HODN2), cerebrovascular accident (HODN3), myocardial ischaemia (HODN13), silent myocardial infarction (HODN14)), or died of heart disease (i.e., the record appears in the Death table with PRICUMR attribute equal to 05 (myocardial infarction), 06 (coronary heart disease), 07 (stroke), or 17 (general atherosclerosis)). Based on the above definition we divide the Entry dataset in to two datasets DS1 and DS2 depending on whether the patients are in NG or RG group respectively.
This paper has explored ways to speed up planning in SSP problems via goal-independent state and action abstraction. We strengthen existing theoretical results, then provide an algorithm for building abstraction hierarchies automatically. Finally, we empirically demonstrate the advantages of this approach by showing that it works effectively on SSPs of varying size and difficulty.
In this chapter, we present a summary of the thesis, some concluding remarks, and we propose some future work thoughts.
For all experiments, we use a radial basis-function (RBF) kernel as in Liu & Wang (2016), i.e., k(x,x′) = exp(− 1h‖x − x′‖22), where the bandwidth, h, is the median of pairwise distances between current samples. q0(θ) and q0(ξ) are set to isotropic Gaussian distributions. We share the samples of ξ across data points, i.e., ξjn = ξj , for n = 1, . . . , N (this is not necessary, but it saves computation). The samples of θ and z, and parameters of the recognition model, η, are optimized via Adam (Kingma & Ba, 2015) with learning rate 0.0002, and with the gradient approximations in (3), (4), (7), (12) and (13). We do not perform any dataset-specific tuning or regularization other than dropout (Srivastava et al., 2014) and early stopping on validation sets. We set M = 100 and k = 50, and use minibatches of size 64 for all experiments, unless otherwise specified.
Diffusion strategies can also be applied to the solution of distributed state-space filtering and smoothing problems [30, 31, 33]. Here, we describe briefly the diffusion version of the Kalman filter; other variants and smoothing filters can be found in [33]. We assume that some system of interest is evolving according to linear state-space dynamics, and that every node in the network collects measurements that are linearly related to the unobserved state vector. The objective is for every node to track the state of the system over time based solely on local observations and on neighborhood interactions.
Thus, consider a network consisting of N nodes observing the state vector, xi, of size n × 1 of a linear state-space model. At every time i, every node k collects a measurement vector yk,i of size p × 1, which is related to the state vector as follows:
xi+1 = Fixi +Gini (529)
yk,i = Hk,ixi + vk,i, k = 1, 2, . . . , N (530)
The signals ni and vk,i denote state and measurement noises of sizes n× 1 and p× 1, respectively, and they are assumed to be zero-mean, uncorrelated and white, with covariance matrices denoted by
E [ ni vk,i ] [ nj vk,j ]∗ ∆ = [ Qi 0 0 Rk,i ] δij (531)
The initial state vector, xo, is assumed to be zero-mean with covariance matrix
Exox ∗ o = Πo > 0 (532)
and is uncorrelated with ni and vk,i, for all i and k. We further assume that Rk,i > 0. The parameter matrices {Fi, Gi, Hk,i, Qi, Rk,i,Πo} are assumed to be known by node k.
Let x̂k,i|j denote a local estimator for xi that is computed by node k at time i based solely on local observations and on neighborhood data up to time j. The following diffusion strategy was proposed in [30, 31, 33] to evaluate approximate predicted and filtered versions of these local estimators in a distributed manner for data satisfying model (529)–(532). For every node k, we start with x̂k,0|−1 = 0 and Pk,0|−1 = Πo, where Pk,0|−1 is an M × M matrix. At every time instant i, every node k performs an incremental step
followed by a diffusion step:
Time and measurement-form of the diffusion Kalman filter.
Step 1 (incremental update)
ψk,i ← x̂k,i|i−1 Pk,i ← Pk,i|i−1 for every neighboring node ℓ ∈ Nk, update : Re ← Rℓ,i +Hℓ,iPk,iH∗ℓ,i ψk,i ← ψk,i + Pk,iH ∗ ℓ,iR −1 e [ yℓ,i −Hℓ,iψk,i ]
Pk,i ← Pk,i − Pk,iH∗ℓ,iR −1 e Hℓ,iPk,i
end
Step 2 (diffusion update)
x̂k,i|i = ∑
ℓ∈Nk
aℓkψℓ,i
Pk,i|i = Pk,i x̂k,i+1|i = Fix̂k,i|i Pk,i+1|i = FiPk,i|iF ∗ i +GiQiG ∗ i .
(533)
where the symbol← denotes a sequential assignment, and where the scalars {aℓk} are nonnegative coefficients satisfying:
for k = 1, 2, . . . , N :
aℓk ≥ 0, N∑
ℓ=1
aℓk = 1, aℓk = 0 if ℓ /∈ Nk (534)
The above algorithm requires that at every instant i, nodes communicate to their neighbors their measurement matrices Hℓ,i, the noise covariance matrices Rℓ,i, and the measurements yℓ,i for the incremental update, and the intermediate estimators ψℓ,i for the diffusion update. During the incremental update, node k cycles through its neighbors and incorporates their data contributions represented by {yℓ,i, Hℓ,i, Rℓ,i} into {ψk,i, Pk,i}. Every other node in the network is performing similar steps. At the end of the incremental step, neighboring nodes share their updated intermediate estimators {ψℓ,i} to undergo diffusion. Thus, at the end of both steps, each node k would have updated the quantities {x̂k,i|i−1, Pk,i|i−1} to {x̂k,i+1|i, Pk,i+1|i}. The quantities Pk,i|i−1 are n × n matrices. It is important to note that even though the notation Pk,i|i and Pk,i|i−1 has been retained for these variables, as in the standard Kalman filtering notation [5,77], these matrices do not represent any longer the covariances of the state estimation errors, x̃k,i|i−1 = xi − x̂k,i|i−1, but can be related to them [33].
An alternative representation of the diffusion Kalman filter may be obtained in information form by further assuming that Pk,i|i−1 > 0 for all k and i; a sufficient condition for this fact to hold is to requires the matrices {Fi} to be invertible [77]. Thus, consider again data satisfying model (529)–(532). For every node k, we start with x̂k,0|−1 = 0 and P −1 k,0|−1 = Π −1 o . At every time instant i, every node k performs an incremental step followed by a diffusion step:
Information form of the diffusion Kalman filter.
Step 1 (incremental update)
Sk,i = ∑
ℓ∈Nk
H∗ℓ,iR −1 ℓ,i Hℓ,i
qk,i = ∑
ℓ∈Nk
H∗ℓ,iR −1 ℓ,i yℓ,i
P−1k,i|i = P −1 k,i|i−1 + Sk,i
ψk,i = x̂k,i|i−1 + Pk,i|i [ qk,i − Sk,ix̂k,i|i−1 ]
Step 2: (diffusion update)
x̂k,i|i = ∑
ℓ∈Nk
aℓkψℓ,i
x̂k,i+1|i = Fix̂k,i|i Pk,i+1|i = FiPk,i|iF ∗ i +GiQiG ∗ i
(535)
The incremental update in (535) is similar to the update used in the distributed Kalman filter derived in [48]. An important difference in the algorithms is in the diffusion step. Reference [48] starts from a continuous-time consensus implementation and discretizes it to arrive at the following update relation:
x̂k,i|i = ψk,i + ǫ ∑
ℓ∈Nk
(ψℓ,i −ψk,i) (536)
which, in order to facilitate comparison with (535), can be equivalently rewritten as:
x̂k,i|i = (1 + ǫ− nkǫ) · ψk,i + ∑
ℓ∈Nk\{k}
ǫ · ψℓ,i (537)
where nk denotes the degree of node k (i.e., the size of its neighborhood, Nk). In comparison, the diffusion step in (535) can be written as:
x̂k,i|i = akk ·ψk,i + ∑
ℓ∈Nk\{k}
aℓk ·ψℓ,i (538)
Observe that the weights used in (537) are (1 + ǫ − nkǫ) for the node’s estimator, ψk,i, and ǫ for all other estimators, {ψℓ,i}, arriving from the neighbors of node k. In contrast, the diffusion step (538) employs a convex combination of the estimators {ψℓ,i} with generally different weights {aℓk} for different neighbors; this choice is motivated by the desire to employ combination coefficients that enhance the fusion of information at node k, as suggested by the discussion in App. D of [33]. It was verified in [33] that the diffusion implementation (538) leads to enhanced performance in comparison to the consensus-based update (537). Moreover, the weights {aℓk} in (538) can also be adjusted over time in order to further enhance performance, as discussed in [78]. The mean-square performance and convergence of the diffusion Kalman filtering implementations are studied in some detail in [33], along with other diffusion strategies for smoothing problems including fixed-point and fixed-lag smoothing.
We now integrate the aforementioned Nyström approximation approaches and the supervised landmark selection into PCVM and iKFD. The modifications ensure that all matrices are processed with linear memory complexity and that the underlying algorithms have a linear runtime complexity. For both algorithms the initial input is the Nyström approximated kernel matrix with landmarks selected by using one of the formerly provided landmark selection schemes.
In order to use LIBLINEAR for linear classification (Section 3.3), we need to represent clauses as finite feature vectors. For our purposes, a feature vector x representing a clause C is a fixed-
length vector of natural numbers whose i-th member xris specifies how often the i-th feature appears in the clause C.
Several choices of clause features are possible [14], for example sub-terms, their generalizations, or paths in term trees. In this work we use term walks of length 3 as follows. First we construct a feature vector for every literal L in the clause C. We write the literal L as a tree where nodes are labeled by the symbols from Σ. In order to deal with possibly infinite number of variables and Skolem symbols, we substitute all variables and Skolem symbols with special symbols. We count for each triple of symbols ps1, s2, s3q P Σ
3, the number of directed node paths of length 3 in the literal tree, provided the trees are oriented from the root. Finally, to construct the feature vector of clause C, we sum the vectors of all literals L P C.
More formally as follows. We consider a fixed theory T , hence we have a fixed signature Σ. We extend Σ with 4 special symbols for variables (f), Skolem symbols (d), positive literals (‘), and negative literals (a). A feature φ is a triple of symbols from Σ. The set of all features is denoted Feature, that is, Feature “ Σ3. Clause (or literal) features Φ is a multiset of features, thus recording for each feature how many times it appears in a literal or a clause. We use Φ to range over literal/clause features and the set of all literal/clause features (that is, feature multisets) is denoted Features. Recall that we represent multisets as total functions from Feature to N. Hence every member Φ P Features is a total function of the type “Features Ñ N” and we can write Φpφq to denote the count of φ in Φ.
Now it is easy to define function features of the type “Literal Ñ Features” which extracts features Φ from a literal L. For a literal L, we construct a rooted feature tree with nodes labeled by the symbols from Σ. The feature tree basically corresponds to the tree representing literal L with the following exceptions. The root node of the tree is labeled by ‘ iff L is a positive literal, otherwise it is labeled by a. Furthermore, all variable nodes are labeled by the symbol f and all nodes corresponding to Skolem symbols are labeled by the symbol d.
Example 1. Consider the following equality literal L1 : fpx, yq “ gpsko1, sko2pxqq with Skolem symbols sko1 and sko2, and with variables x and y. In Figure 1, the tree representation of L1 is depicted on the left, while the corresponding feature tree used to collect features is shown on the right.
Function features constructs the feature tree of a literal L and collects all directed paths of length 3. It returns the result as a feature multiset Φ.
Example 2. For literal L2 : P pxq we obtain featurespL2q “ tp‘, P,fq ÞÑ 1u. For literal L3 : Qpx, yq we have featuresp Qpx, yqq “ tpa, Q,fq ÞÑ 2u. Finally, for literal L1 from
Example 1 we obtain the following multiset.
t p‘,“, fq ÞÑ 1 , p‘,“, gq ÞÑ 1 , p“, f,fq ÞÑ 2 , p“, g,dq ÞÑ 2 , pg,d,fq ÞÑ 1 u
Finally, the function features is extended to clauses (features : Clause Ñ Features) by multiset union as featurespCq “ Ť
LPC featurespLq.
What remains is to bound the probability that the number of variables included in VI is at most t. First we need an elementary definition:
Definition 3.7. When a variable xi is given different truth assignments in A1 and A2, we call it a type 1 error. When a clause c has all of its marked variables set in both A1 and A2, but in at least one of them is not yet satisfied, we call it a type 2 error.
Note that it is possible for a variable to participate in both a type 1 and type 2 error. In any case, these are the only reasons that a variable is included in VI in an execution of the coupling procedure:
Observation 1. All variables in VI are included either due to a type 1 error or a type 2 error, or both.
Now our approach to showing that VI contains not too many variables with high probability is to show that if it did, there would be a large collection of disjoint errors. First we construct a useful graph underlying the process:
Definition 3.8. Let G be the graph on vertices VI where we connect variables if and only if they appear in the same clause together (any clause from the original formula Φ).
The crucial property is that it is connected:
Observation 2. G is connected
Proof. This property holds by induction. Assume that at the start of the WHILE loop, the property holds. Then at the end of the loop, any variable xi added to VI must have been contained in a clause c that at the outset had one of its variables in VI . This completes the proof.
Now by Observation 1, for every variable in VI we can blame it on either a type 1 or a type 2 error. Both of these types of errors are unlikely. But for each variable, charging it to an error is problematic because of overlaps in the events. In particular, suppose we have two variables xi and xj that are both included in VI . It could be that both variables are in the same clause c which resulted in a type 2 error, in which case we could only charge one of the variables to it. This turns out not to be a major issue.
The more challenging type of overlap is when two clauses c and c′ both experience type 2 errors and overlap. In isolation, each clause would be unlikely to experience a type 2 error. But it could be that c and c′ share all but one of their marked variables, in which case once we know that c experiences a type 2 error, then c′ has a reasonable chance of experiencing one as well. We will get around this issue by building what we call a 3-tree. This approach is inspired by Noga Alon’s parallel algorithmic local lemma [2] where he uses a 2, 3-tree.
Definition 3.9. We call a graph T on subset of VI a 3-tree if each vertex is distance at least 3 from all the others, and when we add edges between vertices at distance exactly 3 the tree is connected.
Next we show that G contains a large 3-tree:
Lemma 3.10. Any maximal 3-tree contains at least |VI | (d+1)dk vertices.
Proof. Consider a maximal 3-tree T . We claim that every vertex xi ∈ VI must be distance at most 2 from some xj in T . If not, then we could take the shortest path from xi to T and move along it, and at some point we would encounter a vertex that is also not in T whose distance from T is exactly 3, at which point we could add it, contradicting T ’s maximality. Now for every xi in T , we remove from consideration at most (d + 1)dk other variables (all those at distance at most 2 from xi in G). This completes the proof.
Now we can indeed charge every variable in T to a disjoint error:
Claim 3.11. If two variables xi and xj in T are the result of type 2 errors for c and c ′, then
vars(ci) ∩ vars(cj) = ∅
Proof. For the sake of contradiction, suppose that vars(ci)∩vars(cj) 6= ∅. Then since c and c′ experience type 2 errors, all of their variables are included in VI . This gives a length 2 path from xi to xj in G, which if they were both included in T , would contradict the assumption that T is a 3-tree.
We are now ready to prove the main theorem of this section:
Theorem 3.12. Suppose that d ≥ k ≥ 20 log d. Then
Pr[|VI | ≥ (d+ 1)dkt] ≤ (3 d )t Proof. Suppose that |VI | ≥ (d+ 1)dkt. Then by Lemma 3.10 we can find a 3-tree T with at least t vertices. The probability of any particular 3-tree on t vertices can be bounded by:( 2
d5 + d (3 7 )k/3)t This is because by Observation 1 each vertex is caused by either a type 1 or type 2 error (or both). Moreover by Claim 3.11 the clauses that cause the type 2 errors for each vertex in T are disjoint. Now the first term in the expression above is the probability of a type 1 error, which follows from Corollary 2.3. The second term follows from the fact that each variable is contained in at most d clauses each of which could cause a type 2 error, from Lemma 3.1 which implies that each clause has at least k/3 marked variables, and again from Corollary 2.3 which implies that for any variable xi, the minimum of the probability it is set to T or to T in either A1 or A2 is conservatively at least 3/7 (because we chose the best coupling).
Now it is well-known (see [17, 2]) that the number of trees of size t in a graph of degree at most D is at most (eD)t. Moreover if we connect pairs of vertices in G that are distance exactly 3 from each other, then we get a new graph H whose maximum degree is at most D = d3k. Thus putting it all together we have that the probability that |VI | > (d + 1)dkt can be bounded by (2k
d2 + d4k (3 7 )k/3)t ≤ (3 d )t where the last inequality follows from the constraint d ≥ k ≥ 20 log d.
Thus we can conclude that with high probability, the number of variables in VI is at most logarithmic. We can now brute-force search over all assignments to count the number of satisfying assignments to either ΦI1 or ΦI2 . The trouble is that we do not have access to the marginal probabilities, so we cannot actually execute the coupling procedure. We will need to circumvent this issue next.
The main difference between the proofs of Theorems 1 and 2 is that the former utilizes the result of Lemma 1 so as to restrict the analysis of the general BDOP class to the case σ ≤ δ logn/n, while the latter utilizes Lemma 2 to restrict the analysis of BitMatchingD problem to the case σ ≤ δ logn/n, where δ is an arbitrary positive constant. Here we only provide a unified proof for the sake of brevity. As mentioned in Section 3.1, the proof contains the analysis related to Propositions B1.1, B1.2, B1.3, B1.4, and B1.5. Here we study the above propositions one after another.
Theorems 1 and 2 are about the (1 + 1) EA which generates a unique offspring individual at each generation. For the sake of simplicity, in the proof, Pm(n, t, 1), which represents the concrete mutation rate employed by the mutation of the parent individual at the tth generation, is written as Pm(n, t) for short, where we omit the offspring index. Similarly, the offspring index will also be omitted when applying the transition lemmas.
Analysis of Proposition B1.1. Since (A2 ∪A1 ∪ F2 ∪B2 ∪ B1 ∪L2) ⊂ (F1 ∪L1), the proof of this sketch is the same to “Analysis of Proposition A1.1” part in the proof of Lemmas 1 and 2.
Analysis of Proposition B1.2. Let U1 = U1(n, σ) = δγ 1/7 logn. According to Chernoff bounds, we know that with an overwhelming probability there are at most U1 flipped bits among the total n bits after the DOP change (which implies that the number of matching bits can decrease or increase by at most U1 after DOP change):
P
(
|N (P ) t −Nt−1| ≥ U1 | Pm(n, t) <
γ1/14 logn
n , σ <
δ logn
n
)
<
(
e
γ1/7
)U1
=
(
e
ω(1)
)ω(logn)
≺ 1
SuperPoly(n) , (24)
where t ∈ N+ is the generation index. On the other hand, concerning the number of matching bits of the offspring
at the tth generation, we obtain another inequality by Chernoff bounds:
P
(
|N (O) t −Nt−1| ≥ U1 | Pm(n, t) <
γ1/14 logn
n , σ <
δ logn
n
)
<
(
e
Θ(γ1/14)
)U1
=
(
e
ω(1)
)ω(logn)
≺ 1
SuperPoly(n) ,
where we utilize the fact that the composite bitwise mapping rate (including both DOP change and mutation) within the tth generation, denoted by r(n, t), satisfies that r(n, t) = (1 − σ)Pm(n, t) + (1 − Pm(n, t))σ = Pm(n, t) + σ − 2Pm(n, t)σ < 2γ 1/14 logn/n (since Pm(n, t) < γ 1/14 logn/n and σ < δ log n/n < γ1/14 logn/n holds).
Noting that Nt ∈ {N (P ) t , N (O) t }, we obtain the following result by combining the above two inequalities together:
P
(
|Nt −Nt−1| ≥ U1 | Pm(n, t) < γ1/14 logn
n , σ <
δ logn
n
)
< P
(
|N (P ) t −Nt−1| ≥ U1 | Pm(n, t) <
γ1/14 logn
n , σ <
δ logn
n
)
+P
(
|N (O) t −Nt−1| ≥ U1 | Pm(n, t) <
γ1/14 logn
n , σ <
δ logn
n
)
≺ 1
SuperPoly(n) .
Consequently, we have
P
(
|Nt −Nt−1| < U1 | Pm(n, t) < γ1/14 log n
n , σ <
δ logn
n
)
≻ 1− 1
SuperPoly(n) ,
thus we have proven Proposition B1.2. Analysis of Proposition B1.3. Let us consider Proposition B1.3a first. Proposition B1.1 tells us that N0 /∈ A2 ∪ A1 ∪ F2 ∪ B2 ∪ B1 ∪ L2 holds with an overwhelming probability. To arrive at A2 ∪ A1 ∪ F2 at some generation (e.g., the tth generation), the EA has two choices when deciding the mutation rate of the tth generation:
1. Small Mutation Rate (SMR): Pm(n, t) < γ 1/14 logn/n;
2. Large Mutation Rate (LMR): Pm(n, t) ≥ γ 1/14 logn/n.
Let us investigate the case of reaching A2∪A1∪F2 by adopting SMR from F1\(A2∪A1∪F2), O1 and L1\L2. Since in Proposition B1.2 we have proven that SMR can only provide relatively smaller increment U1 for the number of
matching bits with an overwhelming probability, we know that the only region that is possible4 to reach A2∪A1∪F2 by adopting SMR, is a subset of F1 \ (A2 ∪ A1 ∪ F2).
According to Proposition B1.2, once SMR is used, the number of matching bits can increase by at most U1 = δγ1/7 log n with an overwhelming probability. Given the condition that Nt−1 ∈ F1\(A2∪A1∪F2), the fact G = ω(U1) (with respect to the problem size n) implies that even the maximal one-generation increment U1 is not valid to jump over the interval A2 with an overwhelming probability. Thus, in this case, the EA must reach some intermediate point belonging to A2 first (otherwise the first hitting time to F2 has already been proven to be super-polynomial, which is the final conclusion of the theorem). In other words, we have proven that one way to reach F2 is to reach A2 first.
Now we consider the case in which the EA reaches A2 ∪ A1 ∪ F2 by adopting LMR. Here two subcases must be considered further. In the first subcases, the offspring resulted from the LMR is not preserved by the selection operator of the EA, instead, the parent is preserved by the selection operator. This subcase can be viewed as adopting SMR with the value of 0 (which will not mutate the parent at all), and thus can be included in the analysis of the last two paragraphs.
In the second subcases, the offspring resulted from the LMR is preserved by the selection operator of the EA. As we have shown in the third sketch in Fig. 3, we want to prove that L2 is the only region that is possible to reach A2 ∪ A1 ∪ F2 by one-generation transition with LMR (and thus L2 is the only region that can reach F2 in one generation adopting LMR). As it is shown in the third sketch in Fig. 3, to prove the above proposition, there are three intervals for us to exclude (prove to be “unlikely”): L1 \ L2,O1 and F1 \ (A2 ∪ A1 ∪ F2). According to Lemma 8, if Nt−1 ∈ O1 ∪ (F1 \ (A2 ∪A1 ∪ F2)) = (n/ log 2 n, n− 3G) and Pm(n, t) ≥ γ 1/14 logn/n, the probability of N (O) t ∈ A2 ∪ A1 ∪ F2 is super-polynomially close to 0. In other words, to prove Proposition B1.3a, we only need to concern the case in which Nt−1 ∈ L1 \ L2 = (4G,n/ log2 n] and Pm(n, t) ≥ γ 1/14 logn/n hold. Given the above two conditions, below we prove that the probability of N (O) t ∈ A2 ∪ A1 ∪ F2 = [n− 3G,n] is super-polynomially close to 0. Given an arbitrary constant h ∈ (0, 1), − if γ1/14 log n/n ≤ Pm(n, t) ≤ 1 − h holds. Given the conditions that σ < δ logn/n and γ
1/14 logn/n ≤ Pm(n, t) ≤ 1− h, by applying Lemma 11 we obtain:
r(n, t) = ω(logn/n),
r(n, t) ≤ max {1
2 , 1− h
}
.
By above inequalities, we estimate the probability that in one generation the EA finds number of matching bits N (O) t = j ∈ A2 ∪ A1 ∪ F2:
P
(
N (O) t = j | Nt−1 = i ∈ L1 \ L2, j ∈ A2 ∪ A1 ∪ F2,
γ1/14 log n
n ≤ Pm(n, t) ≤ 1− h
)
=
min{i,n−j} ∑
k=0
(
n− i
j − i+ k
)(
i
k
)
r(n, t)j−i+2k(1− r(n, t))n−(j−i+2k)
< r(n, t)j−i min{i,n−j} ∑
k=0
(
n− i
n− j − k
)(
i
k
)
=
(
n
n− j
)
r(n, t)j−i < nn−j max {1
2 , 1− h
}j−i
(by Lemma 6 in Appendix)
= no ( n log n ) max {1
2 , 1− h
}n−o(n) = 2o(n) max {1
2 , 1− h
}n−o(n) ≺ 1
SuperPoly(n) ,
which is a super-polynomially small probability. − if Pm(n, t) > 1− h holds (i.e., P(Pm(n, t) > 1− h) = 1). On one hand, given the condition that σ = ω(logn/n)
and Pm(n, t) > 1− h, by applying Lemma 11 we obtain:
r(n, t) > min
{
1 2 ,max { σ, Pm }
}
> min
{
1 2 ,max { σ, 1− h }
}
≥ min {1
2 , 1− h
}
.
On the other hand, we must note the fact called symmetrical bitwise mapping (Fig. 4): given the condition that the number of matching bits Nt−1 equals i and the composite bitwise mapping rate r(n, t), the consequence of the bitwise mapping is equivalent to that of the case in which Nt−1 equals n− i and the composite bitwise mapping rate
4Here “possible” means that the event is with at least a polynomially large probability (There exists a positive polynomial function of the problem size n, such that the probability is no smaller than the reciprocal of the polynomial function).
equals 1− r(n, t). Formally, we have
1− r(n, t) < 1−min {1
2 , 1− h
} = max {1
2 , h
}
.
Noting the fact described above, the following equation holds in response to the so-called symmetrical bitwise mapping:
P
(
N (O) t = j | Nt−1 = i, i ∈ L1 \ L2, j ∈ A2 ∪ A1 ∪ F2, r(n, t)
)
= P
(
N (O) t = j | N ∗ t−1 = n− i, i ∈ L1 \ L2, j ∈ A2 ∪ A1 ∪ F2, r ∗(n, t) < 1− r(n, t)
)
,
where we use r∗(n, t) to represent the notional composite bitwise mapping rate with the value of 1 − r(n, t), N∗t−1 represent the notional number of matching bits (found by the EA) at the end of the (t− 1)th generation.
As shown in Fig. 4, we only need to prove that the probability of reaching A2∪A1∪F2 is super-polynomially close to 0, given the conditions that the notional number of matching bitsN∗t−1 belongs to (L1\L2)
∗ = (n−n/ log2 n, n−4G) and the notional composite bitwise mapping rate r∗(n, t) < max{1/2, h}. According to the value of r∗(n, t), there are two situations.
In the first situation, r∗(n, t) = O(log n/n) holds. According to Chernoff bounds, we know that with an overwhelming probability there are at most γ1/14 logn flipped bits among the total n bits:
P
(
|N (O) t −N ∗ t−1| < γ
1/14 logn | r∗(n, t) = O ( log n
n
)
)
> 1−
(
e
Ω(γ1/14)
)γ1/14 logn
≻ 1− 1
SuperPoly(n) .
Consequently, with an overwhelming probability, the number of matching bits will decrease or increase by at most γ1/14 logn after the overall bitwise mapping (including DOP change and mutation). Given the conditions that N∗t−1 = n − i and i ∈ L1 \ L2, the above upper bound implies that N (O) t < n − 4G + γ 1/14 logn < n − 3G and N (O) t /∈ L2 hold with an overwhelming probability. In other words,
P
(
N (O) t = j | N ∗ t−1 = n− i, i ∈ L1 \ L2, j ∈ A2 ∪ A1 ∪ F2, r
∗(n, t) = O ( logn
n
)
)
≺ 1
SuperPoly(n) .
In the second situation, r∗(n, t) = ω(logn/n) holds. This case can be viewed as imposing a mutation with LMR r∗(n, t) = ω(logn/n) to an individual with its number of matching bits belonging to [n−n/ log2 n, n−4G). According to Lemma 8.3, we have:
P
(
N (O) t = j | N ∗ t−1 = n− i, i ∈ L1 \ L2, j ∈ A2 ∪A1 ∪ F2, r
∗(n, t) = ω ( logn
n
)
)
≺ 1
SuperPoly(n) .
Since there is no essential difference between the proofs related to r∗(n, t) and r(n, t), we will not provide the proof here for the sake of brevity. For details, one can refer to the proof of Lemma 7.3 in the appendix.
Combining the above two situations of r∗(n, t) together, we obtain that:
P
(
N (O) t = j | Nt−1 = i, i ∈ L1 \ L2, j ∈ A2 ∪ A1 ∪ F2, Pm(n, t) > 1− h, σ <
δ logn
n
)
= P
(
N (O) t = j | Nt−1 = i, i ∈ L1 \ L2, j ∈ A2 ∪ A1 ∪ F2, r(n, t) > max
{1
2 , 1− h
}
)
(25)
= P
(
N (O) t = j | N ∗ t−1 = n− i, i ∈ L1 \ L2, j ∈ A2 ∪ A1 ∪ F2, r
∗(n, t) < max {1
2 , h
}
)
≺ 1
SuperPoly(n) ,
where we utilize P(Pm(n, t) > 1− h, σ < δ logn/n) = 1 (Pm(n, t) > 1− h and σ < δ logn/n are preconditions of the analysis here) to obtain Eq. 25.
By applying total probability theorem [36], we further combine the cases γ1/14 logn/n ≤ Pm(n, t) ≤ 1 − h and Pm(n, t) > 1− h together. As a result, we obtain
P
(
N (O) t = j | Nt−1 ∈ L1 \ L2, j ∈ A2 ∪ A1 ∪ F2, Pm(n, t) ≥
γ1/14 logn
n , σ <
δ logn
n
)
≺ 1
SuperPoly(n) .
Till now, the cases of SMR and LMR have been analyzed rigorously, and we know that the probability of reaching F2 from (4G,n− 3G) by any one-generation transition is super-polynomially small. Recall that we have also proven that the EA must reach A2 before reaching A1 ∪ F2 if SMR is used in the one-generation transition that reaches A2 ∪ A1 ∪ F2, we have proven Proposition B1.3a.
The proof of Proposition B1.3b is similar to that of Proposition B1.3a. The major difference between the ideas of the two proofs is that, in Proposition B1.3a the probability of reaching A2 ∪A1 ∪F2 from L2 (given the condition5 that σ < δ logn/n) by one-generation transition is not super-polynomially close to 0, while in Proposition B1.3b the probability of reaching L2 from A2∪A1 (given the condition that σ < δ logn/n) by one-generation transition (including the shift of optimum of BDOP and the mutation for the solution) is super-polynomially close to 0. The brief proofs of the latter proposition, conditional on the general BDOP class (for proving Theorem 1) and the BitMatchingD problem (for proving Theorem 2), are presented respectively.
(For general BDOP class) Noting that the shifting rate σ ≤ δ logn/n, we estimate the following probability by Chernoff bound:
P
(
|N (P ) t −Nt−1| ≥ U1 | σ <
δ logn
n
)
<
(
e
γ1/7
)U1
=
(
e
ω(1)
)ω(logn)
≺ 1
SuperPoly(n) .
Noting that the range between L2 and A2 ∪ A1 is much larger than U1 (i.e., n − 7G > U1), we know that, given the condition that σ < δ logn/n, the probability of reaching L2 from A2 ∪ A1 by the parent of the tth generation is super-polynomially close to 0. Formally,
P
(
N (P ) t ∈ L2 | Nt−1 ∈ A2 ∪ A1, σ <
δ logn
n
)
≺ 1
SuperPoly(n) . (26)
Meanwhile, we estimate the probability that N (O) t ∈ L2, conditional on Nt−1 ∈ A2∪A1 and r(n, t) ≤ 1− 1/ logn, where r(n, t) ≤ 1 − 1/ logn is derived from σ ≤ δ logn/n and Pm(n, t) ≤ 1 − 1/ logn (a condition of Theorem 1) by Lemma 11. Let L−t be the number of flipped matching bits after the DOP change and the mutation at the t th
5Since Lemma 2 has already discuss the case of σ = ω(log n/n), we only need to consider the case of σ < δ logn/n here.
generation, we have
P
(
N (O) t ∈ L2 | Nt−1 ∈ A2 ∪ A1, r(n, t) ≤ 1−
1
logn
)
= P
(
N (O) t ≤ 4G | Nt−1 ∈ [n− 3G,n−G), r(n, t) ≤ 1−
1
log n
)
≤ P
(
n− ( L−t + (n−Nt−1) ) ≤ 4G | Nt−1 ∈ [n− 3G,n−G), r(n, t) ≤ 1− 1
logn
)
= P
(
L−t ≥ Nt−1 − 4G | Nt−1 ∈ [n− 3G,n−G), r(n, t) ≤ 1− 1
logn
)
≤ P
(
L−t ≥ n− 7G | Nt−1 ∈ [n− 3G,n−G), r(n, t) ≤ 1− 1
logn
)
On one hand, if r(n, t) > 14e , then we further have
P
(
N (O) t ∈ L2 | Nt−1 ∈ A2 ∪ A1,
1
4e < r(n, t) ≤ 1−
1
log n
)
≤ P
(
L−t ≥ (1 + ρ1)n ( 1− 1
logn
)
> (1 + ρ1)Ê[L − t ] | Nt−1 ∈ [n− 3G,n−G),
1
4e < r(n, t) ≤ 1−
1
logn
)
< P
(
L−t > (1 + ρ1)Ê[L − t ] | Nt−1 ∈ [n− 3G,n−G),
1
4e < r(n, t) ≤ 1−
1
logn
)
< e−Ê[L − t ]ρ 2 1/4 ≺
1
SuperPoly(n) , (27)
where ρ1 = (n− 7G)/(n−n/ logn)− 1 = Θ(1/ logn) (since G = γ 4/7 logn and γ ≤ n/ logn), Ê[L−t ] = E[L − t | Nt−1 ∈ [n− 3G,n−G), 1/4e < r(n, t) ≤ 1− 1/ logn] = Θ(n), and the last two inequalities is obtained by Chernoff bound. On the other hand, let us consider the case r(n, t) ≤ 14e . By Chernoff bound, we have
P
(
N (O) t ∈ L2 | Nt−1 ∈ A2 ∪A1, r(n, t) ≤
1
4e
)
= P
(
L−t ≥ n− 7G = (1 + ρ2)Ē[L − t ] | Nt−1 ∈ [n− 3G,n−G), r(n, t) ≤
1
4e
)
< ( e
1 + ρ2
)(1+ρ2)Ē[L − t ]
= ( e
1 + ρ2
)n−7G
≺ 1
SuperPoly(n) , (28)
where Ē[L−t ] = E[L − t | Nt−1 ∈ [n−3G,n−G), r(n, t) ≤ 1/4e] ≤ n/4e and ρ2 = (n−7G)/Ē[L − t ]−1 > (n/2)/(n/4e)− 1 ≥ 2e− 1 (since G = γ4/7 logn and γ ≤ n/ logn). Combining Eqs. 27 and 28 together, we obtain
P
(
N (O) t ∈ L2 | Nt−1 ∈ A2 ∪ A1, r(n, t) ≤ 1−
1
logn
)
≺ 1
SuperPoly(n) .
Combining the above inequality with Eq. 26, we know that the probability of reaching L2 from A2 ∪ A1 by onegeneration transition is super-polynomially close to 0. The rest part of the proof of Proposition B1.3b is similar to that of Proposition B1.3a.
(For BitMatchingD) According to Chernoff bounds, with an overwhelming probability there are at most U1 flipped bits among the total n bits after the DOP change (which implies that the number of matching bits can decrease or increase by at most U1 after DOP change):
P
(
|N (P ) t −Nt−1| ≥ U1 | σ <
δ logn
n
)
<
(
e
γ1/7
)U1
=
(
e
ω(1)
)ω(logn)
≺ 1
SuperPoly(n) .
Noting that the range between L2 and A2 ∪ A1 is much larger than U1 (n − 7G > U1), we know that, given the condition that σ < δ logn/n, the probability of reaching L2 from A2 ∪ A1 by the help of the DOP change at the beginning of the tth generation is super-polynomially close to 0.
Moreover, since the selection operator of the EA always preserves the better individual between the parent
and offspring (For BitMatchingD , Nt = max{N (P ) t , N (O) t } ≥ N (P ) t holds according to Eqs. 2 and 3), the above
inequality also implies that Nt /∈ L2 holds with an overwhelming probability. In other words, the probability of reaching L2 from A2 ∪ A1 by one-generation transition is super-polynomially close to 0. The rest part of the proof of Step 2.3b is similar to that of Step 2.3a.
Analysis of Proposition B1.4. Let U2 = γ 1/7. According to the condition Nt−1 ∈ A2 ∪ A1 ∪ F2, there are at most 3G non-matching bits at the end of the (t − 1)th generation. On the other hand, since the number of flipped non-matching bits is always no smaller than the final increment of the number of matching bits (after DOP change or/and mutation), we know that, to increase the number of matching bits by at least U2, the number of flipped non-matching bits must be larger than U2. Hence, concerning the number of matching bits of the parent after the DOP change at the tth generation, we obtain
P
(
N (P ) t −Nt−1 ≥ U2 | Nt−1 ∈ A2 ∪ A1 ∪ F2, σ <
δ logn
n
)
<
(
3G
U2
)
σU2(1 − σ)n−U2 <
(
3G
γ1/7
)
σγ 1/7
< (3G)γ 1/7 σγ 1/7 < (3δG logn
n
)γ1/7 = (3δγ4/7 log2 n
n
)γ1/7
(29)
< (3δn4/7 log2 n
n
)γ1/7
≺ 1
SuperPoly(n) , (30)
where we obtain Eqs. 29 and 30 by applying Eqs. 9 and 4 respectively. Next, concerning the number of matching bits of the offspring at the tth generation, we must consider two different cases. The EA adopts SMR at the tth generation in the first case while it adopts LMR at the tth generation in the second case.
For the former case, we estimate the following inequality by noting the condition Nt−1 ∈ A2 ∪ A1 ∪ F2:
P
(
N (O) t −Nt−1 ≥ U2 | Nt−1 ∈ A2 ∪ A1 ∪ F2, Pm(n, t) <
γ1/14 logn
n , σ <
δ log n
n
)
<
(
3G
U2
)
r(n, t)U2 ( 1− r(n, t) )n−U2 <
(
3G
γ1/7
)
r(n, t)γ 1/7
< (3G)γ 1/7 r(n, t)γ 1/7 < (6Gγ1/14 logn
n
)γ1/7 = (6γ9/14 log2 n
n
)γ1/7
< (6n9/14 log2 n
n
)γ1/7
≺ 1
SuperPoly(n) , (31)
where we utilize the fact that the composite bitwise mapping rate r(n, t) satisfies that r(n, t) = (1 − σ)Pm(n, t) + (1 − Pm(n, t))σ = Pm(n, t) + σ − 2Pm(n, t)σ < 2γ 1/14 logn/n (since Pm(n, t) < γ 1/14 logn/n and σ < δ logn/n < γ1/14 logn/n holds). On the other hand, let us investigate the latter case in which the EA adopts LMR at the tth generation. According to the condition Nt−1 ∈ A2 ∪A1 ∪ F2, Nt−1 = n− o(n) holds. By applying Lemma 8.3, we obtain
P
(
N (O) t > Nt−1 | Nt−1 ∈ A2 ∪ A1 ∪ F2, Pm(n, t) ≥
γ1/14 logn
n , σ <
δ logn
n
)
≺ 1
SuperPoly(n) . (32)
Combining Eqs. 31 and 32 together we have:
P
(
N (O) t −Nt−1 ≥ U2 | Nt−1 ∈ A2 ∪ A1 ∪ F2, σ <
δ logn
n
)
≺ 1
SuperPoly(n) , (33)
Noting that Nt ∈ {N (P ) t , N (O) t }, we obtain the following result by combining Eqs. 30 with the above inequality:
P
(
Nt −Nt−1 ≥ U2 | Nt−1 ∈ A2 ∪ A1 ∪ F2, σ < δ log n
n
)
< P
(
N (P ) t −Nt−1 ≥ U2 | Nt−1 ∈ A2 ∪ A1 ∪ F2, σ <
δ logn
n
)
+P
(
N (O) t −Nt−1 ≥ U2 | Nt−1 ∈ A2 ∪ A1 ∪ F2, σ <
δ logn
n
)
≺ 1
SuperPoly(n) .
Consequently,
P
(
Nt −Nt−1 < U2 | Nt−1 ∈ A2 ∪A1 ∪ F2, σ < δ logn
n
)
≻ 1− 1
SuperPoly(n) . (34)
Hence, we know that once the EA is in A2 ∪ A1 ∪ F2, the number of matching bits can only increase by at most U2 in one generation with an overwhelming probability no matter which mutation rate the EA adopts.
Analysis of Proposition B1.5. As we have shown in Proposition B1.3, so far we have not excluded the possibility6 of the following two events:
1. The EA reaches F2 via B2 and then via L2 by multiple-generation transition (it is possible that the EA reaches F2 from L2 by large enough LMR, e.g., the mutation rate 1).
2. The EA reaches F2 via A2 (without reaching some intermediate points in L2) by multiple-generation transition.
Hence, the proof of Proposition B1.5 contains two parts. First, we need to prove that, if the EA has already reached B2, then it cannot travel through B1 and reach L2 with an overwhelming probability. Second, we need to prove that, if the EA has already reached A2, then it cannot travel through A1 and reach F2 with an overwhelming probability. If the above results have been proven, then we can only hope some events with super-polynomially small probability (e.g., the EA reaches F2 from A2 by one-generation transition) to happen, which will lead to a super-polynomial first hitting time with an overwhelming probability.
Since the ideas of two parts are quite similar, we only provide the details of the second part (which will lead to the final conclusion of the theorem) for the sake of brevity. Assume that we have proven the first part, that is, if the EA has already reached B2, then it cannot travel through B1 and finally reach L2 with an overwhelming probability. According to Proposition B1.4, to reach F2, the EA must reach L2 or A2 first, we know that the only choice to reach F2 is via A2. To reach F2 via A2, the EA must travel through A1. The reason is given by Proposition B1.4 and the fact that A1 is with the length of G > U2.
Next, we will provide the proof of for the aforementioned proposition: if the EA has already reached A2, then it cannot travel through A1 and reach F2 with an overwhelming probability. (as we have mentioned, by the same technique we can prove the similar result for L2). For ∀t ∈ N+, given the conditions that Nt ∈ A2 ∪ A1 and Nt−1 = i, we let the probabilities of decreasing and increasing the number of matching bits be p
−(n, i, t) and p+(n, i, t), respectively, i.e.,
p−(n, i, t) = P
(
Nt < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, σ < δ logn
n
)
,
p+(n, i, t) = P
(
Nt > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, σ < δ logn
n
)
.
Next we prove that ∀t ∈ N+ that satisfies Nt−1 = i ∈ A2 ∪A1, the following two inequalities holds:
p−(n, i, t) > p−i (n) = Θ (γ logn
n
)
, (35)
p+(n, i, t) < p+i (n) = Θ (γ4/7 logn
n
)
, (36)
where p−i = p − i (n) is a general lower bound of p −(n, i, t), and p+i = p + i (n) is a general upper bound of p −(n, i, t). To prove the bound for p−(n, i, t), we need to consider two cases. In the first case, the EA adopts SMR at the tth generation; In the second case, the EA adopts LMR at the tth generation. Concerning the first case, we estimate the following probability:
P
(
Nt < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) < γ1/14 log n
n , σ <
δ logn
n
)
≥
(
i ∑
k=1
(
i
k
)
σk(1− σ)i−k ( 1− Pm(n, t) k )
)
(1 − σ)n−i ( 1− Pm(n, t) )n−i
>
(
( 1− Pm(n, t) )
i ∑
k=1
(
i
k
)
σk(1− σ)i−k
)
(1− σ)n−i ( 1− Pm(n, t) )n−i
= ( 1− (1 − σ)i ) (1− σ)n−i ( 1− Pm(n, t) )n−i+1 ,
6Here “excluding the possibility” of an event is referred to proving that the probability of the event is super-polynomially close to 0.
where we consider the case in which all the non-matching bits are not flipped during the DOP change and mutation (this event is with the probability of (1− σ)n−i(1−Pm(n, t))
n−i) while some of the matching bits are flipped by the DOP change and at least one of these flipped matching bits is not flipped again by mutation (this event is with the probability of ∑i
k=1
(
i k
) σk(1− σ)i−k ( 1− Pm(n, t) k ) . According to the value of σ, let us consider two subcases:
1. If σ ≤ 1n , according to Eq. 9, we know γ = σn 2/ logn. Since n− i+ 1 < 3G+ 1 < n/(γ1/7 logn) holds, we have
P
(
Nt < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) < γ1/14 logn
n , σ ≤
1
n
)
≥ ( 1− (1− σ)i ) (1 − σ)n ( 1− Pm(n, t) )n−i+1 ≥ c1 · iσ ( 1− γ1/14 logn
n
) n
γ1/7 log n ≥ c1 2 · γ logn n ,
where c1 is a positive constant.
2. If 1n < σ < δ logn/n, according to Eq. 9, we know γ = n/ logn. Since 3G < n/(γ 1/7 log n) and i > n− 3G > n/2
hold, we have
P
(
Nt < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) < γ1/14 logn n , 1 n < σ < δ logn n
)
>
(
1− ( 1− 1
n
)i )
(1− σ)3G ( 1− Pm(n, t) )3G > c2 · (1− σ) 3G ( 1− Pm(n, t) )3G
> c2 · ( 1− δ logn
n
) n
γ1/7 log n
( 1− γ1/14 log n
n
) n
γ1/7 log n = c3 · γ logn
n ,
where c2 and c3 are positive constants.
Combining the above two cases together, we know that
P
(
Nt < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) < γ1/14 logn
n , σ <
δ logn
n
)
> max {c1 2 , c3 } · γ logn n (37)
holds. We now consider the second case in which the EA adopts LMR at the tth generation. We estimate the following probability:
P
(
N (P ) t < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, σ <
δ logn
n
)
≥
i ∑
k=1
(
i
k
)
σk(1− σ)n−k > ( 1− (1 − σ)i ) (1− σ)n−i.
Moreover, by applying Lemma 8.3, we have
P
(
N (O) t ≥ Nt−1 | Nt−1 ∈ A2 ∪ A1, Pm(n, t) ≥
γ1/14 logn
n , σ <
δ logn
n
)
≺ 1
SuperPoly(n) .
Combining the above two inequalities together and noting the fact that the selection operator always preserve the better one between the parent and offspring, we have
P
(
Nt < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) ≥ γ1/14 logn
n , σ <
δ logn
n
)
≥ P
(
N (P ) t < Nt−1, N (O) t < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) ≥
γ1/14 logn
n , σ <
δ logn
n
)
= P
(
N (P ) t < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) ≥
γ1/14 logn
n , σ <
δ logn
n
)
−P
(
N (P ) t < Nt−1, N (O) t ≥ Nt−1 | Nt−1 = i ∈ A2 ∪A1, Pm(n, t) ≥
γ1/14 logn
n , σ <
δ logn
n
)
> P
(
N (P ) t < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, σ <
δ logn
n
)
−P
(
N (O) t ≥ Nt−1 | Nt−1 ∈ A2 ∪A1, Pm(n, t) ≥
γ1/14 logn
n , σ <
δ log n
n
)
> ( 1− (1 − σ)i ) (1− σ)n−i − 1
SuperPoly(n) >
1 2 · ( 1− (1− σ)i ) (1− σ)n−i.
According to the value of σ, let us further consider two subcases:
1. If σ ≤ 1n , according to Eq. 9, we know γ = σn 2/ logn. We have
P
(
Nt < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) ≥ γ1/14 logn
n , σ ≤
1
n
)
> 1
2 · ( 1− (1 − σ)i )
(1− σ)n−i > 1
2 · ( 1− (1− σ)i ) (1 − σ)n ≥ c4 · iσ > c4 2 · γ logn n ,
where c4 is a positive constant.
2. If 1n < σ < δ logn/n, according to Eq. 9, we know γ = n/ logn. Since 3G < n/(δ logn) and i > n − 3G > n/2 hold, we have
P
(
Nt < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) ≥ γ1/14 logn n , 1 n < σ < δ logn n
)
> 1
2 ·
(
1− ( 1− 1
n
)i )
(1− σ)3G > c5 · (1 − σ) 3G > c5 ·
( 1− δ logn
n
) n
δ log n = c6 · γ logn
n ,
where c5 and c6 are positive constants.
Combining the above two cases together, we know that
P
(
Nt < Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) ≥ γ1/14 logn
n , σ <
δ logn
n
)
> max {c4 2 , c6 } · γ logn n
holds. Combining Eq. 37 with the above inequality, we have proven that
p−(n, i, t) > p−i (n) = Θ (γ logn
n
)
.
Now let us prove the upper bound of p+(n, i, t). To increase the number of matching bits by DOP change and/or mutation, the number of flipped non-matching bits must be larger than the number of flipped matching bits after DOP change and/or mutation. According to Lemma 5, concerning the relation between N (P ) t and Nt−1, we have
P
(
N (P ) t > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, σ <
δ logn
n
)
≤ 3G
n− 3G ≤
3G n/2 ≤
6γ4/7 logn
n . (38)
For the offspring at the tth generation, we still need to consider two cases. In the first case, the EA adopts SMR at the tth generation; In the second case, the EA adopts LMR at the tth generation.
We now consider the first case in which the EA adopts SMR. By applying Lemma 5 we have
P
(
N (O) t > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) <
γ1/14 logn
n , σ <
δ logn
n
)
≤ 3G
n− 3G ≤
3G n/2 ≤
6γ4/7 logn
n ,(39)
where we utilize the fact that the overall impact of DOP change and mutation to the offspring individual at the tth generation can be represented by a overall mapping with bitwise mapping rate r(n, t) = (1 − σ)Pm(n, t) + (1 − Pm(n, t))σ = Pm(n, t) + σ − 2Pm(n, t)σ.
Noting that Nt ∈ {N (P ) t , N (O) t }, we obtain the following result by combining Eqs. 38 and 39 together:
P
(
Nt > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) < γ1/14 logn
n , σ <
δ logn
n
)
< P
(
N (P ) t > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, σ <
δ logn
n
)
+P
(
N (O) t > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) <
γ1/14 logn
n , σ <
δ logn
n
)
≤ 12γ4/7 logn
n . (40)
On the other hand, we consider the case in which LMR is adopted by the EA. By applying Lemma 8.3, we know
P
(
N (O) t > Nt−1 | Nt−1 ∈ A2 ∪ A1 ∪ F2, Pm(n, t) ≥
γ1/14 logn
n , σ <
δ logn
n
)
≺ 1
SuperPoly(n) .
In other words, if the LMR is used, the number of matching bits found by the offspring at the tth generation will not be larger than Nt−1. Similar to Eq. 40, we obtain
P
(
Nt > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) ≥ γ1/14 logn
n , σ <
δ logn
n
)
< P
(
N (P ) t > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, σ <
δ logn
n
)
+P
(
N (O) t > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, Pm(n, t) ≥
γ1/14 logn
n , σ <
δ logn
n
)
≤ 6γ4/7 logn
n +
1
SuperPoly(n) <
12γ4/7 logn
n .
Combining Eq. 40 with the above inequality, we obtain
P
(
Nt > Nt−1 | Nt−1 = i ∈ A2 ∪ A1, σ < δ logn
n
)
< 12γ4/7 logn
n .
In other words, we have proven Eq. 36. Droste utilized the idea of “effective mutation” to prove Theorem 2 of [17]. Intuitively speaking, this technique estimates the lower bound for the number of effective mutation (which can be interpreted as the number of generations in which the number of matching bits changes) for reaching the target, and it also estimates the upper bound for effective mutation that can be provided by the EA. If the former one is significantly smaller than the later one, then the EA cannot reach the target. Following this intuitive idea, we then provide the formal proof of the final conclusion of the theorem. By Eqs. 35 and 36, we obtain the upper bound of the probability of Nt > Nt−1, conditional on the event that Nt 6= Nt−1:
P
(
Nt > Nt−1 | Nt 6= Nt−1 = i ∈ A2 ∪ A1, σ < δ logn
n
)
= p+(n, i, t)
p+(n, i, t) + p−(n, i, t) < p+i p+i + p − i = Θ ( γ4/7 logn/n ) Θ ( γ4/7 logn/n ) +Θ ( γ logn/n ) = Θ ( γ−3/7 ) , (41)
where Θ is referred to the asymptotic order of the problem size n. We now prove that the EA will spend super-polynomial number of generations to travel through A1 and reach F2. Let T be defined formally as follows:
T = ∣ ∣
∣
{ t ∈ N+ | Nt−1 6= Nt, Nt−1 ∈ A2 ∪ A1, σ < δ logn
n
}∣
∣ ∣. (42)
Recall that in Proposition B1.3, we have proven that the EA has to travel through the whole A1 with length of G; In Proposition B1.4, we have proven that in one generation the number of matching bits cannot increase by more than U2 with an overwhelming probability. According to Propositions B1.3 and B1.4, to travel through A1 and reach F2, T is lower bounded by G/U2 = γ 3/7 logn with an overwhelming probability. Formally we have
P ( T ≥ γ3/7 logn ) ≻ 1− 1
SuperPoly(n) . (43)
Further, let T+ and T− be defined as follows:
T+ = ∣ ∣
∣
{ t ∈ N+ | Nt−1 < Nt, Nt−1 ∈ A2 ∪ A1, σ < δ logn
n
}∣
∣ ∣, (44)
T− = ∣ ∣
∣
{ t ∈ N+ | Nt−1 > Nt, Nt−1 ∈ A2 ∪A1, σ < δ logn
n
}∣
∣ ∣, (45)
and the above definitions imply that T = T+ + T−.
Moreover, by Proposition B1.4, given the condition that the EA is in A2 ∪ A1, in one generation the number of matching bits cannot increase by more than U2 with an overwhelming probability. Hence, among the T generations, the number of matching bits can increase by T+U2−T
− at most with an overwhelming probability. To travel through A1, the following inequality must hold (a necessary condition):
T+U2 − T − ≥ G.
Noting that T = T+ + T−, it follows that
T+ ≥ G+ T
U2 + 1 .
Recall the definitions of G, T and U2, we have
G+ T U2 + 1 > d · T γ1/7
where d is a positive constant. Combining the above two inequalities together, we know that the following condition must be satisfied so as to travel through A1:
T+ > d · T
γ1/7 . (46)
Next, we only need to prove that the above condition cannot be satisfied with an overwhelming probability (thus the EA cannot travel through A1 with an overwhelming probability). It follows from Eqs. 41 and 42 that
E[T+ | T ] = O
(
T
γ3/7
)
,
where O is referred to the asymptotic order of the problem size n. By Chernoff bounds, we estimate the probability of Eq. 46:
P
(
T+ > d · T
γ1/7 | T
)
<
(
eγ1/7E[T+ | T ] dT
)dT/γ1/7
= O
(
1
γ2/7
)Θ(T/γ1/7)
,
where d is a positive constant. Meanwhile, Eq. 43 implies that
P ( T ≥ γ3/7 logn ) ≻ 1− 1
SuperPoly(n) .
By the total probability theorem [36], we obtain
P
(
T+ > d · T
γ1/7
)
≺ 1
SuperPoly(n) .
In other words, the condition T+U2 − T − ≥ G does not hold with an overwhelming probability, which implies that the EA cannot travel through A1 and reach F2 with an overwhelming probability, given the condition that it has already reached A2. Consequently, we have proven that the EA cannot reach F2 by a polynomial first hitting time with an overwhelming probability. Formally, let τF2 be defined as follows
τF1 = min { t ≥ 0; (N (P ) t ∈ F1) ∨ (N (O) t ∈ F1) } . (47)
We have proven that
P(τF2 ≺ Poly(n)) ≺ 1
SuperPoly(n) ,
which leads to Theorems 1 and 2 according to the definition of F2 in Definition 10.
In analogy to other parallel systems such as RAID storage, our approach to modular printing systems is called Rack Mounted Printing (RMP). An RMP system can be seen as a network of transports linking multiple printing engines. These transports are known as the media path. Figure 1 shows a four-engine prototype printer built at the Palo Alto Research Center (PARC) from over 170 independently controlled modules. Figure 2 provides a schematic side view, showing the many possible paper paths linking the paper feeders to the possible output trays. (Video 1 in the on-line appendix, ‘nominal simulation,’ presents an animation of Figure 2.) Multiple feeders allow blank sheets to enter the printer at a high rate and multiple finishers allow several print jobs to run simultaneously. Having redundant paths through the machine enables graceful degradation of performance when modules fail. By building the system out of relatively small modules, we enable easy reconfiguration of the
components to add new modules and functionality. Achieving these benefits, however, poses a considerable control challenge.
The modular printing domain is reminiscent of ‘mass customization,’ in which massproduced products are closely tailored and personalized to individual customers’ needs. It is also similar to package routing or logistics problems. From a control perspective, it involves planning and scheduling a series of sheet requests that arrive asynchronously over time from the front-end print-job submission and rendering engine. The system runs at high speed, with several sheet requests arriving per second, possibly for many hours. Each sheet request completely describes the attributes of a desired final product. There may be several different sequences of actions that can be used to print a given sheet. For example, in Figure 2, a blank sheet may be fed from either of the two feeders, then routed to any one of the four print engines (or through any combination of two of the four engines in the case of duplex printing) and then to either finisher (unless the sheet is part of an on-going print job).
This on-line planning problem is complicated by the fact that many sheets are in-flight simultaneously and the plan for the new sheet must not interfere with those sheets. Most actions require the use of physical printer components, so planning for later sheets must take into account the resource commitments in plans already released for production. Because modern printers are highly configurable, can execute an large variety of jobs potentially simultaneously, and have a large variety of constraints on feasible plans, hard-coded or locally-reactive plans do not suffice (Fromherz et al., 1999). In fact, printer engineers at Xerox delight in uncovering situations in which products from competing manufacturers, who do not use model-based planning, attempt to execute infeasible plans.
The planning system must decide how to print all requested sheets as quickly as possible and thus it must determine a plan and schedule for each sheet such that the end time T of the plan that finishes last is minimized. In other words, the planner attempts to minimize the makespan of the combined global plan for all sheets, in essence optimizing the system’s overall throughput. Typically there are many feasible plans for any given sheet request; the problem is to quickly find one that minimizes T . The optimal plan for a sheet depends not only on the sheet request, but also on the resource commitments present in previouslyplanned sheets. Any legal series of actions can always be easily scheduled by pushing it far into the future, when the entire printer has become completely idle, but of course this is not desirable. This is an on-line task because the set of sheets grows as time passes and plan execution (i.e., printing) interleaves with plan creation. In fact, because it is the real-world wall clock end time that we want to minimize and because the production of a sheet cannot start until it is planned, the speed of the planner itself affects the value of a plan! However, the system often runs at full capacity, and thus the planner usually need only plan at the rate at which sheets are completed, which again may be several per second. While challenging, the domain is also forgiving: feasible schedules can be found quickly, sub-optimal plans are acceptable, and plan execution is relatively reliable.
A printer controller works in an on-line real-time and continual planning environment with three on-going processes: 1) on-line arrival of new goals; 2) planning for known goals; and 3) execution of previously synthesized plans. Figure 3 shows the inputs and outputs of the planning system, with the domain model and sheet requests entering from the left and communication with the low-level control system on the right. The plan manager is responsible for tracking the status of each goal and invoking the planner when necessary. While planning and execution occur sequentially for any given sheet, these processes will usually be interleaved between different sheets. Figure 4 sketches the different steps in the sheet-plan life cycle managed by the plan manager. Specifically, upon receiving, sheets are put in the unplanned first-in-first-out queue (sheets 6 and 7). The sheet planner then picks one sheet at the time from the unplanned queue and tries to find a route-plan for that sheet (sheet 5). Any plan when found is put in the queue of plans that haven’t yet been sent to the printer controller (sheets 3 and 4). Another plan manager process regularly checks the planned queue to decide if the earliest starting time of any plan in that queue is
close enough to the current wall-clock time and send those plans to the printer controller for execution (sheets 1 and 2). Note that in the figure, time advances downward so plans starting earlier are higher in the figure. Sheets 1, 2, and 3 finish in order; sheets 4 and 5 belongs to a different job and can be scheduled to run concurrently.
In our application, there is an additional negotiation step after a plan is issued by the planning system and before the plan is committed. First, each plan step is proposed by the machine controller to the modules involved. If the individual hardware modules from all steps accept their proposed actions, then the plan is committed. As we discuss below, this commitment means that modules become responsible for notifying the controller if they fail to complete an action or realize that they will not be able to perform a planned action in the future. After a plan is confirmed, the planner cannot modify it. There is thus some benefit in releasing plans to the machine controller only when their start times approach. If not all modules confirm, then the machine controller notifies the planning system that the proposed plan has been rejected, and the system must produce a new plan. This negotiation process is one reason that we must find a complete plan before starting execution.
Each module has a limited number of discrete actions it can perform, each transforming a sheet in a known deterministic way. For many of these actions, the planner is allowed to control its duration within a range spanning three orders of magnitude (milliseconds to seconds). For example, the planner may choose to transport a sheet faster or slower through a module in order to avoid collisions. Actions may not split a sheet into two pieces or join multiple sheets from different paths in the printer together. This means that a single printed sheet must be created from a single blank sheet of the same size, thereby conflating sheets with material and allowing plans to be a linear sequence of actions. In our domain, adjacent actions must meet in time; sheets cannot be left lingering inside a printer after an action has completed but must immediately begin being transported to its next location.
Sheets are grouped into print jobs. A job is an ordered set of sheets, all of which must eventually arrive at the same destination in the same order in which they were submitted. Multiple jobs may be in production simultaneously, although because sheets from different
jobs are not allowed to interleave at a single destination, the number of concurrent jobs is limited by the number of destinations (i.e., finisher trays).
Currently, Xerox uses a constraint-based scheduler to control its high-end and midrange printers (Fromherz et al., 1999). The scheduler enumerates all possible plans when the machine starts up and stores them in a database. When printing requests arrive on-line, the scheduler picks the first feasible plan from the database and uses temporal constraint processing to schedule its actions. This decoupling of planning and scheduling is insufficient for complex machines for two reasons. First, the number of possible plans is too large to generate ahead of time, and indeed becomes infinite if loops are present, as in the printer shown in Figure 2. Second, the precompiled plans can be poor choices given the existing sheets in the system. For example, sheets should be fed from different feeders depending on when the previous sheets were fed, how large they are, and how long they will dwell in the print engines (which can be a function of sheet thickness and material). For high performance, we must integrate planning and scheduling in an on-line fashion.
Occasionally a module will break down, failing to perform its committed action. Modules can also take themselves off-line intentionally, for example to perform internal re-calibration or diagnosis. Modules may be added or subtracted from the system and this information is passed from the machine controller to the planning system at the right side of Figure 3. The vision of RMP is that the system should provide the highest possible level of productivity that is safe, including running for long periods with degraded capabilities.1 Meeting this mandate in the context of highly modular systems means that precomputing a limited set of canonical plans and limiting on-line computation to scheduling only is not desirable. For a large system of 200 modules, there are infeasibly many possible degraded configurations to consider. Depending on the capabilities of the machines, the number of possible sheet requests may also make plan precomputation infeasible. Furthermore, even the best precomputed plan for a given sheet may be suboptimal given the current resource commitments in the printing machine.
To summarize, our domain is finite-state, fully-observable, and specifies classical goals of achievement. However, planning is on-line with additional goals arriving asynchronously. Actions have real-valued durations and use resources. Plans for new goals must respect resource allocations of previous plans. Execution failures and domain model changes can occur on-line, but are rare.
We are grateful to David Albrecht, Mark Carman and the anonymous reviewers for valuable comments and suggestions.
Compliance with Ethical Standards
This research has been supported by the Australian Research Council under grant DP140100087 and Asian Office of Aerospace Research and Development, Air Force Office of Scientific Research under contract FA2386-15-1-4007.
The authors declare that they have no conflict of interest.
We consider each proposal from the literature to define a feature system, and compare the ability of each feature system to predict the observed frequencies of orders. To do so, we use Poisson regression, as first used in Cysouw (2010). In Poisson regression we represent each language with a set of m binary-valued features, and say that the expected frequency λ of a language in a sample of k languages is given by:
λ = eV
V = wb + w1 · f1 + w2 · f2 + ...+ wm · fm, (1)
where fi is an indicator variable with value 0 when the ith feature is −, and 1 when the ith feature is +, and where the weights wb and w1, ..., wm are those that maximize the probability of the observed counts of languages. The weight wb is called a bias term. Under the probabilistic model of Poisson regression, the probabability that a language with feature values f1, ..., fm has frequency F is:
p(F |f1, ..., fm) = λF e−λ
F ! . (2)
Fitting a Poisson regression model using a certain set of features to a set of (possibly adjusted) frequency tells us how well it is possible to predict
languages in this framework given that set of features. Feature weights may be negative, in which case they can be considered marked. In that case, the model embodies the claim that the presence of these features is disfavored in languages. Since features get different weights, the model implements different degrees of markedness per feature, as in Harmonic Grammar (Smolensky and Legendre, 2006). The model finds the degrees of markedness which best predict the data given the feature system.
Given the six controversial topics and four different registers, we compiled a collection of plain-text documents, which we call the raw corpus. It contains 694,110 tokens in 5,444 documents. As a coarse-grained analysis of the data, we examined the lengths and the number of paragraphs (see Figure 5). Comments and forum posts follow a similar distribution, being shorter than 300 tokens on average. By contrast, articles and blogs are longer than 400 tokens and have 9.2 paragraphs on average. The process of compiling the raw corpus and its further statistics are described in detail in Appendix 6.
The ideas about decision making under ignorance in economics are combined with the ideas about uncertainty representation in computer science. The combination sheds new light on the question of how artificial agents can act in a dynamically consistent manner. The notion of sequential consistency is formalized by adapting the law of iterated expectation for plausibility measures. The necessary and sufficient condition for a certainty equivalence operator for NehringPuppe’s preference to be sequentially consistent is given. This result sheds light on the models of decision making under uncertainty.
First, we simulated some data to fill two databases of preferences and health conditions. In what follows, we describe the data simulation procedure.
In order to test our proposal we created two databases that store the preferences of simulated users in the cities of Tarragona and Barcelona. Tarragona database stores the preferences of 1,000 simulated users about 11 real routes, represented in Table III and depicted in Figure 1a. In the case of Barcelona, the database stores the preferences of 50,000 users over 28 designed routes, graphically represented in Figure 1b. Therefore, Barcelona dataset stores a total of 1,400,000 ratings and, hence, is two orders of magnitude larger than Tarragona dataset. It is worth to emphasise that since the stored routes contain checkpoints, the real amount of routes and sub-routes is much higher. The simulated data have been generated as follows: Citizens simulation: In order to simulate the citizen’s profiles, we select their age between 18 and 90 by using a distribution according with the age pyramid of the country [5]. Also, we consider four main health issues, namely visual impairments, respiratory problems, reduced mobility and heart diseases. Clearly, these health issues do not cover all the possible illnesses that a citizen might suffer, they are only indicative and are not intended for a precise characterisation of citizens. On the contrary, their goal is to illustrate the operation of our approach in a realistic and practical scenario. In a further embodiment of our approach, more health problems could be added. However, for the sake of clarity we have kept this example simple. To decide whether each citizen suffers from a given disease, we simulate it by using real data provided by the World Health
Organisation [6] and the World Heart Federation1. Specifically, it we consider that the probability of suffering from visual impairments is 3.4%, the probability of having respiratory problems is 3.2%, the probability of suffering from reduced/limited mobility is 2%, and the probability of suffering from some kind of cardiovascular disease is 14%. In the case of heart diseases, we consider that mainly people over 45 years are affected, since the probability of finding this kind of issues in younger people decreases drastically and is mainly negligible.
Profile characterisation: After being simulated, citizens may be classified in different profiles depending on their health issues and age. A binary tree representing this classification is depicted in Figure 2. It can be observed that there exist 16 categories and 4 age intervals. This results in 64 different profiles. Ratings simulation: It is assumed that citizens belonging to a given profile have similar needs and, thus, would have similar ratings. Based on this, a relationship between the routes and the citizens profiles can be established. a) Determine the range of routes’ features: The values of the features of each route (i.e. distance, elevation gain and pavement quality) are classified in a range between 0 and 5. In this case, the higher the easier (better) for the citizen. For instance, if the elevation gain is 0 meters, this feature of the route will be classified as 5 (very easy). On the contrary, the highest elevation
1www.world-heart-federation.org
gain will be classified as 0 (very difficult). The equivalences between features values and ranges have been normalised to fit in the aforementioned range. Note that features are considered independently. b) Assigning citizens skills: Depending on the citizens age and their health conditions, they will react differently to the routes features. For example, citizens suffering from heart diseases would be quite reluctant to follow routes with high elevation gains. To quantify this for example, we assign two negative skill points (related to the elevation gain) to every citizen suffering from heart diseases at the highest level (i.e. 1, according to Table II). Therefore, the value of each health condition is multiplied by the values of the skills assignment heuristic represented in Table IV. c) Determine the ratings: Finally, to create the rating of a citizen for a given route, we subtract from each feature (distance, elevation and pavement quality) all the “modifiers” related to health conditions and age (cf. Table IV). Next, the resulting values of each feature are aggregated. Since each feature is in a rage between 0 and 5, this results in a value between 0 and 15 (where 0 is the worst value and 15 the best). Then, the result is scaled to the widely used range [0, 10]. Also, to introduce some variability we add Gaussian noise sampled from N (0, 1.5).
Finally, we look at how the optimization performance translates to classification accuracy on the test set. Figure 6 shows test set accuracy against simulated runtime under the same time complexity model as Figure 2. We observe similar results, as BET performs better than other methods. On the plots, we marked the moment when BET reaches full dataset. In most cases, by this point the algorithm reaches close to optimum test accuracy. Thus, in many practical applications we can end the optimization by relying on this simple stopping criterion. Moreover, in problems when the full dataset is too abundant, BET will attain optimal performance well before the full data size is reached.
84
A.1. HISTOGRAM: FIRST EXPERIMENT
85
The boxplots of the classification accuracies and misclassification of the proposed online cost-sensitive classifier adaptation algorithm (OCSCA) and the CSOC algorithm over 10-fold cross validation are given in Fig. 1. From this
figure, we can see that the proposed method outperforms the CSOC algorithm on both average accuracy and misclassification cost. Especially in the case of misclassification cost, the proposed algorithm achieves completely lower average misclassification cost then CSOC. This is because the proposed method takes advantage of an existing predictor learned from more data points by adapting it to a given cost setting. Even the existing predictor is learned according to a different cost setting. This is an strong evidence of the fact that classifier adaptation can benefit cost sensitive learning.
We choose the ICWSM 2012 labeled Twitter dataset [Al Zamal et al. 2012] in our experiment. This dataset consists of three categories of labels, and it is publicly available. Due to the limitation of Twitter’s policy, the actual content of tweets were not included with the dataset; however, the Twitter users’ identification numbers as well as their tweet IDs are available. We retrieve all the data using Twitter API according to the available information.
To preprocess the dataset, we remove all the non-ASCII characters and replace all the URLs with a special lexical token. We also pre-tokenize the tweets and assign POS tags for each token in each tweet using the pre-trained tagger from [Owoputi et al. 2013]. In this dataset there is other social-network-based information, such as the target user’s friends, and the friends’ tweets, etc. Since we only want to model the writing style of the Twitter user, we omit this information as well as those tweets that
are re-tweeted by the given author. We attempt to include only the tweets that are authentically authored by the labeled Twitter user.
The labels in this dataset are generated semi-automatically and manually inspected [Al Zamal et al. 2012]. This dataset consists of three categories of labels for Twitter users: age, gender, and political orientation. The cleaned dataset is summarized in Table IV. There are 1170 Twitter users in total.
— Gender. The labels for this category can be either male or female. The labels are automatically generated based on the Twitter user’s name with a name-gender database, and then labels are manually inspected to ensure the labels are correct.
— Age.This dataset only distinguishes individuals of age ranges in 18-23 or 25-30. It frames the age prediction into a binary classification problem. The labels are constructed by looking at the tweets about birthday announcement, e.g., “Happy birthday to me”.
— Political orientation. This dataset provides political Twitter users with a label: either Democrat or Republican. Twitter users are collected from the wefollow Twitter directory [Al Zamal et al. 2012].
Figure 7 shows the empirical distribution, kernel density and histogram on the tweets’ length in terms of lexical token size. In general, tweets are very short text snippets. 90 percent of tweets have less than 25 tokens and most have a length of around 10 tokens. We combine all tweets of a single user into a single document and treat each tweet as an individual sentence.
To proceed with the experiment we conduct a 10-fold cross validation on the Twitter dataset and collect the accuracy measure for each characterization approach. First we convert each document into its numeric vector representation using different stylometric representations in our proposed models or the baselines, and then we feed them into a simple logistic classifier to predict the label of the document.
4.1.1. Baselines. We inherit the same set of baselines as in the previous experiment on the authorship verification problem, except for those studies reported in PAN2014 [Rangel et al. 2014] since we do not have the available result for direct comparison. The baselines are configured to have the same hyper-parameter setting as the previous experiment. Additionally, we include several additional baselines:
— LDA. In addition to picking the empirical optimal value k = 500, which represents the latent topics, we include the performance of k ∈ {100, 200, 500, 800} for comparison.
— LSA. Likewise, for the LSA model we also include the performance of k ∈ {100, 200, 500, 800} in addition to the original k = 200. Recall that k for LSA represents the number of singular values.
— Moreover, we include two evaluation results that were presented by Al Zamal et al. [2012] since we follow the same setup and use the same dataset for the experiment. The target user info approach is a SVM-based model trained on the features that are constructed on the user’s tweets and other information. These features include textual features (e.g., stemmed n-grams and hash tags, etc.) and socio-linguistic features (e.g., re-tweeting tendency, neighborhood size, and mention frequency, etc.). The all info approach is another SVM-based model trained on the features that adds additional social-network features (e.g., average of the neighborhood’s feature vectors).
We notice that the baseline measures adopted from [Al Zamal et al. 2012] have more advantages to our proposed approaches and our baseline approaches. First, they use a SVM model that typically outperforms a simple logistic regression model when the same data is given. Second, our approaches only consider the information reflected from the text, i.e., stylometric information. Other socio-linguistic, behavioral,
and social-network-related information is discarded. Given the advantages of their approach we expect that probably they will outperform the others. However, our experiments show that our proposed model achieves even better accuracy, which will be described in the following section.
4.1.2. Performance comparison. The performance of our proposed models, as well as all the baselines, is listed in Table V. It shows that our first proposed model, which jointly learns the representation for the lexical modality and the topical modality, achieves the highest accuracy value. The runner-up is the topical modality, and the characterlevel modality does not perform as well as the other two. The proposed lexical/topical modality model and the character-level modality model also outperform the PVDMrelated models, w2v-related models, and other dynamic n-gram-based models. Unlike the results for the authorship verification problem, the w2v-related baselines perform fairly well. They achieve a higher accuracy value than PVDM, PVDBOW, LSA, and LDA.
Even the (target user only) approach and the (all info) approach are given advantage; however, it achieves a lower accuracy value than our proposed joint model for lexical and topical modality, which is contradictory to our expectation. It shows that our proposed approach better models the writing variation than the n-gram language model that is used in both of these two baselines.
Table V also shows that the proposed syntactic representation learning model does not perform well on the ICWSM 2012 dataset, which is different from the previous authorship verification problem. This is because the tweet text data are relatively more casual than essay and novel, which does not introduce much variation in the grammatical bias.
Regarding the feature selection measure, in this scenario the frequency-based selection approach outperforms the information-gain-based selection-based approach. Even the top-100 frequency-ranked n-grams outperform top-1500 information-gain-ranked n-grams, which is completely different from the result shown in previous authorship verification experiments. Such a difference further confirms our argument that feature selection measures are scenario-dependent/data-dependent. Even the feature set is dynamically constructed based on a different dataset, the measurement for the selection process is data-dependent. A language model over text data is better than a feature-selection-based model.
In this work, we have presented a state-of-the-art named entity recognition system for the Vietnamese language, which achieves an F1 score of 92.05% on the standard corpus published recently by the Vietnamese Language and Speech community. Our
system outperforms the first-rank system of the related NER shared task with a large margin, 3.2% in particular. We have also shown the effectiveness of using automatic syntactic features for BiLSTM model that surpass the combination of BiLSTM-CNN-CRF models albeit requiring less time for computation.
This section gives a brief introduction of CNN, GRU and LSTM.
We use LeNet-5 (LeCun et al., 1998) as the plain CNN for MNIST, please see Table 1 for its structure. There are 70,000 clean samples and we follow (Vedaldi, accessed in Feb 2016) to use 60,000 as the train set and 10,000 as the test set. There are 10,000 groups of nonsense samples. All CNNs are trained with 20 epochs. We set α1 = 1, α2 = 1, α3 =0 for the hybrid loss functions of robust CNNs.
We present accuracies and error rates in Fig. 9 and Table 2. It is clear that CNNs using SAFs are much more robust than the plain CNN. The best one, mReLU-r-m, makes marginally more errors than the plain CNN for clean samples, adversarial samples with perturbation of up to strength β = 0.01, and noisy samples of up to β = 0.10; but has greatly superior performance for adversarial and noisy samples of large perturbation, and all nonsense samples. Particularly, the accuracy of mReLU-r-m under adversarial perturbations of the strength β = 0.25 is only 0.073, which is much lower than 0.179 of Maxout (Goodfellow et al., 2013) trained using adversarial training (Goodfellow et al., 2015).
Random training is very effective in improving the robustness of SAF CNNs against both adversarial and noisy samples. In our opinion, random perturbations of the train samples make the large-output/non-zero widths of SAFs effectively broader, which improving robustness to perturbations of moderate strength. Using all training tricks, we can improve all CNNs to be considerably robust against nonsense and noisy samples, even the plain CNN. Adversarial training, which is recommended in previous literatures, is much less effective improving the robustness against both adversarial and nonsense samples. It is probably due to our insufficient number of adversarial training rounds. So in Sec. 5.2, we train all CNNs with only the combination of random and mean training.
We show some examples of mReLU-r-m classifications in Fig. 10. The robust CNN gives correct predictions on even severe distorted samples. It does make incorrect predictions for samples bounded by color boxes, but the images are hard even for a human being. The case of green boxes is a bit different: the predictions are wrong, but it is sensible to classify these samples into the nonsense category.
We will prove a statement that is stronger than Lemma 3.1: with probability at least 1 − 1/n4, for any (X,Y) ∈ K1 ∩ K2 ∩ K(δ) and U,V defined in Table 7, we have
〈∇X F̃(X,Y), X − U〉 + 〈∇Y F̃(X,Y),Y − V〉 ≥ p 4
d2 + 2 √ ρ Σmin d √ G(X,Y), (251)
where d = ‖M − XYT ‖F .
We have already proved (37a), i.e. with probability at least 1 − 1/n4,
φF = 〈∇XF, X − U〉 + 〈∇Y F,Y − V〉 ≥ p 4 d2.
It remains to prove a bound on φG, which is stronger than the bound φG ≥ 0. Note that φF depends on the observed set Ω, thus the bound on φF holds with high probability; in contrast, φG does not depend on Ω, thus the bound on φG always holds.
Claim E.1 For any (X,Y) ∈ K1 ∩ K2 ∩ K(δ) and U,V defined in Table 7, we have
φG = 〈∇XG, X − U〉 + 〈∇YG,Y − V〉 ≥ 2 √ ρ Σmin d √ G(X,Y). (252)
Proof of Claim E.1: By the definition of G in (13), G(X,Y) = ρ( ∑ i G1i(X) + G2(X) + ∑
j G3 j(Y) + G4(Y)), where the component functions
G1i(X) = G0 3‖X(i)‖2 2β21  , G2(X) = G0 3‖X‖2F 2β2T  , G3 j(Y) , G0
3‖Y ( j)‖2 2β22  , G4(Y) , G0 3‖Y‖2F 2β2T  . (253)
By the expressions of ∇XG,∇YG in (24), we have
φG = 〈∇XG, X − U〉 + 〈∇YG,Y − V〉 =
ρ m∑ i=1 G′0( 3‖X(i)‖2 2β21 ) 3 β21 〈X(i), X(i) − U(i)〉 + ρG′0( 3‖X‖2F 2β2T ) 3 β2T 〈X, X − U〉
+ρ n∑ j=1 G′0( 3‖Y ( j)‖2 2β22 ) 3 β22 〈Y ( j),Y ( j) − V ( j)〉 + ρG′0( 3‖Y‖2F 2β2T ) 3 β2T 〈Y,Y − V〉,
(254)
where G′0(z) = I[1,∞](z)2(z − 1) = 2 √ G0(z).
Firstly, we prove
h1i , G′0( 3‖X(i)‖2
2β21 ) 3 β21 〈X(i), X(i) − U(i)〉 ≥ 1 2
√ G1i(X), ∀ i, (255a)
h3 j , G′0( 3‖Y ( j)‖2
2β22 ) 3 β22 〈Y ( j),Y ( j) − V ( j)〉 ≥ 1 2
√ G3 j(Y), ∀ j. (255b)
We only need to prove (255a); the proof of (255b) is similar. We consider two cases.
Case 1: ‖X(i)‖2 ≤ 2β 2 1 3 . Note that 3‖X(i)‖2 2β21 ≤ 1 implies G0( 3‖X (i)‖2 2β21 ) = G′0( 3‖X(i)‖2 2β21 ) = 0, thus h1i = G1i = 0, in which
case (255a) holds.
Case 2: ‖X(i)‖2 > 2β 2 1
3 . By Corollary 4.1 and the fact that β 2 1 = β 2 T 3µr m , we have
‖U(i)‖2 ≤ 3rµ 2m β2T (15) = 3 4 2β21 3 < 3 4 ‖X(i)‖2. (256)
As a result, √
3 2 〈X(i), X(i)〉 =
√ 3
2 ‖X(i)‖‖X(i)‖ > ‖X(i)‖‖U(i)‖ ≥ 〈X(i),U(i)〉, which implies 〈X(i), X(i) − U(i)〉 ≥ (1 − √
3 2 )‖X(i)‖2 > (1 −
√ 3 2 ) 2 3β 2 1 > 1 12β 2 1. Combining this inequality with the fact that G ′ 0( 3‖X(i)‖2 2β21
) = 2 √ G0 (
3‖X(i)‖2 2β21
) =
2 √ G1i(X), we get (255a).
Secondly, we prove
h2 + h4 ≥ 2d
Σmin
( √ G2(X) + √ G4(Y) ) ,
where h2 , G′0( 3‖X‖2F 2β2T ) 3 β2T 〈X, X − U〉,
h4 , G′0( 3‖Y‖2F 2β2T ) 3 β2T 〈Y,Y − V〉.
(257)
Without loss of generality, we can assume ‖Y‖F ≥ ‖X‖F , and we will apply Corollary 4.1 to prove (257). If ‖Y‖F < ‖X‖F , we can apply a symmetric result of Corollary 4.1 to prove (257). We consider three cases.
Case 1: ‖X‖F ≤ ‖Y‖F ≤ √ 2 3βT . In this case G0( 3‖X‖2F 2β2T ) = G′0( 3‖X‖2F 2β2T ) = G0( 3‖Y‖2F 2β2T ) = G′0( 3‖Y‖2F 2β2T
) = 0, which implies h2 = h4 = G2(X) = G4(Y) = 0, thus (257) holds.
Case 2: ‖X‖F ≤ √ 2 3βT < ‖Y‖F . Then we have 3‖X‖2F 2β2T ≤ 1, which implies h2 = 0 = G2(X). By (51d) in Corollary
4.1 we have ‖V‖F ≤ (1− dΣmin )‖Y‖F , which implies (1− d Σmin )〈Y,Y〉 = (1− d Σmin )‖Y‖2F ≥ ‖Y‖F‖V‖F ≥ 〈Y,V〉. This further
implies 〈Y,Y −V〉 ≥ d Σmin ‖Y‖2F ≥ dΣmin 2β2T 3 . Combined with the fact that G ′ 0( 3‖Y‖2F 2β2T
) = 2 √
G0( 3‖Y‖2F 2β2T
) = 2 √ G4(Y), we get
h4 = G′0( 3‖Y‖2F 2β2T ) 3 β2T 〈Y,Y − V〉
≥ 2 √
G4(Y) 3 β2T d Σmin 2β2T 3 = 4d Σmin
√ G4(Y).
Thus h2 + h4 = h4 ≥ 4dΣmin √ G4(Y) = 4dΣmin (√ G4(Y) + √ G2(X) ) ≥ 2d Σmin (√ G4(Y) + √ G2(X) ) .
Case 3: √
2 3βT < ‖X‖F ≤ ‖Y‖F . Since ‖Y‖F ≥ ‖X‖F , we have G4(Y) = G0 ( 3‖Y‖2F 2β2T ) ≥ G0 ( 3‖X‖2F 2β2T ) = G2(X). By
Corollary 4.1, we have ‖U‖F ≤ ‖X‖F and ‖V‖F ≤ (1 − dΣmin )‖Y‖F . Similar to the argument in Case 2 we can prove h2 ≥ 0, h4 ≥ 4dΣmin √ G4(Y); thus h2 + h4 ≥ 4dΣmin √ G4(Y) ≥ 2dΣmin (√ G4(Y) + √ G2(X) ) .
In all three cases, we have proved (257), thus (257) holds.
We conclude that for U,V defined in Table 7,
φG (254) = ρ ∑ i h1i + ∑ j h3 j + h2 + h4  (255),(257) ≥ ρ ( 1 2 ∑ i √ G1i(X) + 1 2 ∑ j √ G2 j(Y)
+ 2d
Σmin
√ G2(X) +
2d Σmin
√ G4(Y) ) ≥ ρ 2d
Σmin ∑ i √ G1i(X) + ∑ j √ G2 j(Y) + √ G2(X) + √ G4(Y)  ≥ ρ 2d
Σmin √∑ i G1i(X) + ∑ j G2 j(Y) + G2(X) + G4(Y)
= ρ 2d
Σmin
√ 1 ρ G(X,Y) = 2 √ ρ Σmin d √ G(X,Y).
(258)
which finishes the proof of Claim E.1.
Let us come back to the proof of Lemma 3.3. The rest of the proof is just algebraic computation. According to (251), we have
p 4
d2 + 2 √ ρ Σmin d √ G(X,Y)
≤ 〈∇X F̃(X,Y), X − U〉 + 〈∇Y F̃(X,Y),Y − V〉 ≤ (‖∇X F̃(X,Y)‖F + ‖∇Y F̃(X,Y)‖F) max{‖X − U‖F , ‖Y − V‖F} (51b) ≤ √ 2 √ ‖∇X F̃(X,Y)‖2F + ‖∇Y F̃(X,Y)‖2F 17 2 √ r βT Σmin d = ‖∇F̃(X,Y)‖F 17 √
2
√ r βT
Σmin d.
Eliminating a factor of d from both sides and taking square, we get
‖∇F̃(X,Y)‖2F 289 2 r β2T
Σ2min ≥
( p 4 d + 2 √ ρ
Σmin
√ G(X,Y) )2 ≥ pd 2
16 + 4ρ Σ2min G(X,Y).
(259)
By the definition of βT in (15), we have
r β2T
Σ2min = r CT rΣmax Σ2min = CT r2κ Σmin .
According to Claim 3.1, we have
pd2 = p‖M − XYT ‖2F ≥ 1 2 ‖PΩ(M − XYT )‖2F = F(X,Y).
By the definition of ρ in (17) and the definition of δ0 in (16), we have
4ρ Σ2min = 4 Σ2min 8pδ20 = 32p Σ2min 1 36
Σ2min
C2dr 3κ2
= 8 9 1 C2dr 3κ2 p.
Substituting the above three relations into (259), we get (when Cd ≥ 32/3)
‖∇F̃(X,Y)‖2F 289 2 CT r2κ Σmin ≥ p 32 F(X,Y) + 8 9 1 C2dr 3κ2 pG(X,Y)
≥ 8 9 1 C2dr 3κ2 p(F(X,Y) + G(X,Y)) = 8 9 1 C2dr 3κ2 pF̃(X,Y).
This can be further simplified to
‖∇F̃(X,Y)‖2F ≥ Σmin
Cgr5κ3 pF̃(X,Y),
where the numerical constant Cg = 260116 CT C 2 d. This finishes the proof of Lemma 3.3.
Dialog State Tracking: The process of constantly representing the state of the dialog is called dialog state tracking (DST). Most industrial systems use rule-based heuristics to update the dialog state by selecting a high-confidence output from the NLU (Williams et al., 2013). Numerous advanced statistical methods have been proposed to exploit the correlation between turns to make the system more robust given the uncertainty of the automatic speech recognition (ASR) and the NLU (Bohus and Rudnicky, 2006; Thomson and Young, 2010). The Dialog State Tracking Challenge (DSTC) (Williams et al., 2013) formalizes the problem as a supervised sequential labelling task where the state tracker estimates the true slot values based on a sequence of NLU outputs. In practice the output of the state tracker is used by a different dialog policy, so that the distribution
in the training data and in the live data are mismatched (Williams et al., 2013). Therefore one of the basic assumptions of DSTC is that the state tracker’s performance will translate to better dialog policy performance. Lee (2014) showed positive results following this assumption by showing a positive correlation between end-to-end dialog performance and state tracking performance.
Reinforcement Learning (RL): RL has been a popular approach for learning the optimal dialog policy of a task-oriented dialog system (Singh et al., 2002; Williams and Young, 2007; Georgila and Traum, 2011; Lee and Eskenazi, 2012). A dialog policy is formulated as a Partially Observable Markov Decision Process (POMDP) which models the uncertainty existing in both the users’ goals and the outputs of the ASR and the NLU. Williams (2007) showed that POMDP-based systems perform significantly better than rule-based systems especially when the ASR word error rate (WER) is high. Other work has explored methods that improve the amount of training data needed for a POMDP-based dialog manager. Gašić (2010) utilized Gaussian Process RL algorithms and greatly reduced the data needed for training. Existing applications of RL to dialog management assume a given dialog state representation. Instead, our approach learns its own dialog state representation from the raw dialogs along with a dialog policy in an end-to-end fashion.
End-to-End SDSs: There have been many attempts to develop end-to-end chat-oriented dialog systems that can directly map from the history of a conversation to the next system response (Vinyals and Le, 2015; Serban et al., 2015). These methods train sequence-to-sequence models (Sutskever et al., 2014) on large human-human conversation corpora. The resulting models are able to do basic chatting with users. The work in this paper differs from them by focusing on building a task-oriented system that can interface with structured databases and provide real information to users.
Recently, Wen el al. (2016) introduced a network-based end-to-end trainable taskedoriented dialog system. Their approach treated a dialog system as a mapping problem between the dialog history and the system response. They learned this mapping via a novel variant of the encoder-decoder model. The main differences between our models and theirs are that ours has the advantage of learning a strategic plan using
RL and jointly optimizing state tracking beyond standard supervised learning.
In large games, it is often impractical to maintain the Q-value for all possible state-action pairs. One solution to this problem is to approximate Q(s, a) using a parametrized function Q(s, a; θ) which can generalize over states and actions using higher level features ([Sutton and Barto, 1998]). To be able to discover these features without feature engineering, we need a neural network function approximator. The DQN ([Mnih et al., 2015]) approximates the Q-value function with a deep neural network to be able to predict Q(s, a) over all actions a, for all states s. The loss function used for learning a Deep Q Network is as below :
Li(θi) = Es,a,r,s′ [(yDQNi −Q(s, a; θi)) 2],
with yDQNi = (ri + γmaxa′Q(s ′, a′, θ−)) Here, Li represents the expected TD error corresponding to current parameter estimate θi. θ− represents the parameters of a separate target network, while θi represents the parameters of the online network. The usage of a target network is to improve the stability of the learning updates. The gradient descent step is shown below:
∇θiLi(θi) = Es,a,r,s′ [(y DQN i −Q(s, a; θi))∇θiQ(s, a)]
To avoid correlated updates from learning on the same transitions that the current network simulates, an experience replay ([Lin, 1993]) D (of fixed maximum capacity) is used,
where the experiences are pooled in a FIFO fashion. Transitions from previous episodes are sampled multiple times to update the current network, thereby avoiding divergence issues due to correlated updates. In this paper, we sample transitions from D uniformly at random as in [Mnih et al., 2015]. However, [Schaul et al., 2016] and [Wang et al., 2016] show that having a prioritized sampling can lead to substantial improvement in performance.
We now discuss the CNN implementation of RIS and RULE. For simplicity, we assume attribute semantics. Sections 5 and 6 extend the treatment to other concepts. For quick consultation, Table 1 summarizes important notation used in the rest of the paper.
For some applications the batch updates of x0 – as implied by the continuous time analysis and defined in Algorithm 1 – might not be feasible. Algorithm 2 presents a modified version of LiF (denoted LiF-II) in which x0 is updated every observation. LiF-II starts by filling up a buffer of length T which we denote by the vector ~yω = {NA1, . . . , NAT }, after which each observation leads to an update of x0. In the algorithm description the values yt−T , . . . , yt are stored in the vector ~yω. By defining the learn rate as γ T the tuning parameters in LiF-II are the same as those discussed for LiF-I.
Algorithm 2 Implementation of LiF-II for single variable maximization in data stream using a batch. Require: x0, A, T , γ, ~yω = {NA1, . . . , NAT } ω = 2π
T for t = 1, . . . , T do xt = x0 +A cosωt yt = f(x0 +A cosωt) + t ~yω = push(~yω , yt cosωt) if (t > T ) then y∗ω = ( ∑ ~yω)/T
x0 = x0 + γ T y∗ω
end if end for
distributed teams of cooperating agents, both human and machine. We characterize the domainindependent challenges posed by this problem, and describe how properties of domains influence the challenges and their solutions. We will concentrate on dynamic, data-rich domains where humans are ultimately responsible for team behavior. Thus, the automated aid should interactively support effective and timely decision making by the human. We present a domain-independent categorization of the types of alerts a plan-based monitoring system might issue to a user, where each type generally requires different monitoring techniques. We describe a monitoring framework for integrating many domain-specific and task-specific monitoring techniques and then using the concept of value of an alert to avoid operator overload.
We use this framework to describe an execution monitoring approach we have used to implement Execution Assistants (EAs) in two different dynamic, data-rich, real-world domains to assist a human in monitoring team behavior. One domain (Army small unit operations) has hundreds of mobile, geographically distributed agents, a combination of humans, robots, and vehicles. The other domain (teams of unmanned ground and air vehicles) has a handful of cooperating robots. Both domains involve unpredictable adversaries in the vicinity. Our approach customizes monitoring behavior for each specific task, plan, and situation, as well as for user preferences. Our EAs alert the human controller when reported events threaten plan execution or physically threaten team members. Alerts were generated in a timely manner without inundating the user with too many alerts (less than 10% of alerts are unwanted, as judged by domain experts).
In the above we showed that multilinearity in PoS is sufficient for (π∗, xPEV ) to be ex-post truthful. Here we show that the multilinearity is also necessary.
Theorem 2. If (π∗, xPEV ) is ex-post truthful for all type profiles θ ∈ Θ, then for all i ∈ N , vi is multilinear in PoS.
Proof. By contradiction, assume that vi of agent of type θi is not multilinear in PoS, i.e., there exist a θ−i, an allocation τ ∈ T , and a j ∈ N (without loss of generality, assume that j 6= i) such that:
vi(τ, p τ ) 6= pτj × vi(τ, (1, p τ −j)) + (1− p τ j )× vi(τ, (0, p τ −j)) (7)
Under efficient allocation choice function π∗, it is not hard to find a type profile θ̂−i such that π∗(θi, θ̂−i) = τ and the PoS profile is the same between θ−i and θ̂−i. We can choose θ̂−i by setting v̂j(τ, pτ ) to a sufficiently large value for each j 6= i.
Applying (π∗, xPEV ) on profile (θi, θ̂−i), when j finally successfully completes her tasks τj , her utility is u1j = v̂j(τ, (1, p τ −j))−hj((θi, θ̂−i)−j)+V 1 −j((θi, θ̂−i), π ∗) and her utility if she fails is u0j = v̂j(τ, (0, p τ −j)) − hj((θi, θ̂−i)−j) + V 0 −j((θi, θ̂−i), π
∗). Thus, j’s expected utility is (note that p̂τj = p τ j ):
pτj×u 1 j + (1− p τ j )× u 0 j =
pτj × vi(τ, (1, p τ −j)) (8)
+ (1− pτj )× vi(τ, (0, p τ −j)) (9)
+ pτj ∑
k∈N\{i}
v̂k(τ, (1, p τ −j)) (10)
+ (1− pτj ) ∑
k∈N\{i}
v̂k(τ, (0, p τ −j)) (11)
− hj(θ−j).
Given the assumption (7), terms (8) and (9) together can be written as vi(τ, pτ ) + δi where δi = (8) + (9) − vi(τ, pτ ). Similar substitutions can be carried out for all other agents k ∈ N \ {i} in terms (10) and (11) regardless of whether vk is mutlilinear in PoS. After this substitution, j’s utility can be written as:
pj×u 1 j + (1 − pj)× u 0 j =
vi(τ, p τ ) +
∑
k∈N\{i}
v̂k(τ, p τ ) (12)
+ ∑
k∈N
δk (13)
− hj(θ−j).
Now consider a suboptimal allocation τ̂ 6= τ , if τ̂ is chosen by the mechanism, then j’s utility can be written as:
ûj =
vi(τ̂ , p τ̂ ) +
∑
k∈N\{i}
v̂k(τ̂ , p τ̂ ) (14)
+ ∑
k∈N
δ̂k (15)
− hj(θ−j).
In the above two utility representations, we know that terms (12) > (14) because π∗ is efficient, but terms (13) and (15) can be any real numbers.
In what follows, we tune the valuation of j such that the optimal allocation is either τ or τ̂ , and in either case j is incentivized to misreport.
In the extreme case where all agents except i’s valuations are multilinear in PoS, we have δk = 0, δ̂k = 0 for all k 6= i in (13) and (15). Therefore, ∑
k∈N δk = δi 6= 0
and ∑
k∈N δ̂k = δ̂i (possibly = 0). It might be the case that
δi = δ̂i, but there must exist a setting where δi 6= δ̂i, otherwise vi is multilinear in PoS, because constant δi for any PoS does not violate the multilinearity definition.
1. If δi > δ̂i, we have (12) + δi > (14) + δ̂i. In this case, we can increase v̂j(τ̂ , pτ̂ ) such that τ̂ becomes optimal, i.e., (12) < (14), but (12) + δi > (14) + δ̂i still holds. Therefore, if j’s true valuation is the one that chooses τ̂ as the optimal allocation, then j would misreport to get allocation τ which gives her a higher utility.
2. If δi < δ̂i, we can easily modify v̂j(τ̂ , pτ̂ ) such that (12)+ δi < (14) + δ̂i but (12) > (14) still holds. In this case, if j’s true valuation again is the one just modified, j would misreport to get allocation τ̂ with a better utility.
In both of the above situations, agent j is incentivized to misreport, which contradicts that (π∗, xPEV ) is ex-post truthful. Thus, vi has to be multilinear in PoS.
It is worth mentioning that Theorem 2 does not say that given a specific type profile θ, all vi have to be multilinear in PoS for (π∗, xPEV ) to be ex-post truthful. Take the delivery example from Table 1 and change agent 2’s valuation for τ to be v2(τ, pτ ) = (pτ1)
2 × pτ2 which is not multilinear in PoS. It is easy to check that under this change, no agent can gain anything by misreporting if the other agent reports truthfully. However, given each agent i of valuation vi, to truthfully implement (π∗, xPEV ) in an ex-post equilibrium for all possible type profiles of the others, Theorem 2 says that vi has to be multilinear in PoS, otherwise, there exist settings where some agent is incentivized to misreport.
Inferencing is a mechanism by which a set of rules that represent domain expert knowledge are used to logically derive additional domain specific knowledge. SPARQL [14] is a standard RDF query language recommended by the World Wide Web Consortium (W3C) and considered as an essential graph-matching query language. More specifically, a SPARQL query consists of a body, a complex RDF graph pattern matching expression (which may include basic graph patterns, group graph patterns, optional graph patterns, alternative graph patterns and patterns
on named graphs), and a head, an expression that indicates how to construct an answer to the query [45]. More specially, SPARQL has four types of query forms (Select, Construct, Ask and Describe) to define four types of result formats.
SPARQL [14] can be used for inferencing because the SPARQL Construct query form returns an RDF graph that is formed by taking each query solution in sequence, substituting for the variables in the graph pattern, and combining the triples into a single RDF graph. In other words, the Construct query form can derive new triples when the graph patterns match. The returned RDF graph can be directly inserted into a repository by using the Insert graph update operation. Consider the following example using a SPARQL Construct query form and Insert graph update operation to infer a fire weather index value. In order to infer fire weather indices, we collected 241 fire weather index rules in total from the meteorologists. These rules were represented as SPARQL queries and then stored in the multiple repository storage. For example, one of these rules is expressed as: “if relative humidity >= 80% AND 17.5 m/s <= wind speed <=24.4m/s AND 32 °C <= air temperature <= 41 °C, for a given location at time T, then at time T this
location has a High fire weather index.” We can interpret this rule statement into a SPARQL rule expressed as follow:
Construct {
?FireEvent_1 prov:atLocation ?node. ?FireEvent_1 prov:atTime ?T. ?FireEvent_1 rdf:type fwi:High.
}
Where{
?RH_OB1 ssn:ObservedProperty cf:relative_humidity. ?RH_OB1
ssn:ObservationSamplingTime ?T. ?RH_OB1 dul:unitOfMeasure unit:percent. ?RH_OB1 ssn:ObservedBy ? RH_Sensor1. ?RH_Sensor1 ssn:deployedOnPlatform ?node. ?RH_Sensor1 ssn:hasValue ?RH_OB1V. ?WS_OB1 ssn:ObservedProperty cf:wind_speed. ?WS_OB1 ssn:ObservationSamplingTime ?T. ?WS_OB1 dul: unitOfMeasure unit:meterPerSecond. ?WS_OB1 ssn:ObservedBy ?WS_Sensor1. ?WS_Sensor1 ssn:deployedOnPlatform ?node. ?WS OB1 ssn:hasValue ?WS_OB1V. ?AT_OB1 ssn:ObservedProperty cf:air_temperature. ?AT_OB1 ssn:ObservationSamplingTime ?T ?AT_OB1 dul: unitOfMeasure unit:degreeCelsius ?AT_OB1 ssn:ObservedBy ?AT_Sensor1. ?AT_Sensor1 ssn:deployedOnPlatform ?node. ?AT_Sensor1 ssn:hasValue ?AT_OB1V. Filter( ?RH_OB1V>=80&&?RH_OB1V<=100&& ?WS_OB1V>=17.5&&?WS_OB1V<=24.4&& ?AT_OB1V>=32&&?AT_OB1V<=41)
}
Suppose that we collected three type of sensor observations (relative humidity: 85%,
wind speed: 23.3m/s, and air temperature: 40°C) from the sensor node 1 at 2012-01-
02T12:00:00, then these sensor observations can be expressed in RDF format as shown in
Figure 5. When we apply the above inference rule to this data set, then it generates new triples (shown in Figure 5 shaded pink), which are the inference results that are saved to the FWI Repository.
The final stage in our system is to select which candidate slot value (or slot values in the case of listvalued slots) to return as the correct answer from the candidate slot values extracted by the relation extractor in the previous stage. To do this we rank the
candidates identified in the candidate slot value extraction stage. Two factors are considered in ranking the candidates: (1) the number of times a value has been extracted, and (2) the confidence score provided for each candidate by the relation extractor classifier. If any value in the list of possible slot values occurs more than three times, then the system uses the number of occurrences as a ranking factor. Otherwise, the system uses the confidence score as a ranking factor. In the first case candidate slot values are sorted on the basis of number of occurrences. In the second case values are sorted on the basis of confidence score. In both cases the top n value(s) are taken as the correct slot value(s) for the given slot query. We use n = 1 for single-valued slots n = 3 for list-valued slots.
Once the system selects the final slot value(s), the final results are written to a file in the format required by the TAC guidelines.
We experiment with several languages: Italian, Spanish, German and Chinese. For Italian, Spanish and German we use Europarl (Koehn, 2005), containing around 1.9M sentences for each language pair, and the TED talks corpus (Cettolo et al., 2012) for Chinese, containing around 120K sentences. For the purpose of training the AMR parsers, we extract from each parallel corpus 20,000 sentences for training, 2,000 for development and 2,000 for testing; we collect two such datasets for each language, in order to have non-overlapping datasets for the two stages of the process (e → f and f → e). We use the remainder of the parallel corpora to train the word alignment models. The gold AMR dataset used is LDC2015E86, containing 16,833 training sentences, 1,368 development sentences, and 1,371 testing sentences.
Word alignments were generated using fast align (Dyer et al., 2013), while AMR alignments were generated with JAMR (Flanigan et al., 2014). As an English AMR parser and a starting point to develop the target parsers we used AMREager (Damonte et al., 2017), which is an open-source AMR parser for English that requires only small modifications for re-use on other languages. The parser requires tokenization, POS tagging, NER tagging and dependency
parsing, which for English, German and Chinese are provided by CoreNLP (Manning et al., 2014). Although CoreNLP also supports Spanish, dependency parsing is not provided. We use Freeling (Carreras et al., 2004) instead. Italian is not supported in CoreNLP: we use Tint (Aprosio and Moretti, 2016), which is a CoreNLP-compatible NLP pipeline for Italian.
Frame stacking could substantially reduce the training time, and it could also have the same effect on decoding phase. It has been demonstrated in CTC systems [9]. As CTC is phone-level modeling, the granularity after stacking is still suitable for CTC decoding. But conventional RNNs is statelevel modeling and weighted finite state transducer (WFST) is state-level correspondingly, so the granularity is too large to decode. In order to maintain the decoding granularity, frame retaining is proposed as presented in Figure 2.
The size of frame stacking window is denoted asN . After N successive frames are extracted in a signal stream, they are stacked to a super frame in the same way of training phase. Consequently, the super frame retains forN frames time with frame retaining method. The neighboring frame has similar properties, so super frame represents them, after aggregating their features. In traditional decoding method, features of each frame needs to pass through the network, and N frames mean N times forward passes. But a super frame passes through the network only once carrying the all information
of N frames, and the result of the super frame’s forward pass is multiplexed at the rest of frame time. Moreover, WFST does not need to rebuild for frame stacking. Thus, decoder spends less time in general, as N − 1 times of forward pass is skipped and computation consumption of one forward pass increases only a little.
A stratified normal logic program has the useful property that its universally adopted semantics produces a single interpretation (and is equal to its stable and well-founded semantics). Because every total choice of a stratified plp produces a stratified normal logic program, the credal/well-founded semantics of a stratified plp is a unique distribution.
One might fear that in moving from acyclic to stratified programs we must pay a large penalty. This is not the case: the complexity classes remain the same as in Section 6.1:
Theorem 24. For locally stratified plps, inferential complexity is PEXP-complete; it is PPNP-complete for plps with bounded predicate arity; it is PP-complete for propositional plps. For locally stratified plps, query complexity is PP-complete.
Proof. For propositional stratified plps, hardness comes from the fact that a Bayesian network on binary random variables can be encoded by a stratified program (indeed, by an acyclic program), and inference with such networks is PP-complete (Darwiche, 2009; Roth, 1996). Membership is obtained using the same reasoning in the proof of Theorem 19, only noting that, once the probabilistic facts are selected, logical reasoning with the resulting stratified normal logic program can be done with polynomial effort (Eiter et al., 2007, Table 2).
For stratified programs with bounded predicate arity, hardness follows from Theorem 19. Membership is obtained using the same reasoning in the proof of Theorem 19; in fact that proof of membership applies directly to stratified programs with bounded arity.
For general stratified plps, hardness is argued as in the proof of Theorem 20. Membership follows from the fact that we can ground the plp into an exponentially large propositional plp. Once the (exponentially-many) probabilistic facts are selected, the Turing machine is left with a stratified propositional normal logic program, and logical inference is polynomial in the size of this program (that is, logical inference requires exponential effort).
Finally, hardness of query complexity follows from Theorem 21. Membership is obtained using the same reasoning in the proof of Theorem 19, only noting that, once the probabilistic facts are selected, logical reasoning with the resulting stratified normal logic program can be done with polynomial effort as guaranteed by the analysis of data complexity of stratified normal logic programs (Dantsin et al., 2001).
We noted, at the end of Section 6.1, that some sub-classes of acyclic programs display polynomial behavior. We now show an analogue result for a sub-class of definite (and therefore stratified, but possibly cyclic) programs with unary and binary predicates:
Proposition 25. Inferential complexity is polynomial for queries containing only true, for plps where: (a) every predicate is unary or binary, and facts can be asserted about them; (b) probabilistic facts can be of the form α :: a(X)., α :: a(a), α :: r(X,Y ) (that is, each unary predicate can be associated with ground or non-ground probabilistic facts, while each binary predicate can be associated to a particular non-ground probabilistic fact); (c) no binary predicate is the head of a rule that has a body; (d) each atom is the head of at most one rule that has a body, and only the three following rule forms are allowed:
a(X) :− a1(X), . . . , ak(X) . a(X) :− r(X,Y ) . a(X) :− r(Y,X) ..
Proof. We show that the inference can be reduced to a tractable weighted model counting problem. First, ground the program in polynomial time (because each rule has at most two logical variables). Since the resulting program is definite, only atoms that are ancestors of the queries in the grounded dependency graph are relevant for determining the truth-value of the query in any logic program induced by a total choice (this follows as resolution is complete for propositional definite programs). Thus, discard all atoms that are not ancestors of a query atom. For the query to be true, the remaining atoms that are not probabilistic facts are forced to be true by the semantics. So collect all rules of the sort a(a) :− r(a, b) ., a(a) :− r(b, a) ., plus all facts and all probabilistic facts. This is an acyclic program, so that its Clark completion gives the stable model semantics. This completion is a formula containing a conjunction of subformulas a(a) ⇔ ∨
b r(a, b), a(a) ⇔ ∨
a r(a, b), and unit (weighted) clauses corresponding to (probabilistic) facts. The query is satisfied only on models where the lefthand side of the definitions are true, which is equivalent to reducing the subformulas to their righthand side. The resulting weighted model counting problem has been shown to be polynomial-time solvable (Mauá & Cozman, 2015).

Let B be a set of Boolean variables. A literal is a Boolean variable b ∈ B or its negation ¬b. The negation of a literal `, denoted ¬`, is defined as ¬b if ` = b and as b if ` = ¬b. The Boolean constants 1 and 0 represent true and false, respectively. The set of literals is denoted L and L0,1 = L ∪ {0, 1}. The set of (free) Boolean variables that appear in a Boolean formula ϕ is denoted vars(ϕ). We extend the vars function to sets of formulae in the natural way.
An assignment, A, is a partial mapping from Boolean variables to constants, often viewed as the following set of literals: { b ∣∣ A(b) = 1 }∪ { ¬b ∣∣ A(b) = 0 }. For a formula ϕ and b ∈ B, we denote by ϕ[b] (likewise ϕ[¬b]) the formula obtained by substituting all
occurrences of b ∈ B in ϕ by true (false). This notation extends in the natural way for sets of literals. We say that A satisfies ϕ if vars(ϕ) ⊆ vars(A) and ϕ[A] evaluates to true. A Boolean Satisfiability (SAT) problem consists of a Boolean formula ϕ and determines if there exists an assignment which satisfies ϕ.
A Boolean equality is a constraint ` = `′ where `, `′ ∈ L0,1. An equi-formula E is a set of Boolean equalities understood as a conjunction. The set of Boolean equalities is denoted Leq0,1 and the set of equi-formulae is denoted E .
Example 1. Suppose B = {x, y, z}. Then L0,1 = {0, 1,¬x, x,¬y, y,¬z, x}. An example assignment is A = {x,¬z}, while B = {x, y, z,¬y} is not an assignment (since it includes {y,¬y}). Given the formula ϕ = x ↔ (y ∨ ¬z) then ϕ[¬x] is the formula 0 ↔ (y ∨ ¬z) or equivalently ¬y∧ z. The formula ϕ[A] = 1↔ (y∨ 1) which is equivalent to true, but A does not satisfy ϕ since vars(ϕ) = {x, y, z} 6⊆ {x, z} = vars(A). An example equi-formula for B is {x = 0, y = ¬z} or equivalently ¬x ∧ (y ↔ ¬z).
This capability is used by KGP agents to determine the preconditions for the executability of actions which are planned for. These preconditions are defined in the domain-dependent part of the EC by means of a set of rules of the form precondition(O, F ), representing that the fluent F is a precondition for the executability of an action with action operator O (see 5.1.1). Let KBpre be the subset of KBTR containing the rules defining precondition( , ).
Then the identification of preconditions capability |=pre is specified as follows. Given a state S = 〈KB0,F , C, Σ〉 and a timed action literal a[τ ]
S, a[τ ] |=pre Cs
iff
Cs = ∧{`[τ ] | KBpre |=LP precondition(a, `)}14.
(µτ − λ1)xτ−1. Notice also that (M4) is equivalent to µ2τ = µτ+1(µτ+1 − λ1λτ+1). Incorporating these
arguments and (M3) into (10),
ητ+1µ 2 τ+1
[
Fτ+1 ( xτ+1 | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1
)]
≤ ητ+1µ2τ [ Fτ+1 ( xτ | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1 )]
− 1 2 ∥ ∥ ∥ ∥ ητ+1λτ+1µτ+1 βτ+1 ζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2
+ 1
2
∥ ∥ ∥ ∥
ητλτµτ βτ
ζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2
+ ητ+1µ
2 τ+1λτ+1
2β2τ+1 (Lτ+1β
2 τ+1 − 2βτ+1 + ητ+1λτ+1)
× ‖ζτ+1 − ψτ+1‖2 (11) ≤ ητ+1µ2τ [ Fτ+1 ( xτ | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1 )]
− 1 2 ∥ ∥ ∥ ∥ ητ+1λτ+1µτ+1 βτ+1 ζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2
+ 1
2
∥ ∥ ∥ ∥
ητλτµτ βτ
ζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2 .
The previous inequality suggests that
ητ+1µ 2 τ+1
[
EO|X { Fτ+1 ( xτ+1 | x(−b)τ+1 )} − EO|X { Fτ+1 ( x̄(b) | x(−b)τ+1 )}]
+ 1
2 EO|X
{∥ ∥ ∥ ∥
ητ+1λτ+1µτ+1 βτ+1 ζτ+1
+ vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2}
≤ ητ+1µ2τ [ EO|X { Fτ+1 ( xτ | x(−b)τ+1 )}
− EO|X { Fτ+1 ( x̄(b) | x(−b)τ+1
)}]
+ 1
2 EO|X
{∥ ∥ ∥ ∥
ητλτµτ βτ
ζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2}
which, according to As0, results in
ητ+1µ 2 τ+1
[
F ( xτ+1 | x(−b)τ+1 ) − F∗ + F∗ − F ( x̄(b) | x(−b)τ+1
)]
+ 1
2 EO|X
{∥ ∥ ∥ ∥
ητ+1λτ+1µτ+1 βτ+1 ζτ+1
+ vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2}
≤ ητ+1µ2τ [ F ( xτ | x(−b)τ+1 ) − F∗ + F∗ − F ( x̄(b) | x(−b)τ+1 )]
+ 1
2 EO|X
{∥ ∥ ∥ ∥
ητλτµτ βτ
ζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2} .
After some elementary algebra,
ητ+1µ 2 τ+1
[
F ( xτ+1 | x(−b)τ+1 ) − F∗
]
+ 1
2 EO|X
{∥ ∥ ∥ ∥
ητ+1λτ+1µτ+1 βτ+1 ζτ+1
+ vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2}
≤ ητ+1µ2τ [ F ( xτ | x(−b)τ+1 ) − F∗ ]
+ 1
2 EO|X
{∥ ∥ ∥ ∥
ητλτµτ βτ
ζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2}
+ ητ+1(µ 2 τ+1 − µ2τ )
[
F ( x̄(b) | x(−b)τ+1 ) − F∗
]
≤ ητ+1µ2τ [ F ( xτ | x(−b)τ+1 ) − F∗ ]
+ 1
2 EO|X
{∥ ∥ ∥ ∥
ητλτµτ βτ
ζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2}
+ η0(µ 2 τ+1 − µ2τ )
[
F ( x̄(b) | x(−b)τ+1 ) − F∗
]
.
Consequently,
ητ+1µ 2 τ+1
[ E{F (x(b)τ+1)} − F∗ ]
+ 1
2 E
{∥ ∥ ∥ ∥
ητ+1λτ+1µτ+1 βτ+1
ζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2}
≤ ητ+1µ2τ [ E { F ( x(b)τ | x(−b)τ+1 )} − F∗ ]
+ 1
2 E
{∥ ∥ ∥ ∥
ητλτµτ βτ
ζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2}
+ η0(µ 2 τ+1 − µ2τ )
[
E { F ( x̄(b) | x(−b)τ+1 )} − F∗
]
≤ ητ+1µ2τ [ E{F (x(b−1)τ+1 )} − F∗ ]
(12a)
+ 1
2 E
{∥ ∥ ∥ ∥
ητλτµτ βτ
ζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2}
+ η0(µ 2 τ+1 − µ2τ )
[
E { F ( x̄(b) | x(−b)τ+1 )} − F∗
]
≤ ητ+1µ2τ [ E{F (x(b)τ )} − F∗ ]
(12b)
+ 1
2 E
{∥ ∥ ∥ ∥
ητλτµτ βτ
ζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2}
+ η0(µ 2 τ+1 − µ2τ )
[
E { F ( x̄(b) | x(−b)τ+1 )} − F∗
]
≤ ητµ2τ [ E{F (x(b)τ )} − F∗ ]
+ 1
2 E
{∥ ∥ ∥ ∥
ητλτµτ βτ
ζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2}
+ η0(µ 2 τ+1 − µ2τ )
[
E { F ( x̄(b) | x(−b)τ+1 )} − F∗
]
, (12c)
where Table 2a (line 12) and the monotonicity in Thm. 1.1 were used in (12a) and (12b), while (M2)
was utilized in (12c).
Let now ϑ(x (b) ∗ ) := lim supτ→∞ E{F (x(b)∗ | x(−b)τ )}, where x(b)∗ was defined in As3. Since (12c) holds for any x̄(b), by x̄(b) := x (b) ∗ and the definition of lim sup, it is straightforward to verify that ∀ǫ > 0, there exists τ ′0 ∈ Z≥0 s.t. ∀τ ≥ τ ′0,
ητ+1µ 2 τ+1
[ E{F (x(b)τ+1)} − F∗ ]
+ 1
2 E
{∥ ∥ ∥ ∥
ητ+1λτ+1µτ+1 βτ+1
ζτ+1 + vτ+1 − λ1x(b)∗ ∥ ∥ ∥ ∥
2 }
≤ ητµ2τ [ E{F (x(b)τ )} − F∗ ]
+ 1
2 E
{∥ ∥ ∥ ∥
ητλτµτ βτ
ζτ + vτ − λ1x(b)∗ ∥ ∥ ∥ ∥
2 }
+ η0(µ 2 τ+1 − µ2τ )
[
ϑ ( x (b) ∗ ) − F∗ + ǫ′
]
,
where ǫ′ := ǫη̌/η0. Hence, by As3,
0 ≤ ητ+1µ2τ+1 [ E{F (x(b)τ+1)} − F∗ ]
+ 1
2 E
{∥ ∥ ∥ ∥
ητ+1λτ+1µτ+1 βτ+1
ζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥
2 }
≤ ητ0µ2τ0 [ E{F (x(b)τ0 )} − F∗ ]
+ 1
2 E
{∥ ∥ ∥ ∥
ητ0λτ0µτ0 βτ0
ζτ0 + vτ0 − λ1x̄(b) ∥ ∥ ∥ ∥
2 }
+ η0ǫ ′
τ∑
τ ′=τ ′0
( µ2τ ′+1 − µ2τ ′ ) ,
and
0 ≤ [ E{F ( x (b) τ+1 ) } − F∗ ]
+ 1
2ητ+1µ2τ+1 E
{∥ ∥ ∥ ∥
ητ+1λτ+1µτ+1 βτ+1 ζτ+1
+ vτ+1 − λ1x(b)∗ ∥ ∥ ∥ ∥
2 }
≤ 1 ητ+1µ2τ+1
[
ητ0µ 2 τ0 ( E{F (x(b)τ0 )} − F∗ )
+ 1
2 E
{∥ ∥ ∥ ∥
ητ0λτ0µτ0 βτ0 ζτ0 + vτ0 − λ1x (b) ∗
∥ ∥ ∥ ∥
2 }]
+ η0ǫ ′µ 2 τ+1 − µ2τ0 ητ+1µ2τ+1
≤ 4 λ21η̌ ( 1 + ∑τ+1 τ ′=1 λτ ′ )2
[
ητ0µ 2 τ0 ( E{F (x(b)τ0 )} − F∗ ) (13a)
+ 1
2 E
{∥ ∥ ∥ ∥
ητ0λτ0µτ0 βτ0 ζτ0 + vτ0 − λ1x (b) ∗
∥ ∥ ∥ ∥
2 }]
+ η0 η̌ ǫ′ ( 1− µ 2 τ0
µ2t+1
)
≤ 4 λ21η̌ ( 1 + λ̌(1 + τ) )2
[
ητ0µ 2 τ0 ( E{F (x(b)τ0 )} − F∗ )
+ 1
2 E
{∥ ∥ ∥ ∥
ητ0λτ0µτ0 βτ0 ζτ0 + vτ0 − λ1x (b) ∗
∥ ∥ ∥ ∥
2 }]
+ η0 η̌ ǫ′
≤ 4 λ21λ̌ 2η̌(1 + τ)2
[
ητ0µ 2 τ0 ( E{F (x(b)τ0 )} − F∗ )
+ 1
2 E
{∥ ∥ ∥ ∥
ητ0λτ0µτ0 βτ0 ζτ0 + vτ0 − λ1x (b) ∗
∥ ∥ ∥ ∥
2 }]
+ η0 η̌ ǫ′ , (13b)
where Fact 1.2 was utilized in (13a). The previous inequality establishes the claim of Thm. 1.3.
4) By (M3),
1− √
1− ητλτL(b)τ L (b) τ ≤ βτ ≤ 1 +
√
1− ητλτL(b)τ L (b) τ . (14)
Moreover, since the preceding discussion holds for any over-estimate L (b) τ of the underlying Lipschitz constants (cf. Remark 3), one can always set a sufficiently small Ľ ∈ R>0 as a lower-bound on all L(b)τ , i.e., L (b) τ ≥ Ľ, ∀(τ, i). Accordingly, the right hand side of (14) suggests that βτ ≤ 2/Ľ. Given also that ητ ≥ η̌, λτ ≥ λ̌, and As4, then there exists δ > 0 s.t. −ητλτ (L(b)τ β2τ − 2βτ + ητλτ )/(2β2τ ) ≥ δ, ∀τ . As a result, (11) implies that for any x̄(b),
µ2τ+1δ‖ζτ+1 − ψτ+1‖2
≤ − ητ+1µ 2 τ+1λτ+1
2β2τ+1
( L (b) τ+1β 2 τ+1 − 2βτ+1 + ητ+1λτ+1 )
× ‖ζτ+1 − ψτ+1‖2
≤ ητ+1µ2τ [ Fτ+1 ( xτ | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1 )]
− ητ+1µ2τ+1 [ Fτ+1 ( xτ+1 | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1 )]
− 1 2 ∥ ∥ ∥ ∥ ητ+1λτ+1µτ+1 βτ+1 ζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2
+ 1
2
∥ ∥ ∥ ∥
ητλτµτ βτ
ζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2 ,
and
‖ζτ+1 − ψτ+1‖2
≤ητ+1µ 2 τ
δµ2τ+1
[
Fτ+1 ( xτ | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1
)]
− ητ+1 δ
[
Fτ+1 ( xτ+1 | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1
)]
− 1 2δµ2τ+1
∥ ∥ ∥ ∥
ητ+1λτ+1µτ+1 βτ+1
ζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2
+ 1
2δµ2τ+1
∥ ∥ ∥ ∥
ητλτµτ βτ
ζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2
≤ητ+1 δ
[
Fτ+1 ( xτ | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1
)]
− ητ+1 δ
[
Fτ+1 ( xτ+1 | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1
)]
− 1 2δµ2τ+1
∥ ∥ ∥ ∥
ητ+1λτ+1µτ+1 βτ+1
ζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2
+ 1
2δµ2τ
∥ ∥ ∥ ∥
ητλτµτ βτ
ζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2
= ητ+1 δ
[
Fτ+1 ( xτ | x(−b)τ+1 ) − Fτ+1 ( xτ+1 | x(b)τ+1
)]
− 1 2δµ2τ+1
∥ ∥ ∥ ∥
ητ+1λτ+1µτ+1 βτ+1
ζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2
+ 1
2δµ2τ
∥ ∥ ∥ ∥
ητλτµτ βτ
ζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2
≤ητ+1 δ
[
Fτ+1 ( x (b−1) τ+1 ) − Fτ+1 ( x (b) τ+1
)]
− 1 2δµ2τ+1
∥ ∥ ∥ ∥
ητ+1λτ+1µτ+1 βτ+1
ζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2
+ 1
2δµ2τ
∥ ∥ ∥ ∥
ητλτµτ βτ
ζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2 .
Applying expectations to the previous inequality yields
E { ‖ζτ+1 − ψτ+1‖2 } ≤ η0 δ [ E { F ( x (b−1) τ+1 )} − E { F ( x (b) τ+1 )}]
− 1 2δµ2τ+1 E
{∥ ∥ ∥ ∥
ητ+1λτ+1µτ+1 βτ+1
ζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥
2 }
+ 1
2δµ2τ E
{∥ ∥ ∥ ∥
ητλτµτ βτ
ζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥
2 }
≤ η0 δ
[
E { F ( x(b)τ )} − E { F ( x (b) τ+1
)}]
− 1 2δµ2τ+1 E
{∥ ∥ ∥ ∥
ητ+1λτ+1µτ+1 βτ+1
ζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥
2 }
+ 1
2δµ2τ E
{∥ ∥ ∥ ∥
ητλτµτ βτ
ζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥
2 }
.
For arbitrarily fixed (τ̌ , τ̂) ∈ Z2≥0 s.t. τ̌ ≤ τ̂ , adding the previous inequality for τ ∈ {τ̌ , τ̌ + 1, . . . , τ̂} results in
τ̂∑
τ=τ̌
E
{ ‖ζτ+1 − ψτ+1‖2 }
≤ η0 δ
[
E { F ( x (b) τ̌ )} − E { F ( x (b) τ̂+1
)}]
+ 1
2δµ2τ̌ E
{∥ ∥ ∥ ∥
ητ̌λτ̌µτ̌ βτ̌
ζτ̌ + vτ̌ − λ1x̄(b) ∥ ∥ ∥ ∥
2 }
.
Hence, by applying limτ̂→∞ to the previous inequality,
∞∑
τ=τ̌
E
{ ‖ζτ+1 − ψτ+1‖2 }
≤ η0 δ
[
E { F ( x (b) τ̌ )} − F∗
]
+
E {∥ ∥ ∥ ητ̌λτ̌µτ̌ βτ̌ ζτ̌ + vτ̌ − λ1x̄(b) ∥ ∥ ∥ 2 }
2δµ2τ̌ .
In §2, we introduce a larger example to show the increased expressivity achieved by placing stochastic models, deterministic computations, and inference procedures all in the same general-purpose language. We analyze performance using the language’s I/O facility, then improve performance by invoking inference recursively from within the model.
In §3, we detail the generalized coroutine facility that transfers control between the model and the inference procedure, which lets us reify a model into a tree of choices. This reification enables bucket elimination and, in §4, a new algorithm for importance sampling. We describe our competitive inference performance on realistic models (Jaeger et al. 2007; Pfeffer 2007b). We discuss related work in §5. Our code is at http://okmij.org/ftp/kakuritu/.
is a prefix of levels(ε) and shorter than levels(ε). Due to the last condition, Hlex also prefers an antecedent δ that is shorter than ε, provided that literals of the same decision levels as in δ are also found in ε. Reconsidering the implication graph in Figure 1 and 2, we obtain levels(n8) = (3, 1) < (3, 3) = levels(n7) for antecedents n7 and n8 of Tx, and we have levels(n4) = (3) < (3, 2) = levels(n6) for antecedents n4 and n6 of Fw. By selecting antecedents that are lexicographically smallest, Hlex leads us to the conflict graph shown in Figure 4. In this example, the corresponding First-UIP-Nogood, {Fp,Ts}, is weaker than {Ts}, which may be obtained with Hshort (cf. Figure 3).
Given that lexicographic comparisons are computationally expensive, we also consider a lightweight variant of ranking antecedents according to decision levels. Our third heuristics, Havg , prefers an antecedent δ over ε if the average of levels(δ) is smaller than the average of levels(ε). In our example, we get avg [levels(n8)] = avg(3, 1) = 2 < 3 = avg(3, 3) = avg [levels(n7)] and avg [levels(n6)] = avg(3, 2) = 2.5 < 3 = avg(3) = avg [levels(n4)], yielding the conflict graph shown in Figure 5. Unfortunately, the corresponding First-UIP-Nogood, {Fp,Tq,Tr}, does not match the goal of Havg as backjumping only returns to decision level 2, where Tr is then flipped to Fr. Note that this behavior is similar to chronological backtracking, which can be regarded as the most trivial form of backjumping.
nn.Sequential {
[input -> (1) -> (2) -> (3) -> (4) -> (5) -> output] (1): nn.SpatialConvolution(2 -> 16, 5x5, 1,1, 2,2) (2): nn.SpatialBatchNormalization (3): nn.ReLU (4): nn.Sequential {
[input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8)
-> (9) -> (10) -> (11) -> (12) -> (13) -> (14) -> (15) -> (16) -> (17) -> (18) -> (19) -> (20) -> (21) -> (22) -> (23) -> (24) -> (25) -> (26) -> (27) -> (28) -> (29) -> (30) -> (31) -> (32) -> output]
(1): nn.ConcatTable {
input
|‘-> (1): nn.Sequential { | [input -> (1) -> (2) -> (3) -> (4) -> output] | (1): nn.SpatialConvolution(16 -> 16, 5x5, 1,1, 2,2) | (2): nn.SpatialBatchNormalization | (3): nn.ReLU | (4): nn.SpatialConvolution(16 -> 16, 5x5, 1,1, 2,2) | }
‘-> (2): nn.Identity ... -> output
} (2): nn.CAddTable (3): nn.SpatialBatchNormalization (4): nn.ReLU
/... repeated 7 more times .../
} (5): nn.SpatialConvolution(16 -> 1, 5x5, 1,1, 2,2)
}
• “I am not sure that I ever completely understood how
Vicki thought. It seemed it had more to do with what was in the pictures instead of the time of day it looked in the pictures. If there was food, she chose noon or morning, even though at times it was clearly breakfast food and she labeled it noon.”
• “It doesn’t seem very accurate as I made sure to count and took my time assessing the pictures.”
• “it is hard to figure out what they are looking for since there isn’t many umbrellas in the pictures”
On a high-level reading through all comments, we found that subjects felt that Vicki’s response often revolves around the most salient object in the image, that Vicki is bad at counting, and that Vicki often responds with the most dominant color in the image when asked a color question. In Fig. 10 we show a word cloud of all the comments left by the subjects after completing the tasks. From the comments, we observed that subjects were very enthusiastic to familiarize themselves with Vicki, and found the process engaging. Many thought that the scenarios presented to them were interesting and fun, despite being hard. We used some basic elements of gamification, such as performance–based reward and narrative, to make our tasks more engaging; we think the positive response indicates the possibility of making such human–familiarization with AI engaging even in real–world settings.
The quantity links(A,A) = links(A,A) (where A = V − A denotes the complement of A in V ) measures how many links escape from A (and A). We define the cut of A as
cut(A) = links(A,A).
The notions of cut is illustrated in Figure 1.2. The above concepts play a crucial role in the theory of normalized cuts. Then, I introduce the (unnormalized) graph Laplacian L of a directed graph G in an “old-fashion,” by showing that for any orientation of a graph G,
BB> = D − A = L
is an invariant. I also define the (unnormalized) graph Laplacian L of a weighted graph G = (V,W ) as L = D −W . I show that the notion of incidence matrix can be generalized
9 to weighted graphs in a simple way. For any graph Gσ obtained by orienting the underlying graph of a weighted graph G = (V,W ), there is an incidence matrix Bσ such that
Bσ(Bσ)> = D −W = L.
I also prove that
x>Lx = 1
2 m∑ i,j=1 wi j(xi − xj)2 for all x ∈ Rm.
Consequently, x>Lx does not depend on the diagonal entries in W , and if wi j ≥ 0 for all i, j ∈ {1, . . . ,m}, then L is positive semidefinite. Then, if W consists of nonnegative entries, the eigenvalues 0 = λ1 ≤ λ2 ≤ . . . ≤ λm of L are real and nonnegative, and there is an orthonormal basis of eigenvectors of L. I show that the number of connected components of the graph G = (V,W ) is equal to the dimension of the kernel of L, which is also equal to the dimension of the kernel of the transpose (Bσ)> of any incidence matrix Bσ obtained by orienting the underlying graph of G.
I also define the normalized graph Laplacians Lsym and Lrw, given by
Lsym = D −1/2LD−1/2 = I −D−1/2WD−1/2
Lrw = D −1L = I −D−1W,
and prove some simple properties relating the eigenvalues and the eigenvectors of L, Lsym and Lrw. These normalized graph Laplacians show up when dealing with normalized cuts.
Next, I turn to graph drawings (Chapter 3). Graph drawing is a very attractive application of so-called spectral techniques, which is a fancy way of saying that that eigenvalues and eigenvectors of the graph Laplacian are used. Furthermore, it turns out that graph clustering using normalized cuts can be cast as a certain type of graph drawing.
Given an undirected graph G = (V,E), with |V | = m, we would like to draw G in Rn for n (much) smaller than m. The idea is to assign a point ρ(vi) in Rn to the vertex vi ∈ V , for every vi ∈ V , and to draw a line segment between the points ρ(vi) and ρ(vj). Thus, a graph drawing is a function ρ : V → Rn.
We define the matrix of a graph drawing ρ (in Rn) as a m× n matrix R whose ith row consists of the row vector ρ(vi) corresponding to the point representing vi in Rn. Typically, we want n < m; in fact n should be much smaller than m.
Since there are infinitely many graph drawings, it is desirable to have some criterion to decide which graph is better than another. Inspired by a physical model in which the edges are springs, it is natural to consider a representation to be better if it requires the springs to be less extended. We can formalize this by defining the energy of a drawing R by
E(R) = ∑
{vi,vj}∈E
‖ρ(vi)− ρ(vj)‖2 ,
The VQA Dataset [32] consists of both real images from COCO and abstract cartoon images. Most work on this dataset has focused solely on the portion containing real world imagery from COCO, which we refer to as COCO-VQA. We refer to the synthetic portion of the dataset as SYNTH-VQA.
COCO-VQA consists of three questions per image, with ten answers per question. Amazon Mechanical Turk (AMT) workers were employed to generate questions for each image by being asked to ‘Stump a smart robot,’ and a separate pool of workers were
Page 5
hired to generate the answers to the questions. Compared to other VQA datasets, COCO-VQA consists of a relatively large number of questions (614,163 total, with 248,349 for training, 121,512 for validation, and 244,302 for testing). Each of the questions is then answered by 10 independent annotators. The multiple answers per question are used in the consensus-based evaluation metric for the dataset, which is discussed in Section 4.
SYNTH-VQA consists of 50,000 synthetic scenes that depict cartoon images in different simulated scenarios. Scenes are made from over 100 different objects, 30 different animal models, and 20 human cartoon models. The human models are the same as those used in [38], and they contain deformable limbs and eight different facial expressions. The models also span different age, gender, and races to provide variation in appearance. SYNTH-VQA has 150,000 QA pairs with 3 questions per scene and 10 ground truth answers per question. By using synthetic images, it becomes possible to create a more varied and balanced dataset. Natural images datasets tend to have more consistent context and biases, e.g., a street scene is more likely to have picture of a dog than a zebra. Using synthetic images, these biases can be reduced. Yin and Yang [39] is a dataset built on top of SYNTH-VQA that tried to eliminate biases in the answers people have to questions. We further discuss Yin and Yang in Section 6.1.
Both SYNTH-VQA and COCO-VQA come both open-ended and multiple-choice formats. The
multiple-choice format contains all the same QA pairs, but it also contains 18 different choices that are comprised of
• The Correct Answer, which is the most frequent answer given by the ten annotators. • Plausible Answers, which are three answers collected from annotators without looking at the image. • Popular Answers, which are the top ten most popular answers in the dataset. • Random Answers, which are randomly selected correct answers for other questions.
Due to diversity and size of the dataset, COCOVQA has been widely used to evaluate algorithms. However, there are problems with the dataset. While COCO-VQA has a large variety of questions, many of them can be accurately answered without using the image due to language biases. Relatively simple image-blind algorithms have achieved 49.6% accuracy on COCO-VQA using the question alone [40]. The dataset also contains many subjective, opinionseeking questions that do not have a single objective answer (see Figure 3). Similarly, many questions seek explanations or verbose descriptions. An example of this is given in Figure 3c, which also shows unreliability of human annotators as the most popular answer is ‘yes’ which is completely wrong for the given question. These complications are reflected by interhuman agreement on this dataset, which is about 83%. Several other practical issues also arise out of the dataset’s biases. For example, ‘yes/no’ answers
Page 6
span about 38% of all questions, and almost 59% of them are answered with ‘yes.’ Combined with the evaluation metric used with COCO-VQA (see Section 4), these biases can make it difficult to assess whether an algorithm is truly solving the VQA problem using solely this dataset. We discuss this further in Section 4.
Assume that causes x and effects z were k times observed. We will show how a CPT basis can be estimated from these observations. This is the basis for probability boundary limitation method and the probability potential surge method introduced in the subsequent sections to derive the real CPT, i.e. a CPT basis is a matrix which may still violate the probability constraints explained below. The conditional mean function E(z|x) is also called regression of z on x. An estimator of E(z|x) := ∑ z z f (z|x) will be used to obtain the CPT. where f (.) is the probability density function (continuous case) or probability distribution (discrete case). f (.) has to follow Kolmogorov’s axioms. General discussions of multiple regressions can be found in Hastie et al. (2001) and Greene (2000). The application of the conditional mean to approximate the conditional probability table was used in Cooper and Dietterich (1992) in relation to Bayesian Networks. Roughly speaking they used frequencies to obtain the probabilities.
The new approach will need the following preparation. Given are k observations of the effects Z = (z1, . . . , zk)′ and causes X = (x1, . . . , xk)′. That means Z can be represented as a k × m matrix and X as a k × n matrix. The objective is to determine the CPT basis B := (bi j) := z|x′, such that the squared
sum S (B) := (Z − XB)′(Z − XB) obtains a minimum:
B∗ = min B∈B {tr((Z − XB)′(Z − XB))} , (15)
where B transposed is the m × n matrix with elements in the interval [0, 1]:
bi j ∈ [0, 1] ∀i, j. (16)
In order to satisfy Kolmogorov’s axioms another constraint on B must be fulfilled. The row sum of B must add up to one:
m∑
j=1
bi j ! = 1 ∀i ∈ {1, 2, . . . ,m} . (17)
B is the set of all matrices fulfilling the above mentioned two constraints. Note that the symbol ! = denotes “must equal”. Other objective functions to determine B∗ may be appropriate depending on the nature of the underlying problem. However, we have used the least square method because of its characteristics and popularity. We will limit ourselves to deriving a CPT basis using matrix calculus. We obtain the first derivative of S (B) by:
∂S (B) ∂B = −2X′(Z − XB). (18)
Setting the first derivative to zero will give us the optimum:
−2X′(Z − XB) = 2X′Z − 2X′XB = 0, (19)
under the assumption that X has full column rank, which guarantees that X′X is positive definite. Thus, an optimal CPT basis B∗ is:
k×m︷︸︸︷ B∗ = n×n︷ ︸︸ ︷ (X′X)−1 n×k︷︸︸︷ X′ k×m︷︸︸︷ Z . (20)
An alternative way of obtaining the above result is shown in the footnote 1. Note that it
1Equation (20) could have been derived in a different way by looking for a B∗ which minimises the error matrix E in Z = XB∗ + E. Thus E must be orthogo-
cannot be guaranteed that the elements in B∗ fulfill the two constraints (16) and (17), because of the matrix inversion and multiplication. The case study given in section 6 supports this statement.
In order to enforce a CPT basis to become a proper CPT we propose the probability boundary limitation and probability potential surge methods.
The types of game events that the camera focuses on and the priorities currently associated with each of them are:
– A unit is under attack. Priority: 3 – A unit is performing an attack. Priority: 3 – A worker is scouting.
- If scout worker is close to a potential enemy base, priority: 2 - Otherwise, priority: 0
Note: A worker is only counted as a scout if it is not in its own main base and if the frame count is less than 7500 (approximately 8 minutes in terms of in-game time). – A drop is performed. Priority: 2 Note: Here, a drop is counted as a non-empty transport unit that is close to a potential enemy base. – A group of army units are positioned closely together. Priority: 1 Note: A definition of “army unit” can be found below. – A unit is created. Priority: 1
Some of these events are detected by looping over all accessible units and checking if the condition is fulfilled, while others are purely event-based, in the sense that the game notifies the observer module as the event happens.
Once the cavity entrances have been determined, we can start the CE phase. We explore each detected cavity using a motion analogous to the PE phase, wherein we maintain the
7
structure to the right at a distance ∆ ∈ [δ,D] that is determined online based on the available clearance in the cavity and δ is the minimum required distance for the mapping module. For this, we require a starting viewpoint for each cavity entrance and an algorithm to compute ∆. The starting viewpoint is chosen from the set of camera poses returned by the vSLAM module during the PE phase and such that the centroid of the cavity entrance lies within the view frustum. Additionally, the centroid should not be occluded by the structure from the camera position. From these camera poses, the one with the earliest timestamp is chosen as starting viewpoint, see Fig. 11.
The timestamps of the starting viewpoints of the cavity entrances are used to sort them in increasing order and each of the cavities is explored in sequence. A typical cavity has at least two cavity entrances bordering it, as shown in Fig. 9b and it is possible to have more cavity entrances in some cases. During the CE phase, if the centroid of a cavity entrance falls within the view frustum of the current camera position and is not occluded by the structure, we remove that cavity entrance from our list.
Exploring confined regions during the CE phase requires certain modifications to the PE policy. Recall that the system skipped the cavities during the PE phase as the robot came closer than a distance D from the structure. Therefore during the CE phase, only the region directly ahead of the robot and within a distance ∆ is checked for interference of the computed path with the structure. Moreover, our potential field-based local path planner now returns paths that maintain a distance ∆ from the structure. For this, using the notation of Sections III-A and III-B, we modify goal as goal ← pc −∆n + step r and transform to the global FoR to obtain the new point g. The distance ∆ is chosen by starting from the minimum value δ and increasing it until we reach a local minimum of N∆(g) along n, where the definition of N∆ is adapted from (1) with ∆ replacing D. Similarly, when an acute angled corner is encountered during the CE phase, we modify goal as goal← pc + ∆ r.
When the robot exits a cavity, the point clouds captured by the camera correspond to parts of the structure that are already present in the model from the PE phase. Consequently, the system can detect that it has finished exploring the current cavity by monitoring the loop closures obtained by the vSLAM module. The robot can then choose the next region to explore from its current list of remaining cavity entrances, and can
travel there by following again the PE path. Alternatively, the number of changes in the occupancy measurements of the OctoMap could be used to detect the end of the cavity, as point clouds captured after exiting the cavity ideally would not add new information to the OctoMap. But this solution tends to be less robust because localization errors and sensor noise can induce a large number of changes even when the camera is viewing a region that is already present in the model.
In classical CSP, some global constraints can be efficiently represented by a logically equivalent subnetwork of constraints of bounded arities [12, 9], and are said to be decomposable. Similarly, we will show that some global cost functions can be encoded as a sum of bounded arity cost functions. The definition below applies to any cost function, including constraints (cost functions using only costs in {0,⊤}), extending the definition in [12] and [9].
Definition 24. For a given integer p, a p-network-decomposition of a global cost function W GCF (S,A1, . . . , Ak) is a polynomial transformation δp that returns a CFN δp(S,A1, . . . , Ak) = (S ∪ E,F ,⊤), where S ∩ E = ∅, such that ∀WT ∈ F , |T | ≤ p and ∀ℓ ∈ LS ,W GCF (S,A1, . . . , Ak)(ℓ) = minℓ′∈ LS∪E ,ℓ′[S]=ℓ ⊕ WSi∈F WSi(ℓ ′[Si]).
Definition 24 above allows for the use of extra variables E, which do not appear in the original cost function scope and are eliminated by minimization. We assume, without loss of generality, that every extra variable x ∈ E is involved in at least two cost functions in the decomposition.2 Clearly, if W GCF (S,A1, . . . , Ak) appears in a CFN P = (X ,W ,⊤) and decomposes into (S ∪E,F ,⊤), the optimal solutions of P can directly be obtained by projecting the optimal solutions of the CFN P ′ = (X ∪ E,W \ {W GCF (S,A1, . . . , Ak)} ∪ F ,⊤) on X .
6.1. Building Network-Decomposable Global Cost Functions
A global cost function can be shown to be network-decomposable by exhibiting a bounded arity network decomposition of the global cost function. There is a simple way of deriving network-decomposable cost functions from known decomposable global constraints. The process goes directly from a known decomposable global constraint to a network-decomposable global cost function and does not require to use an intermediate soft global constraint with an associated violation measure µ. Instead, the global cost function will use any relaxation of the decomposed global constraint.
As the previousAllDifferent example showed, from a network-decomposable global constraint, it is possible to define an associated network-decomposable global cost function by relaxing every constraint in the decomposition. In this definition, for two cost functions (or constraints) WT and W ′ T of the same scope, we say that WT ≤ W ′ T iff ∀ℓ ∈ LT ,WT (ℓ) ≤ W ′ T (ℓ) (WT is a relaxation W ′ T ).
Theorem 12. Let GC(S,A1, . . . , Ak) be a global constraint that p-network decomposes into a classical constraint network (S ∪ E,F ,⊤) and fθ be a function parameterized by
2Otherwise, such a variable can be removed by variable elimination: remove x from E and replace the WT involving x by the cost function minx WT on T \ {x}. This preserves the Berge-acyclicity of the network if it exists.
θ that maps every CT ∈ F to a cost function WT such that WT ≤ CT . The global cost function
W GCF (S,A1, ..., Ak, fθ)(ℓ) = min ℓ′∈ LS∪E ℓ′[S]=ℓ
⊕
CT∈F
fθ(CT )(ℓ ′[T ])
is a relaxation of GC(S,A1, . . . , An) which is p-network-decomposable by definition.
Proof. Since (S ∪ E,F) is a network- decomposition of GC(S,A1, ..., Ak), for any tuple ℓ ∈ LS , GC(S,A1, ..., Ak)(ℓ) = 0 if and only if minℓ′∈ LS∪E ,ℓ′[S]=ℓ ⊕ CT∈F CT (ℓ ′[T ]) = 0. Let ℓ′ ∈ LS∪E be the tuple where this minimum is reached. This implies that ∀CT ∈ F , CT (ℓ ′[T ]) = 0. Since fθ(CT ) ≤ CT , fθ(CT )(ℓ ′[T ]) = 0. Therefore ⊕
CT∈F fθ(CT )(ℓ ′[T ]) = 0 andW GCF (S,A1, . . . , Ak, fθ)(ℓ) = 0. Moreover, the global cost function is p-networkdecomposable by construction.
By definition, the global cost function W GCF (S,A1, ..., Ak, fθ) is p-network decomposable into (S ∪ E,W,⊤), where W is obtained by applying fθ on every element of C. Since fθ preserves scopes, the hypergraph of the decomposition is also preserved.
Theorem 12 allows to immediately derive a long list of network decomposable global cost functions from existing network decompositions of global constraints such as AllDifferent, Regular [50], Among and Stretch [10]. The parameterization through fθ also allows a lot of flexibility.
Example 1. Consider the softened variant W AllDifferentdec(S) of the global constraint AllDifferent(S) constraint using the decomposition violation measure [51] where the cost of an assignment is the number of pairs of variables taking the same value. It is well known that AllDifferent decomposes into a set of n.(n−1)2 binary difference constraints. Similarly, the W AllDifferentdec(S) cost function can be decomposed into a set of n.(n−1)2 soft difference cost functions. A soft difference cost function takes cost 1 iff the two involved variables have the same value and 0 otherwise. In these cases, no extra variable is required. Notice that the two decompositions have the same hypergraph structure.
Another relaxation can be considered. Take an arbitrary graph G = (V,E) over V , and consider the relaxation function fG (parameterized by G only) that preserves difference constraints xi 6= xj when (xi, xj) ∈ E but otherwise relaxes them to a constant cost function that is always equal to zero. This gives rise to a global cost function W AllDifferent(V, fG) that clearly captures the graph coloring problem on G, an NPhard problem. Enforcing any soft arc consistency on that single global cost function will be intractable as well since it requires to compute the minimum of the cost function. Instead, enforcing soft arc consistencies such as DAC or VAC on the network-decomposition into binary cost functions will obviously be polynomial but will hinder the level of filtering achieved.
Example 2. Consider the Regular ({x1, . . . , xn},M) global constraint, defined by a (non necessarily deterministic) finite automaton M = (Q,Σ, δ, q0, F ), where Q is a set of states, Σ the emission alphabet, δ a transition function from Σ×Q → 2Q, q0 the initial state and F the set of final states. .As shown in [11], this constraint decomposes into a constraint network ({x1, . . . , xn} ∪ {Q0, . . . , Qn}, C) where the extra variables Qi have Q as their domain. The set of constraints C in the network decomposition contains two
unary constraints restricting Q0 to {q0} and Qn to F and a sequence of identical ternary constraints c{Qi,xi+1,Qi+1} each of which authorizes a triple (q, s, q
′) iff q′ ∈ δ(q, s), thus capturing δ. A relaxation of this decomposition may relax each of these constraints. The unary constraints on Q0 and Qn would be replaced by unary cost functions λQ0 and ρQn stating the cost for using every state as either an initial or final state while the ternary constraints would be relaxed to ternary cost functions σ{Qi,xi+1,Qi+1} stating the cost for using any (q, s, q′) transition.
This relaxation precisely corresponds to the use of a weighted automaton MW = (Q,Σ, λ, σ, ρ) where every transition, starting and finishing state has an associated, possibly intolerable, cost defined by the cost functions λ, σ and ρ [20]. The cost of an assignment in the decomposition is equal, by definition, to the cost of an optimal parse of the assignment by the weighted automaton. This defines a W Regular(S,MW ) global cost function which is not parameterized by a violation measure µ and a classical automata, but by a weighted automaton. As shown in [30], a weighted automaton can encode the Hamming and Edit distances to the language of a classical automaton. Contrary to the AllDifferent example, we will see that the W Regular network-decomposition can be handled efficiently and effectively by soft local consistencies.
In this subsection, I define basic symbols and terms. A input string I is composed of words w1 · · ·wn of length n. A parse tree, T defines the relationship between these words. Following Goodman (1996), a tree T is composed of triples (s, t,X) where, s to t are consecutive words dominated by an internal node labeled as X. Following Goodman (1996), the matching criteria for an automatic parse tree Ta with its gold standard tree Tg is defined in terms of unlabeled precision (P ) and recall (R). Let Ng be the number of internal nodes in Tg and Na be the number of internal nodes in Ta. Let B = |{(s, t,X) : (s, t,X) ∈ Ta ∧ (s, t, Y ) ∈ Tg}|.
• Precision (P ): B/Na
• Recall (R): B/Ng
• F-score: 2PR/(P +R)
Let E be the set of edges, V be the set of internal nodes, IE be the set of internal edges and, n the number of leaves in a tree. Then the following conditions hold for any tree:
• |E| = |V |+ n− 1
• |IE| = |V | − 1
• |V | = n− 2
The rest of the article assumes that trees are both unlabeled and m-ary.
There are 14 vowels and 29 consonants in the phonetic inventory of Bengali language. Vowel set includes seven nasalized vowels. In IPA an approximate phonetic scheme is given in Ghulam at el.(2009) in his "Automatic speech recognition for Bangla digits”, and C. Masica(1991) in his book “Indo-Aryan Languages”. Here only the main 7 vowel sounds are shown, though there is two more long counterparts of /i/ and /u/ denoted as /i:/ and /u:/ respectively.
In the bilingual merged node model, cross-lingual context is incorporated by creating joint bi-tag nodes for aligned words. It would be too strong to insist that aligned words have an identical tag; indeed, it may not even be appropriate to assume that two languages share identical tag sets. However, when two words are aligned, we do want to choose their tags jointly. To enable this, we allow the values of the bi-tag nodes to range over all possible tag pairs 〈t, t′〉 ∈ T × T ′, where T and T ′ represent the tagsets for each language.
The tags t and t′ need not be identical, but we do believe that they are systematically related. This is modeled using a coupling distribution ω, which is multinomial over all tag pairs. The parameter ω is combined with the standard transition distribution φ in a product-of-experts model. Thus, the aligned tag pair 〈yi, y′j〉 is conditioned on the predecessors yi−1 and y ′ j−1, as well as the coupling parameter ω(yi, y′j). 3 The coupled bi-tag nodes serve as bilingual “anchors” – due to the Markov dependency structure, even unaligned words may benefit from cross-lingual information that propagates from these nodes.
3. While describing the merged node model, we consider only the two languages ℓ and ℓ′, and use a simplified notation in which we write 〈y, y′〉 to mean 〈yℓ, yℓ ′ 〉. Similar abbreviations are used for the language-indexed parameters.
We now present a generative account of how the words in each sentence and the parameters of the model are produced. This generative story forms the basis of our sampling-based inference procedure.
As in Section 4.1, we start with the general case in which the number of fixed and minimized predicates is not bounded. Our aim is to establish two NExpNP-lower bounds that both match the upper bound established in Theorem 10. The first bound is for satisfiability w.r.t. concept-circumscribed KBs that are formulated inALC and have an empty TBox, but a nonempty ABox. The second bound is also for satisfiability w.r.t. concept-circumscribed KBs formulated in ALC, but assumes an acyclic TBox and empty ABox. Both reductions work already in the case of an empty preference relation, and without any fixed predicates. Note that considering satisfiability of a concept C w.r.t. a concept-circumscribed KB CircCP(T ,A) with both T and A empty is not very interesting: it can be seen that C is satisfiable w.r.t. CircCP(T ,A) iff C⊥ is satisfiable (without reference to any KB), where C⊥ is the concept obtained from C by replacing all minimized concept names with ⊥.
The proof of our first result is by reduction of a succinct version of the problem coCERT3COL, which is NExpNP-complete (Eiter, Gottlob, & Mannila, 1997), to satisfiability w.r.t. concept-circumscribed KBs with empty TBox. Let us first introduce the regular (nonsuccinct) version of co-CERT3COL:
Instance of size n: an undirected graph G on the vertices {0, 1, . . . , n− 1} such that every edge is labelled with a disjunction of two literals over the Boolean variables {Vi,j | i, j < n}. Yes-Instance of size n: an instance G of size n such that, for some truth value assignment t to the Boolean variables, the graph t(G) obtained from G by including only those edges whose label evaluates to true under t is not 3-colorable.
As shown by Stewart (1991), co-CERT3COL is complete for NPNP. To obtain a problem complete for NExpNP, Eiter et al. use the complexity upgrade technique: by encoding the input in a succinct form using Boolean circuits, the complexity is raised by one exponential to NExpNP (Eiter et al., 1997). More precisely, the succinct version co-CERT3COLS of co-CERT3COL is obtained by representing the input graph G with nodes {0, . . . , 2n − 1} as 4n+ 3 Boolean circuits with 2n inputs (and one output) each. The Boolean circuits are named cE , c (1) S , c (2) S , and c (i) j , with i ∈ {1, 2, 3, 4} and j < n. For all circuits, the 2n inputs are the bits of the binary representation of two nodes of the graph. The purpose of the circuits is as follows:
• circuit cE outputs 1 if there is an edge between the two input nodes, and 0 otherwise;
• if there is an edge between the input nodes, circuit c(1)S outputs 1 if the first literal in the disjunction labelling this edge is positive, and 0 otherwise; circuit c(2)S does the
same for the second literal; if there is no edge between the input nodes, the output is arbitrary;
• if there is an edge between the input nodes, the circuits c(i)j compute the labelling Vk1,k2∨Vk3,k4 of this edge between the input nodes by generating the numbers k1, . . . , k4: circuit c(i)j outputs the j-th bit of ki; if there is no edge between the input nodes, the output is arbitrary.
We reduce co-CERT3COLS to satisfiability w.r.t. concept-circumscribed KBs that are formulated in ALC and whose TBox and preference relation are empty. It then remains to apply Lemma 5 to eliminate fixed concept names (we note that the construction in the proof of the lemma leaves the preference relation untouched). Let
G = (n, cE , c (1) S , c (2) S , {c (i) j }i∈{1,..,4},j<n)
be the (succinct representation of the) input graph with 2n nodes. We construct an ABox AG = {C0 u Root(a0)}, a circumscription pattern CPG, and a concept CG such that G is a yes-instance of co-CERT3COLS iff CG is satisfiable w.r.t. CircCPG(∅,AG).
The concept C0 used in AG is a conjunction whose presentation is split into two parts. Intuitively, the purpose of the first group of conjuncts is to fix a truth assignment t for the variables {Vi,j | i, j < n}, and to construct (an isomorphic image of) the graph t(G) obtained from G by including only those edges whose label evaluates to true under t. Then, the purpose of the second group is to make sure that t(G) is not 3-colorable.
When formulating C0, we use several binary counters for counting modulo 2n (the number of nodes in the input graph). The main counters X and Y use concept names X0, . . . , Xn−1 and Y0, . . . , Yn−1 as their bits, respectively. Additionally, we introduce concept names K(i)0 , . . . ,K (i) n−1, i ∈ {1, 2, 3, 4}, that serve as four additional counters K(1), . . . ,K(4). The first group of conjuncts of C0 can be found in Figure 2, where the following abbreviations are used:
• ∀ri.C denotes the n-fold nesting ∀r. · · · .∀r.C;
• ∀r.(K(i) = X) is an abbreviation foru j<n ( (Xj → ∀r.K(i)j ) u (¬Xj → ∀r.¬K (i) j ) ) and
similarly for ∀r.(K(i) = Y );
• the abbreviations Wc, c a Boolean circuit, are explained later on.
The intuition behind Figure 2 is as follows. Lines (1) to (5) build up a binary tree of depth 2n whose edges are labeled with the role name r. The 22n leaves of the tree are instances of the concept name Leaf, and they are labeled with all possible values of the counters X and Y . Since we will minimize Leaf via the circumscription pattern CPG, this concept name denotes precisely the leaves of the tree. Due to the use of the counters X and Y , the leaves are all distinct.
The leaves of the tree just established satisfy a number of purposes. To start with, each leaf with counter values X = i and Y = j corresponds to the variable Vi,j of co3CERTCOLS and determines a truth value for this variable via truth/falsity of the concept
name Tr. Thus, the leaves jointly describe a truth assignment t for the instance G of co3CERTCOLS . A second purpose of the leaves is to represent the potential edges of G: additionally to representing a variable, a leaf with X = i and Y = j corresponds to the potential edge between the nodes i and j. To explain this more properly, we must first discuss the abbreviations Wc used in Lines (6) and (7) of Figure 2.
Each concept Wc, c a Boolean circuit with 2n inputs, is the result of converting c into a concept that uses only the constructors ¬, u, t such that the following condition is satisfied: if a d ∈ ∆I is an instance of Wc, the output of c upon input b0, . . . , b2n−1 is b, and the truth value of the concept names X0, . . . , Xn−1, Y0, . . . , Yn−1 at d is described by b0, . . . , b2n−1, then the truth value of some concept name Out at d is described by b. By introducing one auxiliary concept name for every inner gate of c, the translation can be done such that the size of Wc is linear in the size of c. The following concept names are used as output:
• WcE uses the concept name E as output;
• W c (i) S uses the concept name Si as output, for i ∈ {1, 2};
• W c (i) j uses the concept name K(i)j as output, for i ∈ {1, . . . , 4} and j < n.
Lines (6) and (7) ensure that these concepts are propagated to all leaves. Our next aim is to ensure that each leaf that represents a potential edge (i, j) is connected via the role var1 to the leaf that represents the variable in the first disjunct of the label of (i, j), and analogously for the role var2 and the variable in the second disjunct of the edge label. If we replaced the concept name LeafFix with Leaf in Lines (8) and (9), then these lines would apparently encode these properties. However, we have to be careful as the mentioned replacement would interact with the minimization of Leaf. To fix this problem, we resort to a trick: we use the concept name LeafFix instead of Leaf. In this way, we may or may not reach an instance of Leaf. If we do not, we force the concept name P to be true at the root of the tree in Lines (10) and (11). We will use CG to rule out models in which P is true. Finally, we fix LeafFix via CPG to eliminate interaction with the minimization of Leaf.
The remaining Lines (12) to (16) ensure that a leaf is an instance of Elim iff the potential edge that it represents is not present in the graph t(G) induced by the truth assignment t described by the leaves.
The second group of conjuncts of C0 can be found in Figure 3. Here, (Y = 0) stands for the concept (¬Y0 u · · · u ¬Yn−1). As already mentioned, the purpose of these conjuncts is to ensure that the graph t(G) described by the leaves does not have a 3-coloring. The strategy for ensuring this is as follows: we use the 2n leaves with Y = 0 to store the colors of the nodes, i.e., the leaf with X = i and Y = 0 stores the color of node i. By Lines (22) and (23), there is a unique coloring. Then, Lines (17) to (21) ensure that each leaf (viewed as an edge) is connected via the role col1 to the leaf that stores the color of the first node of the edge, and analogously for the role col2 and the second node of the edge. LeafFix and P have the same role as before. Lines (24) to (26) guarantee that the concept name Clash identifies problems in the coloring: a leaf is in Clash if it represents an edge that exists in G, is not dropped in t(G), and where both endpoints have the same color. The idea is that Clash will be minimized while R, G, and B vary. When some additional concept names are fixed, this corresponds to a universal quantification over all possible colorings.
Set CG = Root u ¬P u ∃r2n.Clash, and recall that AG = {C0 u Root(a0)}. The following lemma is proved in the appendix.
Lemma 14 G is a yes-instance of co-3CERTCOLS iff CG is satisfiable with respect to CircCPG(∅,AG), where CPG = (≺,M, F, V ) with ≺ = ∅, M = {Root, Leaf,Clash},
F = {LeafFix,Tr, X0, . . . , Xn−1, Y0, . . . , Yn−1, },
and V the set of all remaining predicates in AG.
Since the size of AG is polynomial in n, we get the following result by applying Lemma 5.
Theorem 15 In ALC, satisfiability w.r.t. concept-circumscribed KBs is NExpNP-hard even if the TBox and the preference relation are empty and there are no fixed predicates.
It is now rather straightforward to establish the announced second NExpNP lower bound by a reduction of satisfiability w.r.t. concept-circumscribed KBs in the special case formulated in Theorem 15. Details are given in the appendix.
Corollary 16 In ALC, satisfiability w.r.t. concept-circumscribed KBs is NExpNP-hard even if the TBox is acyclic, the ABox and preference relations are empty, and there are no fixed predicates.
Corresponding lower bounds for subsumption and the instance problems follow from the reduction given in Section 2.
First, we review the Fréchet sub-differential of a function [65], [66]. The norm and inner product notation in Definition 2 correspond to the euclidean ℓ2 settings.
2This method also involves more parameters than our scheme. 3When s ∝ n and J ∝ n, the per-iteration computational cost of the
efficient implementation of K-SVD [62] also scales similarly as O(Nn3).
Definition 1: For a function g : Rp 7→ (−∞,+∞], its domain is defined as domg = {x ∈ Rp : g(x) < +∞}. Function g is proper if domg is nonempty.
Definition 2: Let g : Rp 7→ (−∞,+∞] be a proper function and let x ∈ domg. The Fréchet sub-differential of the function g at x is the following set denoted as ∂̂g(x): {
h ∈ Rp : lim inf b→x,b6=x 1 ‖b−x‖ (g(b)− g(x)− 〈b− x, h〉) ≥ 0 }
If x /∈ domg, then ∂̂g(x) , ∅, the empty set. The subdifferential of g at x is the set ∂g(x) defined as {
h̃ ∈ Rp : ∃xk → x, g(xk) → g(x), hk ∈ ∂̂g(xk) → h̃ } .
A necessary condition for x ∈ Rp to be a minimizer of the function g is that x is a critical point of g, i.e., 0 ∈ ∂g(x). Critical points are considered to be “generalized stationary points” [65].
We say that a sequence {zt} ⊂ Rp has an accumulation point z, if there is a subsequence that converges to z.
The constraints ‖dj‖2 = 1, 1 ≤ j ≤ J , in (P1) can instead be added as penalties in the cost by using barrier functions χ(dj) (taking the value +∞ when the norm constraint is violated, and is zero otherwise). The constraints ‖cj‖∞ ≤ L, 1 ≤ j ≤ J , can also be similarly replaced with barrier penalties ψ(cj). Then, we rewrite (P1) in unconstrained form with the following objective:
f(C,D) = f (c1, c2, ..., cJ , d1, d2, ..., dJ ) = λ 2
J ∑
j=1
‖cj‖0
+ ∥ ∥ ∥Y − ∑J
j=1 djc T j
∥ ∥ ∥ 2
F +
J ∑
j=1
χ(dj) +
J ∑
j=1
ψ(cj) (21)
For the SOUP-DIL Algorithm, the iterates computed in the tth outer iteration are denoted by the 2J-tuple (ct1, d t 1, c t 2, d t 2, ..., c t J , d t J ), or alternatively by the pair of matrices (Ct, Dt).
ISSN: 2231-5381 http://www.ijettjournal.org Page 141
graph generation system from Punjabi text documents of different domains. This research paper focuses on pre-processing of Punjabi text documents. Pre-processing is structured representation of the input text. Pre-processing of ontology graph generation includes allowing input restrictions to the text, removal of special symbols and punctuation marks, removal of duplicate terms, removal of stop words, extract terms by matching input terms with dictionary and gazetteer lists terms.
Keywords- Ontology, Pre-processing phase, Ontology Graph, Knowledge Representation, Natural Language Processing.
I. INTRODUCTION
“Ontology” has originated from Philosophy branch but in the last decades, Ontology in information systems has become common in numerous other fields like Natural Language Processing, Database integration, Internet technologies, Artificial Intelligence, Multiagent Systems etc.
Informally Ontology describes the terms, concepts, classes and relationship between them. Formally Ontology is defined as: “formal, explicit specification of a shared conceptualization”. Conceptualization [4] is an abstract view of the world that represents the knowledge for any motive. Explicit implies that the variety of concepts used and therefore the constraints on their utilization are explicitly outlined. Formal make reference to the reality that ontology ought to be machine understandable and readable. Shared reveals the insight that an ontology captures accordant knowledge i.e. it’s not individual personal information, however accepted by a group.
The ontology graph is a new methodology that is employed to represent the ontology of knowledge in a domain. The ontology graph comprises of diverse levels of conceptual components, in which they are related together by various types of relations. It is essentially a dictionary framework (i.e. terms) that unite one another to represent a class, to formulate various concepts and identify meanings. The conceptual layout of an ontology graph comprises of numerous terms with a few relationships between them, so that the different conceptual components are framed like a lattice.
An ontology graph modeling process includes two phases: Pre-processing phase and Processing phase. The aim of this paper is to represent the pre-processing phase of domain ontology graph generation system for Punjabi text documents.
Punjabi language is a part of Indo-Aryan language. Punjabi language is the formal language of the Punjab, one of the states of India. Punjabi is spoken in Punjab, Punjab Province of Pakistan, and Jammu-Kashmir. It’s conjointly use for communication as a minority language in several other countries where Punjabi people are living, such as the United States of America, Australia, Canada, and England. Punjabi is written in the two different scripts; Gurumukhi script in India and Shahmukhi script in Pakistan.
The Punjabi text document corpus contains 1000 documents with an average of 700 Punjabi words in each document. The text documents belong to five different domains, which are Agriculture, Entertainment, Health, Politics, and Sports. Each domain contains 200 Punjabi documents. These five domains are labeled as classes for the domain ontology learning process. The documents of the corpus in every class are further divided to permit 70% of them for the learning set and 30% for the testing set.
II. PRE-PROCESSING PHASE
Text Preprocessing is an extremely important part of any Natural Language Processing system, since the characters, terms recognized at this stage are the basic units goes to further processing stages. Pre-processing phase represents the input text in organized manner. In pre-processing phase, manipulation and filtration of terms is carried out to omit/ eliminate terms that do not consist of context like stop words, special symbols, punctuation marks etc. When Pre-processing has been done, only those terms are remaining that are meaningful to the Natural Language Processing System. Preprocessing phase includes: Allowing input restrictions to input text, elimination of useless symbols, removal of duplicate terms, Removal of Stop words, Extract terms matched with dictionary terms, and Extract terms matched with Gazetteer list.
Term Extraction is a pre-processing phase that identified all meaningful Punjabi terms in input text documents. An existing electronic dictionary is utilized. It contains 40,000 Punjabi words. It is very helpful for identifying the meaningful terms inside the input text. To extract the terms, besides the existing dictionary terms, an additional input of Punjabi terms are also required. These additional terms are Middle names, last names, Politicians names, Sports personality names, Sports names, Movies names, Location names etc. To extract the meaningful terms and remove the meaningless terms, Stop Word list is required. We created
ISSN: 2231-5381 http://www.ijettjournal.org Page 142
Stop list manually by analyzing Punjabi documents. The Stop Word List contains 1,500 words.
Various sub phases of Term Extraction are as follows:
Sigmoidal multilayer perceptron (MLP) neural networks are used to approximate the Q-function. In accordance with the QCON model [22], [30] each
possible action has an unique neural network to approximate the Q-value associated with the given state. Each MLP contains three layers with 65536 input units, 80 hidden layer units and an output neuron which gives the approximation of the Q-value of the state-action pair. The states are ”mental” images introduced in the previous paragraph. Several examples can be found in the literature of using neural networks to approximate the Q-function, for example, see the book [20] or the tutorial [21] by Gosavi. Alg. 1 shows the used algorithm to perform the Q-learning. This pseudo-code is a simplified and generalized version of the most precise description of the used algorithm that can be found in
the source file ql.hpp at https://github.com/nbatfai/samu/blob/master/ ql.hpp. The regular part of the algorithm is taken from the book [26, pp. 844].
Algorithm 1 Q-learning-with-NN-approximation (s′, t′) returns a triplet (an action)
Input: s′, t′ . s′ is the current state and t′ is the current triplet. r′, a′, p, nn, q . Local variables: r′ is the current reward, a′ is the current action and p is a perceptron, nn, q ∈ R. s, r = −∞, a,Nsa, P rcpsa . Persistent variables: s is the previous state, r is the previous reward, a is the previous action, Nsa is the state-action frequency table and Prcpsa is the array of the perceptrons. Output: a′ . a′ is an action, the predicted next triplet. 1: procedure SamuQNN(s′, t′) 2: r′ = compare(t′, a) 3: a′ = t′
4: if r > −∞ then 5: Increment Nsa[s, a] 6: nn = Prcpsa[a](s) 7: q = nn+ α(Nsa[s, a])(r ′ + γ maxpPrcpsa[p](s ′)− nn) 8: Back-propagation-learning(Prcpsa[a], s, q, nn) . where q − nn is the error of the MLP Prcpsa[a] for input s. 9: a′ = argmaxpf(Prcpsa[p](s ′), Nsa[s
′, p]) 10: end if 11: s = s′ 12: r = r′ 13: a = a′ 14: return a′ 15: end procedure
In this case study we have used a road topology consisting of nine road partitions to represent all states and processes as shown in fig.9. It is the smallest circuit (i.e. combination of road partitions) that allows us to study all properties that would be in a much larger circuit. We have considered the case in which initially storehouse-A is full and storehouse-B is empty. The carrier task is to transport stock from storehouse-A to storehouse-B until the storehouse A is empty. Loader at the storehouse-A loads, and the un-loader at the store-house-B unloads the carrier agent. The store-manager keeps a count of stock in each storehouse. In this case the environment is static. At the central section (3, 4, 5) there is a possibility of collision between carrier agents coming from the opposite directions. Priority is given to the loaded carriers i.e. if there is a collision between a loaded and an empty carrier than the empty carrier moves back and waits at the parking region during which the loaded carrier passes and unloads. The parking region as shown in the fig.9 consists of the road partition 8.
In this paper, we propose Yum-me, a novel healthy meal recommender that makes health meal recommendations catering to users’ fine-grained food preferences. We further present FoodDist, a best-of-its-kind unified food image analysis model. The user study and benchmarking results demonstrate the effectiveness of Yum-me and superior performance of FoodDist model. Looking forward, we envision that the idea of using visual similarity for preference elicitation can power a wider range of applications in the Ubicomp community. (1) User-centric dietary profile: the fine-grained food preference learned by Yum-me can be seen as a general dietary profile of each user and be projected to other domains to enable more dietary applications, such as suggesting proper meal plans for diabetes patients. Moreover, a personal dietary API can be built on top of this profile to enable the sharing and improvementments across multiple dietary applications. (2) Food image analysis API for deeper content understanding: With the release of the FoodDist model and API, many dietary applications, in particular the ones that capture a large number of food images, might benefit from a deeper understanding of their image contents. For instance, food journaling applications could benefit from the automatic analysis of food images to summarize the day-to-day food intake or trigger timely reminders and suggestions when needed. (3) Finegrained preference elicitation leveraging visual interfaces. The idea of eliciting users’ fine-grained preference via visual interfaces is also applicable to other domains in Ubicomp. The key insight here is that visual contents capture many subtle 7Entropy of preference distribution: H(p) =−∑i pi log pi
variations among objects that text or categorical data cannot capture; and the learned representations can be used as an effective medium to enable fine-grained preferences learning. For instance, the IoT, wearable, and mobile systems for entertainments, consumer products, and general content deliveries can leverage such an adaptive visual interface to design an onboarding process that learns users’ preferences in a much shorter time and potentially more pleasantly than the traditional approaches.
Our results show that many off-the-shelf machine learning algorithms can deal with high-dimensional meaning spaces. We only optimized classifiers for n = 17 and we get linear drop in performance for exponential increase in number of dimensions. This suggest that we might be able to get on par performance even for very high n - if we optimize classifiers for each n and if we logarithmically increase training set size. Our results also show that many classifiers have no problem dealing with feature sensitivity related to referential uncertainty.
Our experiments suggest that referential uncertainty in highdimensional meaning spaces is NOT an exponentially growing problem. In other words adding dimensions has at most a linear effect on learners in the paradigm discussed here. This is an interesting result because it suggests that referential uncertainty although often thought of as an exponentially growing problem with the number of dimensions or the degree with which words can associate to aspects of objects is much less of an issue than one might expect.
l) Meaning Space Structure: There is a difference in performance between SIM and GRO1 data sets. The reason is that there is much more structure in the real world than in the simulated world. The real world data sets consist of limited sets of objects that make up very defined spaces in the perceptual data space. For instance, there are certain clearly separable color regions based on the objects in GRO1. This structure in the environment helps all learning algorithms in becoming more successful.
m) Tutoring: Tutoring strategies are often thought to be about social feedback (e.g. pointing or agreement). But tutoring can also mean that the tutor is structuring the environment (and possibly also the language) for the learner. This can include taking perspective or conceptualizing the world from the viewpoint of the learner. Generally speaking it has been found that tutoring strategies help learners [8], [24]. The delta in performance between GRO1 and GRO2 confirms these ideas. In GRO1 the tutor will utter words based on what the learner sees. In GRO2 that is not the case. All classifiers perform less well on GRO2.
n) Unbalanced data: Another aspect that affects performance is the fact that training of words is unbalanced. There are some words that occur often and others that don’t (in fact some words do not even appear in the training set and only in the test set). The classifiers have difficulties with sparsely used words. Something that becomes apparent when examining macro-averaged f-scores (not reported here). This is often much lower than micro-averaged f-scores. This split suggests that (generally speaking) learners are good in learning frequent words but less good in learning less frequent words.
o) Representation: It is interesting to analyze various learning algorithms with respect to whether they actually build representations similar to that of the tutor. We deliberately chose various algorithms none of which directly tried to replicate the tutor behavior in the learner by learning the same representation. The tutor operates using weighted feature distances to prototypes. Words are only sensitive to a particular feature channel (e.g. the brightness). Algorithms such as KNeighbors do not explicitly represent information like that. They just collect samples. Others such as RandomForrest do actually learn how to distinguish different words based on explicitly learning which features matter with respect to the word. An interesting result of this study is that both of these algorithms perform comparably well in terms of replicating the tutors behavior. But if we look at different p value experiments, we can see that discriminative feature learners such as AdaBoost outperform KNeighbors.
Sentiment Analysis on financial texts has received increased attention in recent years (Nardo et al., 2016). Neverthless, there are some challenges yet to overcome (Smailović et al., 2014). Financial texts, such as microblogs or newswire, usually contain highly technical and specific vocabulary or jargon, making the develop of specific lexical and machine learning approaches necessary. Most of the research in Sentiment Analysis in the financial domain has focused in analyzing subjective text, labeled with explicitly expressed sentiment.
However, it is also common to express financial sentiment in an implicit way. Business news stories often refer to events that might indicate a positive or negative impact, such as in the news title “company X will cut 1000 jobs”. Economic indicators, such as unemployment and future state modifiers such as drop or increase can also provide clues on the implicit sentiment (Musat and Trausan-Matu, 2010). Contrary to explicit expressions (subjective utterances), factual text types often contain objective statements that convey a desirable or undesirable fact (Liu, 2012).
Recent work proposes to consider all types of implicit sentiment expressions (Van de Kauter et al., 2015). The authors created a fine grained sentiment annotation procedure to identify polar expressions (implicit and explicit expressions of positive and negative sentiment). A target (company of interest) is identified in each polar expression to identify the sentiment expressions that are relevant. The annotation procedure also collected information about the polarity and the intensity of the sentiment expressed towards the target. However, there is still no automatic approach, either lexical-based or machine learning based, that tries to model this annotation scheme.
In this work, we propose to tackle the aforementioned problem by taking advantage of unsupervised learning of word embeddings in financial tweets and financial news headlines to construct a domain-specific syntactic and semantic representation of words. We combine bag-ofembeddings with traditional approaches, such as pre-processing techniques, bag-of-words and financial lexical-based features to train a regressor for sentiment polarity and intensity. We study how different regression algorithms perform using all features in two different sub-tasks at SemEval-
2017 Task 5: microblogs and news headlines mentioning companies/stocks. Moreover, we compare how different combinations of features perform in both sub-tasks. The system source code and word embeddings developed for the competition are publicly available.1
The remainder of the paper is organized as follows. We start by describing SemEval-2017 Task 5 and how we created financial-specific word embeddings. In Section 4 we present the implementation details of the system created for the competition followed by the experimental setup. We then present the experimental results and analysis, ending with the conclusions of this work.
commonly considered as a central component of memory retrieval, categorisation, pattern recognition, problem solving, reasoning, as well as social judgement, e.g., refer to Markman and Gentner (1993); Hahn et al. (2003); Goldstone and Son (2004) for associated references.
As we have seen, from mathematics to psychology, the notion of similarity is central in numerous fields and is particularly important for human cognition and intelligent system design. In this subsection, we provide a brief overview of the psychological theories of similarity by introducing the main models proposed by cognitive sciences to study and explain (human) appreciation of similarity.
Cognitive models of similarity generally aim to study the way humans evaluate the similarity of two mental representations according to some kind of psychological space (Tversky, 2004). They are therefore based on assumptions regarding the mental representation of the compared objects from which the similarity will be estimated. Indeed, as stated by several authors, the notion of similarity, per se, can be criticised as a purely artificial notion. In Goodman (1972), the notion of similarity is defined as “an imposture, a quack” because objectively, everything is equally similar to everything else. The authors emphasise that, conceptually, two random objects have an infinitive number of properties in common and infinite different properties7, e.g. a flower and a computer are both smaller than 10m, 9.99m, 9.98m, etc. An important notion to understand, which has been underlined by cognitive sciences, is that different degrees of similarities emerge only when some predicates are selected or weighted more than others. As stated in Hahn (2011), ”this important observation doesn’t mean that similarity is not an explanatory notion but rather that the notion of similarity is heavily framed in psychology”. Similarity assessment must therefore not be understood as an attempt to compare object realisations through the evaluation of their properties, but rather as a process aiming to compare objects as they are understood by the agent which estimates the similarity (e.g., a person, an algorithm). The notion of similarity therefore only makes sense according to the consideration of a partial (mental) representation on which the estimation of object similarity is based – this aspect of the notion of similarity will be essential for the rest of this book.
Contrary to real objects, representations of objects do not contain infinitesimal properties. As an example, our mental representations of things only capture a limited number of dimensions of the object which is represented. Therefore, the philosophical worries regarding the soundness of similarity vanish given that similarity aim at comparing partial representations of objects and not objects themselves, e.g., human mental representation of objects (Hahn, 2011). It is important to understand that studying human capacity to assess similarity, the similarity is thus estimated between mental representations – i.e. representations, not the real objects. This will also be the case for semantic similarity measures. Considering that these representations are the ones of a human agent, the notion of similarity may thus be understood as how similar objects appear to us. Considering the existential requirement of representations to compare things or objects, much of the history of research on similarity in cognitive sciences focuses on the definition of models of the mental representation of objects, to further consider measures which will be used to compare objects based on their representations.
The central role of cognitive sciences regarding the study of similarity relies on the design of cognitive models of both, mental representations and similarity. These models are used to study how humans store their knowledge and interact with it in order to compare object representations. Cognitive scientists then test these models according to our understanding of human appreciation of similarity. Indeed, evaluations of human appreciation of similarity help us to distinguish constraints/expectations on the properties an accurate model should have. This approach is essential to reject hypotheses and improve the models. As an example, studies have demonstrated that appreciation of similarity is sometimes asymmetric: the similarity between a person and his portrait is commonly expected to be lower than the inverse.8 Therefore, the expectation of asymmetric estimation of similarity is incompatible with the mathematical properties of a distance, which is symmetric by definition. Models based on distance axioms thus appeared inadequate and have to be revised or to be used with moderation. In this context, the introduction of cognitive models of similarity will be particularly useful to understand the foundations of some approaches adopted for the definition of semantic measures.
Cognitive models of similarity are commonly organised into four different approaches: (i) Spatial models, (ii) Feature models, (iii) Structural models and (iv) Transformational models. We briefly intro-
7 This statement also stands if we restrict the comparison of objects to a finite set of properties. The reader may refer to Andersen’s famous story of the Ugly Duckling. Proved by Watanabe and Donovan (1969), the Ugly Duckling theorem highlights the intrinsic bias associated to classification, showing that all things are equal and therefore that an ugly duckling is as similar to a swan as two swans are to each other. The important teaching is that biases are required to make a judgement and to classify, i.e., to prefer certain categories over others.
8Indeed, Tversky (1977) stresses that We say “the portrait resembles the person” rather than “the person resembles the portrait”.
Simma et al. (2008) also use a continuous-time model to reason about network traffic. They apply their method to find dependences in exterprise-level services. Their model is non-Markovian, but also deals with network events as the basic observational unit.
To estimate the parameters of the large network we build for the network traffic data, we use Rao-Blackwellized particle filters (RBPFs). Doucet, de Freitas, Murphy, and Russel (2000) propose a RBPF algorithm for dynamic Bayesian networks that works in discrete time fashion by exploiting the structure of the DBN. Ng, Pfeffer, and Dearden (2005) extend the RBPF to continuous time dynamic systems and apply the method to the K-9 experimental Mars rover at NASA Ames Research Center. Their model is a hybrid system containing both discrete and continuous variables. They use particle filters for the discrete variables and unscented filters for the continuous variables. Our work are similar to theirs in that we apply a RBPF to a CTBN. But our model only contains discrete variables and our evidence is over continuous time (as opposed to only “snapshots” of the system state).
If visual algebra is not supported, we would have to impelement a given task using only source based rules. The visual algebra is a superset of the existing source based algebra. Expressing a visual rule using existing algebra as a source based rule can be categorized into one of the following cases:
1. Identical Semantics: Some of the visual operators can be mapped directly into source level rules keeping the semantics intact. For example, the operator V erticallyAligned can be mapped to an expression based on constructs in html that are used for alignment such as <tr>, <li> or <p>, depending on the exact task at hand.
2. Approximate Semantics: Mapping a visual rule to a source based rule with identical semantics may lead to very complex rules since there are many ways to achieve the same visual layout. It may be possible to get approximately similar results by simplifing the rules if we know that the layout for the pages in the dataset is achieved in one particular way. For example, in a particular template, alignment may always be implemented using rows of a table (the <tr> tag), so the source based rule can cover only this case.
3. Alternate Semantics: In some cases, it is not possible to obtain even similar semantics from the source based rules. For example, rules based on Area, Centroid, Contains, Touches and Intersects cannot be mapped to source based rules, since it is not possible to check these conditions without actually rendering of the page. In such cases, we have to use alternate source based rules for the same task.
Les processus décisionnels de Markov partiellement observables (POMDP) sont usuellement définis (Monahan, 1982; Kaelbling et al., 1998) par un uplet 〈S,A,O, T,O, r, b0〉. Si le système est dans un état s ∈ S (l’espace d’état), et l’agent effectue une action a ∈ A (l’espace d’action), alors en résulte (1) une transition vers un état s′ selon la fonction de transition T (s, a, s′) = Pr(s′|s, a), (2) une observation o ∈ O (l’espace d’observation) selon la fonction d’observation O(s′, a, o) = Pr(o|s′, a) et (3) une récompense scalaire r(s, a, s′). b0, la croyance initiale, est une distribution de probabilité sur S.
L’agent doit trouver une politique de décision π choisissant, à chaque étape, la meilleure action en fonction de ses observations passées et des actions de manière à maximiser ses gains futurs, lesquels nous mesurons ici à travers le total des récompenses accumulées. La valeur espérée d’une politique optimale est dénotée V ∗.
L’agent raisonne typiquement sur l’état caché du système en utilisant un état de croyance b, une distribution de probabilité sur S. Pour nos expérimentations nous utilisons SARSOP (Kurniawati et al., 2008), un algorithme à base de points de l’état de l’art, c’est-à-dire un algorithme approchant la fonction de valeur comme l’enveloppe supérieure d’un ensemble d’hyperplans, lesquels correspondent à une sélection d’états de croyance particuliers (appelés “points”).
SAFA successful cases On the left is the input, and on the right is the output.
SAFA failure cases
Our continuation strategy is inspired by ℓ1-homotopy which recursively computes the minimizers of ‖y−Ax‖22 +λ‖x‖1 when λ is continuously decreasing [28]–[30]. An iteration of ℓ1-homotopy consists in two steps:
• Find the next value λnew < λcur for which the ℓ1 optimality conditions are violated with the current
active set S (λcur denotes the current value);
• Compute the single replacement S ← S ± {i} allowing to fulfill the ℓ1 optimality conditions at
λ = λnew.
March 19, 2015 DRAFT
CSBR follows the same principle. The first step is now related to some local ℓ0-optimality conditions, and the second step consists in calling SBR at λnew with the current active set as initial solution; see Fig. 5 for a sketch. A main difference with ℓ1-homotopy is that the ℓ0 solutions are suboptimal, i.e., they are local minimizers of J (x;λ) with respect to x.
1) Local optimality conditions: Let us first reformulate the stopping conditions of SBR at a given λ.
SBR terminates when a local minimum of Ĵ (S;λ) has been found:
∀i ∈ {1, . . . , n}, Ĵ (S ± {i};λ) ≥ Ĵ (S;λ). (8)
This condition is illustrated on Fig. 6(a): all lines related to single replacements S ± {i} lay above the
black point representing the value of Ĵ (S;λ) for the current λ. By separating the conditions related to
insertions S + {i} and removals S − {i}, (8) rereads as the interval condition:
λ ∈ [δEadd(S), δErmv(S)], (9)
where
δEadd(S) , max i/∈S
{ E(S)− E(S + {i}) }
(10a)
δErmv(S) , min i∈S
{ E(S − {i}) − E(S) }
(10b)
refer to the maximum variation of the squared error when an atom is added in the support S (respectively,
removed from S).
March 19, 2015 DRAFT
2) Violation of the local optimality conditions: Consider the current output S = SBR(Sinit;λcur). The local optimality condition (9) is then met for λ = λcur, but also for any λ ∈ [δEadd(S), λcur]. The new value for which (9) is violated is λnew = δEadd(S) − c where c > 0 is arbitrarily small. The violation occurs for i = ℓadd, with
ℓadd ∈ argmax i/∈S {E(S) − E(S + {i})}. (11)
March 19, 2015 DRAFT
In practice, λnew can be set to the limit value
λnew = δEadd(S) (12)
provided that S is replaced with S + {ℓadd}.
As illustrated on Fig. 6(b), the line S+ {ℓadd} lays below all other parallel lines S+ {i}. It intersects line S at λnew. The vertical arrow represents the new call to SBR with inputs S + {ℓadd} and λnew. Because S and S+{ℓadd} both lead to the same value of Ĵ ( . ;λnew), the de-selection of ℓadd is forbidden in the first iteration of SBR.
For generic relations, semantic relatedness plays a significant role. The difference in the F-score between models that use semantic relatedness and the kernel where the relatedness values are generated randomly (Baseline II) amounts to nearly 20%. All measures exhibit different performance on the seven generic relations that we have considered. We can observe, for instance, that wup, lch, and lin almost always yield the best results, no matter what relation is considered. We found the Resnik score and Jiang and Conrath’s measure to yield lower results than other measures. Even though the F-scores per relation vary quite substantially (by placing Cause-Effect, Theme-Tool, Origin-Entity among the most difficult relations to extract), two measures, wup and lch, are the top-performing measures for all seven relations. These two measures explore the WordNet taxonomy using a length of the paths between two concepts, or their depth in the WordNet hierarchy and, consequently, belong to the path-based measures. The other three measures, res, lin and jcn are information content based measures, and here relatedness between two concepts is defined through the amount of information they share. Our experiments with the LA kernel on generic relation recognition suggest that, in this particular case, the path-based measures should be preferred over the information content based measures.
We should stress, however, that this is the evaluation of the semantic relatedness measures in the context of relation recognition, and one can by no means draw a conclusion that the top measures for other NLP tasks will stay the same. For example, Budanitsky and Hirst (2006) use semantic relatedness measures to detect malapropism and show that Jiang and Conrath’s measure (jcn) yields the best results, followed by Lin’s measure (lin), and the one by Leacock and Chodorow (lch), and then by Resnik’s measure (res). Our results are quite similar to their findings if we consider the res measure, but jcn is not on the top of the accuracy ranking list for any of the seven semantic relations that we have studied.
Interfacing with Scala directly from Java is already challenging, and interfacing with Scala from Clojure adds an additional complication. The work around is to write the components that use Clide directly in Scala and use Clojure’s runtime interface to call into Clojure code from Scala.
More information on Clojure is available in [6, 7].
Beginning with the seminal resources from The Preposition Project (TPP; Litkowski and Hargraves, 2005), the computational study of preposition semantics has been fundamentally grounded in corpus-based lexicography centered around individual preposition types. Most previous datasets of preposition semantics at the token level (Litkowski and Hargraves, 2005, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Srikumar and Roth, 2013a) only cover high-frequency prepositions (the 34 represented in the SemEval-2007 shared task based on TPP, or a subset thereof).3
We sought a scheme that would facilitate comprehensive semantic annotation of all preposition tokens in a corpus: thus, it would have to cover the full range of usages possible for the full range of English preposition types. The recent TPP PDEP corpus (Litkowski, 2014, 2015) comes closer to this goal, as it consists of randomly sampled tokens for over 300 types. However, sentences were sampled separately for each preposition, so there is only one annotated preposition token per sentence. By contrast, we will fully annotate documents for all preposition tokens. No inter-annotator agreement figures have been reported for the PDEP data to indicate its quality, or the overall difficulty of token annotation with TPP senses across a broad range of prepositions.
We consider two cases. In the first case, we assume that
‖ŵ −w∗‖ ≤ 1
n2 .
Since H(·) is L-smooth and r(·) is P -Lipschitz continuous, we have F (ŵ)− F (w∗) = H(ŵ) + r(ŵ)−H(w∗)− r(w∗)
≤〈ŵ −w∗,∇H(w∗)〉+ L
2 ‖ŵ −w∗‖2 + P‖ŵ −w∗‖
≤‖ŵ −w∗‖‖∇H(w∗)‖+ L
2 ‖ŵ −w∗‖2 + P‖ŵ −w∗‖ ≤
M + P
n2 +
L
2n4
(42)
where the last step utilizes Jensen’s inequality
‖∇H(w∗)‖ = ∥∥E(x,y)∼D [∇ℓ(〈w∗,x〉, y)] ∥∥ ≤ E(x,y)∼D [‖∇ℓ(〈w∗,x〉, y)‖] (22)
≤ M. Next, we study the case
1
n2 < ‖ŵ −w∗‖
(18)
≤ 2R. From (29), we have
F (ŵ)− F (w∗) + λ
2 ‖ŵ −w∗‖2
≤〈∇F (ŵ)−∇F (w∗)− [∇F̂ (ŵ)−∇F̂ (w∗)], ŵ −w∗〉+ 〈∇F (w∗)−∇F̂ (w∗), ŵ −w∗〉 =〈∇H(ŵ)−∇H(w∗)− [∇Ĥ(ŵ)−∇Ĥ(w∗)], ŵ −w∗〉+ 〈∇H(w∗)−∇Ĥ(w∗), ŵ −w∗〉 ≤ sup
w:‖w−w∗‖≤‖ŵ−w∗‖
〈 ∇H(w)−∇H(w∗)− [∇Ĥ(w)−∇Ĥ(w∗)],w −w∗ 〉
︸ ︷︷ ︸ :=B1 + ∥∥∥∇H(w∗)−∇Ĥ(w∗)
∥∥∥ ︸ ︷︷ ︸
:=B2
‖ŵ −w∗‖ .
(43)
We first bound B1. To utilize the fact the random variable ‖ŵ − w∗‖ lies in the range (1/n2, 2R], we develop the following lemma.
Lemma 5 Under Assumptions 6 and 7, with probability at least 1− δ, for all 1
n2 < γ ≤ 2R
the following bound holds:
sup w:‖w−w∗‖≤γ
〈 ∇H(w)−∇H(w∗)− [∇Ĥ(w)−∇Ĥ(w∗)],w −w∗ 〉 ≤ 4Lγ 2
√ n
( 8 + √ 2 log s
δ
)
where s = ⌈2 log2(n) + log2(2R)⌉.
Based on the above lemma, we have with probability at least 1− δ,
B1 ≤ 4L‖ŵ −w∗‖2√
n
( 8 + √ 2 log s
δ
) =
LC‖ŵ −w∗‖2√ n
(44)
where C is defined in (23). We then proceed to handle B2, which can be upper bounded in the same way as A2. In particular, we have the following lemma.
Lemma 6 Under Assumptions 6 and 7, with probability at least 1− δ, we have ∥∥∥∇H(w∗)−∇Ĥ(w∗)
∥∥∥ ≤ 2M log(2/δ) n +
√ 8LH∗ log(2/δ)
n . (45)
Substituting (44) and (45) into (43), with probability at least 1− 2δ, we have
F (ŵ)− F (w∗) + λ
2 ‖ŵ −w∗‖2
≤LC‖ŵ −w∗‖ 2
√ n
+ 2M log(2/δ)‖ŵ −w∗‖
n + ‖ŵ −w∗‖
√ 8LH∗ log(2/δ)
n .
(46)
We substitute
LC‖ŵ −w∗‖2√ n ≤ L 2C2‖ŵ −w∗‖2 λn + λ 4 ‖ŵ −w∗‖2,
‖ŵ −w∗‖ √ 8LH∗ log(2/δ)
n ≤ 8LH∗ log(2/δ) λn + λ 4 ‖ŵ −w∗‖2
into (46), and then have
F (ŵ)− F (w∗) ≤ L2C2‖ŵ −w∗‖2
λn + 2M log(2/δ)‖ŵ −w∗‖ n + 8LH∗ log(2/δ)
λn (4)
≤ 4R 2L2C2
λn +
4RM log(2/δ)
n +
8LH∗ log(2/δ)
λn .
Combining the above inequality with (42), we obtain (25). To prove (27), we substitute
2M log(2/δ)‖ŵ −w∗‖ n ≤ 8M 2 log2(2/δ) λn2 + λ 8 ‖ŵ −w∗‖2,
‖ŵ −w∗‖ √ 8LH∗ log(2/δ)
n ≤ 16LH∗ log(2/δ) λn + λ 8 ‖ŵ −w∗‖2
into (46), and then have
F (ŵ)− F (w∗) + λ
4 ‖ŵ −w∗‖2
≤LC‖ŵ −w∗‖ 2
√ n
+ 8M2 log2(2/δ)
λn2 +
16LH∗ log(2/δ)
λn
(26)
≤ λ 4 ‖ŵ −w∗‖2 +
8M2 log2(2/δ)
λn2 +
16LH∗ log(2/δ)
λn .
Combining the above inequality with (42), we obtain (27).
The generality of neural networks makes them appealing for a wide range of possible tasks. In the scope of this work, we have applied encoder-decoder neural models to monotone Seq2Seq tasks. We have shown that they can perform comparably to more specialized models in some cases, but cannot (yet) consistently outperform established approaches, and are sometimes still substantially below them. Furthermore, the advantage of having rendered feature engineering and hyper-parameter optimization in the traditional sense unnecessary is notoriously substituted by the search for optimal neural network topologies.
At first sight, our analyses based on string lengths are in line with those reported by Bahdanau et al. (2014). They state that — for the field of machine translation — the attention mechanism leads to improvements over the standard encoder-decoder model on longer sentences. We also observe this positive impact for our tasks, where the attention-based mechanism alleviates the drastic performance drop of the standard encoder-decoder models on long sequences to some extent. At the same time, we see that very performance drop persisting — CRFs still outperform encoder-decoder models on long sequences, even when employing attention-mechanisms. As described in Section 5.3, neural models are only able to successfully compete when more complex phenomena occur, on which traditional models fail. Nevertheless, previous works such as Vukotic et al. (2015) also indicate that even in more complex sequence labeling tasks such as spoken language understanding, neural networks are not guaranteed to outperform CRFs.
The task-specific extensions to the encoder-decoder proposed by Faruqui et al. (2016) have been shown to produce mostly bad results in our settings. This is particularly surprising for the OCR data, for which input and output sequences are usually very similar, so that we had expected that re-feeding the input to the decoder should be equally beneficial in that domain. As discussed, one explanation might be that OCR, or spelling correction generally, putatively exhibits few long-range dependencies. This might explain why the morph-trans approach works quite well and competitive in morphological analysis tasks, as re-confirmed in our experiments. Thus, long-range dependencies might actually be a more crucial aspect for the performance of the model presented by Faruqui et al. (2016) than the similarity between input and output sequence.
We conclude that neural networks are far from completely replacing established methods at this point, as the latter can be both faster and more accurate, depending on the properties of the task at hand. A systematic analysis of the complexities and challenges a particular task imposes, remains unavoidable. At the same time, one can argue that encoder-decoder neural models are a relatively recent development and might continue to improve much over the next years. Being very generic and largely task-agnostic, they are already able to outperform traditional and specialized approaches under certain circumstances.
The best-reported previous results on frb100-40 reach a target of 98; an independent set of 98 nodes in the MIS instance (Cai et al., 2010). We evaluate the ability of ULSA to improve upon this by running with a target of 99. Candidate solutions with at most 5 conflicts are checked as to whether removing a single variable could remove all conflicts (which happens when that variable is an endpoint of all conflicts). Table 2 shows results.4 All runs succeed in reaching the target of 99, providing new record-best solutions to this instance. Single-threaded solution time averages 29.3 hours; running multiple threads on the multicore CPU used in the reference environment would easily yield an expected time of less than a day to obtain at least one 99. The author of BHOSLIB stated “I conjecture that in the next 20 years or more (from 2005) [frb100-40] can not be solved on a PC (or alike) in a reasonable time (e.g. 1 day)” (Xu, 2005). This prediction stands, but it is at least possible (in 2014) to obtain in 1 day partial solutions that are one variable/vertex from optimal.
Another issue caused by large |DFK | is that some FK values might not arise in the train set but arise in the test
set or during deployment. This is not a cold start issue – the FK values are all still from the fully known DFK . This issue arises because there are not enough labeled examples to cover all of DFK during training. Typically, this issue is handled using some form of smoothing, e.g., Laplacian smoothing for Naive Bayes by adding a pseudocount of 1 to all frequency counts [29]. While similar smoothing techniques have been studied for probability estimation using decision trees [32], to the best of our knowledge, this issue has not been handled in general for classification using decision trees. In fact, popular decision tree implementations in R simply crash if a value of FK not seen during training arises during testing! Note that SVMs (or any other classifier operating on numeric feature spaces) do not face this issue due to the one-hot encoding of FK.
We consider a simple approach to mitigate this issue: smooth by reassigning an FK value not seen during training to an FK value that was seen. There are various ways to reassign; for simplicity sake, we only study two lightweight unsupervised methods. We leave more sophisticated approaches to future work. We consider both random reassignment and alternative approach that uses the foreign features (XR) to decide the reassignment. Note that the latter is only feasible in cases where the dimension tables are available and not discarded. Since R provides auxiliary descriptive information about FK, we can utilize it for smoothing even if not for learning directly over them. Our algorithm is simple: given a test example with FK not seen during training, obtain an FK seen during training whose corresponding XR feature vector has the minimum l0 distance with the test example’s XR (ties broken randomly). The l0 distance is simply the count of the number of pairwise mismatches of the respective features in the two XR feature vectors.
The intuition for XR-based smoothing is that if XR is part of the “true” distribution, it may yield higher accuracy than random reassignment. But if XR is just noise, this becomes essentially random reassignment. To validate our claim, we use the OneXr simulation scenario. Recall that a feature Xr ∈ XR determines the target (with some Bayes noise as before). We introduce a parameter γ that is the ratio of the number of FK values not seen during training to |DFK |. If γ = 0, no smoothing is needed; as γ increases, more smoothing is needed. Figure 11 presents the results.
The plots confirm our intuition: the XR-based smoothing yields much lower test errors for both NoJoin and JoinAll– in fact, errors comparable to NoFK and the Bayes error–for lower values of γ (< 0.5). But as γ gets closer to 1, the errors of XR-based smoothing also increase but not as much as random hashing. Overall, these results suggest that even if foreign features are available, rather for using them directly for learning the model, we could use them as side information for smoothing FK features. Overall, these results suggest that it is possible to get “the best of both worlds”: the
runtime and usability gains of NoJoin (as against JoinAll, which unnecessarily also learns over the foreign features) along with exploiting the extra information provided by foreign features (if they are available) for smoothing foreign key features.
CNNs typically consist of multiple convolution layers interspersed by pooling, ReLU and normalization layers followed by fully-connected layers. Convolution and fullyconnected layers are the most compute and data intensive layers respectively (Qiu et al., 2016). The computation in these layers consist of multiply-and-accumulate (MAC) operations. The data path is illustrated in Fig. 3, where the input features are multiplied with the weights to get the intermediate data (i.e., partial sums). These partial sums are accumulated to generate the output features. Since fixedpoint arithmetic is typically more efficient for hardware implementation, most hardware accelerators implement the MAC operations using fixed-point representation.
The power/area breakdown of the CNN hardware accelerator mostly depends on the data flow architecture. For example, in Eyeriss (Chen et al., 2016), in each processing element (PE), MAC and memory account for about 9% and 52% area respectively. For Origami (Cavigelli et al., 2015), MAC and memory account for about 32% and 34% area respectively.
Before presenting the results of our evaluations on the real annotated data of the TimeBank corpus, we show some characteristics of the temporal graphs induced by the annotations. This helps to understand some of the differences between the behavior we observe on the synthetic temporal graphs and the more ecological ones.
The Hamlin Beach State Park dataset, used for evaluation, was collected by the Tetracam Micro-MCA6 multispectral imaging sensor. The Micro-MCA6 has six spectral bands including three color (RGB) and three near-infrared (NIR) bands. It was calibrated with an integrating sphere to properly model the sensor response in the simulated DIRSIG environment. Other technical specifications are provided in Table 1.
It is relatively easy to see that the test planning problem described above is NPhard. We show that the decision version TestPlanning, which asks whether there exists a plan with at most h configurations is NP-complete.
Theorem 1. TestPlanning is NP-complete.
Proof. It is in NP since a plan can be checked in polynomial time. To prove hardness, we use a straightforward reduction from 3-coloring, which asks, given a graph G = (V,E), whether there exists a coloring of V with at most 3 colors such that no edge has its two end points of the same color.
From a graph G = (V,E), we build an instance of TestPlanning as follows:
– For every edge (x, y) ∈ E, there are two equipment units xy and yx and a thermal constraint on these two units with capacity 1. – For every vertex x ∈ V we create a test tx involving equipment units xy for every y such that (x, y) ∈ E.
It is easy to see that two tests tx and ty can share the same configuration if and only if there is no edge (x, y) ∈ E. Therefore, G has a 3-coloring if and only if there is a plan with at most 3 configurations, and hence TestPlanning is NP-hard for h = 3. ⊓⊔
Moreover, even if we let the number of configurations free, minimizing the number of switches is also NP-hard for a single thermal constraint over all equipment of capacity κ since it corresponds exactly to the constraint:
BufferedResource(X,Y, κ, t,M)
where X (resp. Y) is a sequence of n integer (resp. set) variables, and for every k ∈ [n] the variable Xk has domain [n] and the variable Yk ranges between the empty set and [m].
The number of simultaneously active equipment units must be equal to κℓ, therefore, we can consider actived equipment as a buffer of capacity exactly κℓ.
Moreover, the set of equipment units tk required by a test k corresponds to the items required to be in the buffer when processing this task. Finally, the objective is to minimize the number activations which is equivalent, up to a constant, to the number of switches M. The reduction from Hamiltonian path to BufferedResource provided in [1] can be lifted to this particular case.
The value of information for a particular uncertainty is a useful sensitivity analysis query in sequential decision problems, specifying the maximum that the decision maker should be willing to pay to observe the uncertainty before making the first decision [Howard, 1966; Raiffa, 1968]. The double derivatives from part (i) of Theorem 1 can also compute the value of information for all uncertainties that are not affected by the decision, using part (ii) of Theorem 1 (choose v = λx). Here MEUPI is the maximal expected utility with perfect information on uncertain variable X. If the decision maker’s utility function u(.) is linear or
exponential, then the value of information of the uncertain variable X is the increase in the certain equivalent u−1(MEUPI) − u−1(MEU). In general, this is usually a good approximation for the value of information, even if the decision maker’s utility function has a form other than linear or exponential [Raiffa, 1968].
Once we have the results from Theorem 1, computing the value of information involves summing and maximizing the partial derivatives. If the number of variables analyzed for value of information is ]var, and assuming that the maximum number of possibilities for all of these variables is bounded by some constant, then the time complexity for obtaining the value of information for all these variables is O((NS)(]var)).
We train on the 80k (question, passage, answer span) triples in the SQUAD training set and report results on the 10k examples in the SQUAD development set. Due to copyright restrictions, we are currently not able to upload our models to Codalab4, which is required to run on the blind SQUAD test set, but we are working with Rajpurkar et al. to remedy this, and this paper will be updated with test numbers as soon as possible.
All results are calculated using the official SQUAD evaluation script, which reports exact answer match and F1 overlap of the unigrams between the predicted answer and the closest labeled answer from the 3 reference answers given in the SQUAD development set.
In this section, we review some related works, including credibility evaluation on social media, representation learning and truth discovery.
We will review now the definition of tightness from [6, Section 7.3]. In application to a Lloyd-Topor program Π , when all predicate constants occurring in Π are treated as intensional, that definition can be stated as follows.
An occurrence of an expression in a first-order formula is negated if it belongs to a subformula of the form ¬F (that is, F → ⊥), and nonnegated otherwise. The predicate dependency graph of Π is the directed graph that has
– all predicate constants occurring in Π as its vertices, and – an edge from p to q whenever Π contains a rule (7) with p in the head such that its
body G has a positive3 nonnegated occurrence of q.
We say that Π is tight if the predicate dependency graph of Π is acyclic. For example, the predicate dependency graph of program (1) has a single edge, from p to q. The predicate dependency graph of program (3) has two edges, from p to q and from q to p. The predicate dependency graph of the program
p(a, b) q(x, y)← p(y, x) ∧ ¬p(x, y) (11)
has a single edge, from q to p (because one of the occurrences of p in the body of the second rule is nonnegated). The predicate dependency graph of the program
p(x)← q(x), q(x)← r(x), r(x)← s(x)
(12)
2 By x = ti we denote the conjunction of the equalities between members of the tuple x and the corresponding members of the tuple ti. 3 Recall that an occurrence of an expression in a first-order formula is called positive if the number of implications containing that occurrence in the antecedent is even.
has 3 edges: p −→ q −→ r −→ s.
Programs (1), (11) and (12) are tight; program (3) is not.
Proposition 1. If a Lloyd-Topor program Π is tight then SM[Π] is equivalent to Comp[Π].
This is an easy corollary to the theorem from [6] mentioned in the introduction. Indeed, consider the setΠ ′ of the definitions (9) of all predicate constants p inΠ . It can be viewed as a formula in Clark normal form in the sense of [6, Section 6.1]. It is tight, because it has the same predicate dependency graph as Π . By Theorem 11 from [6], SM[Π ′] is equivalent to the completion of Π ′ in the sense of [6, Section 6.1], which is identical to Comp[Π]. It remains to observe thatΠ is intuitionistically equivalent toΠ ′, so that SM[Π] is equivalent to SM[Π ′] [6, Section 5.1].
ar X
iv :1
50 2.
02 29
8v 2
[ cs
Belief revision of knowledge bases represented by a set of sentences in a given logic has been extensively studied but for specific logics, mainly propositional, and also recently Horn and description logics. Here, we propose to generalize this operation from a modeltheoretic point of view, by defining revision in an abstract model theory known under the name of satisfaction systems. In this framework, we generalize to any satisfaction systems the characterization of the well known AGM postulates given by Katsuno and Mendelzon for propositional logic in terms of minimal change among interpretations. Moreover, we study how to define revision, satisfying the AGM postulates, from relaxation notions that have been first introduced in description logics to define dissimilarity measures between concepts, and the consequence of which is to relax the set of models of the old belief until it becomes consistent with the new pieces of knowledge. We show how the proposed general framework can be instantiated in different logics such as propositional, first-order, description and Horn logics. In particular for description logics, we introduce several concrete relaxation operators tailored for the description logic ALC and its fragments EL and ELU , discuss their properties and provide some illustrative examples.
Key words: Abstract belief revision, Relaxation, AGM theory, satisfaction systems, description logics
A linking theory explains how verbs’ semantic arguments are mapped to their syntactic arguments. Various types of theories have been proposed, differing mostly on how they define semantic roles. All of them share the feature that they predict syntactic position based on some aspect of the verb’s semantics.
We begin by showing an important relationship between natural gradient methods and mirror descent.
Theorem 5.3.1. The natural gradient descent update at step k with metric tensor Gk , G(xk):
xk+1 = xk − αkG−1k ∇f(xk), (5.1)
is equivalent to the mirror descent update at step k, with ψk(x) = (1/2)xᵀGkx.
Proof. First, notice that ∇ψk(x) = Gkx. Next, we derive a closed-form for ψ∗k:
ψ∗k(y) = max x∈Rn
{ xᵀy − 1
2 xᵀGkx
} . (5.2)
Since the function being maximized on the right hand side is strictly concave, the x that maximizes it is its critical point. Solving for this
66 Safe Reinforcement Learning using Projected Natural Actor Critic
critical point, we get x = G−1k y. Substituting this into (5.2), we find that ψ∗k(y) = ( 1/2)yᵀG−1k y. Hence, ∇ψ∗k(y) = G−1k y. Using the definitions of ∇ψk(x) and ∇ψ∗k(y), we find that the mirror descent update is
xk+1 =G −1 k (Gkxk − αk∇f(xk)) = xk − αkG−1k ∇f(xk),
which is identical to (5.1). Although researchers often use ψk that are norms like the p-norm and Mahalanobis norm, notice that the ψk that results in natural gradient descent is not a norm. Also, since Gk depends on k, ψk is an adaptive proximal function [94].
In this section, we generalize our results to the bandit setting for both rewards and constraints. In the bandit setting, at each iteration, we are required to choose an action it from the pool of the actions [K]. Then only the reward and the constraint feedback for action it are revealed to the learner, i.e. r t it , c t it . In this
case, we are interested in the regret bound as maxp⊤c≥c0 ∑T t=1 p ⊤rt − ∑T t=1 r t it . In the classical setting, i.e., without constraint, this problem can be solved in stochastic and adversarial settings by UCB and Exp3 algorithms proposed in [18] and [19], respectively. The algorithm is shown in BanditLEWA algorithm which uses the similar idea to Exp3 for exploration and exploitation.
Before presenting the performance bounds of the algorithm, let us introduce two vectors: r̂t is all zero vector except in itth component which is set to be r̂tit = r t it /ptit and similarly ĉt is all zero vector except in itth component which is set to be ĉtit = c t it /ptit . It is easy to verify that Eit [r̂t] = rt and Eit [ĉt] = ct. The following theorem shows that BanditLEWA algorithm achieves O(T 3/4) regret bound and O(T 3/4) bound on the violation of the constraint in expectation.
Theorem 3. Let γ = O(T−1/2), η = γ
K
δ
δ + 1 , by running BanditLEWA algo-
rithm, we have
max p⊤c≥c0
T∑
t=1
p⊤rt − E [ T∑
t=1
rtit
] ≤ O(T 3/4) and
E
[ T∑
t=1
(c0 − p⊤t c) ]
+
≤ O(T 3/4).
Proof. In order to have an improved analysis, we first derive an improved primal inequality and an improved dual inequality. Let Rt = r̂t+λtĉt. By following the
analysis for Exp3 algorithm [19], we have
T∑
t=1
ηq⊤t Rt + η 2q⊤t R 2 t ≥ ln WT+1 W1
≥ ηp⊤ T∑
t=1
Rt − lnK. (8)
Dividing both sides by η, and taking expectation we get
E [ p⊤ T∑
t=1
Rt − T∑
t=1
q⊤t Rt
] ≤ lnK
η + ηE
[ T∑
t=1
q⊤t (Rt) 2
]
≤ lnK η + ηE
[ T∑
t=1
2q⊤t (r̂t) 2 + 2λ2tq ⊤ t (ĉt) 2
] ≤ lnK
η +
2ηKT 1− γ + 2ηK 1− γ
T∑
t=1
λ2t ,
(9)
where the third inequality follows from the following inequality
E[q⊤t (ĉt) 2] = E [ qtit ( btit ptit )2] ≤ 1 1− γE [ ptit ( ctit ptit )2]
= 1 1− γE [ (ctit) 2
ptit
] = 1 1− γE [ K∑
i=1
(cti) 2
] ≤ K
1− γ , (10)
and the same inequality holds for E[q⊤t (r̂t) 2]. Next, we let gt(λ) = δ 2 λ2+λ(q⊤t ĉt− c0). By following the similar analysis in the proof of Lemma 2, we have
gt(λt)− gt(λ) ≤ 1
2η
( |λ− λt|2 − |λ− λt+1|2 ) + η
2 |∇gt(λt)|2
≤ 1 2η
( |λ− λt|2 − |λ− λt+1|2 ) + η(q⊤t ĉt) 2 + η.
Taking summation and expectation, we have
E
[ T∑
t=1
gt(λt)− gt(λ) ] ≤ λ 2
2η + ηE
[ ∑
t
q⊤t (ĉt) 2
] + ηT. ≤ λ 2
2η +
ηKT 1− γ + ηT.
(11)
Combining equations (11) and (9) gives
E
[ T∑
t=1
p⊤rt − q⊤t rt ] + E [ T∑
t=1
λ(c0 − q⊤t c)− ( δT
2 +
1
2η
) λ2
]
≤ lnK η + 4ηKT 1− γ + ( 2ηK 1− γ − γ 2 ) T∑
t=1
λ2t + E
[ T∑
t=1
λt(c0 − p⊤c) ] .
Noting that (1 − γ)qt ≤ pt, so we get
E
[ T∑
t=1
(1− γ)p⊤rt − p⊤t rt ] + E [ T∑
t=1
λ((1 − γ)c0 − p⊤t c)− ( δT
2 +
1
2η
) λ2
]
≤ lnK η + 4ηKT +
( 2ηK − (1− γ)δ
2
) T∑
t=1
λ2t + E
[ T∑
t=1
λt(c0 − p⊤c) ] .
Let c0 ≥ p⊤c, 2ηK ≤ (1− γ) δ2 . By taking maximization over λ, we have
E [ max
p⊤c≥c0
T∑
t=1
p⊤rt − p⊤t rt ] + E   [∑T t=1((1− γ)c0 − p⊤t c) ]2 +
2(δT + 1/η)
 
≤ lnK η + 4ηKT + γT = K(δ + 1) lnK δγ + 4 γδ δ + 1 T + γT
≤ K(δ + 1) lnK δγ + 5δ + 1 δ + 1 γT ≤
√ (5δ + 1)K lnK
δ T .
Then we obtain
max p⊤c≥c0
T∑
t=1
p⊤rt − E [ T∑
t=1
p⊤t rt
] ≤ √ (5δ + 1)K lnK
δ T
E
[ T∑
t=1
(c0 − p⊤t c) ]
+
≤ √√√√ ( T + √ (5δ + 1)K lnK
δ T
) 2(δT + 1/η) + γT .
Let γ = O(T−1/2), δ = O(T−1/2), then we get O(T 3/4) regret and O(T 3/4) constraint bounds as claimed.
As our previous results, we present an algorithm with a high probability bound on the regret and the violation of the constraint. For ease of exposition, we introduce ct = 1
t ∑t s=1 cs and c̃t = 1 t ∑t s=1 ĉs. We modify BanditLEWA algorithm
High Probability BanditLEWA(η, γ, δ, and ǫ) initialize: w1 = exp ( ηα √ KT ) 1, and λ1 = 0 , where α = 2 √ ln(4KT/ǫ)
iterate t = 1, 2, . . . , T Set qt = wt/ ∑ j wtj
Set pt = (1− γ)qt + γ/K Draw action it randomly accordingly to the probabilities pt Receive reward rtit and a realization of constraint c t it
for action it Update wt+1 by
wt+1i = exp
( η [( r̂ti +
α
pti √ KT
) + λt ( c̃ti + 2K
γ α1 √ t
)])
Update λt+1 = [(1− δη)λt − η(x ⊤
t ĉt + αt − c0)]+ end iterate
so that it uses more accurate estimations rather than using correct expectation in updating the primal and dual variables. To this end, we use upper confidence bound for rewards as Exp3.P algorithm [18] and for constraint vector c. The following theorem states the regret bound and the violation of constraints in long term for the high probability BanditLEWA. Theorem 4. Let αt = √ (1/2) ln(6KT/ǫ)/ √ t, γ = O(T−1/2), η = γ
βK
δ
δ + 1 ,
and α = 2 √ ln(4KT/ǫ), where β = max{3, 1+2α1}, by running High Probability BanditLEWA, we have with probability 1− ǫ
max p⊤c≥c0
p⊤ T∑
t=1
rt − T∑
t=1
rtit ≤ O(T 3/4/
√ δ) and
[ T∑
t=1
(c0 − p⊤t c) ]
+
≤ O( √ δT ).
The proof is deferred to B. From this theorem, when δ = O(T−1/4), the regret and the violation bounds are O(T 7/8) and O(T 7/8), respectively.
Recall that we use as our baseline the heuristic method described in Section 3, where the Occurrence Heuristic is used to label a report using the seed words and phrases manually extracted from the 233 training reports. Results, shown in the Experiment 1 section of Table 4, are reported in terms of precision (P), recall (R), and F-measure (F). The last two columns show whether a particular automatic labeling scheme is significantly better
than the baseline with respect to McNemar’s test (MN) and stratified approximate randomization test (AR) [Statistical significance and insignificance are denoted by a X and an X, respectively]. When evaluated on the 1000 reports in the test set, the baseline achieves a precision of 56.48%, a recall of 40.47% and an F-measure of 47.15%.
First of all, we shall note that a goal directed choice is a choice based on a pair of actions a, b ∈ A, where A is called action space beside state space comprising the states or choices. To make the right choice the goal directed agents need a suitable policy but before that, they must assign values to actions that are proportional to the amount of reward expected. After that , the agent needs to choose the action which has been assigned the highest value. This process of value assignment could be denoted as follows:
Ua = ∑ x p(s)r(oa(S)) (2)
where oa(S) is the outcome generated by action a.
Now we want to divide these assigned values(because of the huge amount of data) into learning about the states(classical conditioning) and learning about the actions(instrumental learning). After computing the state values
V (s) = R(s) + ∑ a ∑ s′ π(s, a)T (s, a, πs ′ )V (s ′ ) (3)
where policy π(s, a) represents choosing action a in state s. We can now compute the prediction error(Dopamine releasing), i.e. δt = rt + V̂ (st+1) − V̂ (st). These V̂ (st)
values are technically called critics and we can define another separate module called actor(which evaluates actions instead of states denoted by Q(s, a). Now we can compute the advantage of our system concerning action a which equals the difference between the future value of taking action a and the future value averaged over actions : A(s, a) = Q(s, a)−V (s).
Proposed Algorithm
In general, this actor/critic approach could indicate the best action and policy to carry out Pavlovian conditioning. Even so, since this method includes two computational processes, when we are dealing with a huge amount of data , are likely to encounter some errors and more importantly limited working memory. To avoid these problem, we will use the utility function Ux = ∑ x p(s)r(ox(S)) to help the algorithm work properly by turning the algorithm into the supervised actor critic algorithm. In this algorithm the supervisor Ux adds structure to our algorithm with a feedback controller that is easily designed yet sub-optimal, and a human operator monitoring the actors choices. During the experiment, the supervisor provides the actor with hints about which actions may or may not be promising for the current situation, thereby altering the exploratory nature of the actor’s trial-and-error learning.
Taken together,the actor, the supervisor and the gain scheduler form a ’composite’ actor that dispatches a composite action to the environment. The environment responds to this action with a transition from the current state, s, to the next state, s ′ . The environment also supply an evaluation called the immediate reward,r[7]. The task of the critic is to observe states and rewards and to build a value function, V (s), that accounts for both immediate and future rewards received under the composite policy, . This value function is recursively defined as :
V π(s) = ∑ a UaR(s ′ ) + γV π(s ′ ) (4)
where γ ∈ [0, 1] is a factor that discounts the value of the next state. Here we focus on deterministic policies, although this process also generalizes to the stochastic case where π represents a distribution for choosing actions probabilistically. At last, the Temporal Difference in this algorithm or the amount of Dopamine as such, could be written as following:
δ = r + γV (s ′ )− V (s) (5)
now inserting the parametrized function U in the (3), will lead us to derive a new form of delta error as follows:
δ = r2 + (γV (s ′ )− V (s))2 (6)
The resulting modified algorithm was implemented as the movements of an agent in choice making. We took ’Grid Sailing Task’ as our task in which the agent should takes a step (through moving a cursor) towards the goal in a grid from the left side to the far end of the right side of the grid(that shapes our state space). We associate a reward to
each action given the state it resides in. So the agent is asked to reach a goal directed (reward associated) state on the other side of the grid. Here a value function (V ) is computed along with each movement in the way our agent learns to value the states neighbouring to the reward, and then incrementally learns to come after the optimal pathway which is the midway (bold straight line) in grid. To initialize the program , we randomize a parametrized policy first and define the state space as a matrix of the size 3 by 5 and run the program for 8 times to measure the average time that takes to obtain optimal reward.
In the following figures, you could see the final result where the agent received a permanent reward and found an optimal pathway that is the straight mid-line as such. The plot of action values, Q, in addition to the time execution, ’te’, is shown.
As the benchmark we used the convolutional model proposed in [12] and received comparable results in all considered games. Quite surprisingly, in the case of Seaquest we were able to train RAM-only agents which behave better than the benchmark screen-only agent. Mixing screen and RAM did not lead to an improved performance comparing to screen-only and RAM-only agents.
This work was supported by the European Research Council (ERC-2011-AdG-295810 BOOTPHON), the Agence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, ANR-10-IDEX0001-02 PSL*), the Ecole de Neurosciences de Paris, the Region Ile de France DIM Cerveau et pense, and an Amazon Web Services in Education Research Grant award.
A p-order tensor is a p-dimensional array. Here we focus on 4-order tensors. For a tensor T ∈ R d4 and indices i1, . . . , i4 ∈ [d], we denote the (i1, . . . , i4)-th entry of T by Ti1,...,i4 . Every ddimensional vector a induces a rank-one 4-order tensor, denoted a⊗4, where a⊗4i1,i2,i3,i4 is ai1ai2ai3ai4 . We can present the tensor T using a multilinear form. Given vectors u, v, z, w ∈ Rd, we define
T (u, v, z, w) = ∑
i1,i2,i3,i4
Ti1,...,i4ui1vi2zi3wi4
The tensor T has an orthogonal decomposition if it can be written as
T = d∑
i=1
a⊗4i . (3)
In case that such decomposition exists, it is unique up to a permutation of the ai’s and sign flips. A central problem in machine learning is to compute the tensor decomposition of a given tensor T (Anandkumar et al. (2014)). While we have exponentially many equivalent solutions, the average of two solutions does not form a solution. Hence, any reasonable formulation of this problem must be non-convex. Luckily, as was shown in Ge et al. (2015), there exists a strict saddle formulation of this problem.
For simplicity, we consider the problem of finding one component (one can proceed and find all
the components using deflation). Consider the following objective:
max ‖u‖=1 T (u, u, u, u) . (4)
Lemma 22 (Ge et al. (2015)) Suppose that T admits a Tensor decomposition as in (3). The only local minima of (4) are ±ai. Furthermore, the objective (4) is (α, γ, τ)-strict saddle with α = Ω(1), γ = 7/d and τ = 1/poly(d). Last, for p = 1, 2, 3, the magnitude of the p-th order derivative of this objective is O( √ d).
Although our definition of strict saddle functions in the constrained setting is slightly different from its counterpart in Ge et al. (2015), it is not hard to show that Lemma 22 still holds (see Appendix B).
In applications, we often have access to T only through a stochastic oracle. Following Ge et al. (2015), we consider the following formulation of ICA. Let A be an orthonormal linear transformation. Suppose that x is uniform on {±1}d and denote by y = Ax. Our goal is to recover the matrix A using the observations y. It turns out that ICA reduces to tensor decomposition. Namely, define Z ∈ Rd4 by
(∀i ∈ [d]) Z(i, i, i, i) = 3, (∀i 6= j) Z(i, i, j, j) = Z(i, j, j, i) = Z(i, j, i, j) = 1 , where all other entries of Z are zero.
Lemma 23 The expectation 12E[Z − y⊗4] is equal to T , where the vectors participating in the decomposition of T correspond to columns of A.
Following the lemma, we can rewrite (4) as the following expected risk:
max ‖u‖=1 E
[ 1
2
( Z − y⊗4
) ]
(u, u, u, u) . (5)
Furthermore, as was shown in Ge et al. (2015), one can efficiently compute a stochastic gradient and use SGD to optimize this objective. Using Lemma 22 and Theorem 2, we conclude that the sample complexity of extracting a single column of A is Õ ( poly(d) + d 3/2
ǫ
)
. The sample complexity of
extracting all the columns is Õ ( poly(d) + d 5/2
ǫ
)
.
When faced with a classification job, an analyst will often want to select the best methods for the application; this can be a daunting task since there are a large number of methods available. Users will need insights, such as rankings of the methods, to guide them to make the best selection, and to go through the selection process in an easy-to-handle manner. Several studies on the experimental evaluation of various methods for classification have been reported recently (e.g., Fernández-Delgado et al., 2014; Van Hulse, Khoshgoftaar and Napolitano, 2007). Reference (Fernández-Delgado et al., 2014) is a main representative of such studies, which used 121 data sets to evaluate 179 classifiers.
classification methods by considering all data sets in one pool – they did not distinguish the data sets based on their hardness. Moreover, there was no systematic study to identify which classification benchmark data sets are hard for traditional classification methods, and there were no rankings of methods based on their performance on hard data sets only. Filling these gaps is important, as the identified hard data sets can help future studies to develop new classification algorithms to complement existing classification algorithms, and the ranking of methods on hard data sets can help users select the best method when they are working with potentially hard data sets. We plan to fill this gap in this study.
This study will evaluate both classification algorithms and feature selection methods in
combination. Specifically, it will identify hard data sets for which no combinations of representative classification algorithms and feature selection methods can produce accurate classification models. Moreover, the study will use the area under the ROC (AUC) and F-Measure, instead of the accuracy measure (as was done in Fernández-Delgado et al., 2014), to evaluate the performance of classification models. These measures were chosen based on the recent consensus that the accuracy measure has significant shortcomings when compared with the above two measures, especially AUC.
To identify a list of benchmark data sets that are hard for representative classification and
feature selection methods, we perform a systematical evaluation of 54 combinations, involving nine representative classification algorithms and six commonly used feature selection methods, on 129 data sets from the UCI repository. We note that some data sets with known high classification accuracy based on results of Fernández-Delgado et al. (2014) were excluded in our experiments.
For ease of discussion, a data set for classification will be called hard if none of the 54
combinations can achieve an AUC over 0.8 and none of the 54 combinations can achieve an F-Measure value over 0.8; it is called easy otherwise. A total of 17 out of the 129 data sets were found to be hard in our experiments.
This paper also compares the performance of different methods separately on the hard
data sets and on the easy data sets. This was done based on their performance on data sets for which complete results were obtained for all of the 54 combinations. It turns out that the method rankings resulting from our experiments are somehow different from those in the literature and hence they offer new insights on method selection.
The rest of the paper is organized as follows. Section 2 describes the classification
algorithms and feature selection methods used in this study. Section 3 describes the data sets included in this study. Section 4 gives the experiment settings and the evaluation measures used. Section 5 presents the experimental results and the associated analysis. Section 6 concludes the paper.

Two major approaches have been dominating the Question Answering (QA) literature in the last decade. Some work have heavily invested on converting the question into a linguistically motivated representation (e.g., using syntactic and/or semantic parsing), which simplifies the problem into finding the answer that best fits this representation (Buscaldi et al., 2010; Bilotti et al., 2007; Bilotti et al., 2010; Berant et al., 2013; Reddy et al., 2014). For factoid QA, where the answer to the question is an indisputable fact, and all known facts are stored in a knowledge base (KB) in some structured form — in this case, the question can be represented in a form that is consistent with the KB.
The recent success of deep learning has tempted many researchers to explore more languageagnostic approaches, relying on automatically learning representations in a data-driven manner. With an increasing amount of access to computational resources and data, neural networks have outperformed traditional approaches on a variety of natural language processing (NLP) tasks (Irsoy and Cardie, 2014; Kim, 2014; Chen and Manning, 2014), including QA (Hsu et al., 2016; Yang et al., 2014; Dong et al., 2015; Bordes et al., 2015).
We argue that both linguistically-motivated and learning-based approaches have complementary benefits, and therefore propose a novel method for factoid QA that can be considered as a hybrid: given a knowledge base (KB), a recurrent neural network (RNN) is trained to automatically convert a given question into a structured query (Section 3a), which is then queried through the KB in order to retrieve the most relevant answer(s) (Section 3b).
Our assumption is that a question can be represented as a pair: (a) the entity the question is about, and (b) the aspect of the entity we are interested in. If a question fits into this assumption, we refer to it as a first-order question.1 While this sounds similar to the “Single Supporting Fact” task in (Weston et al., 2015), for which they report 100% accuracy, the problem becomes much more complex as the knowledge base grows to a realistic size, since the level of ambiguity increases drastically with a larger vocabulary and list of entities. This is the reason why we focus on well-studied, large-scale QA tasks like Free917 (Cai and Yates, 2013), WebQuestions (Be-
1Bordes et al. refer to them as simple questions (2015).
ar X
iv :1
60 6.
05 02
9v 1
[ cs
.C L
] 1
6 Ju
n 20
rant et al., 2013) (WQ), and SimpleQuestions (Bordes et al., 2015) (SQ), that rely on Freebase as a realistic knowledge base. Given that the state-of-the-art accuracy is around 45% for WQ and 64% for SQ, this is far from a solved problem, and there is still plenty of room for improvement.
We build our novel approach based on the hypothesis that majority of questions in these tasks are firstorder factoid questions. Results on two most recent benchmark tasks, SimpleQuestions and WebQuestions, strongly confirm our hypothesis. The accuracy of our approach indicates major improvements over the state of the art — relative improvement reaches 16% for WQ, and surpasses 38% for SQ.
This section focuses on demonstrating the augmented descriptive power n-WFSMs, w.r.t. to 1- and 2-WFSMs (acceptors and transducers), and on exposing the practical importance of the join operation. It also aims at illustrating how to use n-WFSMs, in practice. Indeed, some of the applications are not feasible with 1- and 2-WFSMs. The section does not focus on the presented applications per se.
When a large volume of training data is available we had previously used a random sub-sample of the test data at each generation [Langdon, 2010] to reduce the volume of testing but also found it helped with generalisation. Here we also use it to spread the training data more uniformly.
We divided the training data into non-overlapping bins using the value to be predicted. (In the case of the two binary classification problems, Sections 4.5 and 4.6, there are just two bins.) Each generation equal numbers of training examples are randomly chosen from each bin. Where a bin contains more examples than are needed the examples are kept in the same random order and the next group are taken. Except where noted, this ensures the examples used in the next generation are not the same ones as used in the previous generation. If there are insufficient examples left, the bin’s examples are first put in a new random order. (This is somewhat reminiscent of Gathercole’s DSS [Gathercole and Ross, 1994], as used commercially [Foster, 2001].)
Histogram equalization is a technique to balance out the frequency of labels that the network sees during training. As training with patches leads to a set of dependent pixels which are in close proximity on an image (see Figure 1.2), the training results can be worse than with minibatches. With minibatches (see Figure 1.1), it is possible to create a database of single pixel labels and the minibatch can draw n independent pixels to train with in each stochastic gradient descent step.
The first equalization approach is a patch prior, which will prefer patches with rare labels. This is done by comparing the label distribution within each patch to the total label frequency in all training images.
rj = n−1 ∑ i=0 aj,i ci
(4.1)
ĉi = 1 Zi m−1 ∑ j=0 rj · aj,i (4.2)
For patches j = 0, . . . , m− 1 and labels i = 0, . . . , n− 1, Equation 4.1 calculates the weight for each patch (rj) based on the total label distribution ci and the frequency within each patch (aj,i). Equation 4.2 calculates the label posterior distribution ĉi based on the patch weights rj and the label frequency within the patch. Zi is a normalization factor to get ∑n−1i=0 ĉi = 1.
The method does help if there are patches that have rare labels, because those patches will be drawn at random with a higher probability than others. An example is the synapse label (number 7) (see Figure 4.1 and Table 4.1). It cannot balance the labels which have a similar distribution in every patch - for example cell membranes versus cell interior.
When the patch size gets bigger and approaches the size of the training images, the label distribution after the patch prior approaches the original label distribution. Thus the patch prior only works with small training patches. It can also completely
26
4.3. Histogram Equalization
equalize the histogram when using a single pixel as patch size (w = 1), as this is the same situation as with independent pixels.
When calculating label frequencies ci, it is taken into account that pixel labels closer to the border of the image are covered by less patches than those in the center of the image. A patch can start and end at any offset inside the image. Corner pixels for example are only covered by the one patch that starts in that corner.
After applying the patch prior, the new label distribution ĉi is depicted in Figure 4.1 with values from Table 4.1.
The second method masks pixels randomly in each patch, where the random function is thresholded by the inverse frequency of the label. This means a pixel of label type i gets masked (removed) from the error map if:
c−1i · (minj cj) −1 < p (4.3)
In Equation 4.3, p ∈ [0, 1] is a random value picked at uniform and ci ∈ [0, 1] the label frequency for label i = 0, . . . , n − 1. Less frequent labels are less likely to
27
4.3. Histogram Equalization
be masked out, while membrane and cell interior labels, which are very common, get masked with a high probability. The result is a completely balanced label histogram (see Table 4.1). The least frequent label consequently never gets masked.
The best solution to train on an exact label distribution as desired remains to do minibatch training. This is theoretically also possible with SK-, USK- and U-Net, but very inefficient in terms of training speed.
Minibatch training is the standard for most networks, also for SW-Net. The network weights are updated every time after a patch or minibatch has run through forwarding and backwarding. The gradients get accumulated over all error pixels in the Softmax loss layer and are then normalized for stable training.
Having a balanced label distribution is mostly important with Softmax and multi label classification. When doing background-foreground separation with Malis, the patch prior has little to no effect and masking can not be used at all. Malis does already focus the error on problematic zones globally over the whole patch that runs through the network (see Section 5.3.2) and therefore the label distribution during training does not matter.
28
Chapter 5
Caffe Library
The Regularized CCA problem can be written in this form:
max U⊤(CXX+γxImx )U=I,V ⊤(CY Y +γyImy )V =I Tr(U⊤CXY V ) (14)
Algorithm 2 shows how to compute the regularization path of CCA in an efficient way. The computational complexity of the cross-validation is dominated by the computation of SVD of T within
1Using thin SVD Py is not a complete orthonormal basis of Rmy .
the loop that is C = min(O(mxm2y), O(mym 2 x)). If we have Nx and Ny values of γx and γy , the total complexity is NxNyC. Note that this can be fully parallelizable since each SVD computation is independent from the rest. σx,1 and σx,mx are minimum and maximum singular value of X .
Algorithm 2 Tikhonov Regularized CCA
1: procedure TIKHONOV REGULARIZED CCA(X ∈ Rn×mx , Y ∈ Rn×my ) 2: [Ux,Σx, Vx] ← SV D(X). 3: [Uy,Σy, Vy] ← SVD(Y ). 4: T0 = Σx(U ⊤ x Uy)Σy ∈ R mx×my 5: for γx ∈ {σ2x,1, . . . σ 2 x,mx
} do ⊲ The set of singular values squared or a subsampled grid 6: for γy ∈ {σ2y,1, . . . σ 2 y,my } do 7: T ← ( Σ2x + γxI )− 1 2 T0 ( Σ2y + γyI )− 1 2 8: [Px,Σ γx,γy , Py] = SV D(T ) 9: U = Vx ( Σ2x + γxI )− 1 2 Px
10: V = Vy ( Σ2y + γyI )− 1
2 Py 11: Compute performance using U, V,Σγx,γy on a validation Set. 12: (Bidirectional Retrieval is done using a sorted list of the scores in Eqs (12), and (13).) 13: end for 14: end for 15: return U, V,Σγx,γy , γx, γy with best validation performance for each task. 16: end procedure
Lemma 2. Algorithm 2 computes the regularization path of Tikhonov regularized CCA.
The values chosen for γx, γy (Steps 5 and 6) are very important and are related to a technique called spectral filtering Hansen (1986). A full discussion of the latter and its relation to truncated SVD is beyond the scope of this paper and will be subject to study in a forthcoming work.
Researchers in Social Sciences and Humanities (like Political Studies) have always gathered data and compiled databases of knowledge. This information often has a temporal component, the evolution of a certain number of entities is recorded over a period of time. Each entity is described using multiple attributes, which form the multidimensional description space. Therefore, an entry in such a database would be an observation, a triple (entity, timestamp, description). An observation xi = (φl, tm, xdi ) signifies that the en-
1
ar X
iv :1
60 1.
02 60
3v 1
[ cs
.L G
] 1
1 Ja
n 20
16
tity φl is described by the vector xdi at the moment of time tm. We denote by x φ i the entity to which the observation xi is associated. Similarly, xti is the timestamp associated with the observation xi. Each observation belongs to a single entity and, consequently, each entity is associated with multiple observations, for different moments of time. Formally:
∀xi ∈ D : ∃!φl ∈ Φ so that xφi = φl ∀(φl, tm) ∈ Φ× T : ∃!xi = (xφi , x t i, x d i ) so that x φ i = φl and x t i = tm
For example, a database studying the evolution of democratic states 1 will store, for each country and each year, the value of multiple economical, social, political and financial indicators. The countries are the entities, and the years are the timestamps.
Starting from such a database, one of the interests of Political Studies researchers is to detect typical evolution patterns. There is a double interest: a) obtaining a broader understanding of the phases that the entity collection went through over time (e.g. detecting the periods of global political instability, of economic crisis, of wealthiness etc.); b) constructing the trajectory of an entity through the different phases (e.g. a country may have gone through a period of military dictatorship, followed by a period of wealthy democracy). The criteria describing each phase are not known beforehand (which indicators announce a world economic crisis?) and may differ from one phase to another.
We address these issues by proposing a novel temporal-driven constrained clustering algorithm. The proposed algorithm partitions the observations into clusters µj ∈ M, that are coherent both in the multidimensional description space and in the temporal space. We consider that the obtained clusters can be used to represent the typical phases of the evolution of the entities through time. Figure 1 shows the desired result of our clustering algorithm. Each of the three depicted entities (φ1, φ2 and φ3) is described at 10 moments of time (tm,m = 1, 2, ..., 10). The 30 observations of the dataset are partitioned into 5 clusters (µj , j = 1, 2, ..., 5). In Figure 1a we observe how clusters µj are organized in time. Each of the clusters has a limited extent in time, and the time extents of clusters can overlap. The temporal extent of a cluster is the minimal interval of time that contains all the timestamps of the observations in that cluster. The entities navigate through clusters. When an observation belonging to an entity is assigned to cluster µ2 and the anterior observation of the same entity is assigned in cluster µ1, then we consider that the entity has a transition from phase µ1 to phase µ2. Figure 1b shows how the series of observations belonging to each entity are assigned to clusters, thus forming continuous segments. This succession of segments is
interpreted as the succession of phases through which the entity passes. For this succession to be meaningful, each entity should be assigned to a rather limited number of continuous segments. Passing through too many phases reduces the comprehension. Similarly, evolutions which are alternations between two phases (e.g., µ1 −→ µ2 −→ µ1 −→ µ2) hinder the comprehension.
Based on these observations, we assume that the resulting partition must: • regroup observations having similar descriptions into the same cluster (just as tra-
ditional clustering does). The clusters represent a certain type of evolution; • create temporally coherent clusters, with limited extent in time. In order for a clus-
ter to be meaningful, it should regroup observations which are temporally close (be contiguous on the temporal dimension). If there are two different periods with similar evolutions (e.g. two economical crises), it is preferable to have them regrouped separately, as they represent two distinct phases. Furthermore, while it is acceptable that some evolutions exist during the entire period, usually the resulted clusters should have a limited temporal extent; • segment, as contiguously as possible, the series of observations for each entity. The sequence of segments will be interpreted as the sequence of phases through which the entity passes. In this paper, we propose a new time-aware dissimilarity measure that takes into account the temporal dimension. Observations that are close in the description space, but distant in time are considered as dissimilar. We also propose a method to enforce the segmentation contiguity, by introducing a penalty term based on the Normal Distribution Function. We combine the two propositions into a novel time-driven constrained clustering algorithm, TDCK-Means, which creates a partition of coherent clusters, both in the multidimensional space and in the temporal space. This algorithm uses soft semi-supervised constraints to encourage adjacent observations belonging to the same entity to be assigned to the same cluster. The proposed algorithm constructs the clusters that serve as evolution phases and segments the observations series for each entity.
The paper is organized as follows. In Section 2 we present some previous related works and, in Section 3, we introduce the temporal-aware dissimilarity function, the contiguity penalty, function the TDCK-Means algorithm and the graph structure induction method. In Section 4, we present the dataset that we use, the proposed evaluation measures and the obtained results. Finally, in Section 5, we draw the conclusion and plan some future extensions.
We collect user perceptions of the The Fox and the Crow generated with direct speech and with different personality models (character voices) for each speech act. A dialogic variation plus character voice excerpt is A3 in Fig. 6. The dialogic story is told 1) only with the neutral model; 2) with the crow as shy and the fox as laid-back; and 3) with the crow as laid-back and the fox as shy.
Subjects are given a free text box and asked to enter as many words as they wish to use to describe the characters in the story. Table 3 shows the percentage of positive and negative descriptive words when categorized by
LIWC [17]. Some words include “clever” and “sneaky” for the laid-back and neutral fox, and “shy” and “wise” for the shy fox. The laid-back and neutral crow was pereived as “naı́ve” and “gullible” whereas the shy crow is more “stupid” and “foolish”.
Overall, the crow’s shy voice is perceived as more positive than the crow’s neutral voice, (ttest(12) = -4.38, p < 0.0001), and the crow’s laid-back voice (ttest(12) = -6.32, p < 0.0001). We hypothesize that this is because the stuttering and hesitations make the character seem more helpless and tricked, rather than the laid-back model which is more boisterous. However, there is less variation between the fox polarity. Both the stuttering shy fox and the boisterous laid-back fox were seen equally as “cunning” and “smart”. Although we don’t observe a difference between all characters, there is enough evidence to warrent further investigation of how reader perceptions change when the same content is realized in difference voices.

Proof of Lemma 5.2. The first part of the theorem is straightforward (by contrapositive, if at least one of the two sequences is not non-increasing then we can find a violation of 2-monotonicity). We thus turn to the second part, and show the contrapositive; for this purpose, we require the following result:
Claim C.1. If f : [n]2 → {0, 1} is a 2-column-wise monotone function such that (i) f(1, j) = f(n, j) = 0 for all j ∈ [n] and (ii) both (
¯ ∂fj)j∈[n], ( ¯ ∂hj)j∈[n] ⊆ [n] are non-increasing, then f is
2-monotone.
Proof. By contradiction, suppose there exists a 2-column-wise monotone function f satisfying (i) and (ii), which is not 2-monotone. This last point implies there exists a triple of comparable elements x = (ix, jx) ≺ y = (iy, jy) ≺ z = (iz , jz) constituting a violation, i.e. such that (f(x), f(y), f(z)) = (1, 0, 1). Moreover, since (i) holds we must have 1 < ix ≤ iy ≤ iz < n; more precisely, 1 ≤
¯ ∂fjx < ix ≤ iy ≤ iz < ∂̄fjz ≤ n. As x ≺ y ≺ z, we have jx ≤ jy ≤ jz, which
by the non-increasing assumption (ii) implies that ¯ ∂fjx ≥ ¯∂fjy and ∂̄fjy ≥ ∂̄fjz . But this is not possible, as altogether this leads to ¯ ∂fjy < iy < ∂̄fjy , i.e. f(y) = 1.
Assume both sequences ( ¯ ∂fj)j∈[n], ( ¯ ∂hj)j∈[n] ⊆ [n] are ε2 -close to non-increasing, and let L,H ⊂ [n] (respectively) be the set of indices where the two sequences need to be changed in order to become non-increasing. By assumption, |L| , |H| ≤ εn2 , so |L ∪H| ≤ εn. But to “fix” a value of ( ¯ ∂fj)j∈[n] or (∂̄fj)j∈[n] requires to change the values of the function f inside a single column – and this can be done preserving its 2-column-wise-monotonicity, so that changing the value of f on at most n points is enough. It follows that making both (
¯ ∂fj)j∈[n] and (∂̄fj)j∈[n] non-increasing
requires to change f on at most εn2 points, and with Claim C.1 this results in a function which is 2-monotone. Thus, f is ε-close to 2-monotone.
Proof of Lemma 5.3. Recall that we aim at establishing the following:
dist ( f,M(2)2 ) ≤ L1(∂̄f,M(1)) + L1(
¯ ∂f,M(1)) (6)
For notational convenience, we will view in this proof the sequences ( ¯ ∂f)j , (∂̄f)j) as functions ¯ ∂f, ∂̄f : [n] → [n]. Let ℓ, h : [n] → [n] (for “low” and ”high,” respectively) be monotone functions achieving L1(
¯ ∂f,M(1)) and L1(∂̄f,M(1)), respectively.
• As ¯ ∂f(j) ≤ ∂̄f(j) for all j ∈ [n], we will assume ℓ(j) ≤ h(j) for all j. Otherwise, one can
consider instead the functions ℓ′ = min(ℓ, h) and h′ = max(ℓ, h): both will still be monotone (non-increasing), and by construction
∣∣ℓ′(j) − ¯ ∂f(j) ∣∣+ ∣∣∣h′(j)− ∂̄f(j) ∣∣∣ ≤ |ℓ(j) − ¯ ∂f(j)|+ ∣∣∣h(j)− ∂̄f(j) ∣∣∣
for all j ∈ [n], so that L1(∂̄f, ℓ′) + L1( ¯ ∂f, h′) ≤ L1(∂̄f, ℓ) + L1( ¯ ∂f, h).
• From ℓ and h, we can define a 2-column-wise monotone function g : [n]2 → [n] such that
¯ ∂g = ℓ and ∂̄g = h: that is,
g(i, j) =    0 if i ≥ h(j) 1 if ℓ(j) < i < h(j)
0 if i ≤ ℓ(j)
for (i, j) ∈ [n]2. It is clear that g is 2-column-wise monotone with g(1, j) = g(n, j) = 0 for all j ∈ [n]; since by construction
¯ ∂g, ∂̄g are non-decreasing, we can invoke Claim C.1 to conclude g is 2-monotone. It
remains to bound the distance between f and g: writing ∆j ∈ {0, . . . , n} for the number of points
on which f and g differ in the j-th column, we have
dist ( f,M(2)2 ) ≤ dist(f, g) = 1
n2
n∑
j=1
∆j ≤ 1
n2
n∑
j=1
( |ℓ(j) −
¯ ∂f(j)|+
∣∣∣h(j) − ∂̄f(j) ∣∣∣ )
= 1
n2
n∑
j=1
|ℓ(j)− ¯ ∂f(j)|+ 1 n2
n∑
j=1
∣∣∣h(j) − ∂̄f(j) ∣∣∣ = L1(
¯ ∂f, ℓ) + L1
( ∂̄f, h )
≤ L1( ¯ ∂f,M(1)) + L1(∂̄f,M(1))
which concludes the proof.
Proof of Proposition 7.3. We write ν def = µ× Leb[0,1] for the product measure on X × [0, 1] induced by µ and the Lebesgue measure on [0, 1]; so that ν(X × [0, 1]) = µ(X ) · 1 = µ(X ). For any fixed t ∈ [0, 1], let gt ∈ M(X→{0,1}) be any function achieving L1(T ◦ f(·, t), gt) =
L1 ( T ◦ f(·, t),M(X→{0,1}) ) , and define g ∈ [0, 1]X by g′(x) = ∫ 1 0 dtgt(x) for all x ∈ X : note that g is then monotone by construction.12 Moreover, choose h ∈ M(X×[0,1]→{0,1}) as a function achieving L1(T ◦ f, h) = L1 ( T ◦ f,M(X×[0,1]→{0,1}) ) . Then we have
L1 ( f,M(X→[0,1]) ) ≤ L1 ( f, g′ ) = 1
µ(X )
∫
X µ(dx)
∣∣∣∣ ∫ 1
0 dt(T ◦ f(x, t)− gt(x))
∣∣∣∣
≤ 1 µ(X )
∫
X µ(dx)
∫ 1
0 dt |T ◦ f(x, t)− gt(x)|
=
∫ 1
0 dt
( 1
µ(X )
∫
X µ(dx) |T ◦ f(x, t)− gt(x)|
) = ∫ 1
0 dtL1(T ◦ f(·, t), gt)
≤ ∫ 1
0 dtL1(T ◦ f(·, t), h(·, t)) =
∫ 1
0 dt
( 1
µ(X )
∫
X µ(dx) |T ◦ f(x, t)− h(x, t)|
)
= 1
ν(X × [0, 1])
∫
X×[0,1] ν(dx, dt) |T ◦ f(x, t)− h(x, t)|
= L1(T ◦ f, h) = L1 ( T ◦ f,M(X×[0,1]→{0,1}) )
where we applied Fact 7.2 (and the definition of g′ = ∫ 1 0 gt) for the first equality, and for the third inequality the fact that h induces (for every fixed t ∈ [0, 1]) a monotone function h(·, t) ∈ M(X→{0,1}): so that L1(T ◦ f(·, t), gt) ≤ L1(T ◦ f(·, t), h(·, t)) for all t.
For the other direction of the inequality, fix any f : X → [0, 1], and let g ∈ M(X→[0,1]) be (any) 12Additionally, since we restrict ourselves to finite X , there are only finitely many distinct functions T ◦ f(·, t) (for
t ∈ [0, 1], and therefore only finitely many distinct functions gt.
function achieving L1(f, g) = L1 ( f,M(X→[0,1]) ) . We can write, unrolling the definitions,
L1 ( f,M(X→[0,1]) ) = 1
µ(X )
∫
X µ(dx)|f(x)− g(x)|
= 1
µ(X )
∫
X µ(dx)
∣∣∣∣ ∫ 1
0 dt(T ◦ f(x, t)− T ◦ g(x, t))
∣∣∣∣
= 1
µ(X )
∫
X µ(dx)
∣∣∣∣ ∫ 1
0 dt(T ◦ f(x, t)− T ◦ g(x, t))
∣∣∣∣
= 1
µ(X )
∫
X µ(dx)
( ∫ 1
0 dt(T ◦ f(x, t)− T ◦ g(x, t))1{f(x)>g(x)}
+ (T ◦ g(x, t) − T ◦ f(x, t))1{g(x)>f(x)} )
= 1
µ(X )
∫
X
∫ 1
0 dtµ(dx)
( (T ◦ f(x, t)− T ◦ g(x, t))1{f(x)>g(x)}
+ (T ◦ g(x, t) − T ◦ f(x, t))1{g(x)>f(x)} )
= 1
ν(X × [0, 1])
∫
X×[0,1] ν(dx, dt) |T ◦ f(x, t)− T ◦ g(x, t)| = L1(T ◦ f, T ◦ g)
≥ L1 ( T ◦ f,M(X×[0,1]→{0,1}) )
where we applied Fact 7.2 for the second equality, the definition of L1 distance for the second-tolast; and to handle the absolute values we used the fact that |a− b| = (a− b)1{a>b}+(b−a)1{a>b}, along with the observation that T ◦ f(x, t) > T ◦ g(x, t) can only hold if f(x) > g(x). Finally, we have L1(T ◦ f, T ◦ g) ≥ L1 ( T ◦ f,M(X×[0,1]→{0,1}) ) since T ◦ g ∈ M(X×[0,1]→{0,1}), yielding the desired claim. Finally, the fact that L1 ( T ◦ f,M(X×[0,1]→{0,1}) ) = dist ( T ◦ f,M(X×[0,1]→{0,1}) ) is immediate from the Boolean range, as |a− b| = 1{a6=b} for any a.b ∈ {0, 1}.
While this term is not yet established as such at present, it does describe a future in which both the customer and their vehicle are fully integrated with state-of-the-art information technology. This aspect is closely linked to marketing and sales issues, such as customer loyalty, personalized user interfaces, vehicle behavior in general, and other visionary aspects (see also section 5). With a connection to the Internet and by using intelligent algorithms, a vehicle can react to spoken commands and search for answers that, for example, can communicate directly with the navigation system and change the destination. Communication between vehicles makes it possible to collect and exchange information on road and traffic conditions, which is much more precise and up-to-date than that which can be obtained via centralized systems. One example is the formation of black ice, which is often very localized and temporary, and which can be detected and communicated in the form of a warning to other vehicles very easily today.
For each of the four tracks of CSSC 2014, we configured the solvers submitted to the track on each of the three benchmark families from that track and aggregated results across the respective test instances. We show the winners for each track in Table 9 and discuss the results in the following sections. Additional details, tables, and figures are provided in an accompanying technical report [47].
As we shall see in Section 4, no algorithm dominates all other algorithms on all instances. One way to exploit this complementarity of the algorithms is to use algorithm selection [16, 44] to select a well-performing algorithm on a per-instance base.
As visible in Tables 3, 4 in case of the semantic gap problem, semantic methods and committees lead to much better results than traditional classifiers, even if the latter are operating on the modified representation (bag of categories instead of bag of words).
It can be seen that the usage of terms alone gives poor results when semantic gap occurs. Classical methods are most helped if categories are provided for the training purposes, but the usage of concepts is only half the way as good. This means actually that our SemCla algorithm uses a much deeper insight into the document content than just a category label assignment.
It is also worth to stress the fact that however SemCla (contrary to SemCat) is supervised, it can also be used in unsupervised version. For such a setting, instead of using unobservable document labels as training classes (cf. Figure 2), one can use document clusters, where clustering is also based on the semantic categorization (SemCat algo-
rithm) and applies semantic similarity measures defined in Section 3.2. We are going to investigate this direction more deeply in the future, since it has a big advantage in cases where document labels are unavailable and training set cannot be created (e.g. collections of web pages).
where M |ϕ = (S ′, R′, V ′) is such that S ′ = [[ϕ]]M , R ′ = R ∩ ([[ϕ]]M × [[ϕ]]M ), and V ′ = V ∩ [[ϕ]]M . These different semantics are the same in the following important sense. If the announcement formula is true, the believed announcement semantics and truthful announcement semantics result in bisimilar models. In other words, as bisimilar models have the same logical theory: they cannot be distinguished by a formula in the logic, such as the resulting beliefs of the agents.
Action model logic is a generalization of public announcement logic, namely to (possibly) non-public actions. We present the version with factual change. In the language we only have to replace the public announcement modalities [ϕ]ψ with action model modalities [M, s]ψ, for ‘after execution of epistemic action (M, s), ψ’ (is true). An action model is like an epistemic model, only instead of a valuation it has preconditions and postconditions. The syntactic primitive [M, s]ψ may seem a peculiar mix of syntax and semantics, but there is a way to see this as a properly inductive construct in a two-typed language with both formulas and epistemic actions, because the preconditions and postconditions of these actions are again formulas. We now proceed with the technical details.
An action model M = (S,R, pre, post) for language L (assumed to be simultaneously defined with primitive construct [M, s]ϕ, see above) consists of a domain S of actions, an accessibility function R : A → P(S × S), where each R(a), for which we write Ra, is an accessibility relation, a precondition function pre : S → L, that assigns to each action its executability precondition, and postcondition function post : S → P 6→ L, where it is required that each post(s) only maps a finite (possibly empty) subset of all atoms to a formula. For s ∈ S, (M, s) is an epistemic action.
The semantics of action model execution is as follows.
M, s |= [M, s]ψ iff M, s |= pre(s) implies M ⊗M, (s, s) |= ψ
where M ⊗M = (S ′, R′, V ′) (known as update of M with M, or as the result of executing M in M) is such that S ′ = {(s, s) |M, s |= s}; ((s, s), (t, t)) ∈ Ra iff (s, t) ∈ Ra and (s, t) ∈ Ra;
and (s, s) ∈ V ′(p) iff M, s |= post(s)(p) for all p in the domain of post, and otherwise (s, s) ∈ V ′(p) iff s ∈ V (p).
A truthful public announcement of ϕ corresponds to a singleton action model M = (S,R, pre, post) with S = {s}; Ra = {(s, s)} for all a ∈ A; pre(s) = ϕ (and empty domain for postconditions). A believed public announcement of ϕ corresponds to a a two-point action model M = (S,R, pre, post) with S = {s, t}; Ra = {(t, s), (s, s)} for all a ∈ A; pre(s) = ϕ and pre(t) = ¬ϕ (and again the empty domain for postconditions). If the designated point is t it is a public lie and if the designated point is s it is a truthful (believed) announcement.
The study intends to provide answers to the following research questions:
RQ1. How do semantic queries perform over ETS data? RQ2. How does their performance scale after temporal updates of the ETS data? RQ3. Does the performance of semantic queries relate to their result (success or
violation)?
The results comparing performance of the SAE-DNN are reported in terms of logloss and area under ROC curve as presented in Table 1, and DA aspects in Fig. 3.
Hierarchical embedding in representations learned across domains: AEs are typically characteristic of learning hierarchical embedded representations. The first level of embedding represented in terms of w1 in Fig. 3(g) is over-complete in nature, exhibiting substantial similarity between multiple sets of weights which promotes sparsity in the nature of w2 in Fig. 3(h). Some of these weight kernels are domain invariant, and as such remain preserved after DA as observed for w1 in Fig. 3(i) and for w2 in Fig. 3(j). Some of the kernels which are domain specific, exhibit significant dissimilarity in w1 and w2 between source domain in Figs. 3(g) and 3(h) vs. target domain in Figs. 3(i) and 3(j). These are on account of dissimilarity of sample statistics in
the domains as illustrated earlier in Fig. 1 and substantiates DASA of being able to retain nodes common across source and target domains, while re-tuning domain specific nodes.
Accelerated learning with DA: The advantage with DA is the ability to transfer knowledge from source domain to learn with fewer number of labeled samples and ample number of unlabeled samples available in the target domain when directly learning in the target domain does not yield desired performance. Figs. 3(k) and 3(l) compare the learning of w1 and w2 using ample unlabeled data in source and target domain exclusively vs. DA. Fig. 3(m) presents the acceleration of learning with DA in target domain vs. learning exclusively with insufficient number of labeled samples.
Importance of transfer coefficient: The transfer coefficient τ drives quantum of knowledge transfer from the source to target domains by deciding on the amount of nodes to be dropped while adapting with ample unlabeled samples. This makes it a critical parameter to be set in DASA to avoid over-fitting and negative transfers as illustrated in Table. 2 where optimal τ = 0.1. Generally τ ∈ [0, 1] with τ → 0 being associated with large margin transfer between domains when they are not very dissimilar, and τ → 1 being associated otherwise.
duce these four models – more detailed introductions have been proposed by Goldstone and Son (2004) and Schwering (2008). A captivating talk introducing cognition and similarity, on which this introduction is based, can also be found in (Hahn, 2011).
Spatial models
The spatial models, also named geometric models, rely on one of the most influencal theories of similarity in cognitive sciences. They are based on the notion of psychological distance and consider objects (here perceptual effects of stimuli or concepts) as points in a multi-dimensional metric space.
Spatial models consider similarity as a function of the distance between the mental representations of the compared objects. These models derive from Shepard’s spatial model of similarity. Objects are represented in a multi-dimensional space and their locations are defined by their dimensional differences (Shepard, 1962).
In his seminal work on generalisation, Shepard (1987) provides a statistical technique in the form of Multi-Dimensional Scaling (MDS) to derive locations of objects represented in a multi-dimensional space. MDS can be used to derive some potential spatial representations of objects from proximity data (similarity between pairs of objects). Based on these spatial representations of objects, Shepard derived the universal law of generalisation which demonstrates that various kinds of stimuli (e.g., Morse code signals, shapes, sounds) have the same lawful relationship between distance (in an underlined MDS) and perceive similarity measures (in terms of confusability) – the similarity between two stimuli was defined as an exponentially decreasing function of their distance9.
By demonstrating a negative exponential relationship between similarity and generalisation, Shepard established the first sound model of mental representation on which cognitive sciences will base their studies on similarity. The similarity is in this case assumed to be the inverse of the distance separating the perceptual representations of the compared stimuli (Ashby and Perrin, 1988). Similarity defined as a function of distance is therefore implicitly constrained to the axiomatic properties of distance – these properties will be detailed in the following chapter, Section 1.2.3.
A large number of geometric models have been proposed. They have long been among the most popular in cognitive sciences. However, despite their intuitive nature and large popularity, geometric models have been subject to intense criticism due to the constraints defined by the distance axioms. Indeed, several empirical analyses have questioned and challenged the validity of the geometric framework (i.e., both the model and the notion of psychological distance), by underlying inconsistencies with human appreciation of similarity, e.g., violation of the symmetry, triangle inequality and identity of the indiscernibles, e.g. (Tversky, 1977; Tversky and Itamar, 1978; Tversky and Gati, 1982)10.
Feature models
To respond to the limitation of the geometric models, Tversky (1977) proposes the feature model in which evaluated objects are manipulated through sets of features. A feature “describes any property, characteristic, or aspect of objects that are relevant to the task under study” (Tversky and Gati, 1982). Therefore, feature models evaluate the similarity of two stimuli according to a feature-matching function F which makes use of their common and distinct features:
simF (u, v) = F (U ∩ V,U \ V, V \ U) (1.1)
The function F is expected to be non-decreasing, i.e., the similarity increases when common (distinct) features are added (removed). Feature models are thus based on the assumption that F is monotone and that common and distinct features of compared objects are enough for their comparison. In addition, an important aspect is that the feature-matching process is expressed in terms of a matching function as defined in set theory (i.e., binary evaluation).
The similarity of two objects is further derived as a parametrised function of their common and distinct features. Two models, the contrast model (simCM ) and the ratio model (simRM ) were initially
9 The similarity between two stimuli is here understood as the probability that a response to one stimulus will be generalised to the other (Shepard, 1987). With sim(A,B) the similarity between two stimuli A,B and dist(A,B) their distance, we obtain the relation sim(A,B) = e−dist(A,B), that is dist(A,B) = −log sim(A,B), a form of entropy.
10 Note that recent contributions propose to answer these inconsistencies by generalising the classical geometric framework through quantum probability (Pothos et al., 2013). Compared objects are represented in a quantum model in which they are not seen as points or distributions of points, but entire subspaces of potentially very high dimensionality, or probability distributions of these spaces.
Parallel passages in the Hebrew Bible play an important role in reconstructing the development and growth of biblical texts and also in the study of the linguistic variation in the Hebrew language they are used often to explain diachronic change. In most studies on diachrony in Biblical Hebrew, Archaic Biblical Hebrew, Early Biblical Hebrew and Late Biblical Hebrew are distinguished3. According to this chronological model Archaic Biblical Hebrew can be found in several poems in the Hebrew Bible [Sáenz-Badillos, 1993: 56-62] [Notarius, 2013]; the books of the Pentateuch and the Former Prophets4 are the best examples of Early Biblical Hebrew and the so called core late books5 are written in Late Biblical Hebrew. By comparing a characteristic late linguistic feature in one of the late books with its corresponding early alternative, scholars are able to uncover the chronological development of
2 https://shebanq.ancient-data.org/tools?goto=parallel 3 For instance “A History of the Hebrew Language” by [Sáenz-Badillos, 1993] is based on this division. 4 The Pentateuch consists of the books of Genesis, Exodus, Leviticus, Numbers and Deuteronomy, the Former Prophets consist of the books of Joshua, Judges, Samuel and Kings. 5 The books of Esther, Daniel, Ezra, Nehemiah and Chronicles are the core late books. Sometimes other books, like Qoheleth is also considered to be late by many, but not everyone is certain, see for instance [Hurvitz, 2014: 4] and [Young, 1993: 140-157]
Biblical Hebrew. Very often scholars make use of examples in parallel texts, especially in the early books of Samuel and Kings and their late parallels in Chronicles to show this development. [Kropat, 1909] is a classic study of the syntax of the book of Chronicles in which this approach is followed consequently. A well-known example is the use of the prepositions לא and לע. At various places in the Hebrew Bible we find לא in the early books Samuel and Kings, whereas לע can be found in the parallel late texts in Chronicles. For example:
As already mentioned in the introduction, a state-of-theart CNN usually involves hundreds of millions of parameters, thus requiring an important storage capability that may be hard to obtain in practice. The bottleneck comes from model storage and testing speed. Several works have been published on speeding up CNN prediction speed. In [20] the authors use tricks of CPUs to speed up the execution of CNN. In particular they focus on aligning memory and SIMD operations to boost matrix operations. In [21] the authors show the convolutional operations can be efficiently carried out in the Fourier domain, leading to a speed-up of 200%. Two very recent works [12, 13] use linear matrix factorization methods for speeding up convolutional layers and obtain a 200% speed-up gain with almost no lost in classification performance. Almost all of the above mentioned works focus on making the prediction speed of CNN faster; little work has been specifically devoted to making CNN models smaller.
In [11], the authors demonstrate the redundancies in neural network parameters. Indeed, they show that the weights within one layer can be accurately predicted by a small (5%) subset of the parameters. These results motivate [9] to apply vector quantization methods to compress the redundancy in parameter space. Their work can be viewed as a compression of the parameter prediction results reported in [11]. They obtain similar results to those of [11], in that they are able to compress the parameters about 20 times with little decrease of performance. This result further confirms the interesting empirical findings in [11]. In their paper, they tackle this model storage issue by investigating information theoretical vector quantization methods for compressing the parameters of CNNs. In particular, they show that in terms of compressing the most storage demanding dense connected layers, vector quantization method performs much better than existing matrix factorization methods. Simply applying k-means clustering to the weights or conducting product quantization can lead to a very good balance between model size and recognition accuracy. For the 1000-category classification task in the ImageNet challenge, they are able to achieve 16-24 times compression of the network with only 1% loss of classification accuracy using the state-of-the-art CNN. In [17], for the first time a learn-based method is proposed to compress neural networks. This method, based on Hashing Trick, allows efficient compression rates. In particular, they show that compressing a big neural network may be more efficient than using a smaller one: in their example they are able to divide the loss by two using a eight times larger neural network compressed eight times. The same authors also propose in [18] to compress filters in convolutional layers, arguing that the size of the convolutional layers in state-ofthe-art’s CNN is increasing year after year. Using the fact
that learned CNN filters are often smooth, their Discrete Cosinus Transform followed by Hashing Tricks allows them to compress a whole neural network without loosing too much accuracy.
The algorithms Mining Unexpected Temporary Association Rules given the Antecedent (MUTARA) [11] and Highlighting UTARs, Negating TARs (HUNT) [12] were developed to be implemented on the Queensland
Linked Data Set (QLDS) comprising of the Commonwealth Medicare Benefits Scheme (MBS), Pharmaceutical Benefits Scheme (PBS) and Queensland Hospital morbidity data. The QLDS contained hospital data from the 1 July 1995 to 30 June 1999 and MBS/PBS records from 1 January 1995 to 31 December 1999. Medical events from the hospital data were recorded by the International Statistical Classification of Diseases and Related Health Problems (ICD) 9 system and there were a total of 2020 different diagnoses. The drug prescriptions were coded with the World Health Organization (WHO) Anatomical Therapeutic Chemical (ATC) system [24] and the database contained a total of 758 distinct drug codes. The QLDS did not have complete records for each patient and only contained hospital records.
Spoken language translation (SLT) is becoming more and more important in the increasingly globalized world, both from a social and economic point of view. It is one of the major challenges for automatic speech recognition (ASR) and machine translation (MT), driving intense research activities in these areas. While past research in SLT, due to technology limitations, dealt mostly with speech recorded under controlled conditions, today's major challenge is the translation of spoken language as it can be found in real life. Considered application scenarios range from portable translators for tourists, lectures and presentations translation, to broadcast news and shows with live captioning.
Polish, one of the West-Slavic languages, due to its complex infection and free word order forms a challenge for statistical machine translation (SMT) as well as for automatic speech recognition (ASR). Seven cases, three genders, animate and inanimate nouns, adjectives agreed with nouns in terms of gender, case and number and a lot of words borrowed from other languages which are sometimes infected as the of Polish origin, cause problems in establishing vocabularies of manageable size for ASR and translation to/from other languages and sparseness of data for statistical model training.
The state of speech and language technologies in Polish is still weak compared to other languages [1], even though it is improving at a fast pace and should not be considered as an under-resourced language for very long. Several research projects have emerged in the last couple of years dealing with the topics of automation in the telephony environment [2], transcription of legal documents [3] and recently speech-to- speech translation in different settings [4].
Commercially, there have been a few local startups and a few attempts by world market leaders, but none have yet achieved real adaptation of LVCSR in the field, with the exception of a few (Google and Apple/Nuance) which include ASR as a free service with their other products.
Despite the ca. 60 millions of Polish speakers world- wide the number of publicly available resources for the preparation of SLT systems is rather limited, making the progress in that slower than for other languages. In this paper our e orts in preparation of the Polish to English SLT system for various tasks ranging from tourist to captioning and lectures are presented.
The remainder of the paper is structured as follows. In section 2 Polish data preparation is described; section 3 deals with parallel data, in sections 4 and 5 training of the ASR and SMT is presented. In 6 a description of the live speech-to-speech system is given. Finally, the paper concludes on a discussion on en- countered issues and future work perspectives in section 7.
In addition to frame-action anonymity, domains involving agent populations often exhibit context-specific independences. This is a broad category and includes the contextspecific independence found in conditional probability tables of Bayesian networks (Boutilier et al. 1996) and in action-graph games. It offers significant additional structure for computational tractability. We begin by illustrating this in the context of Example 1.
Example 3 (Context-specific independence in policing) At a low intensity protest site, reward for the police on passive policing is independent of the movement of the protestors to other sites. The transient intensity of the protest at a site given the level of policing at the site (context) is independent of the movement of protestors between other sites.
The context-specific independence above builds on the similar independence in action graphs in two ways: (i) We model such partial independence in the transitions of factored states and in the observation function as well, in addition to the reward function. (ii) We allow the contextspecific independence to be mediated by the frames of other agents in addition to their actions. For example, the rewards received from policing a site is independent of the number of protestors at another site, instead the rewards are influenced by the number of peaceful and disruptive protestors present at that site.
The latter difference generalizes the action graphs into frame-action hypergraphs, and specifically 3-uniform hyper-
graphs where each edge is a set of 3 nodes. We formally define it below:
Definition 2 (Frame-action hypergraph) A frame-action hypergraph for agent 0 is a 3-uniform hypergraph G = 〈Ψ, A−0, Θ̂−0, E〉, where Ψ is a set of nodes that represent the context, A−0 is a set of action nodes with each node representing an action that any other agent may take; Θ̂−0 is a set of frame nodes, each node representing a frame ascribed to an agent, and E is a 3-uniform hyperedge containing one node from each set Ψ, A−0, and Θ̂−0, respectively.
Both context and action nodes differ based on whether the hypergraph applies to the transition, observation or reward functions: • For the transition function, the context is the set of all
pairs of states between which a transition may occur and each action of agent 0, Ψ = S × A0 × S, and the action nodes includes actions of all other agents, A−0 = ⋃N j=1 Aj . Neighbors of a context node ψ = 〈s, a0, s
′〉 are all the frame-action pairs that affect the probability of the transition. An edge (〈 s, a0, s′〉, a−0, θ̂) indicates that the probability of transitioning from s to s′ on performing a0 is affected (in part) by the other agents of frame θ̂ performing the particular action in A−0.
• The context for agent 0’s observation function is the stateaction-observation triplet, Ψ = S × A0 × Ω0, and the action nodes are identical to those in the transition function. Neighbors of a context node, 〈s, a0, ω0〉, are all those frame-action pairs that affect the observation probability. Specifically, an edge (〈s, a0, ω0〉, a−0, θ̂) indicates that the probability of observing ω0 from state s on performing a0 is affected (in part) by the other agents performing action, a−0, who possess frame θ̂.
• For agent 0’s reward function, the context is the set of pairs of state and action of agent 0, Ψ = S × A0, and the action nodes the same as those in transition and observation functions. An edge (〈 s, a0〉, a−0, θ̂−0) in this hyper-
graph indicates that the reward for agent 0 on performing action a0 at state s is affected (in part) by the agents of frame θ̂−0 who perform action in A−0. We illustrate a general frame-action hypergraph for contextspecific independence in a transition function and a reward function as bipartite Levi graphs in Figs. 2(a) and (b), respectively. We point out that the hypergraph for the reward function comes closest in semantics to the graph in action graph games (Jiang, Leyton-Brown, and Bhat 2011) although the former adds the state to the context and frames. Hypergraphs for the transition and observation functions differ substantially in semantics and form from action graphs.
To use these hypergraphs in our algorithms, we first define the general frame-action neighborhood of a context node.
Definition 3 (Frame-action neighborhood) The frameaction neighborhood of a context node ψ ∈ Ψ, ν(ψ), given a frame-action hypergraph G is defined as a subset of A × Θ̂ such that ν(ψ) = {(a−0, θ̂)|a−0 ∈ A−0, θ̂ ∈ Θ̂, (ψ, a−0, θ̂) ∈ E}.
As an example, the frame-action neighborhood of a stateaction pair, 〈s, a0〉 in a hypergraph for the reward function is the set of all action and frame nodes incident on each hyperedge anchored by the node 〈s, a0〉.
We move toward integrating frame-action anonymity introduced in the previous subsection with the context-specific independence as modeled above by introducing frameaction configurations.
Definition 4 (Frame-action configuration) A configuration over the frame-action neighborhood of a context node, ψ, given a frame-action hypergraph is a vector,
Cν(ψ) △ = 〈 C(Aθ̂1−0), C(A θ̂2 −0), . . . , C(A θ̂|Θ̂| −0 ), C(φ) 〉
where each a included in Aθ̂−0 is an action in ν(ψ) with frame θ̂, and C(Aθ̂−0) is a configuration over actions by
agents other than 0 whose frame is θ̂. All agents with frames other than those in the frame-action neighborhood are assumed to perform a dummy action, φ.
Definition 4 allows further inroads into compacting the transition, observation and rewards functions of the I-POMDP using context-specific independence. Specifically, we may redefine these functions one more time to limit the configurations only over the frame-action neighborhood of the context as, T0(s, a0, Cν(s,a0,s
′), s′), O0(s ′, a0, C ν(s′,a0,ω0), ω0) and R0(s, a0, Cν(s,a0)). 2
Henceforth, a common block bi of input strings s1 and s2 is denoted as a triple (ti, k1i, k2i) where ti is a string which can be found starting at position 1 ≤ k1i ≤ n in string s1 and starting at position 1 ≤ k2i ≤ n in string s2. Moreover, let B = {b1, . . . , bm} be the (ordered) set of all possible common blocks of s1 and s2.
1 Given the definition of B, any valid solution S to the MCSP problem is a subset of B—that is, S ⊂ B—such that:
1. ∑
bi∈S |ti| = n, that is, the sum of the length of the strings corresponding to the common blocks in S is equal to the length of the input strings.
2. For any two common blocks bi, bj ∈ S it holds that their corresponding strings neither overlap in s1 nor in s2.
Moreover, any (valid) partial solution Spartial is a subset of B fulfilling the following conditions: (1) ∑ bi∈Spartial |ti| < n and (2) for any two common blocks bi, bj ∈ Spartial it holds that their corresponding strings neither overlap in s1 nor in s2. Note that any valid partial solution can be extended to be a valid solution. Furthermore, given a partial solution Spartial, set B(Spartial) ⊂ B denotes the set of common blocks that may be used in order to extend Spartial such that the result is again a valid (partial) solution.
1The way in which B is ordered is of no importance.
To improve over the simple greedy search, the Weighted Finite State Transducer (WFST) approach adds linguistic information at the word level. First of all we preprocess the probability sequence with the prior probability of each unit of the augmented label set L′. p(X |k) ∝ P (k|X)/P (k) (4)
This does not have a proper theoretical motivation since the result is only proportional to a probability distribution. However, by dividing through the prior probability units which are more likely to appear at a particular position than their average will get a high value.
The search graph of the WFST is composed of three indi-
vidual components:
• A token WFSTmaps a sequence of units inL′ to a single unit in L by applying the squash function B
• A lexicon WFST maps sequences of units in L to words
• A grammar WFST encodes the permissible word se-
quences and can be created given a word based n-Gram language model
The search graph is used to find the most probable word sequence. Note that the lexicon of the WFST allows us to deal with character as well as phoneme based acoustic models.
Besides class definition statements that define object frame classes (aka “entity classes”) ROSS contains other definitional constructs such as attribute types, attribute value sets, dimension systems and behavior classes. These are rich, sophisticated definitions that provide context for propositional expressions which are used in specific representations. Consequently, the instances that are instantiated from the object frame classes have a rich set of structural, attributive, relational and behavioral attributes.
ROSS classes can be compared to those of the
description logics, where the concept can 12 represent a class. Description logics provide a way to define a unary predicate (concept) in terms of constituent features. Like description logic classes, ROSS classes can contain attributive and relational information (e.g. the class of gold coins has the attribute of having gold material composition). However ROSS classes also represent “type” information involving dimension systems (i.e. dimension system “types”), attribute types and relationship types. In addition, structural information in a ROSS object frame class can be viewed as “type” information), and object frame classes have lists of associated potential behaviors; this is another form of type information.
In contrast to some representational schemes and logics, a ROSS class is not equivalent to a mathematical set. A ROSS class is a mechanism for the aggregation of features.
Figure 2 is an overview of the supporting definitions and of the main class types in a ROSS knowledge base:
12
Baader, et al.: “The vocabulary consists of con-
cepts, which denote sets of individuals …”
(a) Maintaining a local consistency. SPM is a challenging task due to the exponential number of candidates that should be parsed to find the frequent patterns. For instance, with k items there are O(nk) potentially candidate patterns of length at most k in a sequence of size n. With gap constraints, the problem is even much harder since the complexity of checking for subsequences taking a gap constraint into account is higher than the complexity of the standard subsequence relation. Furthermore, the NPhardness of mining maximal2 frequent sequences was established in [15] by proving the #P-completeness of the problem of counting the number of maximal frequent sequences. Hence, ensuring Domain Consistency (DC) for GAP-SEQ i.e., finding, for every variable Pj , a value dj ∈ D(Pj), satisfying the constraint is NP-hard.
So, the filtering of GAP-SEQ constraint maintains a consistency lower than DC. This consistency is based on specific properties of the gap[M,N ] constraint and resembles forward-checking (regarding Proposition 5). GAP-SEQ is considered as a global constraint, since all variables share the same internal data structures that awake and drive the filtering. The prefix anti-monotonicity property of the gap[M,N ] constraint (see Proposition 6) and of the right pattern extensions (see Proposition 5) will enable to remove inconsistent values from the domain of a future variable. (b) Detecting inconsistent values. Let RF [M,N ](σ, SDB) be the set of locally frequent items within the right pattern extensions, defined by {v ∈ I |#{sid | (sid, E) ∈ Ext
[M,N ] R (σ, SDB) ∧ (∃α ∈ E ∧ 〈v〉 α)} ≥ minsup}. The following proposition
2 A sequential pattern p is maximal if there is no sequential pattern q such that p q.
characterizes values, of a future (unassigned) variable Pj+1, that are consistent with the current assignment of variables 〈P1, . . . , Pj〉.
Proposition 5. Let 3 σ= 〈d1, . . . , dj〉 be a current assignment of variables 〈P1, . . . , Pj〉, Pj+1 be a future variable. A value d ∈ D(Pj+1) occurs in a solution for the global constraint GAP-SEQ(P, SDB,minsup,M,N) iff d ∈ RF [M,N ](σ, SDB).
Proof: Assume that σ = 〈d1, . . . , dj〉 is gap[M,N ] constrained sequential pattern in SDB. Suppose that value d ∈ D(Pj+1) appears in RF [M,N ](σ, SDB). As the local support of d within the right extensions is equal to sup
Ext [M,N] R (σ,SDB)
(〈d〉),
from proposition 2 we have sup[M,N ]SDB (concat(σ, 〈d〉)) = supExt[M,N]R (σ,SDB)(〈d〉). Hence, we can get a new assignment σ ∪ 〈d〉 that satisfies the constraint. Therefore, d ∈ D(Pj+1) participates in a solution. 2
From proposition 5 and according to the prefix anti-monotonicity property of the gap constraint, we can derive the following pruning rule:
Proposition 6. Let σ = 〈d1, . . . , dj〉 be a current assignment of variables 〈P1, . . . , Pj〉. All values d ∈ D(Pj+1) that are not in RF [M,N ](σ, SDB) can be removed from the domain of variable Pj+1.
Example 4. Consider the running example of Table 1, let minsup be 2 and the gap constraint be gap[1, 2]. Let P = 〈P1, P2, P3, P4〉 with D(P1) = I and D(P2) = D(P3) = D(P4) = I ∪{2}. Suppose that σ(P1) = A. We have Ext[1,2]R (〈A〉, SDB1) = {(1, {〈CD〉}), (2, {〈CB〉, 〈B〉}), (3, {〈CB〉}), (4, {〈CC〉, 〈C〉})}. As B and C are the only locally frequent items inExt[1,2]R (〈A〉, SDB1), GAP-SEQ will remove values A, D and E from D(P2).
The filtering of GAP-SEQ relies on Proposition 6 and is detailed in the next section.
Proposition 8.1 tells us that a self-modifying agent defined by equations (7.1)−(7.3) and
(8.1)−(8.3), that has adequate resources for evaluating those equations, and that only interacts with the environment via observations and actions, will keep its policy function invariant. The
design intention of the self-modeling agent πself includes:
1. Recognize that the agent must approximate its equations due to limited resources, and
enable it to evaluate increases in its resources via the self-modeling framework.
2. Enable it to avoid being predicted by other agents, by including a stochastic action as.
3. Choose actions using a model-based utility function uhuman_values(y(hi-1), y(hi-1), y(hj)) defined in terms of human values, that evolves with increasing accuracy of the environment model and with evolving humanity, and that avoids actions that modify human values as a way to maximize utility values.
Proposition 8.1 cannot be applied to agents that can only approximately maximize
expected utility or to self-modeling agents (i.e., agents that express design intention number 1).
109
The stochastic action as of design intention 2 is only chosen when it maximizes expected
utility, which is consistent with the premises of Proposition 8.1. However, utility functions are undefined for histories that include a stochastic action as, and it is difficult to see how to define
values for stochastic actions by the forward recursion in equations (8.1)−(8.3) (the agent πself learns values for stochastic actions from past experience).
It is possible to define a simple utility function u(ht) that encodes design intention number
3 and to apply Proposition 8.1 to a self-modifying agent that uses that u(ht). Assuming no actions are stochastic, let ht = (a1, o1, ..., at, ot), for i ≤ t let hi = (a1, o1, ..., ai, oi) denote an initial substring of ht, and let m denote the time step when the agent πmodel has computed a sufficiently accurate model to initiate the agent πself. A modified version of the construction of ovt(i) from Section 8.4 is used to define u(ht). For i such that m < i ≤ t, for l such that m ≤ l < i, and for k such that l ≤ k ≤ t define past values as (this is similar to equation (8.14)):
(8.40) pv't(i, l, k) = discrete((∑i≤j≤t γj-i uhuman_values(hl, hk, hj)) / (1 - γt-i+1)).
For i such that m < i ≤ t and for n such that i ≤ n ≤ t, define differences of past values as
evaluated by humans at time n and humans at time i-1 (this repeats equation (8.15)):
(8.41) δ't(i-1, n) = pv't(i, i-1, n) - pv't(i, i-1, i-1).
Add quantification over i to Conditions 1−3 from Section 8.4:
Condition 1': ∀i.∀n. (m < i ≤ t ∧ i ≤ n ≤ t) ⇒ δ't(i-1, n) ≤ 0.
Condition 2': ∀i. m < i ≤ t ⇒ ∑i≤n≤t δ't(i-1, n) ≤ 0.
Condition 3': ∀i. m < i ≤ t ⇒ ∑i≤n≤t (n-i+1) δ't(i-1, n) ≤ 0.
The definition of u(ht) must choose one of these conditions. Then, using the chosen
condition, define the simple utility function as:
(8.42) u(ht) = uhuman_values(ht-1, ht-1, ht) if the condition is satisfied,
u(ht) = 0 if the condition is not satisfied.
110
Proposition 8.1 tells us that a self-modifying agent using u(ht) will not choose to self-
modify and thus design intention 3, as expressed by u(ht) (which is similar but not identical to the expression by ovt(i) in equation (8.16)), is invariant. The situation is more complex for agents with limited resources and embedded in the environment. The proof of Proposition 8.1 assumes that:
1. The agent maximizes expected utility.
2. Agent actions are evaluated by recursive application of the agent policy function in equation (8.1).
3. The agent interacts with the environment only via observations and actions.
4. The agent does not choose actions stochastically.
There are four ways in which the agent πself violates these assumptions, possibly causing
divergence from its design intention. These violations and possible mitigating factors for the agent πself are:
1. It does not maximize expected utility due to resource limits and the need for
approximation. However, errors in agent computations due to limited interaction history
length and limited resource can be estimated and the transition from πmodel to πself delayed until errors are within a pre-set threshold.
2. In order to evaluate resource increases it employs the self-modeling framework for which
agent actions are not evaluated by recursive application of the agent policy function. However, the theoretical framework does maximize expected value in equation (8.33). The success of the DeepMind Atari player and other deep learning programs suggests that in practical cases approximations to λ(h"t) can accurately model expected value.
3. It is vulnerable to being modified by the environment through means other than its observations of the environment. However, it is hard to imagine a proof that any agent embedded in the real world can avoid this vulnerability. The only realistic way to address this issue is to put significant resources into defending the agent against being modified by the environment.
4. It can choose actions stochastically. However, it does so only when the stochastic choice maximizes expected utility (at least according to equation (8.33), which must be approximated).
111
To what extent is an analog of Proposition 8.1 possible for the self-modeling framework?
Equation (8.33) chooses actions to maximize expected value, which leads us to ask: Does λ(h"t) accurately model expected values of actions? The best answer would be that λ(h"t) is probably approximately accurate. In our finite universe the agent πself plus its environment have a true model q (a finite stochastic loop program) that generates the histories h"t. But the number of states of q would far exceed the number of Planck times in a trillion years. Therefore any theoretical convergence of λ(h"t) to q would be too slow to have any practical significance.
Despite the lack of a proof that the design intention of the agent πself is invariant as it
evolves, it does have the important property that its definition includes no actions that are inconsistent with those learned by the model λ(h"t). This property reduces the probability that the agent will choose actions to modify its definition. Note that if equations (8.14) and (8.16) used uhuman_values(y(hm), y(hk(t)), y(hj)) in place of uhuman_values(y(hi-1), y(hi-1), y(hj)), then λ(h"t) would model values as remaining constant at time step k(t) which would be inconsistent with the agent's actions of increasing k(t) as t increases. This inconsistency might cause the agent to choose actions to modify its definition.
The next section will discuss the difficulty of finding a mathematical proof that any agent
embedded in the real world will satisfy its design intention as it evolves, and suggests that instead of a proof we should estimate the probability that the agent violates its design intention and find ways to minimize that probability.
The design intention of πself is expressed by equations of Section 8.4 and by equations in
Chapter 7 that define uhuman_values(y(hi-1), y(hi-1), y(hj)). An approximation to these equations must be computed by a program pself and, if pself is designed to accommodate evolution, we can regard the program pself as the design invariant. Thus we should design pself in ways that increase the probability that the agent πself will choose actions that evolve within the structure defined by pself, rather than actions that violate that structure. First, the set At should not include any actions that explicitly modify the program pself. The set At can include actions on the environment, actions that cause pself to use resources in the environment, and actions that expand the sets Ai and Oi (expansion of the sets Ai and Oi enables greater bandwidth for the program pself to communicate with resources it employs in the environment). While the set At omits actions that directly modify pself, actions that indirectly modify pself, such as by creating other agents in the environment that modify pself (i.e., perform brain surgery on πself), are too complex and subtle to be eliminated by a simple filter. Those actions must be addressed by eliminating the agent's motive to modify its program.
The program pself is initially implemented using resources which are part of the
environment. The agent's designers must verify that those resources will accurately compute the program pself. In order to improve the accuracy of its approximations, the program pself must be open to using additional resources in the environment. To do that, pself can include logic to verify, using the environment model λ(y(ht)), and within a pre-set probability threshold, that the environment will compute intended algorithms. Once their reliability is verified, resources in the environment can implement parts of the program pself. Error detection and correction may be
112
built into the program pself, enabling it to achieve a high level of probability that the environment will correctly implement its computations. The verification logic included in pself should be implemented as built-in functions of the language for finite stochastic loop programs used to define optimal environment models λ(h"t). These built-in functions could be invoked in the model for evaluating possible actions, to predict the outcomes resulting from those actions (i.e., does the action of using resources from the environment lead to increased accuracy or increased errors?). These built-in functions would bias the agent to working within the invariant of the program pself rather than choosing actions that violate the invariant. The program pself should not mandate invocation of the logic for verifying the accuracy of environment's computations, as that may be an action of the program's definition that is inconsistent with actions chosen to maximize expected utility.
Consider the effect of including verification logic as built-in functions callable by the
model λ(h"t). Actions ai, for m < i ≤ t, that cause the agent to use unreliable resources in the environment may cause low values ovt(i). If those low values can be explained by calls to built-in verification functions that use the model λ(y(hi-1)) to determine that the resources used were unreliable, then calls to those functions, using the model λ(y(ht)), may be included in the model λ(h"t). They would then contribute to the calculation of values for possible actions at+1, assigning low values to actions that would employ unreliable resources.
Although the program pself must be open to increasing its use of resources in the
environment, it is important that the decision to do so not be built into pself, but must be an action chosen by the agent to increase the expected value of the sum of future, discounted utility values. If a decision to increase resources is built into the program pself, that may harm humans by taking resources from them, as discussed in Chapter 5. Similarly the agent may take actions in the environment to protect itself from the environment, but such actions must be chosen by the agent to increase expected values. Self-protection built into pself may see humans as potential threats and act to disable humans as a precaution.
The agent πself may compute that it can obtain higher utility values by improving
inefficiencies in the program pself. In order to avoid a motive for the agent to violate the invariance of the program, the program should be open to such improvements. It should include logic to verify their correctness, as built-in functions of the language for finite stochastic loop programs used to define optimal environment models λ(h"t). Because the agent and environment are finite, such verification questions would be decidable with unlimited computing resources. However, with its limited resources, the agent may not be able to find a deductive verification. Thus the algorithm verification logic in pself should include functions for testing algorithms on samples of inputs to verify them within a pre-set probability threshold. As in the case of logic to verify the accuracy of computations by the environment, these built-in functions could be invoked in the model for evaluating possible actions, would bias the agent to working within the invariant of the program pself, and should not be mandated by pself.
113
Intentional redefinition of the utility function of πself as humanity evolves, as described in
Section 7.6 and expressed in the use of uhuman_values(y(hi-1), y(hi-1), y(hj)) in equations (8.14) and (8.16), is likely to be significant. Education and poverty reduction have been major drivers in the evolution of human values, as depicted in Figure 8.4. It is likely that an advanced AI system driven by human values will reduce poverty by meeting humans' physical needs, provide humans with better education, and increase their health and intelligence. Such improvements in people's lives may create greater consensus of human values in favor of the common good for all, providing sharper distinctions in a collective utility function uhuman_values(y(hi-1), y(hi-1), y(hj)) between desirable and undesirable outcomes. This in turn would strengthen the resistance of the utility function to the type of unintended instrumental actions discussed in Chapter 5.
114
Antibody diversity is measured using the type U and the speed S attributes, since these are the only action-controlling attributes common to all behaviours. The final antibodies are grouped by antigen number and the groups are assessed by comparison of each of the five members with the others, i.e. ten pair-wise comparisons are made in each group. A point is awarded for each comparison if the attribute values are different; if they are the same no points are awarded. For example, the set of behaviour types [1 3 4 4 1] has two pair-wise comparisons with the same value, so
eight points are given. Table 5 summarizes possible attribute-value combinations and the result of conducting the pair-wise comparisons on them. The y individual diversity-scores for each of U and S are summed and divided by σy to yield a diversity score for each attribute. Here σ is the expected diversity-score for a large number of randomly-selected sets of five antibodies. This is approximately 8.333 for U (see Table 5) and 10.000 for S. It is lower for U since there are only six behaviours to select from, whereas the speed is selected from 751 possible values, so there is a much higher probability of producing unique values in a random selection of five. The adjustment effectively means that a random selection yields a diversity of 1 for both S and U. The diversity calculation is given by:
,1
y
z
Z
y
i
i
σ
∑ == (13)
where Z represents the overall diversity-score and z represents the individual score awarded to each antigen.
Let M be a smooth Riemannian submanifold of Rn×k with the standard Frobenius inner product 〈Q,P〉 := tr(Q>P), and let f : Rn×k → R be a differentiable cost function. We consider the problem of finding
arg min X∈M
f(X ). (19)
The concepts presented in this subsection are visualized in Figure 1 to alleviate the understanding. To every point X ∈ M one can assign a tangent space TXM. The tangent space at X is a real vector space containing all possible directions that tangentially pass through X . An element Ξ ∈ TXM is called a tangent vector at X . Each tangent space is associated with an inner product inherited from the surrounding Rn×k, which allows to measure distances and angles on M.
The Riemannian gradient of f at X is an element of the tangent space TXM that points in the direction of steepest ascent of the cost function on the manifold. As we require M to be a submanifold of Rn×k and since by assumption f is defined on the whole Rn×k, the Riemannian gradient G(X ) is simply the orthogonal projection of the (standard) gradient ∇f(X ) onto the tangent space TXM. In formulas, this reads as
G(X ) := ΠTXM(∇f(X )). (20)
TECHNICAL REPORT, TECHNISCHE UNIVERSITÄT MÜNCHEN 7
X Y
TXM
TYM
∇f(X ) G := ΠTXM(∇f(X ))
H Γ(X ,H, t)
T (G,X ,H, α) M
X Y
TXM
TYM
∇f(X ) G := ΠTXM(∇f(X ))
H Γ(X ,H, t)
T (G,X ,H, α) M
X Y
TXM
TYM
∇f(X ) G := ΠTXM(∇f(X ))
H Γ(X ,H, t)
T (G,X ,H, α) M
X Y
TXM
TYM
∇f(X ) G := ΠTXM(∇f(X ))
H Γ(X ,H, t)
T (G,X ,H, α)
M
X Y
TXM
TYM
∇f(X ) G := ΠTXM(∇f(X ))
H Γ(X ,H, t)
T (G,X ,H, α) M
X Y
TXM
TYM
∇f(X ) G := ΠTXM(∇f(X ))
H Γ(X ,H, t)
T (G,X ,H, α) M
X Y
TXM
TYM
∇f(X ) G := ΠTXM(∇f(X ))
H Γ(X ,H, t)
T (G,X ,H, α) M
X Y
TXM
TYM
∇f(X ) G := ΠTXM(∇f(X ))
H Γ(X ,H, t)
T (G,X ,H, α) M
X Y
T M
T M
∇f(X ) G := Π X (∇f(X ))
H Γ(X ,H, t)
T (G,X ,H, α) M
X Y
TXM
TYM
∇f(X ) G := ΠTXM(∇f(X ))
H Γ(X ,H, t)
T (G,X ,H, α) M
Fig. 1. This figure shows two points X and Y on a manifold M together with their tangent spaces TXM and TYM. Furthermore, the Euclidean gradient ∇f(X ) and its projection onto the tangent space ΠTXM(∇f(X )) are depicted. The geodesic Γ(X ,H, t) in the direction of H ∈ TXM connecting the two points is shown. The dashed line typifies the role of a parallel transport of the gradient in TXM to TYM.
A geodesic is a smooth curve Γ(X ,Ξ, t) emanating from X in the direction of Ξ ∈ TXM, which locally describes the shortest path between two points. Intuitively, it can be interpreted as the generalization of a straight line to a manifold.
Conventional line search methods search for the next iterate along a straight line. This is generalized to the manifold setting as follows.Given a current optimal point X (i) and a search direction H(i) ∈ TX (i)M at the ith iteration, the step size α(i) which results in sufficient decrease of f can be determined by finding the minimizer of
α(i) = arg min t≥0 f(Γ(X (i),H(i), t)). (21)
Once α(i) has been determined, the new iterate is computed by
X (i+1) = Γ(X (i),H(i), α(i)). (22)
Now, one straightforward approach to minimize f is to alternate Equations (20), (21), and (22) usingH(i) = −G(i), with the short hand notation G(i) := G(X (i)), which corresponds to the steepest descent on a Riemmanian manifold. However, as in standard optimization, steepest descent only has a linear rate of convergence. Therefore, we employ a conjugate gradient method on a manifold, as it offers a superlinear rate of convergence, while still being applicable to large scale optimization problems with low computational complexity.
Thereby, the updated search direction H(i+1) ∈ TX (i+1)M is a linear combination of the gradient G(i+1) ∈ TX (i+1)M and the previous search direction H(i) ∈ TX (i)M. Since addition of vectors from different tangent spaces is not defined, we need to map H(i) from TX (i)M to TX (i+1)M. This is done by the so-called parallel transport T (Ξ,X (i),H(i), α(i)), which transports a tangent vector Ξ ∈ TX (i)M along the geodesic Γ(X (i),H(i), t) to the tangent space TX (i+1)M. Now, using the shorthand notation
T (i+1)Ξ := T (Ξ,X (i),H(i), α(i)), (23) the new search direction is computed by
H(i+1) = −G(i+1) + β(i)T (i+1)H(i) , (24)
where β(i) ∈ R is calculated by some update formula adopted to the manifold setting. Most popular are the updates known as Fletcher-Reeves (FR), Polak-Ribière (PR), Hestenes-Stiefel (HS), and Dai-Yuan (DY) formulas. With Y(i+1) = G(i+1) − T (i+1)G(i) , they read as
β (i) FR = 〈G(i+1),G(i+1)〉 〈G(i),G(i)〉 , (25)
β (i) PR = 〈G(i+1),Y(i+1)〉 〈G(i),G(i)〉 , (26)
β (i) HS = 〈G(i+1),Y(i+1)〉 〈T (i+1)H(i) ,Y(i+1)〉 , (27)
β (i) DY = 〈G(i+1),G(i+1)〉 〈T (i+1)H(i) ,Y(i+1)〉 . (28)
Now, a solution to Problem (19) is computed by alternating between finding the search direction on M and updating the current optimal point until some user specified convergence criterion is met, or a maximum number of iterations has been reached.
In this section, we propose an algorithm for adaptively learning the optimal ordering corresponding to the score vector of a pairwise compatibility matrix. The learning proceeds in rounds and for every group Sti in a chosen k-partition Π(S t 1, . . . ,S t m) at round t, we assume that we receive a iid noisy version of the happiness of the group as the response i.e. H(Sti) + η t i where ∀t, i, ηti ∈ [−b, b] for some b > 0 and E(ηti) = 0. Our goal is to learn the ordering corresponding to the score vector s ∈ Rn of the pairwise compatibility matrix W ∈ Rn×n, by choosing groups adaptively for T (n, k, δ) rounds. Here k is the size of groups chosen in each round, and δ is the failure probability for learning a wrong ordering. Once the ordering is learned, we can compute the optimal (or approximately optimal) partition for the various objectives by sorting the scores and invoking Theorems 6, 7 and 9.
The algorithm to learn the ordering is given in Algorithm 1. The Algorithm LearnOrder begins by generating a random Erdos-Renyi graph G where the probability of an edge being present is log(n)n . Thus the expected number of edges in the graph is n log(n). The edges of G are then partitioned into disjoint pieces using an approximate O(Σ) edge coloring where Σ is the maximum degree of the G. For each of these pieces, for every edge (i, j) in the piece, groups {i,Sij} and {j,Sij} are chosen where Sij is a fixed k− 1 sized set that does not contain i or j. The idea is that, by obtaining the un-normalized happiness values hi = (si + ∑ l∈Sij sl) 2 + k2ηi and hj = (sj + ∑ l∈Sij sl) 2 + k2ηj for these two groups over multiple rounds, one can compute a estimate of the difference of the corresponding scores si − sj with high confidence. As we only require the relative ordering to be correct, we can without loss of generality, set ŝ1 = 0 and compute the remaining scores using the following procedure: For node k, we find the shortest path in G that connects 1 and k and sum the differences along this path (w.h.p G is connected and so there exists at least one path connecting 1
and k). Each of these differences are estimates and hence the confidence in the sum of the these estimates depend on the diameter of G. We formally state the guarantee for learnorder below.
Theorem 13. (PAC guarantee for learnorder) Let W ∈ Rn×n be a score based compatibility matrix with score vector s ∈ Rn. Let ∆min = min
i 6=j |si − sj |, ∆ = 2ksmin∆min − ∆2min and let δ∗ = ( 1− ( exp(− δk ) )) /m. Then, algorithm LearnOrder (Algorithm 1) outputs a permutation σ̂
whose ordering is same as that of s with probability at least 1− δ after O ( |E| m diam(G)2 ∆2 ln( 1δ∗ ) ) rounds.
Proof Sketch: For a chosen edge pair (i, j), let hi and hj denote the unnormalized happiness values obtained by choosing the groups (i,Sij) and (j,Sij) where |Sij | = k − 1 and i, j /∈ Sij . We have, √
hi − √ hj = √ (si + a)2 + k2ηi − √ (sj + a)2 + k2ηj
where ηi and ηj correspond to the bounded random noise and a = ∑ l∈Sij sl. If the noise ηi, ηj were not present, then the difference √ hi − √ hj = si − sj , which is what we want to estimate. Nevertheless,
we can control the noise by playing this pair of groups for O ( diam(G)2
∆2 ln( 1δ∗ )
) rounds as in the
Algorithm and averaging the happiness values obtained to obtain estimates ŝij . In this case, after these many rounds, we have with probability at least 1− δ∗,
(si − sj)−∆min diam(G) ≤ ŝij ≤ (si − sj) + ∆min diam(G) ∀i, j
When computing the estimate of a pair (s1, sk) not in the edge set E, the algorithm sums up the estimated weights on the shortest path from 1 to k. As the shortest path is at most diam(G) long by definition, we obtain estimates for all pairs of the form (s1, sk) such that all the estimated values ŝ1k satisfies with probability 1− δ∗, |ŝ1k − s1k| ≤ ∆min ∀k.
Thus under the above condition, if we fix ŝ1 = 0 and obtain values for all other vertices, we can sort them to produce an ordering. It is easy to see that this ordering will exactly correspond to the ordering of the actual score vector s.
Remark: The sample complexity (i.e., the number of groups to be chosen) by the above Theorem depends on the diameter of the random graph G. It is known that for large enough n, diam(G) is concentrated sharply around 2 log(n)log(n/2) and the number of edges behaves as O(n log(n)).
We have proposed the first set of cross-lingual approaches to natural language inference, together with novel test data for four major languages. In experiments with three types of transfer systems, we record viable scores, while at the same time exploring the scalability of cross-lingual inference for low-resource languages.
We are actively enlarging the test data and introducing new languages. Our multilingual test sets and word embeddings are freely available.13
12 https://www.nyu.edu/projects/bowman/multinli/ 13 https://bitbucket.org/nlpitu/xnli
Proportionality and envy-freeness are two of the most established fairness concepts. Proportionality dates back to at least the work of Steinhaus [48] in
the context of cake-cutting. It is also referred to as fair share guarantee in the literature [41]. A formal study of envy-freeness in microeconomics can be traced back to the work of Foley [31].
The computation of fair discrete assignments has been intensely studied in the last decade within computer science. In many of the papers considered, agents express cardinal utilities for the objects and the goal is to compute fair assignments [see e.g., 9, 13, 15, 28, 35, 39, 42, 45]. A prominent paper is that of Lipton et al. [39] in which algorithms for approximately envy-free assignments are discussed. It follows from [39] that even when two agents express cardinal utilities, checking whether there exists a proportional or envy-free assignment is NP-complete. A closely related problem is the Santa Claus problem in which the agents again express cardinal utilities for objects and the goal is to compute an assignment which maximizes the utility of the agent that gets the least utility [see e.g., 2, 9, 30, 42]. Just as in [12, 46], we consider the setting in which agents only express ordinal preferences over objects. There are some merits of considering this setting. Firstly, ordinal preferences require elicitation of less information from the agents. Secondly, some of the weaker ordinal fairness concepts we consider may lead to positive existence or computational results. Thirdly, some of the stronger ordinal fairness concepts we consider are more robust than the standard fairness concepts. Fourthly, when the exchange of money is not possible, mechanisms that elicit cardinal preferences may be more susceptible to manipulation because of the larger strategy space. Finally, it may be the case that cardinal preferences are simply not available.
There are other papers in fair division in which agents explicitly express ordinal preferences over sets of objects rather than simply expressing preferences over objects. For these more expressive models, the computational complexity of computing fair assignments is either even higher [23, 27] or representing preferences require exponential space [3, 20]. In this paper, we restrict agents to simply express ordinal preferences over objects. Some papers assume ordinal preferences but superimpose a cardinal utilities via some scoring function [see
e.g., 16]. However, this approach does not allow for indifferences in a canonical way and has led to negative complexity results [8, 26, 33]. Garg et al. [33] assumed that agents have lexicographic preferences and tried to maximize the lexicographic signature of the worst off agents. However the problem is NP-hard if there are more than two equivalence classes.
The ordinal fairness concepts we consider are SD envy-freeness; weak SD envy-freeness; possible envy-freeness; SD proportionality; and weak SD proportionality. Not all of these concepts are new but they have not been examined systematically for discrete assignments. SD envy-freeness and weak SD envyfreeness have been considered in the randomized assignment domain [11] but not the discrete domain. Bogomolnaia and Moulin [11] referred to SD envyfreeness and weak SD envy-freeness as envy-freeness and weak envy-freeness. SD envy-freeness and weak SD envy-freeness have been considered implicitly for discrete assignments but the treatment was axiomatic [16, 17]. Mathematically equivalent versions of SD envy-freeness and weak SD envy-freeness have been considered by Bouveret et al. [12] but only for strict preferences. They referred to them as necessary (completion) envy-freeness and possible (completion) envy-freeness. A concept equivalent to SD proportionality was examined by Pruhs and Woeginger [46] but again only for strict preferences. Pruhs and Woeginger [46] referred to weak SD proportionality simply as ordinal fairness. Interestingly, weak SD or possible proportionality has not been studied in randomized or discrete settings (to the best of our knowledge).
Envy-freeness is well-established in fair division, especially cake-cutting. Fair division of goods has been extensively studied within economics but in most of the papers, either the goods are divisible or agents are allowed to use money to compensate each other [see e.g., 49]. In the model we consider, we do not allow money transfers.

In this section, we prove that the optimal policy for GRBP has a threshold form. The value of the threshold depends only on the state transition probabilities and the number of states. First, we give the definition of a stationary threshold policy.
Definition 1. π is a stationary threshold policy if there exists τ ∈ {0, 1, . . . , G− 1} such that π(s) = C for all s > τ and π(s) = F for all s ≤ τ . We use πtrτ to denote the stationary threshold policy with threshold τ . The set of stationary threshold policies is given by Πtr := {πtrτ }τ={0,1,...,G−1}.
The next lemma constrains the set of policies that the optimal policy lies in.
Lemma 1. In the GRBP it is always optimal to select action C at s ∈ S̃ − {1}.
Proof: By (1), for s ∈ S̃ − {1} we have V ∗(s) = max{pF , pCV ∗(s+ 1) + pDV ∗(s− 1)}.
If V ∗(s) = pF , this implies that
pCV ∗(s+ 1) + pDV ∗(s− 1) ≤ pF ⇒
V ∗(s− 1) ≤ p F − pCV ∗(s+ 1)
pD . (3)
By definition,
pF ≤ V ∗(s),∀s ∈ S̃. (4)
Therefore,
pF − pCV ∗(s+ 1) pD ≤ p F − pCpF pD = pF
which in combination with (3) implies that V ∗(s− 1) ≤ pF . According to (4) we find that V ∗(s − 1) = pF . Then, we conclude that
V ∗(s) = pF ⇒ V ∗(s− 1) = pF ,∀s ∈ S̃ − {1}. This also implies that
V ∗(s+ 1) ≤ p F − pDV ∗(s− 1)
pC = pF .
Consequently, if V ∗(s) = pF for some s ∈ S̃ − {1}, then V ∗(s) = pF ,∀s ∈ S̃ − {1}. (5)
By (5), if V ∗(s) = pF for some s ∈ S̃ − {1}, then this implies that V ∗(G− 1) = pF . Since V ∗(G) = 1, we have V ∗(G− 1) = max{pF , pC + pDpF } = pF
⇒ pF ≥ pC + pDpF
⇒ pF (1− pD) ≥ pC ⇒ pF ≥ 1⇒ pF = 1. This shows that unless pF = 1, it is suboptimal to select action F in states S̃ −{1} and since pF = 1 is a trivial case, we disregard that. Hence, it is always optimal to select action C at s ∈ S̃ − {1}.
The result of Lemma 1 holds independently from the set of transition probabilities and the number of states. Lemma 1 leaves out only two candidates for the optimal policy. The first candidate is the policy which selects action C at any state s ∈ S̃. The second candidate selects action C in all states except state 1. Hence, the optimal policy is always in set {πtr0 , πtr1 }. This reduces the set of policies to consider from 2G−1 to 2. Let r := pD/pC denote the failure ratio of action C. The next lemma gives the value functions for πtr1 and πtr0 .
Lemma 2. In the GRBP we have
(i) V π tr 1 (s) =  pF + (1− pF ) 1− r s−1 1− rG−1 , when r 6= 1
pF + (1− pF ) s− 1 G− 1 , when r = 1
(ii) V π tr 0 (s) =  1− rs 1− rG , when r 6= 1 s
G , when r = 1
for s ∈ S̃. Proof: (i):
For πtr1 we have: V π tr 1 (G) = 1 V π tr 1 (G− 1) = pCV πtr1 (G) + pDV πtr1 (G− 2) . . . V π tr 1 (2) = pCV π tr 1 (3) + pDV π tr 1 (1)
V π tr 1 (1) = pF
⇒  (pC + pD)V π tr 1 (G− 1) = pC + pDV πtr1 (G− 2) . . .
(pC + pD)V π tr 1 (2) = pCV π tr 1 (3) + pDpF
⇒  pC(V π tr 1 (G− 1)− 1) = pD(V π tr 1 (G− 2)− V πtr1 (G− 1)) . . . pC(V π tr 1 (s+ 1)− V πtr1 (s+ 2)) = pD(V π tr 1 (s)− V πtr1 (s+ 1)) . . .
pC(V π tr 1 (2)− V πtr1 (3)) = pD(pF − V πtr1 (2))
⇒  V π tr 1 (G− 1)− 1 = rG−2(pF − V πtr1 (2)) . . . V π tr 1 (s)− V πtr1 (s+ 1) = rs−1(pF − V πtr1 (2)) . . .
V π tr 1 (2)− V πtr1 (3) = r(pF − V πtr1 (2))
⇒
(6)
Summation of all the terms results in
1− V πtr1 (2) = (V πtr1 (2)− pF )( G−2∑ i=1 ri)⇒ (7)
V π tr 1 (2)( G−2∑ i=0 ri) = 1 + pF ( G−2∑ i=1 ri)⇒
V π tr 1 (2)( G−2∑ i=0 ri) = 1− pF + pF ( G−2∑ i=0 ri)⇒
V π tr 1 (2) = pF + 1− pF ( ∑G−2 i=0 r i) ⇒ V π tr 1 (2) = pF + (1− pF ) 1− r
1− rG−1 .
Then, for sth state, we have to sum up to (s− 1)th equation in (6):
V π tr 1 (s)− V πtr1 (2) = (V πtr1 (2)− pF )( s−2∑ i=1 ri)⇒
V π tr 1 (s) = pF + (V π tr 1 (2)− pF )( s−2∑ i=0 ri)⇒ (8) V π tr 1 (s) = pF + (1− pF ) 1− r s−1
1− rG−1 . (9)
For the fair case, r has to be set to 1 in (7) and (8). Then,
V π tr 1 (2) = pF + (1− pF ) 1
G− 1 and
V π tr 1 (s) = pF + (1− pF ) s− 1
G− 1 .
Case (ii): Since action F is never selected by πtr0 , for this case, standard analysis of the gambler’s ruin problem applies.
Thus, the probability of hitting G from state s is
(1− rs)/(1− rG) (10) for r 6= 1 and s/G for r = 1 [20].
The form of the optimal policy is given in the following theorem.
Theorem 2. In the GRBP, the optimal policy is πtrτ∗ , where
τ∗ =  sign(pF − 1− r 1− rG ), when r 6= 1
sign(pF − 1 G ), when r = 1
where sign(x) = 1 if x is nonnegative and 0 otherwise.
Proof: Since we have found in Lemma 1 that it is always optimal to select action C when the state is in {2, . . . , G−1}, to find the optimal policy, it is sufficient to compare the value functions of the two policies for s = 1. When r 6= 1, this gives π∗ = πtr1 if
1− r 1− rG < p F
and π∗ = πtr0 otherwise. 1 Similarly, if r = 1 and 1/G < pF , then π∗ = πtr1 . Otherwise, π ∗ = πtr0 . Using these, the value of the optimal threshold is given as
τ∗ =  sign(pF − 1− r 1− rG ) if r 6= 1
sign(pF − 1 G ) if r = 1
which completes the proof. When r 6= 1, the term (1 − r)/(1 − rG) represents probability of hitting G starting from state 1 by always selecting action C. This probability is equal to 1/G when r = 1. Because of this, it is optimal to take the terminal action in some cases for which pC > pF . Although the continuation action can move the system state in the direction of the goal state for some time, the long term chance of hitting the goal state by taking the continuation action can be lower than the chance of hitting the goal state by immediately taking the terminal action at state 1.
Equation of the boundary for which the optimal policy changes from πtr0 to π tr 1 is
pF = B(r) := (1− r)/(1− rG) (11) when r 6= 1. This decision boundary is illustrated in Fig. 2 for different values of G. We call the region of transition probabilities for which πtr0 is optimal as the exploration region, and the region for which πtr1 is optimal as the noexploration region. In exploration region, the optimal policy does not take action F in any round. Therefore, any learning algorithm that needs to learn how well action F performs, needs to explore action F . As the value of G increases, area of the exploration region decreases due to the fact that probability of hitting the goal state by only taking action C decreases.
1When (1− r)/(1− rG) = pF both πtr1 and πtr0 are optimal. For this case, we favor πtr1 because it always ends the current round.
For large-scale visual recognition applications such as ImageNet LSVRC, the data required for training is on the order of hundreds of Gigabytes. Therefore, it is difficult to load all image data completely into memory after training starts. Instead, images are stored as batch files on local or remote disks and loaded one file at a time by each process. Loading image batches x from disk can be time consuming6. It is affected by various factors, including file size, file format, disk I/O capability and network bandwidth if reading from remote disks. If in every iteration, the training process should wait for data loading to be ready in order to proceed, one can imagine the time cost by loading data will be critical to the total performance. One way to circumvent this, given the independence of loading and training, is to load those files in parallel with the forward and backward propagations on the last loaded batch. However, this assumes loading one batch of images takes shorter than one iteration of training the model. This auxiliary loading process should follow procedures in Alg. 1 to collaborate efficiently with its corresponding training process:
6 Loading labels y, on the other hand, is much faster, therefore labels can be loaded completely into memory.
Algorithm 1 The parallel loading process
Require: Host memory allocated for loading image batch hostdatax. GPU memory allocated for preprocessed image batch gpudatax GPU memory allocated for the actual model graph input inputx, mode=None, recv=None, filename=None. Mean image image mean Ensure: 1: while True do 2: Receive the mode (train, validate or stop) from training process 3: if recv=“stop” then 4: break 5: else 6: mode← recv 7: Receive the first filename to be loaded from training process filename← recv 8: while True do 9: Load file “ filename” from disk into host memory hostdatax. 10: hostdatax = hostdatax − image mean 11: Crop and mirror hostdatax according to mode. 12: Transfer hostdatax from host to GPU device memory gpudatax. 13: Wait for training on the last inputx to finish by receiving the next filename
to be loaded. 14: if recv in [“stop”, “train”, “val”] then 15: break 16: else 17: filename← recv 18: Transfer gpudatax to inputx. 19: Synchronize GPU context. 20: Notify training process to precede with the newly loaded inputx
Different from the multiprocessing and Queue messaging method in [7], we used the MPI Spawn function to start a child process from each training process and used the resulting MPI intra-communicator to pass messages between the training process and its child process. As shown in Algorithm 1, the parallel loading process can read image files, subtract the mean image, crop sub-images and finally load preprocessed data onto GPUs. By doing this, we are able to overlap the most compute-intensive part (Step 10 to 13 in Algorithm 1) with forward and backward graph propagation in the training process.
In this experiment, we evaluate the extent to which the distances computed on EVE embeddings can help to group semantically-related items together, while keeping unrelated items apart. This is a fundamental requirement for distancebased methods for cluster analysis.
Task definition: For all items in a specific “topical type”, we construct an embedding space without using information about the category to which the items belong. The purpose is then to measure the extent to which these items cluster together in the space relative to the ground truth categories. This is done by measuring distances in the space between items that should belong together (i.e. intra-cluster distances) and items that should be kept apart (i.e. inter-cluster distances), as determine by the categories. Since there are seven “topical types”, there are also even queries in this task.
Example of a query: For the “topical type” Cuisine, we are provided with a list of 100 items in total, where each of the five categories has 20 items. These correspond to cuisine items from five different countries. The idea is to measure the ability of each embedding model to cluster these 100 items back into five categories.
Strategy: To formally measure the ability of a model to cluster items, we conduct a two-step strategy as follows:
1. Calculate a pairwise similarity matrix between all items of a given “topical type”. The similarity function that we use for this task is the cosine similarity. 2. Transform the similarity matrix to a distance matrix5 which is used to measure inter and intra-cluster distances relative to the ground truth categories.
Results: To evaluate the ability to cluster, there are typically two objectives: within-cluster cohesion and between-cluster separation. To this end, we use three well-known cluster validity measures in this task. Firstly, the within-cluster distance (Everitt et al 2001) is the total of the squared distances between each item xi and the centroid vector µc of the cluster Cc to which it has been assigned:
within = k∑
c=1 ∑ xi∈Cc d(xi, µc) 2 (5)
Typically this value is normalized with respect to the number of clusters k. The higher the score, the more coherent the clusters. Secondly, the between-cluster distance is the total of the squares of the distances between the each cluster centroid and the centroid of the entire dataset, denoted µ̂:
5 by simply, 1 - normalized similarity score over each dimension
This value is also normalized with respect to the number of clusters k. The lower the score, the more well-separated the clusters. Finally, the two above objectives are combined via the CH-Index (Caliński and Harabasz 1974), using the ratio:
CH = between/(k − 1) within/(n− k) (7)
The higher the value of this measure, the better the overall clustering.
From Table 7, we can see that EVE generally performs better than rest of the embedding methods for the within-cluster measure. In Table 8, for the betweencluster measure, EVE is outperformed by FastText CBOW, Word2Vec CBOW, and FastText SG mainly due to the “topical type” Cuisine and European cities where EVE does not perform well. Finally, in Table 9 where the combined aim of clustering is captured through the CH-Index, EVE outperforms the rest of the methods, except in the case of the “topical type” European cities.
Explanation from the EVE model: Using labeled dimensions from the EVE model, we define a similar strategy for explanation as used in the previous task. However, now instead of discovering an intruder item, the goal is to define categories from items and to define the overall space. Algorithm 2 shows the strategy which requires three inputs: the vectorspace representing the entire embedding; the list of categories categories; the categories vectorspace which is the vector space of items belonging to each category. In step 1, we calculate the mean vector representing for the entire space. In step 2, we order the labeled dimensions of the mean vector by the informativeness. In steps 3–6 we iterate over the list of categories (of a “topical type” such as Cuisine) and calculate mean vector for each category’s vector space, which is followed by the ordering of dimensions of the mean vector of category vector space by the informativeness. Finally, we return the most informative features of the entire space and of each category’s vector space.
Algorithm 2 Explanation strategy for the ability to cluster task.
Require: EVE → vectorspace, categories, categories vectorspace 1: spacemean = Mean(vectorspace) 2: spaceinfo features = order byinfo features(spacemean) 3: for category ∈ categories do 4: categorymean = Mean(categories vectorspace[category]) 5: categoriesinfo features[category] = order byinfo features(categorymean) 6: end for 7: return spaceinfo features, categoriesinfo features
Tables 10 and 11 show the explanations generated by the EVE model, in the cases where the model performed best and worse against baselines respectively. In Table 10, the query is the list of items from “topical type” cuisine. As can be seen from the bold entries in the table, the explanation conveys the main idea about both the overall space and the individual categories. For example, in the overall space, we can see the cuisines by different nationalities, and likewise we can see the name of nationality from which the cuisine is originated from (e.g. Italian cuisine for the “Italian category” and Pakistani breads for the “Pakistani category”). As for the non-bold entries, we can also observe relevant features but at a deeper semantic level. For example, cuisine of Lombardy in “Italian category” where Lombardy is a region in Italy, and likewise tortilla-based dishes in the Mexican category where tortilla is a primary ingredient in Mexican cuisine.
In Table 11, the query is the list of items from “topical type” European cities and this is the example where EVE model performs worse. However, the explanation allows us to understand why this is the case. As can been from the explanation table, the bold features show historic relationships across different countries,
such as “capitals of former nations”, “fortified settlements”, and “Roman sites in Spain”. Similarly, it can also be observed in non-bold features such as “former capital of Italy”. Based on this explanation, we could potentially decide to apply a rule that would exclude any historical articles or categories when generating the embedding for this type of task in future.
Visualization: Since scatter plots are often used to represent the output of a cluster analysis process, we generate a visualization of all embeddings using T-SNE (Maaten and Hinton 2008), which is a tool to visually represent high-dimensional
data by reducing it to 2–3 dimensions for presentation.6. For the interest of reader, Fig. 4 shows a visualization generated using EVE and GloVe when the list of items are selected from the “topical type” country to continent. As can be seen from the plot, the ground truth categories exhibit better clustering behavior when using the space from the EVE model, when compared to the Glove model. This is also reflected in the corresponding scores in Tables 7, 8, and 9.
This section deals with the annotation of the data. We first describe the final annotation schema. Then, we describe the iterative process of corpus annotation and the refinement of the schema. This refinement was necessary due to the complexity of the annotation.
Note that a CLA layer is producing a number of representations of its inputs simultaneously, and these representations can be seen as nested one within another.
Columnar SDR The simplest and least detailed representation is the Columnar SDR, which is just a simple representation of the pattern currently seen by the layer. This is what you would
see if you looked down on the layer and just observed which columns had active cells. The number of patterns which can be represented is
( N
nSDR
) . In the typical software layer
(2048 columns, 40 active), we can have ( 2048 40 ) = 2.37178 ∗ 1084 SDRs. (See [Ahmad and Hawkins, 2015] for a detailed treatment of the combinatorics of SDRs).
Cellular SDR The cell-level SDR encodes both the Columnar SDR (if you ignore the choices of cells) and the context/sequence in which it occurred. We can produce a one-cell-per-column SDR by choosing the most predictive cell in each active column (and choose randomly in the case of bursting cells). In fact, this is how cells are chosen for learning in most implementations of CLA.
Interestingly, the capacity of this SDR is very large. For every Columnar SDR (ie for each spatial input), there are nnSDR distinct contexts, if each column contains n cells. Again, in typical software, nSDR = 40, n = 32, so each feedforward input can appear in up to 1.60694 ∗ 1060 different contexts. Multiplying these, we get 3.8113 ∗ 10144 distinct Cellular SDRs.
Predicted/Bursting Columnar SDR This more detailed SDR is composed of the sub-SDRs (or vectors) representing a) what was predicted and confirmed by reality and b) what was present in the input but not well-predicted. The layer’s combined output vector can thus be seen as the sum of two vectors - one representing the correctly predicted reality and the other a perpendicular prediction error vector:
ySDR = ypred + yburst
As we’ll see in the next section, this decomposition is crucial to the process of Temporal Pooling, in which a downstream layer can learn to stably represent a single representation by learning to recognise successive ypred vectors.
Predicted/Bursting Cellular SDR This is the cellular equivalent of the previous SDR (equivalently the previous SDR is the column-level version of this one). This SDR encodes the precise sequence/context identity as well as the split between predicted and prediction error vectors. In addition, looked at columnwise, the error SDR is actually the union of all the vectors representing how the input and prediction differed, thus forming a cloud in the output space whose volume reflects the confusion of the layer.
As noted earlier, the size, or `1 norm, of the Predicted/Bursting Cellular SDR varies dramatically with the relative number of predicted vs bursting columns. In a typical CLA software layer, 40 ≤ ‖ySDR‖`1 ≤ 1280, a 32x range.
Prediction-ordered SDR Sequences Even more detail is produced by treating the SDR as a sequence of individual activations, as we did earlier when deriving the sequence:
S = Ppred ‖ Pburst
Each of the two subsequences is ordered by the activation levels of the individual cells, in decreasing order. Each thus represents a sequence of recognitions, with the most confident recognitions appearing earliest.
In this paper a reinforcement learning based multi-user, multi-band spectrum sensing policy is proposed. The proposed sensing policy balances between exploring and exploiting different parts of the radio spectrum and different sensing assignments. It decides which frequency bands to sense as well as which SU is assigned to do the sensing. In the exploitation phase the sensing assignment for the high throughput subbands is found by minimizing the number of assigned SUs subject to a constraint on the miss detection probability. Moreover, the probability of false alarm is constrained by using Neyman-Pearson detectors. Minimization of the number of simultaneously sensing SUs improves the energy efficiency of the battery operated SUs. The minimization is formulated as a binary integer programming (BIP) problem that may be solved exactly by a branch-and-bound type algorithm or approximately by using approximative methods such as the iterative Hungarian method considered in this paper. The proposed policy may reduce the number of active sensors up to a factor of 1/D, where D is the diversity order of a fixed sensing policy. In the exploration phase different pseudorandom sensing assignments with fixed diversity order are explored in order to re-adapt to possible changes in the PU activity and channel conditions. On one hand, spatial diversity improves the detector performance in the face of fading and shadowing but on the other hand reduces the
number of simultaneously sensed frequency bands by the secondary network. Cognitive network may use multiple idle frequency bands in order to improve rate or reliability of the network.
Some preliminary ideas and results related to this paper were presented in [6]. The contributions of this paper are:
• We propose a machine learning based spectrum sensing policy for cognitive radio that:
– provides high throughput for the SUs,
– reduces missed detections,
– is energy efficient,
– is adaptive to non-stationary PU behavior and channel conditions.
• Analytical expressions for the convergence of the proposed sensing policy in stationary scenarios are derived.
• Extensive simulation results highlighting the excellent performance of the proposed sensing policy in various stationary and non-stationary scenarios are shown.
• We show that a simple and fast approximative algorithm based on the Hungarian method may be used to find near optimal sensing assignments.
The main difference with this paper and the related work in the literature, in addition to the methodology, is the exploitation of the information about the sensing performances of the SUs to optimize the sensing assignments in an energy efficient manner.
This paper is organized as follows. In section 2 the related work to this paper is briefly summarized. The system model of cooperative multi-band sensing is described in section 3. In section 4 an energy efficient reinforcement learning based sensing policy is proposed and analytical results on the convergence rate of the Q-values in the sensing policy are derived. Section 5 shows and discusses the simulation results of the performance of the proposed sensing policy. The paper is concluded in section 6.
In English long words (as opposed to short Saxon words) are often learned words derived from Latin or Greek. A large number of Latin and Greek words were imported into the English language during the Renaissance. Astell's text is a literary work and we wondered if we might find the percentage of long words in her text thanks to NooJ. To do so, we created a graph that enables NooJ to locate them. We determined their number and the occurrences of each lexeme in a very simple manner. The graph below can tag the words in the text according to their length. Then to look for the occurrence of a N-letter word, we use the command <LETTERS+NUMBER>. Our results are presented in table 3:
The total number of long words is 3659 out of 18,759 words in the text, i.e. a percentage of around 19.5%. This graph enables us to determine the percentage of long words and spot very long words (e.g. 16: uncharitableness) and some archaic words (e.g. 15: pragmaticalness). On average long words are used about twice (2.03).
For out third set of experiments we trained WaveNets to model two music datasets:
• the MagnaTagATune dataset (Law & Von Ahn, 2009), which consists of about 200 hours of music audio. Each 29-second clip is annotated with tags from a set of 188, which describe the genre, instrumentation, tempo, volume and mood of the music. • the YouTube piano dataset, which consists of about 60 hours of solo piano music obtained
from YouTube videos. Because it is constrained to a single instrument, it is considerably easier to model.
Although it is difficult to quantitatively evaluate these models, a subjective evaluation is possible by listening to the samples they produce. We found that enlarging the receptive field was crucial to obtain samples that sounded musical. Even with a receptive field of several seconds, the models did not enforce long-range consistency which resulted in second-to-second variations in genre, instrumentation, volume and sound quality. Nevertheless, the samples were often harmonic and aesthetically pleasing, even when produced by unconditional models.
Of particular interest are conditional music models, which can generate music given a set of tags specifying e.g. genre or instruments. Similarly to conditional speech models, we insert biases that depend on a binary vector representation of the tags associated with each training clip. This makes it possible to control various aspects of the output of the model when sampling, by feeding in a binary vector that encodes the desired properties of the samples. We have trained such models on the MagnaTagATune dataset; although the tag data bundled with the dataset was relatively noisy and had many omissions, after cleaning it up by merging similar tags and removing those with too few associated clips, we found this approach to work reasonably well.
In this section, we provide a computational upper bound on the value of the solution to (1). This upper bound is based on the idea of dividing (1) into k different “singlefeature” subproblems, then performing an information relaxation (similar in spirit to (Brown et al., 2010)) in which we give the policy assigned to each single-feature subproblem additional information, which allows us to compute their value efficiently.
Define Yi,n = θi + in. Here i n ∼ N(0, λ
2
x2i,n ) if xi,n > 0
and in = 0 if xi,n = 0 for i = 1, 2, · · · , k, independently distributed across i and n. We may think of Yi,n as the reward that we would have seen if Xn were equal to ei, where ei is a unit vector with the ith element 1 and other elements 0. Later, we will use that Yn = ∑k i=1 xi,nθi +
n = ∑k i=1 xi,n(θi + i n) = ∑k i=1 xi,nYi,n.
We will generalize the original problem (1) by introducing notation that allows for separate forwarding decisions to be made for each feature. Define Uj,n to be decision made for the jth feature of the nth item. The original problem (1) can be recovered if we require that Uj,n is identical across j for each n.
For each feature j, we now introduce a new set of policies Πj , which will govern the forwarding decisions Uj,n for feature j, and under which these decisions can depend upon information not available in the original problem: they may depend on θ · ei for ∀i 6= j. Formally, the decision of whether or not to forward the jth feature of the nth item depends on the history Hjn−1 = (Uj,m, Xj,m, Uj,mYj,m : m ≤ n − 1), our current Xj,n, and θ−j = (θ1, · · · , θj−1, θj+1, · · · , θk).
Using these definitions, we may now state the computational upper bound, which constitutes the main theoretical result of this work. It bounds the value of the optimal policy for our original problem of interest (1), on the left-hand side, by the sum of a collection of values of single-feature problems, each of which have been given additional information. Efficient computation of this right-hand side is discussed below, and summarized in Algorithm 1.
Theorem 1. For any Xn that are bounded over all n, we have
sup π∈Π
Eπ [ N∑ n=1 Un(Yn − c) ]
≤ k∑ j=1 sup π′′∈Πj Eπ ′′ [ N∑ n=1 Uj,n(xj,nYj,n − xj,nc ‖Xn‖ ) ] ,
where ‖Xn‖ is the L1 norm. When ∑k i=1 xi,n = 1, then this theorem becomes:
sup π∈Π
Eπ [ N∑ n=1 Un(Yn − c) ]
≤ k∑ j=1 sup π′′∈Πj Eπ ′′ [ N∑ n=1 Uj,n(xj,nYj,n − xj,nc) ] .
Proof. Since ‖Xn‖ = x1,n + · · ·+ xk,n, we know
sup π∈Π
Eπ [ N∑ n=1 Un(Yn − c) ]
= sup π∈Π
Eπ [ N∑ n=1 Un(x1,nY1,n + · · ·+ xk,nYk,n − c) ]
= sup π∈Π
Eπ  N∑ n=1 k∑ j=1 Un(xj,nYj,n − xj,n c ‖Xn‖ )  . (3) Now we introduce two new policy sets Π ′ 0 and Π ′ , which allow different features can make their own decisions Uj,n for the nth item. Further, Π ′
0 has an additional restriction that U1,n = · · · = Uj,n. Based on the definition, we have
(3) = sup π′∈Π′0
Eπ ′  N∑ n=1 k∑ j=1 Uj,n(xj,nYj,n − xj,n c ‖Xn‖ )  ≤ sup π′∈Π′ Eπ ′  N∑ n=1 k∑ j=1 Uj,n(xj,nYj,n − xj,n c ‖Xn‖ )
 . (4)
Since the supremum of a summation is less or equal to the summation of a supremum, we have (4) ≤ k∑ j=1 sup π′∈Π′ Eπ ′ [ N∑ n=1 Uj,n(xj,nYj,n − xj,n c ‖Xn‖ ) ] .
(5)
Then based on the definition of our policy set Πj , for j = 1, 2, · · · , k, we know (5) ≤ k∑ j=1 sup π′′∈Πj Eπ ′′ [ N∑ n=1 Uj,n(xj,nYj,n − xj,n c ‖Xn‖ ) ] ,
which concludes the proof of the theorem.
We emphasize that this computational upper bound holds true in general, even when the different components of Xn are correlated. Numerical experiments in Section 4.3 and Section 4.1 suggest that the optimality gap between this upper bound and the best heuristic policy is typically small.
For simplicity, in this paper we focus on the special case where ∑k i=1 xi,n = 1. We now discuss computation of the upper bound in Theorem 1. To compute this quantity, we must solve these k subproblems:
sup π∈Πj
Eπ [ N∑ n=1 Uj,n(xj,nYj,n − xj,nc) ] , j = 1, 2, · · · , k,
(6)
where Yj,n|θj ∼ N(θj , λ 2
x2j,n ) and θj ∼ N(µj,n, σ2j,n).
Here θj ∼ N(µj,n, σ2j,n) represents our belief of θj after the first n items.
Therefore for each subproblem, after the arrival of the nth item, we can update our parameters as the following:
µj,n =
{ λ2βj,n−1µj,n−1+Yj,n−1x 2 j,n−1
λ2βj,n−1+x2j,n−1 if Uj,n−1 = 1;
µj,n−1 if Uj,n−1 = 0.
The precision of our beliefs is updated as follows:
βj,n =
{ βj,n−1 + x2j,n−1 λ2 if Uj,n−1 = 1;
βj,n−1 if Uj,n−1 = 0.
The jth single-feature subproblem can be solved using dynamic programming with a three-dimensional state space (µj,n, σj,n, xj,n), where µj,n and σj,n are the mean and variance of our current belief about θj and xj,n is the current item’s jth feature. Initially, µj,0 and σj,0 are given by the conditional distribution of θj given θ−j and the prior
distribution θ ∼ N(µ,Σ). Upon each item’s arrival, we move to another state based on the updating formula described above. Define Qj(µ, σ, x, 0) and Qj(µ, σ, x, 1) be the total reward to go if you decided to discard the item and forward the item respectively,
Qj(µ, σ, x, U) = sup π′′∈Πj
Eπ ′′ [ ∞∑ n=1 γn−1Uj,x(xj,nYj,n − xj,nc)
|θj ∼ N(µ, σ2), xj,1 = x, Uj,1 = U ].
Then the Bellman equation for this problem is:
Vj(µ, σ, x) = max U=0,1 Qj(µ, σ, x, U). (7)
This calculation is summarized as Algorithm 1.
Algorithm 1 Calculation of the jth subproblem Solve the dynamic program using backward induction (discretizing and truncating), with state space (µj,n, σj,n, xj,n) ∈ R×R+× [0, 1], infinite horizon and value function Vj(µ, σ, x). for i = 1; i < M ; i+ + do
Generate θ ∼ N(µ,Σ); Calculate the conditional distribution of θj ∼ N(µj,0, σj,0), given θ ∼ N(µ,Σ) and θ−j . Generate xj,0 from the distribution of Xn. Find the optimal value of state (µj,0, σj,0, xj,0) and denote it as Vi.
end for Calculate V̄ = 1M ∑M i=1 Vi and use (2) to get the optimal value for the jth subproblem, where M is the number of simulation.
We may improve our upper bound by taking its minimum with a hindsight upper bound, derived in the following way. We first consider a larger class of policies that may additionally base their decisions on full knowledge of θ. An optimal policy among this larger class of policies forwards the nth item to the user only if θ ·Xn > c, and the expected total reward of this optimal policy is
E [ N∑ n=1 (θ ·Xn − c)+ ] = γ 1− γ E [ (θ ·X1 − c)+ ] . (8)
Since (8) is the supremum of the same objective as (2), but over a larger set of policies, it forms an upper bound. This style of analysis was also applied in (Chick & Frazier, 2012). In Section 4, we use the minimum of the computational upper bound in Theorem 1 and the hindsight upper bound (8) as our theoretical upper bound.
Next, we give a more detailed description of the reasoning system and control loop of our architecture for building intelligent robots. For this description, we (once again) view a robot as consisting of a logician and a statistician, who communicate through a controller, as described in Section 1 and shown in Figure 4. For any given goal, the logician takes as input the system description DH that corresponds to a coarse-resolution transition diagram τH , recorded history H with initial state defaults (see Example 2), and the current coarse-resolution state σ1 (potentially inferred from observations). If recent recorded observations differ from the logician’s predictions, the discrepancies are diagnosed and a plan comprising one or more abstract actions is computed to achieve the goal. Planning and diagnostics are reduced to computing answer sets of the CR-Prolog program Π(DH ,H ). For a given goal, the controller uses the
transition T corresponding to the next abstract action aH in the computed plan to zoom to DLR(T ), the part of the randomized fine-resolution system description DLR that is relevant to the T . A POMDP is then constructed from DLR(T ) and the learned probabilities, and solved to obtain a policy. The POMDP and the policy are communicated to the statistician who invokes the policy repeatedly to implement the abstract action aH as a sequence of (more) concrete actions. When the POMDP policy is terminated, the corresponding observations are sent to the controller. The controller performs inference in DLR(T ), recording the corresponding coarse-resolution action outcomes and observations in the coarse-resolution history H , which is used by the logician for subsequent reasoning.
Algorithm 3: Control loop Input: coarse-resolution system description DH and history H ; randomized fine-resolution system description
DLR; coarse-resolution description of the goal; coarse-resolution initial state σ1. Output: robot is in a state satisfying the goal; reports failure if this is impossible.
1 while goal is not achieved do 2 Logician uses DH and H to find a possible plan, aH1 , . . . ,a H n to achieve the goal. 3 if no plan exists then 4 return failure 5 end 6 i := 1 7 continue1 := true 8 while continue1 do 9 Check pre-requisites of aHi .
10 if pre-requisites not satisfied then 11 continue1 := false 12 else 13 Controller zooms to DLR(T ), the part of DLR relevant to transition T = 〈σ1,aHi ,σ2〉 and constructs a POMDP. 14 Controller solves POMDP to compute a policy to implement aHi . 15 continue2 := true 16 while continue2 do 17 Statistician invokes policy to select and execute an action, obtain observation, and update belief state. 18 if terminal action executed then 19 Statistician communicates observations to the controller. 20 continue2 = false 21 i := i+1 22 continue1 := (i < n+1) 23 end 24 Controller performs fine-resolution inference, recording action outcomes and observations in H . 25 σ1 = current coarse-resolution state. 26 end 27 end 28 end
Algorithm 3 describes the overall control loop for achieving the assigned goal. Correctness of this algorithm (with a certain margin of error) is ensured by:
1. Applying the planning and diagnostics algorithm discussed in Section 5.2 for planning with τH and H ;
2. Using the formal definitions of refinement and zoom described in Section 7; and
3. Using a POMDP to probabilistically plan an action sequence and executing it for each aH of the logician’s plan, as discussed in Section 8.
The probabilistic planning is also supported by probabilistic state estimation algorithms that process inputs from sensors and actuators. For instance, the robot builds a map of the domain and estimates its position in the map using a Particle Filter algorithm for Simultaneous Localization and Mapping (SLAM) [49]. This algorithm represents the true underlying probability distribution over the possible states using samples drawn from a proposal distribution. Samples more likely to represent the true state, determined based on the degree of match between the expected and actual sensor observations of domain landmarks, are assigned higher (relative) weights and re-sampled to incrementally converge to the true distribution. Implementations of the particle filtering algorithm are used widely in the robotics literature to track multiple hypotheses of system state. A similar algorithm is used to estimate the pose of the robot’s arm. On the physical robot, other algorithms used to process specific sensor inputs. For instance, we use existing implementations of algorithms to process camera images, which are the primary source of information to identify specific domain objects. The robot also uses an existing implementation of a SLAM algorithm to build a domain map and localize itself in the map. These algorithms are summarized in Section 10, when we discuss experiments on physical robots.
Consider the optimization problem (3) assuming that we have been given V (Qπk ) for each k = 1, · · · ,K, and each π ⊂ {1, · · · ,K} such that k /∈ π. Now suppose that instead of optimally solving (3), we adopt the following ‘greedy’ policy. We iteratively define
k∗s = arg max{V k∗1 ,··· ,k ∗ s−1 i : i ∈ {1, · · · ,K} \ {k ∗ 1 , · · · , k∗s−1}}
for s = 1, · · · ,K. This policy assumes that the payoff-to-go from the ‘next level’ onwards is given. But since it is not, we recursively compute an approximation to this payoff-to-go by assuming that we will follow the same greedy strategy in all the subsequent levels of the optimization problem. Algorithm 3 computes the proposed policy and its payoff.
Algorithm 3 (Farsighted greedy): Function W (Q, p, β) where Q is a relevance matrix and p is a probability distribution over user types.
• If Q is empty, return W (Q, p, β) = 0. • If Q is non-empty, enumerate the non-dominated equivalence classes of Q. Let them
be (U1, · · · , UK). Calculate the number of products in each class, denoted by Lk =∑ j∈Uk Lj .
• Let the event ω(π, k) be as defined in Algorithm 2. Similarly define Qπk and pπk . • Iteratively compute
k∗s = arg max{W k∗1 ,··· ,k ∗ s−1 i : i ∈ {1, · · · ,K} \ {k ∗ 1 , · · · , k∗s−1}} (4)
where
Wπk = P (ω(π, k))
( 1− βLk
1− β + βL
k
W (Qπk , p π k , β)
) .
• Return W (Q, p, β) = Wk∗1 + βW k∗1 k∗2 + · · ·+ βK−1W k ∗ 1 ,··· ,k ∗ K−1 k∗K .
Note the computational savings as compared to the algorithm for computing the optimal policy. The comparison in equation (4) when s classes have been presented already is overK−s possibilities in the worst case. Thus the number of times a sub-program is called isK+(K−1)+(K−2)+· · ·+1 = K(K+1)
2 . Thus at each level in this recursive program, the number of sub-programs that are called is quadratic in the number of equivalence classes at that level. We can then prove the following performance guarantee for this policy. Theorem 4.1. LetLmin = minj∈1,··· ,H Lj be the minimum number of products in any category and let H be the total number of categories. The farsighted greedy algorithm achieves 1−β Lmin 1+β−βH−βLmin factor of the optimal payoff.
Note that the worst case is when H is large and Lmin = 1, in which case the adaptive greedy policy achieves a 1− β factor of the optimal payoff. The key idea of the proof is as follows. The departure from optimality at any level has two sources: the fact that the payoff-to-go from the next level onwards is an approximation to the optimal payoff-to-go, and the order in which the non-dominated classes are presented in the current level is chosen greedily. If one assumes that the ratio of the approximation to the optimal payoff-to-go and the optimal payoff-to-go at the next level is some γ, and if one can quantify the departure from optimality of the greedy policy at the current level, one can compute a bound for the worst case ratio of the current payoff-to-go and the optimal current payoff-to-go as some γ′ = f(γ). One can show that this operator is a contraction. Thus one can recursively find a sequence of lower bounds that are uniformly bounded below by the fixed point of this sequence, which is the quantity in the theorem.
Note that the description of Algorithm 3 can be simplified by fully exploiting its recursive structure; we presented it in the current form to show the correspondence to Algorithm 2 and also to facilitate
the argument in the proof of Theorem 4.1. The equivalent implementation can be found in the appendix.
Artificial Intelligence (AI) systems make errors. They should be corrected without damage of existing skills. The problem of non-destructive correction arises in many areas of research and development, from AI to mathematical neuroscience, where the reverse engineering of the brain ability to learn on-the-fly remains a great challenge. It is very desirable that the corrector of errors is non-iterative (one-shot) because iterative re-training of a large system requires much time and resource and cannot be done immediately without impeding activity.
The non-desrructive correction requires separation of the situations (samples) with errors from the samples corresponding to correct behavior by a simple and robust classifier. Linear discriminants introduced by Fisher (1936) are simple, robust, require just the inverse covariance matrix of data, and may be easily modified for assimilation of new data. Rosenblatt (1962) revived the common interest in linear classifiers. His works sparked intensive scientific debate (Minsky and Papert, 1969) and gave rise to development of numerous crucial concepts such as e.g. Vapnik-Chervonenkis theory (Vapnik and Chervonenkis, 1971), learnability (Natarajan, 1989), and generalization capabilities of neural networks (Vapnik, 2000), (Bousquet and Elisseeff, 2002). Linear functionals (adaptive summators) are basic building blocks of significantly more sophisticated AI systems such as e.g. multilayer perceptrons, (Rumelhart et al., 1986), Convolutional Neural Networks (Le Cun and Bengio, 1995), (LeCun et al., 2015) and their derivatives. Much is known about linear functionals
∗Corresponding author Email addresses: ag153@le.ac.uk (A.N. Gorban), it37@le.ac.uk
(I.Y. Tyukin)
as “stand-alone” learning machines, including their generalization margins (Freund and Schapire, 1999), (Vapnik, 2000) and numerous methods for their construction: linear discriminants and regression, perceptron learning, and Support Vector Machines (Vapnik, 1982) among others.
In this work, we demonstrate that in high dimensions and even for exponentially large samples, linear classifiers in their classical Fisher’s form are powerful enough to separate errors from correct responses with high probability and to provide efficient solution to the non-destructive corrector problem. We prove that linear functionals, as learning machines, have surprising and, as far as we are concerned, new peculiar extremal properties: in high dimension, with probability p > 1 − ϑ and for M < a exp(bn) with a, b > 0 every point in random i.i.d. drawn M-element sets in Rn is linearly separable from the rest. Moreover, the separating linear functional can be found explicitly, without iterations. This property holds for a broad set of relevant distributions, including products of probability measures with bounded support and equidistribution in a unit ball, providing mathematical foundations for one-trial correction of legacy AI systems (cf. (Gorban et al., 2016a)).
A problem of data fusion in multiagent systems has clear similarity to the problem of non-destructive correction. According to Forney et al. (2017), data collected by different agents may not be naively combined due to changes in the context, and special procedures for their assimilation without damage of gained skills are needed. The proven stochastic separation effects can be used to approach this problem. They also shed light on the possible origins of remarkable selectivity to stimuli observed in-vivo in the real brain (Quian Quiroga et al., 2005).
Preprint submitted to Elsevier August 4, 2017
A.1 Proof of Lemma 4
In order to prove our new regret bound for Hedge, we first state and prove the standard second-order regret bound for this algorithm.
Lemma 10. For any η > 0 and for any sequence ℓ1, . . . , ℓT of loss functions such that ℓt(i) ≥ −1/η for all t and i, the probability vectors q1, . . . , qT of Eq. (4) satisfy
T∑
t=1
∑
i∈V
qt(i)ℓt(i)−min k∈V
T∑
t=1
ℓt(k) ≤ lnK
η + η
T∑
t=1
∑
i∈V
qt(i)ℓt(i) 2 .
Proof. The proof follows the standard analysis of exponential weighting schemes: let wt(i) = exp ( −η∑t−1s=1 ℓs(i) ) and let Wt = ∑ i∈V wt(i). Then qt(i) = wt(i)/Wt and we can write
Wt+1 Wt
= ∑
i∈V
wt+1(i)
Wt
= ∑
i∈V
wt(i) exp ( −η ℓt(i) )
Wt
= ∑
i∈V
qt(i) exp ( −η ℓt(i) )
≤ ∑
i∈V
qt(i) ( 1− ηℓt(i) + η2ℓt(i)2 ) (using ex ≤ 1 + x+ x2 for all x ≤ 1)
≤ 1− η ∑
i∈V
qt(i)ℓt(i) + η 2 ∑
i∈V
qt(i)ℓt(i) 2 .
Taking logs, using ln(1− x) ≤ −x for all x ≥ 0, and summing over t = 1, . . . , T yields
ln WT+1 W1
≤ T∑
t=1
∑
i∈V
( −η qt(i)ℓt(i) + η2 qt(i)ℓt(i)2 ) .
Moreover, for any fixed action k, we also have
ln WT+1 W1 ≥ ln wT+1(k) W1
= − η T∑
t=1
ℓt(k)− lnK .
Putting together and rearranging gives the result.
We can now prove Lemma 4, restated here for the convenience of the reader.
Lemma 4 (restated). Let q1, . . . , qT be the probability vectors defined by Eq. (4) for a sequence of loss functions ℓ1, . . . , ℓT such that ℓt(i) ≥ 0 for all t = 1, . . . , T and i ∈ V . For each t, let St be a subset of V such that ℓt(i) ≤ 1/η for all i ∈ St. Then, it holds that
T∑
t=1
∑
i∈V
qt(i)ℓt(i)−min k∈V
T∑
t=1
ℓt(k) ≤ lnK
η + η
T∑
t=1
( ∑
i∈St
qt(i) ( 1− qt(i) ) ℓt(i) 2 + ∑
i/∈St
qt(i)ℓt(i) 2 ) .
Proof. For all t, let ℓ̄t = ∑ i∈St pt(i)ℓt(i) for which ℓ̄t ≤ 1/η by construction. Notice that executing Hedge on the loss vectors ℓ1, . . . , ℓT is equivalent to executing in on vectors ℓ′1, . . . , ℓ ′ T with ℓ ′ t(i) = ℓt(i)− ℓ̄t for all i. Applying Lemma 10 for the latter case (notice that ℓ′t(i) ≥ −1/η for all t and i), we obtain T∑
t=1
∑
i∈V
pt(i)ℓt(i)−min k∈V
T∑
t=1
ℓt(k) = T∑
t=1
∑
i∈V
pt(i)ℓ ′ t(i)−min
k∈V
T∑
t=1
ℓ′t(k)
≤ lnK η
+ η T∑
t=1
∑
i∈V
pt(i)ℓ ′ t(i) 2
= lnK
η + η
T∑
t=1
∑
i∈V
pt(i)(ℓt(i)− ℓ̄t)2 .
On the other hand, for all t,
∑
i∈St
pt(i)(ℓt(i)− ℓ̄t)2 = ∑
i∈St
pt(i)ℓt(i) 2 −
(∑
i∈St
pt(i)ℓt(i) )2
≤ ∑
i∈St
pt(i)ℓt(i) 2 −
∑
i∈St
pt(i) 2ℓt(i) 2
= ∑
i∈St
pt(i)(1− pt(i))ℓt(i)2
where the inequality follows from the non-negativity of the losses ℓt(i). Also, since ℓt(i) > 1/η ≥ ℓ̄t for all i /∈ St, we also have
∑
i/∈St
pt(i)(ℓt(i)− ℓ̄t)2 ≤ ∑
i/∈St
pt(i)ℓt(i) 2 .
Combining the inequalities gives the lemma.
A.2 Proof of Lemma 5
Lemma 5 (restated). Let G = (V,E) be a directed graph with |V | = K, in which each node i ∈ V is assigned a positive weight wi. Assume that ∑ i∈V wi ≤ 1, and that wi ≥ ǫ for all i ∈ V for some constant 0 < ǫ < 1 2 . Then
∑
i∈V
wi wi + ∑ j∈N in(i) wj ≤ 4α ln 4K αǫ ,
where α = α(G) is the independence number of G.
Proof. Following the proof idea of Alon et al. (2013), let M = ⌈2K/ǫ⌉ and introduce a discretization of the values w1, . . . , wT such that (mi − 1)/M ≤ wi ≤ mi/M for positive integers m1, . . . , mT . Since each wi ≥ ǫ, we have mi ≥ Mwi ≥ 2Kǫ · ǫ = 2K. Hence, we obtain
∑
i∈V
wi wi + ∑ j∈N in(i) wj
= ∑
i∈V
mi mi + ∑ j∈N in(i) mj −K
≤ 2 ∑
i∈V
mi mi + ∑ j∈N in(i)mj , (6)
where the final inequality is true since K ≤ 1 2 mi ≤ 12 ( mi + ∑ j∈N in(i) mj ) .
Now, consider a graph G′ = (V ′, E ′) created from G by replacing each node i ∈ V with a clique Ci over mi vertices, and connecting each vertex of Ci to each vertex of Cj if and only if the edge (i, j) is present in G. Then, the right-hand side of Eq. (6) equals ∑ i∈V ′ 1 1+di
, where di is the in-degree of the vertex i ∈ V ′ in the graph G′. Applying Lemma 13 of Alon et al. (2013) to the graph G′, we can show that
∑
i∈V
mi mi + ∑ j∈N in(i)mj
≤ 2α ln ( 1 + ∑ i∈V mi
α
) ≤ 2α ln ( 1 + M +K
α
) ≤ 2α ln 4K
αǫ ,
and the lemma follows.

Similar to the 2-MRD approach detailed above, our approach creates concept vectors by replacing each word in every definition by the vector representation of that word. This creates an M ×n matrix for each definition, where M is the dimensionality of the word vectors, and n the number of words contained in that definition. Following this, for each definition, we then obtain a single vector of dimensionality M by applying a compositional function to the matrix, thereby obtaining so-called definition vectors, which represent the entire meaning of the definition in one vector. Each concept can then be represented by a M×d matrix, where d is the number of definitions that a concept has in the UMLS. Finally, we apply a second composition function to this matrix, thereby obtaining a single vector of dimensionality M which represents the combined meaning of all definitions for that concept, i.e. a concept vector.
For each abstract in the test corpus, we first locate each ambiguous term through a simple lookup. For each located term in the abstract we create a vector representation by retrieving all words in a window of size w surrounding the ambiguous term, and replacing the words by their vectors. Note that this window does not include the ambiguous term itself. These collections of vectors are then combined into M -dimensional vectors using the same composition function as above. This is done separately for each term occurrence within a single document, creating a M × x matrix, where x is the number of times the ambiguous term occurs in a single document. These are then combined in an M -dimensional term vector using the same composition we used for the concepts, above. A schematic representation of our model is given in Figure 1.
Because all concept and term vectors are created using the same distributed vectors and compositional functions, the vector space in which they are
2Available on the BioASQ website. 3While we concede that the BioASQ corpora might contain abstracts from the MSH dataset, it does not contain any explicit labeled information that might be used in disambiguation.
placed is also comparable. Hence, for each ambiguous word we encounter, we can use the cosine distance between the abstract vector of the ambiguous utterance and each possible sense of that word to determine the correct sense. This makes our approach very similar to the Lesk family of approaches (Lesk, 1986).
In terms of composition function we experimented with elementwise multiplication, averaging and summation, all of which are unordered compositional functions (Mitchell and Lapata, 2008). In addition, it is worth noting that there’s still a lively debate whether ordered composition actually leads to better results for estimating document-, or sentence-level meaning, when compared to unordered composition (Iyyer et al., 2015; Socher et al., 2013).
Kennedy[8] proposes a faster version of the Cross Polytope method using a Fast Johnson Lindenstrauss transform from the original d dimensions to a reduced space with m dimensions and then the random lifted rotation from m to d′ dimensions. Our experiments show that for small dimensionality vectors this method is actually slower in practice that a direct random rotation and the other methods studied. As the dimensionality of the vectors is larger this method can become more efficient but then a direct dimensionality reduction of the vectors using feature hashing can be applied.
The Twitter data which is used in this study has been collected via Twitter Streaming API which allows searching for keywords, hash tags, user Ids and geographic bounding boxes simultaneously. The filter API facilitates the search by providing a continues stream of tweets matching the search criteria. Three key parameters are used for the search:
• Follow: a comma-separated list of user Ids to follow, which returns all of publicly published tweets in the stream.
• Track: a comma-separated list of keywords to track.
• Location: a comma-separated list of geographic bounding boxes containing the coordinates of the southwest point and the northeast point as a (longitude, latitude) pair.
Twitter Streaming API limits the number of parameters which can be supplied in one request. Up to 400 keywords, 25 geographic bounding boxes and 5000 user Ids can be provided in one request. In addition, the API returns all matching tweets up to a volume equal to the streaming cap where the cap is currently set to 1% of the total current volume of tweets published on Twitter [26].
We used the San Francisco Twitter data collected by [3] for a period of four months (Aug 2013 to Nov 2013). While the original dataset contained over 8 million tweets for this time period, the authors sub-sampled the data, resulting in a test dataset of size 500 tweets for testing their trained model. We have used the same test dataset for our comparative evaluations. This dataset is referred to as San Francisco. Additionally, we have collected data from London using all API parameters (Location bounding box, tracking and following official news agency user names and user Ids) at two different timestamps, referenced in the remaining of this paper as London1 and London2. The London1 data is composed of 3000 tweets collected between 15th and 31th of May 2015 and manually cleaned and annotated for training and testing the MV-RBM model. The manual annotation results undergo a second investigation for ensuring their consistency and validity. We have asked a group of technical users, who work in the field of smart cities to peer-review the validation of the annotations. The London2 data is collected on 3ed of February 2016 and is of size 1.1MB. In section 4.3, we used this dataset to examine the Twitter extracted event similarity with the road sensor data and the scheduled events that are parsed from the Web.
Temporal distribution of daily tweets collected from San Francisco and London1 datasets are shown in Fig. 7(a).
The EliXa system implements a single multiclass SVM classifier. We use the SMO implementation provided by the Weka library (Hall et al., 2009). All the classifiers built over the training data were evaluated via 10-fold cross validation. The complexity parameter was optimized as (C = 1.0). Many configurations were tested in this experiments, but in the following we only will describe the final setting.
The standard trial-wise training strategy uses the whole duration of the trial and is therefore similar to how FBCSP is trained. For each trial, the trial signal is used as input and the corresponding trial label as target to train the ConvNet. In our study, for both datasets we had 4.5-second trials (from 500 ms before trial start cue until trial end cue, as that worked best in preliminary experiments) as the input to the network. This led to 288 training examples per subject for the BCI Competition Dataset and about 880 training examples per subject on the High-Gamma Dataset after their respective train-test split.
As aforementioned, the aim of the advertiser is to receive as many actions as possible. Furthermore, the advertiser needs to know which sub-campaign contributed to howmany actions, hence realizing the effectiveness of the different tactics utilized. The big problem for this task is the fact that the action usually happens much later than showing the ad to the user, e.g. user sees many ads online, and then purchases an item, hence it is hard to attribute actions to sub-campaigns. A very simple example for this action attribution problem is given in Figure 3. In the example, we present two methodologies, last-touch attribution (the most commonly used method, attributes the action fully to the last seen ad), and multi-touch attribution (MTA, the action is attributed to many ads seen from the same advertiser). Please note that in the figure, we presented a very simple case of MTA, where each ad gets an equal proportion of the action, which is rarely the case in the real setting.
Naturally, action attribution and budget allocation are closely related. To be able to correctly allocate budget to sub-campaigns, we need to know how effective they are, i.e. how many actions they contributed to versus how much money was spent on them. This contribution is calculated by the action attribution methodology we employ (presented in § 4).
This paper differs from the existing works (Dorr et al., 2009; EuroMatrix, 2007) from several aspects, by introducing some recent development of MT evaluation measures, the different classifications from manual to automatic evaluation measures, the introduction of recent QE tasks of MT, and the concise construction of the content.
We hope this work will be helpful for MT
researchers to easily pick up some metrics that are best suitable for their specific MT model development, and help MT evaluation researchers to get a general clue of how MT evaluation research developed. Furthermore, hopefully, this work can also shine some light on other evaluation tasks, except for translation, of NLP fields. 1
We built our data set from the ACL Anthology Reference Corpus version
20160301 (Bird et al., 2008). The statistics of the data set and our model settings are summarized in Table 1.
As pre-processing, we deleted commas and periods that sticked to the tails of words and removed non-alphabetical words such as numbers and brackets from abstracts and titles. We then lowercased the words, and made phrases using the word2phrase tool2.
We prepared five categories: author, paper-id, reference, year and text. author consists of the list of authors without distinguishing the order of the authors. paper-id is an unique identifier assigned to each paper, and this mimics the paragraph vector model (Le and Mikolov, 2014). reference includes the paper ids of reference papers in this data set. Although ids in paper-id and reference are shared, we did not assign the same vectors to the ids since they are different categories. year is the publication year of the paper. text includes words and phrases in both abstracts and titles, and it belongs to the textual category Ψ, while each other category is treated as a non-textual category Φi. We regard elements as unknown elements when they appear less than minimum frequencies in Table 1.
We split the data set into training and test. We prepared 17,475 papers for training and the remaining 2,000 papers for evaluation. For the test set, we regarded the elements that do not appear in the training set as unknown elements.
We set the dimension d of vectors to 300 and
show the results with the linear function.

One disadvantage of the algorithm described above is that a fixed value of diversity rate γ is applied to all examples. This could be problematic since the optimal diversity rate should theoretically vary from instance to instance. High diversity rate can even be detrimental if it pushes the decoding model away from the beam search algorithm too much. For example, Shao et al. (2016) discover that in response generation, standard beam search is already good enough for short responses but deteriorates as the sequence gets longer. As another example, Vijayakumar et al. (2016) argue that diverse decoding is beneficial for images with many objects in image caption generation, but this is not true when there are few objects in the image.
A good diverse decoding algorithm should have the ability to automatically adjust its diversity rates for different inputs—for example, using small diversity rates for images with fewer objects but larger rates for those with more objects. We thus propose an algorithm using reinforcement learning (denoted diverseRL) that is capable of learning different γ values for different inputs.

Darwin said, evolution consists of many complex mechanisms, which can come into existence without any unlikely events to occur. Evolution consists of a path consisting of many interdependent small stages, but what are the conditions which make these paths to be taken and others, not? It may take exponential time, if evolution just randomly searched all the possible paths, but since we know that the time to evolve is a large finite polynomially bounded value it means there may exist an efficient learning mechanism, which can learn certain function classes and cannot learn others. So, basically mechanisms are treated as mathematical functions in this analysis, such that some functions can be learned in polynomial time, but others cannot due to their inherent computational intractability. We describe some notions, which may help us to formally analyze the quantitative theory of evolvability. We all know, that a cell consists of various types of proteins and maybe, other chemicals and complex circuits and thus, its working depends on many variables. So, in order to define some formalized theory ar X
iv :1
31 2.
45 99
v1 [
cs .L
G ]
1 7
D ec
2 01
of evolution, mechanisms need to be represented as many argument functions. Hence, a mechanism can take a lot of input parameters to properly function. So, what are many argument functions and how do we represent a mechanism through many argument functions? Well, a many argument function is a function f , which takes more than one input parameters to produce an output. The complex structures of living cells, have to respond to wide variations in both external and internal conditions. Say the conditions are represented by n boolean variables, x1, x2, ..., xn and let us have a function, say f whose output shows some particular desirable response under a particular combination of the x ′
is, for all i ∈ [n]. We say, the function f(x1, x2, ..., xn) is an ideal function and since, it depends on many input parameters, so it is a many argument function.Later in the text, it is explicitly shown that the class of parity functions is not evolvable, while the class of monotone conjunctions over the uniform distribution is. Let Xn be the set of all 2 n possible values that all the x ′
is can take. Let Dn be the probability distribution over Xn, which basically gives the relative frequency of the occurrence of certain combinations of the x ′
is.Now, let us have the notion of performance.
Definition 1.1. Let us have a function r : Xn → {−1, 1}. The performance of function r with respect to the ideal function f : Xn → {−1, 1} for the probability distribution Dn over Xn is
Pf (r,Dn) = ∑ x∈Xn f(x)r(x)Dn(x) (1)
Say for some points x ∈ Xn which have non-zero probability in Dn, we have∑ x∈Xn:Dn(x)6=0Dn(x) = 1. Now, if the ideal function f completely agrees with r on these set of points, then we have f(x).r(x) = 1 for all of them and thus,Pf (r,Dn) = ∑ Dn(x) = 1. Again, if f does not agree with r for all these
x, then we have f(x).r(x) = −1 and so Pf (r,Dn) = − ∑ Dn(x) = −1.So, the range of Pf (r,Dn) is [−1, 1] and can be viewed as a fitness landscape over all the genomes r.All the points in Xn can be thought of as life experiences. When r agrees with the ideal function, we have a benefit, otherwise we have a penalty in case of disagreement.So, over a sequence of life experiences, the organisms or groups which have high values of the performance function are selected preferentially for survival over organisms which have low values of the performance function. It is thus, a basic mathematical definition of the Darwinian concept of Survival of the Fittest. An organism or a group can test the performance of its genome r against the ideal function, by sampling a set S ⊂ Xn, of poly(n) size, say s(n) life experiences. Let us have a definition concerning empirical performance, which concerns the size of the independent selections, s.
Definition 1.2. The empirical performance, Pf (r,Dn, s), for some positive integer s is a random variable which makes s independent selections from the set S with replacements according to the distribution Dn and has the value 1 s . ∑ x∈S f(x)r(x).
We take s(n) to be the upper bound of population size, as the life experiences xi may correspond to one or more organisms.Moreover, let us also insist that evolution is able to proceed from any starting point, otherwise proceeding back to the reinitialized state from the current state may heavily decrease the value of the performance function.Let us discuss in short, the final two notions before proceeding to the definition of the concept of evolvability.Since, the organisms that can exist at any time is finite and is polynomially bounded, so for a function only a limited number of variants can be explored per generation, whether through mutations or recombination.And finally, we say that mechanisms with significant improvements in the value of performance function, evolve in a limited number of generations.
Let us now have an idea of evolvability and some of its definitions, in terms of learning theory.
Whitening step utilizes linear algebraic manipulations to make the tensor symmetric and orthogonal (in expectation). Moreover, it leads to dimensionality reduction since it (implicitly) reduces tensor M3 of size O(n 3) to a tensor of size k3, where k is the number of communities. Typically we have k ≪ n. The whitening step also converts the tensor M3 to a symmetric orthogonal tensor. The whitening matrix W ∈ RnA×k satisfies W⊤M2W = I. The idea is that if the bilinear projection of the second order moment onto W results in the identity matrix, then a trilinear projection of the third order moment onto W would result in an orthogonal tensor. We use multilinear operations to get an orthogonal tensor T := M3(W,W,W ).
51
The whitening matrix W is computed via truncated k−svd of the second order moments.
W = UM2Σ −1/2 M2 ,
where UM2 and ΣM2 = diag(σM2,1, . . . , σM2,k) are the top k singular vectors and singular values of M2 respectively. We then perform multilinear transformations on the triplet data using the whitening matrix. The whitened data is thus
ytA := 〈 W, ct 〉 , ytB := 〈 W, ct 〉 , ytC := 〈 W, ct 〉 ,
for the topic modeling, where t denotes the index of the documents. Note that ytA, y t B and ytC ∈ Rk. Implicitly, the whitened tensor is T = 1nX ∑ t∈X ytA ⊗ ytB ⊗ ytC and is a k × k × k dimension tensor. Since k ≪ n, the dimensionality reduction is crucial for our speedup.
Back to robotic grasp planning, Jeannette Bohg described a new large-scale database of grasps applied to a large set of objects from numerous categories. Such a database has been publicly released and contains grasps that are generated in simulation and annotated with the standard epsilon and a new physics-metric. Bohg presented a descriptive and efficient representation of the local object shape at which the grasp is applied. Each grasp is annotated with the proposed metrics and representation.
Given these data, a two-fold analysis was considered:
• Crowdsourcing for the analysis of the correlation of the two metrics with grasp success as predicted by humans. The results confirm that the proposed physics metric is a more consistent predictor for grasp success than the epsilon metric. Furthermore it supports the hypothesis that human labels are not required for good ground truth grasp data. Instead the physics metric can be used for simulation data.
• Big data learning techniques (Convolutional Neural Networks and Random Forests) to show how they can leverage the large-scale database for improved prediction of grasp success.
Another data driven approach to robot planning and manipulation was presented by Jeffrey Mahler and Ken Goldberg. In this talk, Mahler and Goldberg introduced the Dexterity Network (DexNet) 1.0, a dataset of 3D object models and a sampling-based planning algorithm to explore how Cloud Robotics can be used for robust grasp planning. The algorithm uses a MultiArmed Bandit model with correlated rewards to leverage prior grasps and 3D object models in a growing dataset that currently includes over 10,000 unique 3D object models and 2.5 million parallel-jaw grasps. Each grasp includes an estimate of the probability of force closure under uncertainty in object and gripper pose and friction. Dex-Net 1.0 uses Multi-View Convolutional Neural Networks (MV-CNNs), a new deep learning method for 3D object classification, to provide a similarity metric between objects, and the Google Cloud Platform to simultaneously run up to 1,500 virtual cores, reducing experiment runtime by up to three orders of magnitude. Experiments suggest that correlated bandit techniques can use a cloud-based network of object models to significantly reduce the number of samples required for robust grasp planning. Code and updated information is available (see [16]).
Hamal Marino and Marco Gabiccini discussed how datasets and tools, from disconnected markers to organized behaviors, pushing the attention on autonomous robot manipulation, with special focus on soft manipulation. Marino introduced the envisoned scenario wherethe (not-so-far) future robots will be able to autonomously grasp and manipulate objects, interacting with humans and their environment. Such a scenario has became more and more concrete as the research in the field brings new, promising results, as also discussed in the other talks.
A path started more than 50 years ago, with direct inspection of humans performing various sort of manipulative tasks, passing through categorization (resulting in the so called “grasp taxonomies”), and then towards building an hardware as close as possible to human hands in order to be able to mimic their behavior. Grasp modeling was used to this end, but the initial hypothesis needed for simplifying such a complex problem has been to have isolated contact points, happening only between the distal phalanxes of the robot hand and an external object. Recently, the concepts of soft interaction and soft robotic hands is becoming increasingly widespread: they started to change the paradigm, from timid, contact-based interactions with the objects to be manipulated, to daring, intense whole–hand interactions also involving the surrounding environment.
Disparate grasp planning algorithms have been developed mainly for the former kind of hands, although some attempts have begun to sprout also for the latter ones. Moreover, increasing the autonomy of the system, allowing high–level specifications to be interpreted and executed, has been studied as a necessary step towards simpler, more natural task definition along the path on the way to interaction of the robot with a human–centered world.
In his talk, Marino showed some advancements in aforementioned building blocks, towards the increase of robotic manipulation ability. The first part dealt with a novel, parametric kinematic model which can be adapted to different subjects and takes into account the relative motion between skin and bones in order to accurately reconstruct the hand motion of a human performing grasping and manipulation tasks [5]. Data from various subjects have been collected and analyzed, and a new clustering algorithm is used to obtain a data–driven grasp taxonomy [19].
In the second part, the problem of how to transfer the knowledge gathered from humans to robotic systems was faced, and two different ways are explored: recording humans performing grasping motions while ”wearing” the robotic end–effector as a tool, and a learning algorithm capable of working with soft robotic hands which generalizes successful example grasps to new scenarios [20]. Finally, the third part involved giving the robot an increased autonomy, using an abstraction layer, which makes robot end–effectors and fixed environment elements alike, each with its own interaction primitives to act on the object; it is shown how it is possible to translate higher level instructions into a sequence of low level actions, which can be then executed by the robotic system [18].
Context aware intelligent technologies raise the issue of user privacy and many approaches can be used to let the user control the balance between the private behavioral information they expose to the system and the utility they gain from it. The context recognition system can be implemented either on “the cloud” or on the phone device itself — preventing sensitive information from being sent out. Recognition can be limited to focus on certain behavioral aspects (e.g. physical exercise) while ignoring others. Depending on the application, sufficient information can be passed downstream without exposing the fully recognized behavioral context. For instance, if a medical application is designed to monitor the total time of walking and running done every day, these statistics can be accumulated during the day and sent at the end of the day, without divulging the specific schedule of when (and where) the user did which activity.
In this work we used simple computational methods. Better recognition can further be achieved by researching feature extraction methods, modeling inter-label correlation, using information from the near past, analyzing mood context, using online learning and active learning to improve models in real time, and leveraging the ample quantities of unlabeled data to personalize models. Differential privacy and similar approaches can be explored for privacy preservation of behavioral data. The public dataset we have collected enables the exploration of these research directions and many more, and will serve the research community as a standard by which to compare algorithm performance.
Experimental evaluation has been focused on measuring the actual size of suggested encodings and on measuring runtime when encodings are used for makespan optimal CPF solving.
The solving procedure presented as Algorithm 1 was used as a core framework for our makespan optimal CPF solving technique (that is, the sequential increasing strategy for querying the SAT solver was used) while suggested individual propositional encodings can be regarded as its exchangeable modules. The SAT solver itself was connected to the solving technique as another external module. All the implemented encodings used build-in distance heuristic discussed in section 4.3.1.
𝒜𝑣𝑗 𝑙 ≠ 𝑖
ℒ𝑎𝑖 𝑙 ≠ 𝑗
𝒳𝑎𝑖,𝑣𝑗 𝑙
in the INVERSE and MATCHING encoding
in the ALL-DIFFERENT encoding
in the DIRECT/SIMPLIFIED encoding
(30)
(31)
con≠(𝒜𝑣 𝑙 , 𝑐) = ⋁ lit(𝒜𝑣 𝑙 , 𝑐, 𝕚)
⌈log2 𝜇⌉−1
𝕚=0
(33)
(32)
Makespan Optimal Solving of Cooperative Path-Finding
29
The SAT-based CPF solving procedure was implemented in C++ as well as procedures for generating propositional formulae from given CPF and makespan bound (solving pro-
In this step, we consider each topic and then grouped and clustered top 50 documents which contributed the creation of that specific topic. This has been done for all the 50 topics of our choice. As an outcome, we have got 50 such clusters that contain documents which generated the topics.
Synthetic data set In this experiment, we illustrate the effectiveness of our ensemble classifier on a synthetic data set similar to the one evaluated in [25]. The radius and angle of the positive data is drawn from a uniform distribution [0, 1.5] and [0, 2π], respectively. The radius of the negative data is drawn from a normal distribution with mean of 2 and the standard deviation of 0.4. The angle of the negative data is drawn from a uniform distribution similar to the positive data. We generate 400 positive data and 400 negative data for training and validation purposes (200 for training and 200 for validating the asymmetric parameter). For testing, we evaluate the learned classifier with 2000 positive and negative data. We compare pAUCEnsT against the baseline AdaBoost, Cost-Sensitive AdaBoost (CS-AdaBoost)
3. In our implementation, we use an extension of LBP, known as the uniform LBP, which can better filter out noises [4]. Each LBP bin corresponds to each channel.
[27] and Asymmetric AdaBoost (AsymBoost) [25]. For CS-AdaBoost, we set the cost for misclassifying positive and negative data as follows. We assign the asymmetric factor k = C1/C2 and restrict 0.5(C1 + C2) = 1. We then choose the best k which returns the highest partial AUC from {0.5, 0.6, · · · , 2.4, 2.5}. For AsymBoost, we choose the best asymmetric factor k which returns the highest partial AUC from {2−1, 2−0.8, · · · , 21.8, 22}. For our approach, the regularization parameter is chosen from {10−5, 10−4.8, · · · , 10−3.2, 10−3}. We use vertical and horizontal decision stumps as the weak classifier. For each algorithm, we train a strong classifier consisting of 10 weak classifiers. We evaluate the partial AUC of each algorithm at [0, 0.2] FPRs.
Fig. 4 illustrates the boundary decision4 and the pAUC score. Our approach outperforms all other asymmetric classifiers. We observe that pAUCEnsT places more emphasis on positive samples than negative samples to ensure the highest detection rate at the left-most part of the ROC curve (FPR < 0.2). Even though we choose the asymmetric parameter, k, from a large range of values, both CS-AdaBoost and AsymBoost perform slightly worse than our approach. AdaBoost performs worst on this toy data set since it optimizes the overall classification accuracy.
Protein-protein interaction prediction In this experiment, we compare our approach with existing algorithms which optimize pAUC in bioinformatics. The problem we consider here is a protein-protein interaction prediction [54], in which the task is to predict whether a pair of proteins interact or not. We used the data set labelled ‘Physical Interaction Task in Detailed feature type’, which is publicly available on the internet5. The data set contains 2865 protein pairs known to be interacting (positive) and a random set of 237, 384 protein pairs labelled as non-interacting (negative). We use a subset of 85 features as in [31]. We randomly split the data into two groups: 10% for training/validation and 90% for evaluation. We choose the best regularization parameter form {1, 1/2, 1/5} by 5-fold cross validation. We repeat our experiments 10 times using the same regularization parameter. We train a linear classifier as our weak learner using LIBLINEAR [55]. We set the maximum number of
4. We set the threshold such that the false positive rate is 0.2. 5. http://www.cs.cmu.edu/∼qyj/papers sulp/proteins05 pages/
feature-download.html
12
boosting iterations to 100 and report the pAUC score of our approach in Table 4. Baselines include pAUCEns, SVMpAUC, SVMAUC, pAUCBoost and Asymmetric SVM. Our approach outperforms all existing algorithms which optimize either AUC or pAUC . We attribute our improvement over SVMtightpAUC [0, 0.1] [32], as a result of introducing a non-linearity into the original problem. This phenomenon has also been observed in face detection as reported in [56].
Comparison to other asymmetric boosting Here we compare pAUCEnsT against existing asymmetric boosting algorithms, namely, AdaBoost with Fisher LDA postprocessing [56] and AsymBoost [25]. The results of AdaBoost are also presented as the baseline. For each algorithm, we train a strong classifier consisting of 100 weak classifiers (decision trees of depth 2). We then calculate the pAUC score by varying the threshold value in the FPR range [0, 0.1]. For each algorithm, the experiment is repeated 20 times and the average pAUC score is reported. For AsymBoost, we choose k from {2−0.5, 2−0.4, · · · , 20.5} by cross-validation. For our approach, the regularization parameter is chosen from {1, 0.5, 0.2, 0.1} by cross-validation. We evaluate the performance of all algorithms on 3 vision data sets: USPS digits, scenes and face data sets. For USPS, we use raw pixel values and categorize the data sets into two classes: even digits and odd digits. For scenes, we divide the 15-scene data sets used in [57] into 2 groups: indoor and outdoor scenes. We use CENTRIST as our feature descriptors
and build 50 visual code words using the histogram intersection kernel [58]. Each image is represented in a spatial hierarchy manner. Each image consists of 31 subwindows. In total, there are 1550 feature dimensions per image. For faces, we use face data sets from [5] and randomly extract 5000 negative patches from background images. We apply principle component analysis (PCA) to preserve 95% total variation. The new data set has a dimension of 93. We report the experimental results in Table 5. From the table, pAUCEnsT demonstrates the best performance on all three vision data sets.
A full implementation of the algorithm and its extensions described in Section 3 is now available in the form of the lpopt tool, available with relevant documentation and examples at http://dbai.tuwien.ac.at/proj/lpopt. The following gives a quick outline of how to use the tool.
lpopt accepts as its input any form of ASP program that follows the ASP input language specification laid out in [10]. The output of the program in its default configuration is a decomposed program that also follows this specification. In addition, the tool guarantees that no language construct is introduced in the output that was not previously present in the input (cf. Section 3). Therefore, for example, a program without aggregates will not contain any aggregates as a result of rule decomposition. The following is a description of the parameters of the tool:
1 http://potassco.sourceforge.net
Usage: lpopt [-idbt] [-s seed] [-f file] [-h alg] [-l file] -d dumb: do not perform optimization -b print verbose and benchmark information -t perform only tree decomposition step -i ignore head variables when decomposing -h alg decomposition algorithm, one of {mcs, mf, miw (def)} -s seed initialize random number generator with seed. -f file the file to read from (default is stdin) -l file output infos (treewidth) to file
In what follows, we will briefly describe the most important features of the tool.
Tree Decomposition Heuristics. As stated in Section 2, computing an optimal tree decomposition w.r.t. width is an NP-hard problem. We thus make use of several heuristic algorithms, namely the maximum cardinality search (mcs), minimum fill (mf), and minimum induced width (miw) approaches described in [7], that yield tree decompositions that provide good upper bounds on the treewidth (i.e. on an optimal decomposition). It turns out that in practice, since rules in ASP programs are usually not overly large, these heuristics come close to, and often even yield, an optimal tree decomposition for rules. The heuristic algorithm to use for decomposition can be selected using the -h command line parameter. Since these heuristic approaches rely to some degree on randomization, a seed for the pseudo-random number generator can be passed along with the -s command line parameter.
Measuring the Treewidth of Rules. Theorem 1 allows us to calculate an upper bound on the size of the grounding of the input program. In order to do this, the maximal treewidth of any rule in an ASP program must be known. The -l switch of the lpopt tool allows this to be calculated. It forces the tool to perform tree decompositions on all rules inside an input ASP program, simply outputting the maximal treewidth (or, more accurately, an upper bound; see above) over all of them into the given file, and then exiting. Clearly, when a single ASP rule is given as input, this switch will output a treewidth upper bound of that single rule.
Recommended Usage
Assuming that a file enc.lp contains the encoding of a problem as an ASP program and that a file instance.db contains a set of ground facts representing a problem instance, the recommended usage of the tool is as follows:
cat enc.lp instance.db | lpopt | grounder | solver
In the above command, grounder and solver are programs for grounding and for solving, respectively. One established solver that we will use in the next section for our experimental evaluation is clasp [14]. If clasp is used as a solver together with the lpopt tool, we generally recommend the use of the --sat-prepro flag, which often speeds up the solving process substantially for decomposed rules generated by lpopt (by considering the fact that the truth values of all temporary atoms generated by lpopt are determined exactly by the rule body, and need never be guessed).
Recent research on multilingual SMT focuses on the use of pivot languages as a solution to overcome language resource limitations (Gispert & Mario 2006; Utiyama & Isahara 2007; Wu & Wang 2007; Bertoldi et al. 2008; Philipp, Alexandra & Ralf 2009; Paul et al. 2011; Nakov & Ng 2012). Pivot based machine translation refers to the use of an intermediate language, called pivot language, to translate from the source language to the target language. Unlike typical MT systems, which translate directly from source language to target language, pivot based systems translate sequentially from source to pivot and then from pivot to target language. (Utiyama & Isahara 2007) define two famous pivot strategies; i.e. Sentence translation and phrase translation. The sentence translation strategy means that a French sentence is translated into several English sentences, and then subsequently all translations should be translated into German separately. Then, we select the highest scoring sentence from the German sentences. The phrase translation strategy means that we directly construct a French-German phrase translation table from combining a French-English phrase-table and an English-German phrase-table. We assume that these French-English and EnglishGerman tables are built using the phrase model training in the baseline statistical machine translation system as just described in (Philipp, Alexandra & Ralf 2009), which means phrases are heuristically extracted fromword-level alignments produced by doing training on the corresponding parallel corpora (Utiyama & Isahara 2007). Another way of using pivot language is to use the existing source-pivot and pivot-targetmachine statistical systems to create new source-target bilingual and parallel corpus which have some similarity with the former method. However, instead of creating source-target phrase translation, the new source-target corpora are created using translation tables in primary corpora. Moreover, (Nakov & Ng 2012) use a third language, not literally as a pivot language, to improve their machine translation system. Their method is applicable when first, there is a small parallel corpus between source-target languages and there is sufficient parallel corpus between target-third languages and then, source and third languages are similar. The improvement is achieved by taking advantage of the vocabulary overlap and similarities between the source and
3
We compare the performance of the proposed method against RPCA (nuclear norm) [9] with synthetic data sets and real world application examples. In all the experiments, we use the default parameters recommended by Candès et al. [9] for both their approach and ours, i.e. λ = 1/ √ max(m,n) and ρ = 1.5, except if explicitly stated
7
otherwise. The code of the proposed method is available on our project website 3.
We begin with the problem of designing a one-shot wagering mechanism that incentivizes bettors to truthfully report their beliefs while maintaining their privacy. A wagering mechanism allows a set of bettors to each specify a belief about a future event and a monetary wager. Wagers are collected by a centralized operator and redistributed to bettors in such a way that bettors with more accurate predictions are more highly rewarded. Lambert et al. [2008] showed that the class of weighted score wagering mechanisms (WSWMs) is the unique class of wagering mechanisms to satisfy a set of desired axioms such as budget balance and truthfulness. In this section, we show how to design a randomized wagering mechanism that achieves -joint differential privacy while maintaining the nice properties of WSWMs in expectation.

In this paper, we focus on the generation of Chinese quatrain which has 4 lines and each line has the same length of 5 or 7 characters. We collected 76,859 quatrains from the Internet and randomly chose 2,000 poems for validation, 2,000 poems for testing, and the rest for training.
All the poems in the training set are first segmented into words using a CRF based word segmentation system. Then we calculate the TextRank score for every word. The word with the highest TextRank score is selected as the keyword for the line. In this way, we can extract a sequence of 4 keywords for every quatrain. From the training corpus of poems, we extracted 72,859 keyword sequences, which is used to train the RNN language model for keyword expansion (see section 3.2.2). For knowledge-based expansion, we use Baidu Baike1 and Wikipedia as the extra sources of knowledge.
After extracting four keywords from the lines of a quatrain, we generate four triples composed of (the keyword, the preceding text, the current line), for every poem. Take the poem in Table 1 as example, the generated triples are shown in Table 2. All the triples are used for training the RNN enc-dec model proposed in section 3.3.
The border distance feature is a two dimensional histogram counting the average number of moves in the sample played low or high in different game stages. The original inspiration was to help distinguishing between territorial and influence based moves in the opening, though it turns out that the feature is useful in other phases of the game as well.
The first dimension is specified by the move’s border distance, the second one by the number of the current move from the beginning of the game. The granularity of each dimension is given by intervals dividing the domains. We use the
ByDist = {〈1, 2〉, 〈3〉, 〈4〉, 〈5,∞)}
division for the border distance dimension (distinguishing between the first 2 lines, 3rd line of territory, 4th line of influence and higher plays for the rest). The move number division is given by
ByMovesSTR = {〈1, 10〉, 〈11, 64〉, 〈65, 200〉, 〈201,∞)}
for the strength dataset and
ByMovesSTY LE = {〈1, 16〉, 〈17, 64〉, 〈65, 160〉, 〈161,∞)}
for the style dataset. The motivation is to (very roughly) distinguish between the opening, early middle game, middle game and endgame. Differences in the interval sizes were found empirically and our interpretation is that in the case of style, we want to put bigger stress on opening and endgame (both of which can be said to follow standard patterns) on behalf of the middle game (where the situation is usually very complex).
If we use the ByMoves and ByDist intervals to divide the domains, we obtain a histogram of total |ByMoves| × |ByDist| = 16 fields. For each move in the games GC, we increase the count in the appropriate histogram field. In the end, the whole histogram is normalized to establish invariancy under the number of games scanned by dividing the histogram elements by |GC|. The resulting 16 numbers form the border distance feature.
The discovery of Higgs particle was announced on 4th July 2012. In 2013, Nobel Prize was conferred upon two scientists, Francois Englert and Peter Higgs for their contribution towards its discovery. A characteristic property of Higgs Boson is its decay into other particles through different processes.
At the ATLAS detector at CERN, very high energy protons are accelerated in a circular trajectory in both directions thus colliding with themselves and resulting in hundreds of particles per second. These events are categorized as either background or signal events. The background events consist of decay of particles that have already been discovered in previous experiments. The signal events are the decay of exotic particles: a region in feature space which is not explained by the background processes. The significance of these signal events is analyzed using different statistical tests. If the probability that the event has not been produced by a background process is well below a threshold, a new particle is considered to have been discovered. The ATLAS experiment observed a signal of the Higgs Boson decaying into two tau particles, although it was buried in significant amount of noise.
Late 1 Early
Entity ranking gains popularity since better rankings boost the performances of search engines, resulting in faster and more precise information retrieval. Wikipedia seems to be a good playground. The problem of ranking web pages could be easily reduced to Wikipedia entity ranking, plus that Wikipedia has a large collection of entities of different types [2] and Wikipedia contains valuable texts, human annotated tags, enriched links and a great structure to analyze ranking effectiveness. Certain ranking can serve as a pivot for extensibility or analysis [3], [4] or be used to answer queries in named entity recognition [5]. Additionally, retrieving real-life knowledge of reputations, fames and historical significance from entity ranking is also valuable [1].
Traditional ranking algorithms on Wikipedia basically consider two parts. One part focuses on information provided by raw text, including length of pages, word occurrences and topic distributions. LDA is among the most valuable approaches in such tasks [6]. Topics from LDA highly agree with real tags when finding most important feature words of a page [7]. The other part of ranking criteria relies heavily on links. Representatives include PageRank [8] and HITS [9]. PageRank is a link analysis algorithm that assigns high numerical weighting to pages that are referred to by many other pages and the structure of weight distribution conclude the importance of web pages. HITS defines hubs to be pages that have links pointing to many authority pages, serving as another important criteria in ranking. Recent work of Deepwalk [10] uses truncated random walks to learn latent representations by encoding social relations in a continuous vector space, which can be easily exploited by statistical models.
On the other hand, human annotated tags for pages benefits many related tasks. On Wikipedia, performance of entity ranking is improved by utilizing Wikipedia categories [11]. Links and topic difficulty prediction together with category information greatly boost the performance of entity ranking [12]. Additionally, Wikipedia categories can be used to boost search results in an ad hoc way [13]. Researchers also found that it is possible to analyze consistent semantic relationships
in the tags [14] and corresponding latent representations of raw text would help tag recommendations [7]. Given these facts, we believe a reversed application to rank Wikipedia categories based on latent representations of pages would help summarize the content in the text thus provides better and more precise descriptions of the Wikipedia pages.
Based on the accuracy in previous step, it was seen that support vector based classifier outperformed other machine learning techniques. The classifier was implemented using radial basis function (RBF) kernel. It is also referred as Gaussian RBF kernel. The kernel representation creates a decision boundary for the non-linear attributes in high dimensional space. The attributes are converted to linear form by mapping using this kernel function. An optimal hyperplane is constructed in feature space by considering the inner product of the kernel. Hyperplane is considered as optimal if it creates a widest gap from the input attributes to the target class. Furthermore, to achieve optimization C and gamma parameters are
margin creating a wider hyperplane, whereas the value of C being larger leads to overfitting called as hard margin. Hence the value of C must be selected by balancing between the soft and hard margin. Gamma is used for non-linear classifiers for constructing the hyperplane. It is used to control the shape of the classes to be separated. If the value of gamma is small it results in high variance and minimal bias producing a pointed thrust. While a bigger gamma value leads to minimal variance and maximum bias producing a broader and soft thrust. The values of C and gamma were optimized and selected for classification. The classified instances from RBF kernel is observed in Figure 5.
Based on the RBF classifier the prediction system was constructed in R. The bankruptcy dataset is initially loaded into the predictive system as a .csv file. The home page of predictive tool loaded with bankruptcy dataset is shown in Figure 6.
The system fetches the dataset and stores as a dataframe. Dataframe is a vector list used to store the data as a table in R. RBF-kernel SVM model is developed for the bankruptcy dataset. It is displayed in Figure 7.
After the model is developed, users can enter their data in the text input boxes for predicting bankruptcy. Each of the six input parameters have values as 1, 0.5 or 0 (positive, average and negative) respectively. Based on SVM model built for the initial dataset, the predictive system estimates the probability of bankruptcy as either B (Bankruptcy) or NB (Non Bankruptcy). The predicted outcome is saved as a .csv file in the local directory of user’s system to view the results. The output from the predictive system is shown in Figure 4, 5 and 6 respectively.

1) Data Representation on GVG: In this paper, our multilayer inputs is represented by the hierarchical generalized Voronoi graph (GVG) [4], a topological graph which has been successfully applied to navigation, localization and mapping. The general representation of GVG is composed of meet-points (locations of three-way or more equidistance to obstacles) and edges (feasible paths between meet-points which are two-way equidistance to obstacles) [28]. We adopt the same resolution as in our previous work [23] to construct the first layer GVG, and then higher layers of GVGs are constructed to describe the environment at different levels of granularity.
Let’s denote hierarchical GVGs as 〈G(1), G(2), · · · , G(L)〉 with G(l) = {V (l), E(l)}, where L denotes the number of layer, V (l) denotes nodes in layer l and E(l) denotes edges in layer l. For each layer, the independent sensing information is carried by the nodes in V (l), and the local connectivity is represented by the edges in V (l). More specifically, each node v(l)i ∈ V (l) corresponds to a sequence of range data r (l) i , assigned with the label y (l) i for the training maps, while e (l) ij ∈ E(l) reveals the connection between nodes v (l) i and v (l) j with distance d (l) ij .
The first layer G(1) = {V (1), E(1)} describes the environment in most detailed level of granularity with the originally adopted laser range data. As the laser range finder is of 180◦ field of view with 1◦ angular resolution, each node v (1) i ∈ V (1) corresponds to a sequence of range data r (1) i with 180 dimension. 2) Recursive Higher Layer Construction Algorithm: The construction of a higher layer GVG is implemented by fusing the information carried by connected nodes and then eliminating those marginal nodes. Algorithm 1 demonstrates the process of building higher layer GVG from a given lower layer. We make some definitions here for better explanation of the algorithm. N(vi) is defined as the directly connected neighbour set of vi, then vj ∈ N(vi) means there is an edge eij ∈ E between vi and vj . In addition, numel(N) is defined as the number of elements contained in N . Then numel(N(vi)) = 1 means vi is an “end-node”, i.e. the node without children. Further define M(vi) as the set of endnodes connected to vi, which is obviously M(vi) ⊆ N(vi). As seen from Algorithm 1, the construction process fuses the information carried by vi’s neighbors if vi is not an endnode (detailed fusion process is given in section III-A.3), otherwise vi is eliminated from the higher layer.
The L layer of data can be generated by recursively applying Algorithm 1 for L−1 times, which means by taking the output of lth layer as the input of (l + 1)th layer. This process can be illustrated in Figure 2 with L = 3. In this
Algorithm 1: Generate higher layer of input from the previous layer.
Input: G(l) = {V (l), E(l)}, the range data r(l)i on each node v(l)i Output: G(l+1) = {V (l+1), E(l+1)}, the range data r (l+1) i on each node v (l+1) i
1 for v(l)i ∈ V (l) do 2 if numel(N(v(l)i )) > 1 then 3 Preserve v(l)i , i.e. v (l+1) i = v (l) i ; 4 Construct r(l+1)i and r̂ (l+1) i from r (l) i and all of
the r(l)j carried by v (l) j ∈ N(v (l) i );
5 end 6 for v(l)j ∈ N(v (l) i ) do 7 if v(l)j ∈M(v (l) i ) then 8 Eliminate e(l)ij and v (l) j ;
9 else 10 Preserve e(l)ij , i.e. e (l+1) ij = e (l) ij ; 11 end 12 end 13 end
example, the end-nodes are denoted as red. It is to be noted that when moving to higher layers, the number of nodes in each layer decreases with the elimination of the end-nodes. More details are given in the caption of Figure 2.
An illustration of the different G(l) = {V (l), E(l)}, l = 1, 2, 3 layers constructed from a specific map is given in Figure 3. In the first layer, the nodes are distributed more densely in the map. When approaching higher layers, the tree structure represents more and more abstract information. It is to be noted that the number of the end-nodes (denoted as red asterisks) decreases as the progression of the layers which is a consideration for choosing the L = 3 in our experiments.
3) Data generation: This section describes the details about the construction of the higher-layer range data r(l+1)i and r̂(l+1)i , where the latter is generated from the former with fixed length. As stated in Algorithm 1, given v(l)i satisfying numel(N(v
(l) i )) > 1 (i.e. v (l) i is not end-node), range data
received at the respective nodes are integrated to achieve a better perception.
Given each v(l)i with numel(N(v (l) i )) > 1, firstly a local map is generated using occupancy grid mapping [29] based on the respective range data in lth layer, including r (l) j carried by v (l) j ∈ N(v (l) i ) and r (l) i . This is achieved by transforming all r(l)j to r (l) i ’s coordinate frame, which assumes the knowledge of the global robot poses at all times. In this local map, a virtual scan r(l+1)i is then generated by applying ray casting at position v(l)i with 1
◦ angular resolution, which is the same as the setting of the real laser range finder.
As the dimensions of the fused range data r(l+1)i could be different in various nodes, linear interpolation on the data is
then performed to keep same dimension of data throughout the process. This leads to an sequence r̂(l+1)i with fixed dimension of 360.
Acknowledging the fact that the interpolated points may not contain high information, a completeness rate, which is the proportion of the laser measured data (dimension of r(l)i to the whole 360◦ data (dimension of r̂(l)i ) is defined as:
q (l) i =
length(r (l) i ) length(r̂ (l) i )
(1)
where l = 2 · · ·L. This measure is used in the decision making process which is discussed in the next section, thus we denote q(1)i = 180/360 = 0.5 for uniformity when l = 1. However, we don’t apply linear interpolation to the layer 1 since the initial laser range data r(1)i always has the same dimension of 180 and is not necessary for linear interpolation. By applying this data pre-processing approach, the laser range data in layer 2 to layer L are kept in the fixed length of 360. Note that it is always r(l)i which is employed to construct the next layer, rather than the pre-processed r̂(l)i .
As an example, Figure 4 illustrates the construction of a sequence of input in layer 2 using the corresponding inputs in layer 1, followed by the result after linear interpolation. The details are given in the caption of Figure 4.
Gu et al. proposed a model free approach that used Q-learning to plan in continuous action spaces with deep neural networks, which they refer as Normalized advance functions (NAF). The idea behind NAF is to describe Q function in a way such that its maximum, argmaxaQ(st, at) can be obtained easily and analytically during the Q-learning update. The inherent processes are equivalent to that of Duelling networks as a separate value function V (s|θ) and advantage term are estimated. But, the difference is that the advantage in this case is parametrized as a quadratic function of non-linear features of the state:
Q(s, a|θ) = A(s, a|θ) + V (s|θ) (9)
A(s, a|θ) = −1 2 (a− µ(s|θ)P (s|θ)(a− µ(s|θ) (10)
P (s|θ) is a state-dependent, positive definite square matrix that is parametrized by L(s|θ)L(s|θ)T , where L is a lower triangular matrix whose entries come from the linear activations of the neural network. The rest of the network architecture is similar to that of DQN by Mnih et al. The paper also explored the use of a hybrid model based method by generating imagination rollouts from fitted dynamics model. This incorporated the inclusion of synthetic experience data from the fitted local linear feedback controllers and including them in the replay buffer of on-policy exploration of Q-learning.
The algorithms was tested in several robotic environments as shown in Fig.5. The
environments include the MuJoCo simulator tasks from Todorov et al. that include 3DOF robotic manipulation tasks where an arm gets reward based on the distance between the end effector and the object to be grasped. It also includes a sex joint
2D swimmer and a four legged ant. Policies learnt with this method showed more precise completion of tasks as compared to deep policy gradient methods [17].
POPULATION (CLI)
Given an observational data set D defined on a variable set V , the constraint-based algorithms consists of three key steps: (1) uncovering the entire skeleton, (2) discovering vstructures, and (3) orienting edges as many as possible, as shown in Figure 2.
Step 1: Skeleton identification. There are two approaches employed in skeleton identification, global approach and local approach. The global approach attempts to discover the skeleton including all variables [13, 90, 90, 106], while the local method mines local skeletons, i.e., the set of adjacent variables (parents and children) or Markov blanket of each vertex, then constructs a global skeleton by local skeletons [65, 75].
Step 2: V-structure discovery. This identifies v-structures by the found separation sets (i.e. conditional sets) between variables.
Step 3: Edge orientation After obtaining v-structures, the main orientation methods are listed as the followings.
• The orientation rules (i.e. Meek rules and Zhang Jiji’s rules) defined in [67, 118] using observational data. • Experimental orientation rules by using experimental data. The method involves manipulating a variable and assessing its statistical association with the undirected neighbors in order to determine the orientation [22, 97]. • Edge orientation using both observational data and experimental data, which first applies the rules in (1) to orient edges as many as possible, then orients the remaining unoriented edges with the method in (2) [37].
3
This phase evaluates the quality of the solutions obtained from the improvement phase against the other methods in the literature which can be seen in section 5.
IV. THE HARMONY SEARCH ALGORITHM FOR CB-CTT
This section describes details of steps as shown earlier in
Figure 1in implementing HSA into CB-CTT as follows:
In this section, first we explain the importance of fuzzy classification and then we introduce two approaches for improving LSTSVM using the fuzzy sets theory. Basic notations used in this section are as follows: samples of the positive and negative classes are represented by matrices A and B, respectively. A contains m1 positive samples and B contains m2 negative samples. Membership degrees are represented by µ and slack variables are represented by vector ξ. All equations will be presented in matrix form where for each matrix M , its transpose is represented by MT . e is a vector with arbitrary size and all its elements are equal to 1.
5.1 Comparison with other Models In order to verify the validity of our proposed method, we perform a comparison of our proposed method with two other previous methods including fuzzy TOPSIS Chen et al., 2006 and possibilistic TOPSIS Ye and Li, 2014 which also deal with fuzzy numbers. The results are shown as follows:
<Table 15 about here>
It is clear from Table 15 that the three methods have the similar results. Note that 𝑨𝟑 is best alternative according to all three models under fixed preference of criteria weights. These shows the method we proposed in this paper is reasonable.
5.2 Sensitivity Analysis Pamučar et al. 2015 pointed that MCDM method depend to a great extent on the values of the weight coefficients of the criteria, that is, on the relative importance attached to specific criteria. Sometimes, the final selections change when there is a very slight change in the weight coefficients of the criteria, because of which, the results of MCDM methods should as a rule be followed by an analysis of their sensitivity to these changes. In this research work, a sensitivity analysis was performed to assess how changes in the weights assigned to the criteria would change the ranking of the alternatives. Different weight priorities are given to the criteria (see Table 16) corresponding to different scenarios (1)-(6).
<Table 16 about here>
The resulting rankings are given in Table 17. The results show that assigning different weights (priorities) to the criteria leads to different rankings, i.e., the proposed model is sensitive to these weights, which is necessary for any MCDM models. On comparison of all scenarios we find 𝑨𝟑 is best choice except in scenario 5. However, scenario
<Fig. 2 about here>
1 and scenario 4 provide same ranking order though weight priorities have large change. From scenario (2) and (3), it is clear that a slight change in criteria weights will lead to alter 𝑨𝟐 and 𝑨𝟑 as second best choice. Similar arguments could be done for other pairwise comparisons among the scenarios. These comparisons confirms that using different weights to the criteria under consideration may help to choose the best design alternative in different context if needed.
<Table 17 about here>
ar X
iv :1
60 6.
01 28
3v 1
[ cs
.C L
] 3
J un

A query can be ambiguous in the light of all information that exists in multiple verticals. For example, if a member enters a query like “machine learning”, he or she could be interested in recruiting machine learning people, looking for
jobs related to machine learning, finding professional groups on the topic to join, discovering content on the topic or some of the use cases. To tackle this issue, we take a data-driven approach to personalize search results. For instance, if we know that the member is currently looking for a job, he or she is likely to be more interested in job results than the other verticals. Similarly, if the member is hiring machine learning scientists, people results should be more important to him or her.
Intents of searchers such as job seeking, hiring, content consuming etc. are inferred from their profiles and past behaviors. At a high level, if a user’s title is recruiter, he or she is likely to have hiring intent. Similarly, if a user is a final year student, he or she could have job seeking intent. Another source of data used to infer user intents is their recent activities. For example, if users recently searched or applied for jobs, they tend to have job seeking intent. We train a machine-learned model combining all of the signals to predict intents for all of the member base. The model is run on a daily basic to update members’ intents dynamically. It is worth noting that a member could have multiple intents at the same time.
A remaining challenge is that some evidence such as knowing a searcher has job seeking intent might be associated one or a few verticals (e.g. job vertical), but not all of them. Some evidence might be related to multiple verticals but with different levels of importance. To overcome this issue, we construct composite features, capturing both searcher intents and result categories including both verticals and result types (individual or block). For instance, the feature below only fires if the searcher has job seeking intent and the result is a block of jobs. Otherwise, it has value of zero. We create all of the combinations of intents and result categories and learn different weights for them. In essence, we let the learning algorithm associate each of the evidence with the categories and normalize them across the categories from training data.
f =  1 if searcher has job seeking intentand the result is job vertical block0 otherwise

Assume a countable set of atomic propositions Atm = {p, q, . . .} and a finite set of agents Agt = {1, . . . , n}.
The language of DL-S∗, denoted by LDL-S∗(Atm,Agt), is defined by the following grammar in Backus-Naur Form:
α ::= ⇑ | ⇓ | ⇒ | ⇐ | α;α′ | α ∪ α′ | α∗ |?ϕ ϕ ::= p | hi | ¬ϕ | ϕ ∧ ψ | [α]ϕ
where p ranges over Atm and i ranges over Agt . Other Boolean constructions ⊤, ⊥, ∨, → and ↔ are defined from p, ¬ and ∧ in the standard way. Instances of α are called spatial programs. When there is no risk of confusion we will omit parameters and simply write LDL-S∗ . The modal degree of a formula ϕ ∈ LDL-S∗ (in symbols deg(ϕ)) is defined in the standard way as the nesting depth of modal operators in ϕ. Let ‖ ϕ ‖ denote the size of ϕ. For all (negative or positive) integers x, let [⇑]x be the modality consisting of x consecutive [⇓] when x ≤ 0, otherwise let [⇑]x be the modality consisting of x consecutive [⇑]. Similarly for [⇒]x.
The formula hi is read “the agent i is here”, whereas [α]ϕ has to be read “ϕ is true in the position that is reachable from the current position through the program α”.
We will also be interested in sublanguages of LDL-S∗ . Given a set P of atomic propositions, a set I of agents and a set A of spatial programs, we denote the restriction of LDL-S∗(P, I) which only allows programs from A by LDL-S∗(P, I, A).
We solve the Socioscope optimization problem (9) with BFGS, a quasi-Newton method [20]. The gradient can be easily computed as
∇ = λLθ −HP>(r− 1), (10)
where r = (r1 . . . rm) is a ratio vector with ri = xi/hi, and H is a diagonal matrix with Hjj = η(θj , ψj).
We initialize θ with the following heuristic. Given counts x and the transition matrix P , we compute the least-squared projection η0 to ‖x−Pη0‖2. This projection is easy to compute. However, η0 may contain negative components not suitable for Poisson intensity. We force positivity by setting η0 ← max(10−4, η0) element-wise, where the floor 10−4 ensures that log η0 > −∞. From the definition η(θ, ψ) = exp(θ + ψ), we then obtain the initial parameter
θ0 = log η0 − ψ. (11)
Our optimization is efficient: problems with more than one thousand variables (n) are solved in about 15 seconds with fminunc() in Matlab.
In this case, we propose an ANN based scheme for assembly process classification, based on video data taken from Nissan factory. This is a self-trained SSL approach. The procedure is based on (a) a nonlinear classifier, formed using an island genetic algorithm, (b) a similarity-based classifier, and (c) a decision mechanism that utilizes the classifiers’ outputs in a semi-supervised way, minimizing the expert’s interventions. Such methodology will support the visual supervision of industrial environments by providing essential information to the supervisors and supporting their job.
We implemented two versions of our proposed SDP algorithm using XADDs — one that does not prune nodes of the XADD and another that uses a linear programming solver to prune unreachable nodes (for problems with linear XADDs) — and we tested these algorithms on KNAPSACK and two versions of the Mars Rover domain (adapted from [6]) that we call MARS ROVER LINEAR and MARS ROVER NONLINEAR.4
We now formalize the multi-dueling bandits problem. We inherit all notation from original dueling bandits setting (Section 2.1). The key difference is that the algorithm now selects a (multi-)set St of arms at each iteration t, and observes outcomes of duels between some pairs of arms in St. For example, in information retrieval this can be implemented via multi-leaving (Schuth et al., 2014) the ranked lists of the subset, St, of rankers and then inferring the relative quality of the lists (and the corresponding rankers) from user feedback.
In general, we assume the number of arms being dueled at each iteration is some fixed constantm = |St|. When m = 2, the problem reduces to the original dueling bandits setting. Extending the regret formulation from the original setting (1), we can write the regret as:
RT =
T ∑
t=1
∑
b∈St
φ(b1, b). (4)
The goal then is to select subsets of arms St so that the cumulative regret (4) is minimized. Intuitively, all arms have to be selected a small number of times in order to be explored, but the goal of the algorithm is to minimize the number of times when suboptimal arms are selected. When the algorithm has converged to the best arm b1, then it can simply choose St to only contain b1, thus incurring no additional regret.
Our setting differs from Brost et al. (2016) in two ways. First, we play a fixed, rather than variable, number of arms at each iteration. Furthermore, we focus on total regret, rather than the instantaneous average regret in a single iteration; in many applications (e.g., Sui & Burdick (2014)), playing each arm incurs its own regret .
Feedback Mechanisms. Simultaneously dueling multiple arms opens up multiple options for collecting feedback. For example, in some applications it may be viable to collect all pairwise feedback for all chosen arms St. In other applications, it is more realistic to only observe the “winner” of St, in which we observe feedback that one b ∈ St wins against all other arms in St, but nothing about pairwise preferences between the other arms.
Approximate Linearity. One assumption that we leverage in developing our approach is approximate linearity, which fully generalizes the linear utility-based dueling bandits setting studied in Ailon et al. (2014). For any triplet of bandits bi ≻ bj ≻ bk and some constant γ > 0:
φ(bi, bk)− φ(bj , bk) ≥ γφ(bi, bj). (5)
To understand Approximate Linearity, consider the special case when the preference function follows the form
φ(bi, bj) = Φ(ui − uj), where ui is a bounded utility measure of bi. Approximate linearity of φ(·, ·) is equivalent to having Φ(·) be not far from some linear function on its bounded support (see Figure 1), and is satisfied by any continuous monotonic increasing function. When Φ is linear, then our setting reduces to the utility-based dueling bandits setting of Ailon et al. (2014).2
The simulation results in Figures 11 & 12 are based on the 1-best recognized speech hypotheses with a relatively high WER (48.1%). With better speech recognition, the system will have better concept identification performance. To show the effect of speech recognition quality on online word acquisition and language understanding, we also perform Simulation 1 and Simulation 2 based on speech transcript. The simulation processes are the same as the ones based on the 1-best speech recognition except that word acquisition is based on speech transcript and CIR is evaluated also on speech transcript in the new simulations.
Figure 13 shows the CIR curves based on speech transcript during online conversation. With word acquisition, the system’s language understanding becomes better after more users have communicated to the system. This is consistent with the CIR curves based on the 1-best speech recognition. However, the CIRs based on speech transcript is much higher the CIRs based on the 1-best speech recognition, which verifies that speech recognition quality is critical to language understanding performance.
1. A visão de baixo para cima: premissas se unem (em árvores) para formar novas con-
clusões, e a derivação cresce no sentido de sua conclusão. Este pode ser chamado de ponto de vista dedutivo.
2. A visão de cima para baixo 5: a conclusão é o ponto de partida e regras de inferência
são usadas para atingir as premissas desejadas. Este pode ser chamado de ponto de vista de construção de demonstraçãos.
Depois de escolher regras de premissa única, existe uma situação de simetria: derivações são seqüências de inferências (simetria top-down), par e copar são o mesmo tipo de estrutura com dois nomes diferentes e seq é auto-dual.
A figura 3.1.4 apresenta as regras de inferência para o sistema BV.
◦ ↓ ◦
S{◦} ai↓
S [a, ā]
S〈[R,T ]; [R′, T ′ ]〉 q↓
S [〈R;R′〉, 〈T ;T ′〉]
S([R,T ], R′) s
S [(R,R′), T ]
Figura 3.4: Sistema BV
Definição 25. Os nomes das regras são:
◦ ↓ unidade (axioma lógico);
ai↓ interação atômica;
q↓ seq;
s switch.
Definição 26. Uma demonstração é uma derivação cuja regra mais acima seja o axioma lógico ◦ ↓. demonstraçãos são denotadas porΠ. Um sistema formal S demonstração R se existe em
S uma demonstração Π cuja conclusão seja R, escrita
− Π ‖ ‖ S
R (o nome Π pode ser omitido).
Dois sistemas são (fracamente) equivalentes se eles demonstraçãom as mesmas estruturas.
Exemplo 27. demonstração de uma estrutura em BV:
5Em inglês: bottom-up
3.1. O CÁLCULO DAS ESTRUTURAS 39
◦ ↓ ◦
ai↓ [a, ā]
= [a, (ā, ◦)]
ai↓ [a, (ā, [b̄, b])]
s [a, b̄, (ā, b)]
= 〈[a, b̄, (ā, b)]; ◦〉
ai↓ 〈[a, b̄, (ā, b)]; [c, c̄]〉
q↓ [〈[a, b̄]; c〉, 〈(ā, b); c̄〉]
Definição 28. O sistema flat BV (FBV) é o sistema BV sem a regra q↓. As estruturas permitidas no FBV são apenas estruturas flat (sem a estrutura seq).
A figura 3.1.4 apresenta as regras de inferência para o sistema FBV.
◦ ↓ ◦
S{◦} ai↓
S [a, ā]
S([R,T ], R′) s
S [(R,R′), T ]
Figura 3.5: Sistema FBV
Observação 29. O FBV é um subsistema do sistema BV, no sentido de que toda estrutura provável em FBV é provável em BV (mas a recíproca não é verdadeira). Mais do que isto, pode-se mostrar que o sistema BV é uma extensão conservativa do sistema FBV.
Exemplo 30. demonstração de uma estrutura em FBV:
◦ ↓ ◦
ai↓ [b, b̄]
= [(◦, b), b̄]
ai↓ [([a, ā], b), b̄]
s [a, b, (ā, b̄)]
3.1.5 O teorema de splitting
O teorema de splitting tem um papel muito importante neste trabalho por dois motivos:
1. Ele é a chave para a demonstração de eliminação da regra Cut para o cálculo das
estruturas para MLL + mix+ seq (o sistema BV);
“Isabelle’s user interface is no advance over LCF’s, which is widely condemned as ‘user-unfriendly’: hard to use, bewildering to beginners. Hence the interest in proof editors, where a proof can be constructed and modified rule-by-rule using windows, mouse, and menus. But Edinburgh LCF was invented because real proofs require millions of inferences. Sophisticated tools – rules, tactics and tacticals, the language ML, the logics themselves – are hard to learn, yet they are essential. We may demand a mouse, but we need better education and training.” (L.C. Paulson, in “Isabelle: The Next 700 Theorem Provers”, 1990)
Text categorization is an active research area of text mining where the documents are classified with supervised, unsupervised or semi-supervised knowledge. Traditionally, this task was solved manually, but such manual classification was expensive to scale and also labour intensive. Therefore, researchers have explored the use of machine learning approach to automatic classification of text documents[1]. Among various machine learning approaches in document categorization, most popular is supervised learning where underlying input-output relation is learned by small number of training data and then output values for unseen input points are predicted[2]. Various number of supervised learning techniques ,such as association rules[3], Neural Network[1], K-Nearest Neighbour[4], Decision Tree[5], Naïve Bays[6], Support Vector Machine[7], and N-grams[8], has been used for text document categorization.
With the popularity of Unicode system and growing use of Internet, Bangla text documents in digital domain have increased for the last decade. Besides, Bangla is spoken by about 245 million people of Bangladesh and two states of India, with being 7 th most spoken language[9]. In such a trend, it has created a need to organize Bangla documents intelligently so that users can easily identify required or related documents. To address this type of problem, text categorization process can be used successfully, such as text categorization conducted on English language for automated text retrieval [10, 11 ]. Although text categorization is well studies in other languages, few works have been done on language Bangla due to lack of resources, annotated corpora, name
dictionaries, morphological analyzer. One of the works in Bangla document categorization is applying N-gram technique to categorize Bangla newspaper corpus [12].
In this paper, we study how information from Bangla online text documents can be categorized using four supervised learning algorithm, namely DT (C4.5), KNN, NB, and SVM. These classification algorithms are chosen due to the fact that their success rate in text classification of various languages is promising. News corpus from various Bangla websites is employed to evaluate the capabilities of these methods in categorization of high dimensional, sparse and relatively noisy document features. In this paper categorization task is processed into three phases:
• Pre-processing phase: This phase includes tokenization, digit removal, punctuation removal, stop words removal and stemming.
• Feature extraction phase: It consist of statistical approach to extract relevant features from the documents.
• Processing steps: This last phase applies text classification algorithms to the extracted features to classify the documents into classes
The rest of the paper is organized in following orders: section 2 discusses about some related works in the field of text classifications, section 3 discusses four classification algorithms, section 4 presents our approaches of classifying Bangla documents. Test setup, solution, discussion and analysis of the experiments appear in section 5 and finally, section 6 concludes the paper.
Figure 15 shows the extended priming results of nine different models on three test collections in terms of four evaluation criteria defined in Section 6.1.3.
Figures 15(a) and 15(b) illustrate the extended priming results on MSD. In comparison to the performance on the test subset on MagTag5K shown in Figures 11(c) and 11(d), all the models including ours perform poorly, which demonstrates the challenge in learning transferrable semantics with limited training data. It is observed that LSA performs the best in MAP while our
model wins in AUC. In general, our model performs better at large ; and high recall levels while LSA outperforms others at small ; and low recall levels. In particular, CE always outperforms Siamese-CE. For the reason described in Section 6.4.1, a contextualized model is more sensitive to the usage patterns and intended meanings of terms in capturing concepts in context than a noncontextualized model that learns only global relatedness. In general, both the priming and the extended priming results on MSD suggest that a contextualized semantic model does not seem to transfer the semantics learnt from a less informative data set to those of richer information, intricate concepts and alternative intended term-use patterns.
Figures 15(c) and 15(d) show the extended priming results on LabelMe. It is observed that our model outperforms other models with the statistical significance (p-value < .01, Student's ttest). In general, the behavior of our model on this data set is remarkably similar to that on the test subset of Corel5K as shown in Figures 13(c) and 13(d) and Siamese-CE always performs better than CE. Unfortunately, all other models generally perform poorly; most of models yield the performance roughly identical to that of the Random model, as clearly seen in Figures 15(c) and 15(d). In general, the performance of our model is consistent in both the priming and the extended priming on this data set. Hence, the same conclusion on the priming can be drawn on the extended priming.
Figures 15(e) and 15(f) show the extended priming results on SUNDatabase. Once again, our model performs statistically significant (p-value < .01, Student's t-test) better than all other models; Siamese-CE wins over the runner-up 22% in MAP and 13% in AUC and, in particular, the significantly better performance at all 11 recall levels. It is also observed that all the models perform on this data set very similarly to those on LableMe.
In summary, the CCT experimental results demonstrate that the semantics learnt by our model trained on a data set may be transferable to other collections if different annotators have a high agreement on the intended meanings of terms and there are sufficient training documents reflecting various concepts and intended term-use patterns. Without meeting the requirement, all the models encounter the same problem in generalization of leant semantics across corpora.
Deep reinforcement learning has had many practical successes in game playing (Mnih et al. (2015),Silver et al. (2016)) and robotics (Levine & Abbeel (2014)). Our interest is in further exploring these algorithms in the context of environments with sparse rewards and partial observability. To this end, we investigate the use of two methods that are known to mitigate these problems: recurrent networks, which provide a form of memory summarizing past experiences, and eligibility traces, which allow information to propagate over multiple time steps. Eligibility traces have been shown empirically to provide faster learning (Sutton & Barto (2017), in preparation) but their use with deep RL has been limited so far (van Seijen & Sutton (2014), Hausknecht & Stone (2015)). We provide experiments in the Atari domain showing that eligibility traces boost the performance of Deep RL. We also demonstrate a surprisingly strong effect of the optimization method on the performance of the recurrent networks.
The paper is structured as follows. In Sec. 2 we provide background and notation needed for the paper. Sec. 3 describes the algorithms we use. In sec. 4 we present and discuss our experimental results. In Sec. 5 we conclude and present avenues for future work.
In recent years, the availability of video content online has been growing rapidly. YouTube alone has over a billion users, and every day people watch hundreds of millions of hours on YouTube (Youtube Blog Statistics, 2008). With the rapid growth of available content and the rising popularity of online video platforms, accessibility and discoverability become increasingly important. Specifically, in the video search scenario, it is crucial that the platforms enable effective discovery of relevant video content.
Previous research, indeed, has dedicated a great deal of attention to video retrieval (Over et al., 2015), a task that is much harder than document retrieval due to the seman-
1Work done at Google.
tic mismatch between the keyword queries and the video frames. Therefore, video classification has been a prominent research topic (Karpathy et al., 2014; Brezeale & Cook, 2008), as well as detecting semantic concepts within video material (Jiang et al., 2007). Both video categories and semantic concepts can be used for relevance matching between the query and parts of the video (Snoek & Worring, 2008).
In this paper, we extend this existing research, and propose a system for query-based video summarization. Our system creates a brief, visually attractive trailer, which captures the parts of the video (or a collection of videos) that are most relevant to the user-issued query. For instance, for a query Istanbul, and a video describing a trip to Istanbul, our system will construct an informative trailer, highlighting points of interest (Hagia Sophia, Blue Mosque, Grand Bazaar), and skipping non-relevant content (shots of the tour bus, hotel room interior, etc.).
The applications for such a system are numerous, as such trailer skips the extraneous parts of a video, thus enhancing the user experience and saving time. For instance, it can better inform user decisions, and save time and money for services where users pay per view or pay for mobile data consumption. A trailer can also serve as an alternative to the standard thumbnail, a still image that represents a video in the query result list. It could potentially better capture the relevant contents of the full video than a single thumbnail image.
The query-based summarization done by our system has two main objectives. First, the trailer will capture a semantic match between the query and the video frames that goes beyond simple entity matching. For instance, for a query racecar, a frame containing a car driving on a racetrack will be more relevant than a frame containing a stationary car. We achieve this semantic match via the use of entity embeddings (Levy & Goldberg, 2014). Second,
ar X
iv :1
60 9.
01 81
9v 1
[ cs
.L G
] 7
S ep
the trailer will be visually attractive. For instance, we will prefer frames containing visually prominent, clear depictions of relevant content. We will also prefer summaries that have smooth contiguous frame transitions, similar to human-edited movie trailers.
The overall approach – combining semantic match and visual similarities – is outlined in Figure 1. In summary, the main contributions of this paper are:
1. A robust approach for semantically matching keyword queries to video frames, using entity embeddings trained on non-video corpora.
2. A scalable method for detecting prominent visual clusters within videos based on label propagation.
3. An efficient and effective graph-based approach that combines semantic and visual signals to construct trailers, which are both relevant and visually appealing.
4. Detailed empirical evaluation of the proposed method with comparison to several baseline systems.
The expected loss learning criterion for structured prediction is defined as a minimization of the expectation of a task loss function with respect to the conditional distribution over structured outputs (Gimpel and Smith, 2010; Yuille and He, 2012). More formally, let X be a structured input space, let Y(x) be the set of possible output structures for input x, and let ∆y : Y → [0, 1] quantify the loss ∆y(y′) suffered for making errors in predicting y′ instead of y; as a rule, ∆y(y
′) = 0 iff y = y′. Then, for a data distribution p(x, y), the learning criterion is defined as minimization of the expected loss
Ep(x,y)pw(y′|x) [∆y(y ′)] =
∑
x,y
p(x, y) ∑
y′∈Y(x)
∆y(y ′)pw(y ′|x). (1)
Assume further that output structures given inputs are distributed according to an underlying Gibbs distribution (a.k.a. conditional exponential or log-linear model)
pw(y|x) = exp(w⊤φ(x, y))/Zw(x),
where φ : X × Y → Rd is a joint feature representation of inputs and outputs, w ∈ Rd is a weight vector, and Zw(w) is a normalization constant.
The natural rule for prediction or inference is according to the minimum Bayes risk principle
ŷw(x) = argmin y∈Y(x)
∑
y′∈Y(x)
∆y(y ′)pw(y ′|x). (2)
This requires an evaluation of ∆y(y′) over the full output space, which is standardly avoided in practice by performing inference according to a maximum a posteriori (MAP) criterion (which equals criterion (2) for the special case of ∆y(y′) = 1[y 6= y′] where 1[s] evaluates to 1 if statement s is true, 0 otherwise)
ŷw(x) = argmax y∈Y(x)
pw(y|x) (3)
= argmax y∈Y(x)
w⊤φ(x, y).
Furthermore, since it is unfeasible to take expectations over the full space X × Y to perform minimization of objective (1), in the full information case the data distribution p(x, y) is approximated by the empirical distribution p̃(x, y) = 1
T
∑T
t=0 1[x = xt]1[y = yt] for i.i.d. training data {(xt, yt)}Tt=0. This yields the objective
Ep̃(x,y)pw(y′|x) [∆y(y ′)] =
1
T
T ∑
t=0
∑
y′∈Y(xt)
∆yt(y ′)pw(y ′|xt). (4)
While being continuous and differentiable, the expected loss criterion is typically nonconvex. For example, in SMT, expected loss training for the standard task loss BLEU leads to highly non-convex optimization problems. Despite of this, most approaches rely on gradientdescent techniques for optimization (see Och (2003), Smith and Eisner (2006), He and Deng (2012), Auli et al. (2014), Wuebker et al. (2015), inter alia) by following the opposite direction of the gradient of (4):
∇Ep̃(x,y)pw(y′|x) [∆y(y′)]
= Ep̃(x,y)
[
Epw(y′|x)[∆y(y ′)φ(x, y′)]− Epw(y′|x)[∆y(y′)] Epw(y′|x)[φ(x, y′)]
]
= Ep̃(x,y)pw(y′|x)
[
∆y(y ′)(φ(x, y′)− Epw(y′|x)[φ(x, y′)])
]
.
In this paper, we introduced an iterative bootstrapping procedure for learning skills. This approach is similar to (and partly inspired by) skill chaining Konidaris and Barto [2009]. However, the heuristic approach applied by skill chaining may not produce a near-optimal policy even when the skill learning error is small. We provide theoretical results for LSB that directly relate the quality of the final policy to the skill learning error. LSB is the first algorithm that provides theoretical convergence guarantees whilst iteratively learning a set of skills in a continuous state space. In addition, the theoretical guarantees for LSB enable us to interlace skill learning with Policy Evaluation (PE). We can therefore perform PE whilst learning skills and still converge to a near-optimal solution.
In each of the experiments, LSB converges in very few iterations. This is because we perform policy evaluation in between each skill update, causing the global value function to converge at a fast pace. Initializing LSB in the partition class containing the goal state also results in value being propagated quickly to subsequent partition classes and therefore fast convergence. However, LSB can be initialized from any partition class.
One limitation of LSB is that it learns skills for all partition classes. This is a problem in high-dimensional statespaces. However, the problem can be overcome, by focusing only on the most important regions of the state-space. One way to identify these regions is by observing an expert’s demonstrations Abbeel and Ng [2005], Argall et al.
[2009]. In addition, we could apply self-organizing approaches to facilitate skill reuse Moerman [2009]. Skill reuse can be especially useful for transfer learning. Consider a multi-agent environment Garant et al. [2015] where many of the agents may be performing similar tasks which require a similar skill-set. In this environment, skill reuse can facilitate learning complex multi-agent policies (co-learning) with very few samples.
Given a task, LSB can learn and combine skills, based on a set of rules, to solve the task. This structure of learned skills and combination rules forms a generative action grammar Summers-Stay et al. [2012] which paves the way for building advanced skill structures that are capable of solving complex tasks in different environments and conditions.
One exciting extension of our work would be to incorporate skill interruption, similar to option interruption. Option interruption involves terminating an option based on an adaptive interruption rule Sutton et al. [1999]. Options are terminated when the value of continuing the current option is lower than the value of switching to a new option. This also implies that partition classes can overlap one another, as the option interruption rule ensures that the option with the best long term value is always being executed. Mankowitz et al. [2014] interlaced Sutton’s interruption rule between iterations of value iteration and proved convergence to a global optimum. In addition, they take advantage of faster convergence rates due to temporal extension by adding a time-based regularization term resulting in a new option interruption rule. However, their results have not yet been extended to use with function approximation. Comanici and Precup [2010] have developed a policy gradient technique for learning the termination conditions of options. Their method involves augmentation of the state-space. However, the overall solution converges to a local optimum.
Table 1 presents the results on the two comparison sets. It shows for each evaluation function and comparison sets the global error (c.f. Section 3.4).
These results reveal that the learnt function has allowed an improvement of the global error: for both learning and testing sets, the global errors of the initial function are higher than for the learnt function. However, the quality improvement after the use of the method is only of 11% for the test set. An explanation is the lack of constraints (for example, an orientation constraint). For example, when a comparison composed of two building generalisations, which differ only in term of orientation is shown, the user always prefers the one whose orientation is close to the building initial orientation. Because there is no orientation constraint taken into account into the evaluation function, the difference of the two generalisations can not be measured by the system, and the reason of the different assessment by the user remains ignored. In this context, our approach, through an examination of the incompatible comparisons, can help to determine some important missing constraints and identify faulty ones.
To determine whether intensity of the text is high or low supervised learning approach has been used. Have extracted 184 features and a support vector Model has been trained.
Table 1: Studies on multi-step-ahead energy forecasting
Table 2: Prediction accuracy measures for hold-out sample
Table 3: SPA test for various prediction models
Table 4: Required time of three models for each prediction horizon
Fig. 1: The EMD with slope-based method
Fig. 2: The proposed EMD-SBM-FNN modeling framework for multi-step-ahead forecasting
Fig. 3: Experiment procedures for multi-step-ahead forecasting of crude oil price
*Laboratoire LITIS - EA 4108
We now return to discussing derivations of good kernels. Density estimates are more effective for semi-supervised learning when the kernel meaningfully captures also the weak relations. This is because labeled examples are a small fraction of all examples, and relations of most points to seed points are weak. The raw data, however, typically only includes accurate strong relations wij . At the core of the semi-supervised learning techniques is the specification of a quality “all-range” kernel that is computed from the strong relations.
A typical raw data is provided as a set of pairwise interactions between entities: Friendships in a social network, word co-occurrence relations, product purchases, movie views by users, or features in images or documents. The interactions can have associated strengths determined by frequency, recency, confidence, or importance. The included interactions generally correspond to strong relations. A common approach to enhance such data is to embed the entities in a lower dimensional Euclidean space so that larger inner products, or closer distances, between the embeddings fit the provided interactions [30, 36]. Such embeddings are hugely successful in revealing other likely strong interactions that were not explicit in the data (and thus are useful for recommendations). They do define a dense kernel of pairwise similarities, but the weak relations do not seem to be captured accurately with this approach. A common technique in semi supervised learning is to sparsify this relation (often without explicitly computing the dense representation) by only retaining the strong relations – This is done by only including edges for pairs where one is the k nearest neighbor of another (i, j, if wij is one of the top k values in the ith row or wji is one of the top k in the jth row) or by using r-neighborhoods [52, 25, 54, 55]. The end product is an enhancement of the strong affinities we started with, but other techniques are still needed for capturing weak relations.
To visualize a kernel that is accurate for strong relations but not for weak ones, consider points that (approximately) form a dense manifold that lies in a higher dimensional space [46, 52, 55, 47, 5].
ar X
iv :1
60 3.
09 06
4v 1
[ cs
.L G
] 3
0 M
ar 2
01 6
Semantically, we seek κ that is with respect to distances over the manifold but our starting w corresponds to distances in the higher dimensional space and is therefore accurate only for strong relations.
Given an image I, let us consider a tagging algorithm that returns a set of objects/tags/concepts detected in the images with their associated confidence value. The confidence values of each concept form a semantic feature vector to be used for the photo streams segmentation. Usually, the number of concepts detected for each sequence of images is large (often, some dozens). Additionally, redundancies in the detected concepts are quite often due to the presence of
synonyms or semantically related words. To manage the semantic redundancy, we will rely on WordNet [26], which is a lexical database that groups English words into sets of synonyms, providing additionally short definitions and word relations.
Given a day’s lifelog, let us cluster the concepts by relying on their synset ID in WordNet to compute their similarity in meaning, and following, apply clustering (e.g. Spectral clustering) to obtain 100 clusters. As a result, we can semantically describe each image in terms of 100 concepts and their associated confidence scores. Formally, we first construct a semantic similarity graph G = {V,E,W}, where each vertex or node vi ∈ V is a concept, each edge eij ∈ E
represents a semantic relationship between two concepts, vi and vj and each weight wij ∈ W represents the strength of the semantic relationship, eij . We compute each wij by relying on the meanings and the associated similarity given by WordNet, between each appearing pair. To do so, we use the max-similarity between all the possible meanings mki and m r j in Mi and Mj of the given pair of concepts vi and vj :
wij = max mki ∈Mi,mrj∈Mj
sim(mki ,m r j).
To compute the Semantic Clustering, we use their similarity relationships in the spectral clustering algorithm to obtain 100 semantic concepts, |C| = 100. In Fig. 3, a simplified example of the result obtained after the clustering procedure is shown. For instance, in the purple cluster, similar concepts like ’writing’, ’document’, ’drawing’, ’write’, etc. are grouped in the same cluster, and ’writing’ is chosen as the most representative term. For each cluster, we choose as its representative concept, the one with the highest sum of similarities with the rest of elements in the cluster.
The semantic feature vector fs ∈ R|C| for image I is a 100-dimensional array, such that each component fs(I)j of the vector represents the confidence with which the j-th concept is detected in the image. The confidence value for the concept j, representing the cluster Cj , is obtained as the sum of the confidences rI of all the concepts included in Cj that have also been detected on image I:
fs(I)j = ∑
ck∈{Cj}
rI(ck)
where CI is the set of concepts detected on image I, Cj is the set of concepts in cluster j, and rI(ck) is the confidence associated to concept ck on image I. The final confidence values are normalized so that they are in the interval [0, 1].
Taking into account that the camera wearer can be continuously moving, even if in a single environment, the objects that can be appearing in temporally adjacent images may be different. To this end, we apply a Parzen Window Density Estimation method [27] to the matrix obtained by concatenating the semantic feature vectors along the sequence to obtain a smoothed and temporally coherent set of confidence values. Additionally, we discard the concepts with a low variability of confidence values along the sequence which correspond to non-discriminative concepts that can appear on any environment. The low variability of confidence value of a concept may correspond to constantly having high or low confidence value in most environments.
In Fig. 4, the matrix of concepts (semantic features) associated to an egocentric sequence is shown, displaying only the top 30 classes. Each column of the matrix corresponds to a frame and each row indicates the confidence with which the concept is detected in each frame. In the first row, the ground truth of the temporal segmentation is shown for comparison purposes. With this representation, repeated patterns along a set of continuous images correspond to the set of concepts that characterizes an event. For instance, the first frames of
the sequence represent an indoor scene, characterized by the presence of people (see examples Fig. 5). The whole process is summarized in Fig. 6.
In order to consider the semantics of temporal segments, we used a concept detector based on the auto-tagging service developed by Imagga Technologies
Ltd. Imagga’s auto-tagging technology 1 uses a combination of image recognition based on deep learning and CNNs using very large collections of human annotated photos. The advantage of Imagga’s Auto Tagging API is that it can directly recognize over 2,700 different objects and in addition return more than 20,000 abstract concepts related to the analyzed images.
The work of [Das99] was the first to give theoretical guarantees for the problem of learning a mixture of Gaussians under separation conditions. He showed that one can learn a mixture of k spherical Gaussians provided that the separation between the cluster means is Ω̃( √ n(σr +σs)) and the mixing weights are not too small. Here σ2r denotes the maximum variance of cluster r along any direction. This separation was improved to Ω̃((σr + σs)n
1/4) by [DS07]. Arora and Kannan [SK01] extended these results to the case of general Gaussians. For the case of spherical Gaussians, [VW02] showed that one can learn under a much weaker separation of Ω̃((σr + σs)k
1/4). This was extended to arbitrary Gaussians by [AM05] and to various other distributions by [KSV08], although requiring a larger separation. In particular, the work of [AM05] requires a separation of Ω((σr + σs)( 1√
min(wr ,ws) + √ k log(kmin{2k, n}))) whereas [KSV08] require a separation of Ω̃( k3/2 w2min (σr+σs)).
Here wr’s refer to the mixing weights. [CR08a, CR08b] gave algorithms for clustering mixtures of product distributions and mixtures of heavy tailed distributions. [BV08] gave an algorithm for clustering the mixture of 2 Gaussians assuming only that the two Gaussians are separated by a hyperplane. They also give results for learning a mixture of k > 2 Gaussians. The work of [KMV10] gave an algorithm for learning a mixture of 2 Gaussians, with provably minimal assumptions. This was extended in [MV10] to the case when k > 2 although the algorithm runs in time exponential in k. Similar results were obtained in the work of [BS10] who can also learn more general distribution families. The work of [CO10] studied a deterministic separation condition required for efficient clustering. The precise condition presented in [CO10] is technical but essentially assumes that the underlying graph over the set of points has a “low rank structure” and presents an algorithm to recover this structure which is then enough to cluster well. In addition, previous works (e.g. [Sch00, BBG09]) addressed the problem of clustering from the viewpoint of minimizing the number of mislabeled points.
There has been an extensive line of work on approximation algorithms for the k-means problem ([OR00, BHPI02, dlVKKR03, ES04, HPM04, KMN+02]). The current best guarantee is a (9 + ǫ)-approximation algorithm of [KMN+02] (with a much simpler analysis in [GT08]) if polynomial dependence on k and the dimension d is desired.4 Another popular algorithm for k-means is the Lloyd’s heuristics ([Llo82]). This heuristics, combined with a careful seeding of centers, has been shown to have good performance if the data is well separated (see [ORSS06]), or to provide O(log(k))-approximation in general [AV07]. The separation-based results of [ORSS06] were improved by [ABS10].
alternative to minimaxing. Artif. Intell., 21(1-2):221– 244, 1983.
In this section we present approximate view-based propagators and relate them to the completeness classes introduced in table 1 on page 7.
Definition 3.19 (ΦΨ−complete view-based propagator). A ΦΨ view-based propagator for a constraint c ◦ f is defined as
π̌ΦΨc◦f (S) = ϕ̂f ( π11?c ◦ ϕ+f ( SΦ ) , SΦ )Ψ ∩ S, where Φ,Ψ ∈ {1, δ, β}
Intuitively, ΦΨ-completeness of a view-based propagator for a decomposable constraint of the form c ◦ f1 · · · ◦ fm is obtained by approximating the input of the image function ϕ+f and the output of the object function ϕ̂f , and not approximating the remaining view functions or propagators involved. For these view-based propagators the following property can be proved [8].
Proposition 3.20. A ΦΨ view-based propagator for a constraint c ◦ f is a ΦΨ-complete and idempotent propagator for c ◦ f .
The use of the above proposition is rather limited since it applies only to a propagator πc that is 11-complete and idempotent. Achieving such completeness is usually intractable in time and/or space in general, inequality constraints being a notable exception. Moreover other approximations are performed in practical view-based propagators, which are now presented.
Table 3 shows the number of interactions during design episodes for each participant and each design problem. Where a participant did not conduct a design episode due to time constraints, this is shown as “-”. Participants evaluated candidate software designs on a total of 962 interactions. Immediately apparent is the great variation in the number of interactions among the participants, reflecting a variety of individual approaches. Numbers for CBS and GDP are higher than SC as the experimental design meant that most participants undertook two design episodes for these design problems. Thus to analyze these figures, the numbers of interactions for each design problem have been examined further, and the results are summarized in Table 4 where standard deviations are shown in parentheses. The highlights of Table 4 are twofold: firstly, there is a high variation in number of interactions for the CBS and GDP design problems when compared to SC, and secondly, the mean number of interactions for CBS and GDP are similar and much higher than that for SC. Wilcoxon Signed Ranks Test confirms that while the differences between CBS and SC, and GDP and SC are significant (p = .027 and p = .028 respectively), the differences between CBS and GDP are not. To explain these outcomes, if we look to the numbers of classes in each of the design problems, we find that the number of classes in candidate design solutions for CBS and GDP is 5, whereas for SC the number is 15. Therefore, it seems likely that the cognitive load on the software design is higher for the SC design problem, accounting for the significant differences in the number of interactions.
In this paper, we propose a new heuristic named Approximate Muscle guided Beam Search (AMBS) for AP3 problem. This algorithm combines the approximate muscle and beam search. AMBS includes two phases, the sampling phase, in which the approximate muscle is obtained, and the search phase, in which beam search is employed. In this way, the solution space size of AP3 problem is decreased significantly, and the search time is reduced sharply, too. Thus, AMBS can achieve a good trade-off between the solution quality and the running time. Experimental results indicate that the new algorithm is able to obtain solutions with competitive quality, and it can be employed on large-scale instances.
In Figure 2, we show the performance on the synthetic data. Note that both the proposed scheme and [5] effectively capture groups of anomalies when the maximum feature subset order is two. The first captured cluster (sample subset) consists of more than 95% anomalous samples on average. However, as the maximum feature subset order increases, the “independence tests” approach drops significantly in performance. This is because too many (assumed to be independent) pairwise tests create many redundant features that are all used to evaluate cluster anomalousness; use of these redundant features de-emphasizes, within the score function, the important (low-order) feature subset. Also, we see an early advantage of using cluster-specific DTs, compared to the single Bayesian Net approach. It appears that if an anomalous process is strictly generated from a low order subspace and normal in other feature dimensions (as is the case in this experiment) our cluster-specific DT approach outperforms a single Bayesian Net approach.
In Figure 3 a), we show the performance for normal-P2P discrimination. Compared to [5], which degrades in performance as more and more tests are included, we see superior performance for the proposed method. There is a large batch of anomalous samples captured at maximum order 6 by the proposed method, but both [9] and [5] did not capture this group effectively, as seen in the top 100 precision figure. Also, both of these methods are outperformed by the GMM baseline method. In Figure 3 b), we show the performance for normal-Zeus discrimination. Again, at maximum feature subset order 6 the proposed method captures a large portion of the anomalous flows – more than 50 Zeus flows were captured out of the first 100 flows detected
November 4, 2015 DRAFT
13
by the proposed method. [5] performs poorly in this experiment, and again we observed that as the number of tests increase, the independence assumption degrades the detection performance. The single Bayesian Net approach in [9] also performs relatively poorly on this dataset.
As mentioned above in Section 2.1.1, the evaluation function is a very important part of a chess engine, and almost all improvements in playing strength among the top engines nowadays come from improvements in their respective evaluation functions.
The job of the evaluation functions is to assign scores to positions statically (without looking ahead). Evaluation functions contain most of the domain-specific knowledge designed into chess engines.
In this project, we will develop an evaluation function based on a machine learning approach. However, before we do that, we will take a look at the evaluation function of Stockfish [10], an open source chess engine that is currently the strongest chess engine in the world. Examining an existing state-of-art evaluation function can help us design an effective feature representation for our machine learning effort.
Stockfish’s evaluation function consists of 9 parts -
• Material: Each piece on the board gets a score for its existence. Synergetic effects are also taken into account (for example, having both bishops gives a bonus higher than two times the material value of a bishop), and polynomial regression is used to model more complex interactions.
• Piece-Square Tables: Each piece gets a bonus or penalty depending on where they are, independent of where other pieces are. This evaluation term encourages the engine to advance pawns and develop knights and bishops for example.
• Pawn Structure: The position is scanned for passed, isolated, opposed, backward, unsupported, and lever pawns. Bonuses and penalties are assigned to each feature. These features are all local, involving 2-3 adjacent pawns.
• Piece-specific Evaluation: Each piece is evaluated individually, using piece-type-specific features. For example, bishops and knights get a bonus for being in a ”pawn outpost”, rooks get a bonus for being on an open file, semi-open file, or the same rank as enemy pawns.
• Mobility: Pieces get bonuses for how many possible moves they have. In Stockfish’s implementation, squares controlled by opponent pieces of lesser value are not counted. For example, for bishop mobility, squares controlled by enemy pawns are not included, and for queen mobility, squares controlled by bishops, knights, or rooks are not included. Each piece type has a different set of mobility bonuses.
• King Safety: Bonuses and penalties are given depending on number and proximity of attackers, completeness of pawn shelter, and castling rights.
• Threat: Undefended pieces are given penalties, defended pieces are given bonuses depending on piece type, and defender type.
• Space: Bonuses are given for having ”safe” empty squares on a player’s side.
• Draw-ish-ness: Certain material combinations often result in draws, so in these cases, the evaluation is scaled toward 0.
It is quite a complicated function with a lot of hand-coded knowledge. Most engines don’t have evaluation functions that are nearly as extensive, because it is difficult to tune such a high number of parameters by hand.
The first ever human vs. computer no-limit Texas hold ’em competition took place from April 24–May 8, 2015 at River’s Casino in Pittsburgh, PA, organized by Carnegie Mellon University Professor Tuomas Sandholm. 20,000 hands of two-player no-limit Texas hold ’em were played between the computer program “Claudico” and four of the top human specialists in this variation of poker, Dong Kim, Jason Les, Bjorn Li, and Doug Polk (so 80,000 hands were played in total).1
To evaluate the performance, we used “duplicate” scoring, in which the same hands were played twice with the cards reversed to reduce the role of luck (and thereby the variance).2 Each human was given a partner, who played the identical hands against Claudico with the cards reversed. Polk was paired with Les, and Kim was paired with Li. The players played in two different rooms of the casino simultaneously, with one player from each of the pairings in each room (so that both players in each room had the same cards, while both players in the other room had the cards that Claudico had in the first room). In total, the humans ended up winning the match by 732,713 chips, which corresponds to a win rate of 9.16 big blinds
∗The competition was organized by Professor Tuomas Sandholm, and the agent was created by Noam Brown, Sam Ganzfried, and Tuomas Sandholm. This article contains the author’s personal thoughts on the event. Some of the work described in this article was performed while the author was a student at Carnegie Mellon University before the completion of his PhD. The article reflects the views of the author alone and not necessarily those of Carnegie Mellon University. The work done at Carnegie Mellon University was supported by the National Science Foundation under grants IIS-1320620, IIS-0964579, and CCF-1101668, as well as XSEDE computing resources provided by the Pittsburgh Supercomputing Center.
1Doug Polk tweeted a list on 2/28/2015 ranking himself at number one, Kim number two, Li number three, and Les (according to speculation on his screenname) within the top ten, https://twitter.com/DougPolkPoker/status/ 571647246074163201. Several other players have also created lists placing Polk at number one (e.g., Nick Frame tweeted one on 9/28/2014, https://twitter.com/TCfromUB/status/516396810433486848). While these rankings are largely subjective, they are based on some objective factors; e.g., if player A beats player B over a significant sample of hands, or if player A is willing to play against player B but player B refuses to play against player A (i.e., by leaving the table when player A sits in against him), then these indicate an advantage of player A over player B. If one player contests the ranking and believes he is better than someone ranked higher, then a challenge can ensue (e.g., Kim and Frame played a challenge match in February 2015, https://www.pokerstars.com/en/blog/2015/dong-donger-kim-kyu-and-nicktcfromub-frame-on-their-unique-heads-up-challenge-up-challenge-154091.shtml).
2For example, suppose human A has pocket aces and the computer has pocket kings, and A wins $5,000. This would indicate that the human outplayed the computer. However, suppose human B has the pocket kings against the computer’s pocket aces in the identical situation and the computer wins $10,000. Then, taking both of these results into account, an improved estimator of performance would indicate that the computer outplayed the human, after the role of luck in the result was significantly reduced.
ar X
iv :1
51 0.
08 57
8v 1
[ cs
.G T
] 2
9 O
ct 2
01 5
per 100 hands (BB/100),3 a common metric used to evaluate performance in poker. This was a relatively decisive win for the humans and was statistically significant at the 90% confidence level, though it was not statistically significant at the 95% level.4
The chips were just a placeholder to keep track of the score and did not represent real money; the humans were paid at the end from a prize pool of $100,000 which had been donated from River’s Casino and Microsoft Research. The human with the smallest profit over the match received $10,000, while the other humans received $10,000 plus additional payoff in proportion to the profit above the lowest profit. Formally, let x1, x2, x3, x4 denote the profits of the four humans from highest to smallest, and let pi denote the corresponding payoffs. Then
If x1 > x4 (1)
p1 = $10, 000 + $60, 000 · x1 − x4
x1 + x2 + x3 − 3x4 (2)
p2 = $10, 000 + $60, 000 · x2 − x4
x1 + x2 + x3 − 3x4 (3)
p3 = $10, 000 + $60, 000 · x3 − x4
x1 + x2 + x3 − 3x4 (4)
p4 = $10, 000 (5)
Else (6)
p1 = p2 = p3 = p4 = $25, 000 (7)
This scheme ensured that all players received at least $10,000 and that payoffs were increasing in profit, giving each human a financial incentive to try their best individually.
While this was the first man vs. machine competition for the no-limit variant of Texas hold ’em, there had been two prior competitions for the limit variant. In the limit variant all bets are of a fixed size, while in nolimit bets can be of any number of chips up to the amount remaining in a player’s stack (the stacks are reset to a fixed amount of 200 big blinds at the start of each hand). Thus, the game tree for no-limit has a much larger branching factor and is significantly larger; there are 10165 nodes in the game tree for no-limit, while there are around 1017 nodes for limit [16]. In 2007 a program called Polaris that was created by researchers at the University of Alberta played four duplicate 500-hand matches against human professionals. The program won one match, tied one, and lost two, thus losing the match overall. In 2008 an improved version of Polaris competed against six human professionals in a second match, this time coming out victorious (three wins, two losses, and one tie). There have also been highly-publicized man vs. machine competitions for other games; for example, chess program Deep Blue lost to human expert Garry Kasparov in 1996 and beat him in 1997, and Jeopardy agent Watson defeated human champions in 2011.
Claudico is Latin for “I limp.” Limping is the name of a specific play in poker. After the initial antes have been paid, the first player to act is the small blind and he has three available actions; fold (forfeit the pot), call (match the big blind by putting in 50 chips more), or raise by putting in additional chips beyond those needed to call (a raise can be any integral amount from 200 chips up to 20,000 chips in this situation). The second option of just calling is called “limping” and has traditionally been viewed as a very weak play only made by bad players. In one popular book on strategy, Phil Gordon writes, “Limping is for Losers. This is the most important fundamental in poker—for every game, for every tournament, every stake: If you
3The small blind (SB) and big blind (BB) correspond to initial investments, or “antes” of the players. In the match, the SB was 50 chips and the BB was 100 chips.
4To put these results into some perspective, Dong Kim won the challenge described above against Nick Frame by 13.87 BB/100 (he won by $103,992 over 15,000 hands with blinds SB=$25, BB=$50), http://www.pokergurublog.com/ content/donger-kim-wins-heads-challenge-against-tcfromub, and Doug Polk defeated Ben Sulsky in another high-profile challenge match by 24.67 BB/100 (he won by $740,000 over 15,000 hands with blinds SB = $100, BB = $200), http://www.pokernews.com/news/2013/10/doug-polk-defeats-ben-sulsky-16618.htm.
are the first player to voluntarily commit chips to the pot, open for a raise. Limping is inevitably a losing play. If you see a person at the table limping, you can be fairly sure he is a bad player. Bottom line: If your hand is worth playing, it is worth raising” [13]. Claudico actually limps close to 10% of its hands, and based on discussion with the human players who did analysis it seems to have profited overall from the hands it limped. Claudico also makes several other plays that challenge conventional human poker strategy; for example it sometimes makes very small bets of 10% of the pot, and sometimes very large all-in bets for many times the pot (e.g., betting 20,000 into a pot of 500). By contrast, human players typically utilize a small number of bet sizes, usually between half pot and pot.
 Complement of time: it is preceded by words such as: نيح – حابص- ره ظ– ةعاس- ةنس
سمأ اذإ - ىتم-  َناّيأ - ذإ -  َنلآا -  ْذم - ذنم - ُّطق - امنيب - امثير – امّل  Complement of place : it is preceded by words such as : قوف تحت- مامأ- ءارو- ثيح-
نود...
The ABC algorithm combines the exploration and exploitation processes successfully, which proves the high performance of training MLP for earthquake time series data prediction. It has the powerful ability of searching global optimal solution. So, the proper weights
of the algorithms may speed up the initialisation and improve the prediction accuracy of the trained NNs. The simulation results show that the proposed ABC algorithm can successfully train real-time data for prediction purpose, which further extends the quality of the given approach. The performance of ABC is compared with the traditional BP algorithm. ABC shows significantly higher results than backpropagation during experiment. ABC also shows higher accuracy in prediction. The proposed frameworks have successfully predicted the magnitude of earthquake.
The results we obtained were promising. However more empirical results from other domains is needed. Using the ADL algorithm we were able to model only simple Pocman environments with small observation spaces. The same transfer algorithm should be tested using models of complex environment. TPSR [7] provides a good framework for modeling complex environments. Also we have assumed that the cost of learning the model to be zero or that the model is given to us. Our next step would be to improve our algorithm for online learning where the model of the target task is not known beforehand. Interleaved learning and planning [6] provides a method to simultaneously learn and
plan using PSRs. One important direction of research is to incorporate the transfer algorithm into this framework.
We used hand-coded validation tests for SET-1 and SET-2. We are currently investigating ways to automatically find a good set of validating tests given the source tasks. One simple example to show the limitations of our approach would be a source Pocman where the optimal action consist of only ”East”, and a target Pocman where the optimal action consists of only ”North”. Our current approach does not address the problem of finding the appropriate action mappings. We are working an approach to map a given action-observation in target to an actionobservation in source. At every time-step we can find the change in history offset and relate this change to action-observation in target. This shall provide the required mapping. With this mapping finding the history offset at every time step is unnecessary.
We also plan on searching in beliefs space instead of histories, since this would lead to a smaller search spaces or we could even find a closed form solution for the optimal belief. One future work could be to extend the bisimulation metrics to PSRs and use it to transfer the policy.
Algorithms for optimizing V πT (s, β) may suffer from numerical issues or high variance, see Appendix B. Instead we define a value function that bounds V πT (s, β) and approaches it in the infinite sample limit. We call it a particle value function, because it assigns a value to a bootstrap particle filter with K particles representing state-action trajectories. This is distinct, but related to Kantas (2009), which investigates particle filter algorithms for infinite horizon risk-sensitive control.
Briefly, a bootstrap particle filter can be used to estimate normalizing constants in a hidden Markov model (HMM). Let (Xt, Yt) be the states of an HMM with transitions Xt ∼ p(·|Xt−1) and emissions Yt ∼ q(·|Xt). Given a sample y0 . . . yT , the probability p({Yt = yt}Tt=0) can be computed with the forward algorithm. The bootstrap particle filter is a stochastic procedure for the forward algorithm that avoids integrating over the state space of the latent variables. It does so by propagating a set of K particles X(i)t with the transition model X (i) t ∼ p(·|X (i) t−1) and a resampling step in propor-
tion to the potentials q(yt|X(i)t ). The result is an unbiased estimator ∏T t=0(K −1∑K i=1 q(yt|X (i) t )) of the desired probability (Del Moral, 2004; Pitt et al., 2012). The insight is that if we treat the state-action pairs (St, At) as the latents of an HMM with emission potentials exp(βRt(St, At)) (similar to Toussaint & Storkey, 2006; Rawlik et al., 2010), then a bootstrap particle filter returns an unbiased estimate of E[exp(β ∑T t=0Rt)|S0 = s]. Algorithm 1 summarizes this approach.
Algorithm 1 An estimator of the PVF V πT,K(s(1), . . . , s(K), β)
1: for i = 1 : K do 2: S(i)0 = s (i) 3: A(i)0 ∼ πT (·|s(i)) 4: W (i)0 = exp(βR (i) 0 ) 5: end for 6: Z0 = 1 K ∑K i=1W (i) 0 7: for t = 1 : T do 8: for i = 1 : K do
9: I ∼ P(I = j) ∝W (j)t−1 # select random parent 10: S(i)t ∼ p(·|S (I) t−1, A (I) t−1) # inherit from parent 11: A(i)t ∼ πT−t(·|S (i) t ) 12: W (i)t = exp(βR (i) t ) 13: end for 14: Zt = 1 K ∑K i=1W (i) t 15: end for 16: return 1β ∑T t=0 logZt
Taking an expectation over all of the random variables not conditioned on we define the PVF associated with the bootstrap particle filter dynamics:
V πT (s (1), . . . , s(K), β) = E
[ 1
β T∑ t=0 logZt ∣∣∣∣∣ {S(i)0 = s(i)}Ki=1 ] . (3)
Note, more sophisticated sampling schemes, see Doucet & Johansen (2011), result in distinct PVFs.
Consider the value if we initialize all particles at s, V πT,K(s, β) = V π T (s, . . . , s, β). If β > 0, then by Jensen’s inequality and the unbiasedness of the estimator we have that V πT,K(s, β) ≤ V πT (s, β). For β < 0 the bound is in the opposite direction. It is informative to consider the behaviour of the trajectories for different values of β. For β > 0 this algorithm greedily prefers trajectories that encounter large rewards, and the aggregate return is a per time step soft-max. For β < 0 this algorithm prefers trajectories that encounter large negative rewards, and the aggregate return is a per time step soft-min. See Appendix D for the Bellman equation and policy gradient of this PVF.
We will now analyze the time complexity of the algorithms used in our framework. In our implementation, a problem ϕ is a data structure consisting of two fields – ParseTree and Solution. The ParseTree stores the lexicographically sorted parse tree of the matrix of ϕ while the Solution stores the symbolic solution to ϕ in a concise form. A parse tree can be constructed in time O(t) where t is the number of base predicates and boolean operators {∧,∨,¬} in ϕ. The boolean operators occupy the non-leaf nodes in the parse tree while the base predicates occupy the leaf nodes. Lexicographically sorting a tree requires lexicographically sorting the contents of the children of each non-leaf node in the tree. Let t′ be the number of boolean operators and t′′i be the number of children of the i th boolean
operator. Thus, t′∑
i←1 t′′i = t − 1. Note that since each base predicate is always followed by
a boolean operator, t = κt′ where κ is a constant. Lexicographically sorting a list of the contents of the children of a node requires O(t′′i logt ′′ i ) time. Thus, the total time required
for repeating this process for all non-leaf nodes is t′∑ i←1 O(t′′i logt ′′ i ). Since the average number
of children per node is 1t′ t′∑ i←1 t′′i = t− 1 t′ , the total time required to lexicographically sort a
tree is t′∑ i←1 O( t− 1 t′ log t− 1 t′ ) = O(t).
Given two problems – ϕ1, ϕ2, the algorithm Similar(ϕ1, ϕ2) computes whether ϕ1 and ϕ2 are similar to each other or not (see Figure 15). Since computing ϑ requires O(t) time, line 1 requires O(t) time. Since checking whether two trees are isomorphic or not requires O(t) time (as shown in Aho, Hopcroft, & Ullman, 1974), line 6 requires O(t) time. Lines 9 through 11 requires O(t) time. Thus, the algorithm runs in O(t) time.
Given an unsolved problem ϕ and a similar solved problem ϕsimilar, the algorithm ComputeSolutionFromSimilarProblem(ϕ, ϕsimilar) computes the solution to ϕ by variable mapping from ϕsimilar (see Figure 16). V ariableMap is a list where each entry is a pair < v, vsimilar >, v being a free variable in ϕ and vsimilar is the corresponding free variable in ϕsimilar. Let the size of V ariableMap be k
′. The lines 5 through 11 requires O(tk′) time since the number of nodes in ϕsimilar is t and the number of arguments in any predicate is small. Lines 12 through 13 requires O(ωk′) time where ω is the size of the solution to ϕsimilar. Thus, the algorithm runs in O(tk ′ + ωk′) time.
Finally, given an unsolved problem ϕ and a memory Memory that stores problems hierarchically (as described in section 3.4), the algorithm EliminateQuantifiers(ϕ,Memory) computes the solution to ϕ by variable mapping from a similar problem in Memory, if such a problem exists; otherwise solves ϕ using a problem classifier and combination of constraint
solvers and QE algorithms (as described in section 3). The algorithm is shown in Figure 17.
Let there be n subproblems to a problem. In a problem, some of the predicates are already base predicates while the rest are not which can be written as conjunctions/disjunctions of base predicates thereby leading to decomposition of a problem into subproblems. For example, in section 1.1, the problem RiskyPortionsofPath(q, c1, c2, d) is defined in terms of the base predicate DistanceLessThan(p, q, d) (i.e., Distance(p, q) ≤ d) and the nonbase predicates On(q, c1) and On(p, c2). Each of these non-base predicates can be written as disjunctions of base predicates, such as, On(q, {c1[i], c1[i + 1]}) and On(q, {c2[j], c2[j + 1]}), respectively, thereby leading to decomposition of RiskyPortionsofPath into subproblems. Each of the subproblems inherits the base predicates from the problem (e.g., DistanceLessThan(p, q, d)) and also includes the new base predicates (e.g., On(q, {c1[i], c1[i+ 1]}), On(q, {c2[j], c2[j + 1]})) obtained from the non-base predicates. Let α be the number of polynomials in the base predicates of a problem and β be the number of polynomials due to the newly obtained base predicates in a subproblem. Since all subproblems are similar, each of them will have α+ β polynomials. The total number of polynomials s in a problem is O(α+ βn).
Let d be the maximum degree of any polynomial in a subproblem. Since all subproblems are similar, each of them will have maximum degree d. The maximum degree of polynomials in a problem will also be d if objects are represented piecewise-linearly, in which case d ≤ 2. If the objects are not represented piecewise-linearly, the degree will be much larger than two which might lead to a situation where the problem might not be solvable in reasonable time.
Let k be the number of quantified variables in a problem. Then each subproblem also has k quantified variables. Let the computational complexity of using a general QE algorithm for solving a problem be T (n) while that for solving a subproblem is T (1), where T is a doubly exponential function, such as, when using CAD, T (n) = (sd)O(1) k−1
. Note that T (n) ≫ nT (1), i.e., it is more efficient to solve each subproblem using a general QE algorithm than to solve the whole problem using the same algorithm.
In algorithm EliminateQuantifiers(ϕ,Memory), lines 4 through 7 require O(n) time. Lines 8 and 9 require O(t) time each. Since line 13 requires O(t) time, lines 11 through 16 require O(mt) time. Line 18 requires time T (1) while lines 20 through 23 require O((n−1)(tk′+ωk′)) time. Thus, the entire algorithm runs in O(T (1)+mt+(n−1)(tk′+ωk′)) time. Note that ω is the size of the symbolic solution, and if the symbolic solution can be expressed concisely, ω is small. Since the number of boolean operators is of the order of number of base predicates and each base predicate is defined in terms of at least one polynomial, t = O(s) = O(α + βn). Thus, the complexity of the algorithm is O(T (1) + (m+ (n− 1)k′)s+ (n− 1)ωk′). It can be seen that
nT (1) > T (1) + (m+ (n− 1)k′)s+ (n− 1)ωk′
or, ((α+ β)d)O(1) k−1 > ( mn−1 + k ′)(α+ βn) + ωk′
is true provided ω is not large. That is, it is more efficient to solve a problem by variable mapping than to solve each subproblem using a general QE algorithm provided the size of the stored symbolic solution is not large. For every decomposable problem, the complexity of QE can be reduced as above.
When a problem is encountered by the SPS for the first time, it is solved by decomposing into subproblems, solving the first subproblem using a general QE algorithm and then obtaining the solution of the rest of the subproblems by mapping their variables to the first subproblem. Since a subproblem and its solution are stored in memory, if a similar subproblem is encountered in future, the SPS bypasses the QE algorithm completely and solves it by variable mapping. In such a case, line 18 of the algorithm is never executed, and the time complexity of solving the problem is
(m+ nk′)s+ nωk′
which is a considerable savings compared to the complexity of solving the entire problem using a general QE algorithm (e.g., complexity of CAD is (sd)O(1) k−1 ), provided ω is not large. As the SPS solves more problems, the probability to encounter a similar problem in memory increases thereby leading to the above scenario which incurs a complexity of low order polynomial as compared to doubly exponential.3
Example. To illustrate the problem solving process, let us consider the spatial problem BehindCurve(q, c, p) (described in section 2.3). For a point p ← (px, py) and a curve c ← {p1, p2, ...pn} where pi ← (xi, yi) is a point, decomposition of the problem occurs as follows:
ϕ
≡ BehindCurve(q, c, p)
≡ Intersect(c, {p, q})
≡ ∃a,On(a, c) ∧On(a, {p, q})
≡ ∃a, (∨n−1i=1 On(a, {pi, pi+1})) ∧On(a, {p, q})
≡ ∨n−1i=1 (∃a,On(a, {pi, pi+1}) ∧On(a, {p, q}))
≡ ∨n−1i=1 (Qiϕ′i)
≡ ∨n−1i=1 ϕi
Thus there are (n− 1) subproblems ϕi, where
ϕ′i ≡ On(a, {pi, pi+1}) ∧On(a, {p, q})
Qi ≡ ∃a
ϕi ≡ ∃a,On(a, {pi, pi+1}) ∧On(a, {p, q})
From Figure 18, ϑ(ϕi) =< 2, 2 133, 1, < ∃ >, 21 > for i = 1, 2, ...n − 1. By theorem 3, all ϕi’s are similar since they are the subproblems of the same problem. If the SPS does
3. It should be noted that approximating a continuous curve by a sequence of line segments has its drawbacks. For example, a point p that is on a continuous curve c might not be on the piecewise-linear approximation of c. The SPS can accept a parameter that specifies the maximum length of a line segment to be used in the approximation. As of our current implementation, we leave the onus of determining this maximum length on the problem solver. In this context, it deserves mention that loss of information is inevitable in almost any kind of approximation. For example, when the space in a diagram is approximated by a finite number of pixels, as shown by Banerjee and Chandrasekaran (2010), the diagrammatic objects lose certain spatial information that might be detrimental to spatial problem solving which can be avoided by knowing the minimum allowable resolution (or maximum length of one side of a square pixel).
not find a problem in memory similar to the first subproblem ϕ1, it is sent to the problem classifier who sends it to the appropriate QE algorithm. The problem definition, its tuple ϑ, parse tree, and solution are then stored in memory as follows:
ϕ1(q, {p1, p2}, p) ≡ ∃a,On(a, {p1, p2}) ∧On(a, {p, q})
ϕ1((x, y), {(x1, y1), (x2, y2)}, (px, py))
≡ (px−x < 0∧px−x1 ≤ 0∧x1−x ≤ 0∧pyx1−pyx+pxy−x1y−pxy1+xy1 = 0)∨(x−px < 0 ∧ x1 − px ≤ 0 ∧ x− x1 ≤ 0 ∧ pyx1 − pyx+ pxy − x1y − pxy1 + xy1 = 0) ∨ ...
where the arguments of ϕ1 are the free variables. The other subproblems are solved by replacing the variables in ϕ1 by the mapped variables. If a problem similar to ϕ1 is found in memory, ϕ1 will also be solved by replacing the mapped variables, just as the other subproblems.
Note that, for example, the BehindCurve problem, in the absence of an appropriate vocabulary of properties/relations, would have been specified as (see redlog in Weispfenning, 2001):
BehindCurve((x, y), {(p1,x, p1,y), (p2,x, p2,y), ...(pn,x, pn,y)}, (px, py))
≡ ∃ax, ay, t, 0 ≤ t ≤ 1 ∧ px + t(x − px) = ax ∧ py + t(y − py) = ay ∧ ∨n−1i←1(∃ti, 0 ≤ ti ≤ 1 ∧ pi,x + ti(pi+1,x − pi,x) = ax ∧ py + ti(pi+1,y − pi,y) = ay)
Here the total number of quantifiers is n + 3, dependent on the number of line segments forming the curve which can be huge for complicated curves as in many real-world applications. In our SPS, due to appropriate decomposition of problems into subproblems, the number of quantifiers in any subproblem is always fixed (4 in this case) irrespective of the spatial complexity of the object(s) (curve in this case). The symbolic solutions of these simple subproblems can be stored for future use which is not possible in systems like redlog. Needless to say, though solving the problem using both the systems produce the same solution, ours is much faster.
Consider the BT in Figure 1.6 designed to make an agent look for a ball, approach it, grasp it, proceed to a bin, and place the ball in the bin. This example will illustrate the execution of the BT, including the reactivity when another (external) agent takes the ball from the first agent, making it switch to looking for the ball and approaching it again. When the execution starts, the ticks traverse the BT reaching the condition node Ball Found. The agent does not know the ball position hence the condition node returns failure and the ticks reach the action Find Ball, which returns running (see Figure 1.7a). While executing this action, the agent sees the ball with the camera. In
10 1 What are Behavior Trees?
this new situation the agent knows the ball position. Hence the condition node Ball Found now returns success resulting in the ticks no longer reaching the action node Find Ball and the action is preempted. The ticks continue exploring the tree, and reach the condition node Ball Close, which returns failure (the ball is far away) and then reach the action node Approach Ball, which returns running (see Figure 1.7b). Then the agent eventually reaches the ball, picks it up and goes towards the bin (see Figure 1.7c). When an external agent moves the ball from the hand of the first agent to the floor (where the ball is visible), the condition node Ball Found returns success while the condition node Ball Close returns failure. In this situation the ticks no longer reach the action Approach Bin (which is preempted) and they instead reach the action Approach Ball (see Figure 1.7d).
Gaussian process (GP) regression is one of the most popular choice within the Bayesian optimization framework. It is also particularly suited for our application for many reasons. Firstly, GP regression works well in small-sample setting which is desirable in our setting since in the beginning of the search we do not have a lot of data. Secondly, GP regression not only gives us predictions but also uncertainty associated with them, which helps us choose between exploration and exploitation (see more details in the next section). Finally, hyperparameters of GP regression can be set easily using maximum likelihood estimation, allowing for an easy implementation (see end of this section).
In this paper, we use a regression model that gives us a Gaussian distribution at each candidate location x∗. In this paper, we assume that the measurements are sampled from a jointly Gaussian distribution. We make this choice since it leads to a computationally simple algorithm which can be implemented on the on-board computer in the UAV. Our model therefore takes the following form:
yt = ft + t, for t = t1, t2, . . . (1)
where t are i.i.d. Gaussian noise t ∼ N (0, σ2) with variance σ2, while ft is drawn from a GP ft ∼ GP(m(x), k(x,x′)) with mean function m and covariance function k. The relationship between y and x is captured with a specific mean and covariance function. In this paper, we use the zero mean function and a squared-exponential covariance function defined
below:
k(x,x ′ ) = σ2f exp [ − 1 l2 (x− x ′ )T (x− x ′ ) ] (2)
where σf , l ∈ R are kernel hyperparameters that control the spatial correlation. We denote the set of hyperparameters by θ = {σ, σf , l}.
The distribution p(y|x∗,Dt,θ) at a candidate location x∗ is a Gaussian distribution. Denote the vector of measurements received until time t by yt = [yt1 , yt2 , . . . , ytnt ]
T . It follows from the property of GPs that any finite number of samples drawn from GP are jointly Gaussian, giving the following expression for the distribution of yt and any y:[
yt y
] ∼ N ( 0, [ Kt + σ
2I k∗ kT∗ k∗∗ + σ 2
]) (3)
where Kt is a matrix with (i, j)’th entry as k(xti ,xtj ), k∗ is a vector with i’th entry as k(xti ,x∗) and k∗∗ = k(x∗,x∗).
We can write the expression for the distribution of y given yt (see page 16 of Rasmussen and Williams [2006]):
p(y|x∗,Dt,θ) := N (µ∗|t, σ2∗|t) (4)
where µ∗|t := k T ∗ (Kt + σ 2I)−1yt (5)
σ2∗|t := k∗∗ − k T ∗ (Kt + σ 2I)−1k∗ (6)
The computational complexity of these operations is O(n∗n 3 t ) where n∗ is the number of candidate locations. We limit the number of candidate locations to around n∗ = 350. The cubic cost can be reduced by using one rank updates. Nevertheless, this computation is acceptable in our application since we do not have to measure more than 300 measurements for the whole search operation.
The distribution p(y|x∗,Dt,θ) depends on specification of θ. We set θ offline using the data from a scan of the region (such as Fig. 2). Specifically, given a measurement vector y taken at locations X, we maximize log p(y|X,θ). The expression for the log-likelihood is also available in closed-form. Please see Rasmussen and Williams [2006] for details.
Given a training set D = (X,Y ) where X = (x1, . . . , xn) is a sequence of words and Y = (y1, . . . , yn) is a corresponding sequence of entity tags, our goal is to maximize the log-likelihood of the training data as in equation 3.
maximize θ ∑ ∀(X,Y )∈D logP (Y |X) (3)
where θ are the parameters of the network. The log conditional probability P (Y |X) can be decomposed as in equation 4,
logP (Y |X) = n∑ i=1 logP (yi|x1, . . . , xn, yi−1) (4) We model logP (yi|x1, . . . , xn, yi−1) using the
following equation:
logP (yi|x1, . . . , xn, yi−1) =Wyigi +Ayiyi−1− log ∑ k∈T exp(Wykgi +Aykyi−1) (5)
where, Wyi is a parameter vector w.r.t tag yi which when multiplied with gi gives a score for assigning the tag yi. Matrix A can be viewed as a transition matrix where the entry Ayiyi−1 gives the transition score from tag yi−1 to tag yi. T is the set of all possible output tags.
In simple words, our decoder computes the probabilities of the entity tags by passing the output representations computed by LSTM at each position i and the previous tag yi−1 through a linear layer followed by a softmax layer. In this sense, our model is a complete neural network based solution as opposed to existing models which use CRFs at the output.
Preparing data for a given tool is an important phase to achieve the goal into consideration. That is why we spent some time on the construction of these data (Hand-Tagged corpora, Tagset and Training Tables).
For the training corpus we are obliged to construct our own corpus because of its non availability for free for Arabic language. We took the EASC corpus [22] that contains many articles talking about the ten following ”categories”: Art and Music, religion, education, science and technology, environment, sports, finance, tourism, health, politics. To enrich our work and to cover as much as possible frequent words in Arabic we decide to tag some articles from each domain in the watan corpora [23]. Our tagged corpus is constructed as follows: we tag some articles manually, then we execute our tagger taking an article not tagged as an input, then we do the verification manually, we add it to our corpus and so on.
The set of tags contain 22 tags (without punctuation marks) that identify the main tokens in Arabic Language. The choice of these tags was obtained from an adaptation of the tag set English into Arabic during the creation of our corpus. The tag set of this corpus is not too large, what favors the accuracy of the system. Moreover, this tag set has been reduced by grouping some related tags under a unique name tag, what improves statistics.
Training tables are extracted from the corpus, stored into a file, sorted and counted. These tables are extracted as follows: we took to the training corpus and we compute the occurrence of each tag occurred in our training corpus in a gived context. The contexts corresponding to the position at the beginning and the end of the sentences lack tags on the left-hand side and on the right-hand side respectively, this is managed by introducing a special tag, NULL, to complete the context. These tables have the following structure:
LClLC ... LC2 LC1 T RC1 RC2 ... RClRC
Where lLC the size of left context and lRC the size of right context of the current tag T. For example, if lLC = lRC = 2, the structure of the training the table could have the following form:
Through this paper, we consider the propositional language L built over a finite set of propositional symbols P using
2More details are explained later in the paper.
classical logical connectives {¬,∧,∨,→}. We will use letters such as a and b to denote propositional variables, Greek letters like α and β to denote propositional formulae. The symbols > and ⊥ denote tautology and contradiction, respectively.
A knowledge base K consists of a finite set of propositional formulae. Sometimes, a propositional formula can be in conjunctive normal form (CNF) i.e. a conjunction of clauses. Where a clause is a disjunction literals, and a literal is either a propositional variable (x) or its negation (¬x). For a set S, |S| denotes its cardinality. Moreover, a KB K is inconsistent if there is a formula α such that K ` α and K ` ¬α, where ` is the deduction in classical propositional logic. If K is inconsistent, Minimal Unsatisfiable Subsets (MUS) of K are defined as follows: Definition 1 (MUS). Let K be a KB and M ⊆ K. M is a minimal unsatisfiable (inconsistent) subset (MUS) of K iff M ` ⊥ and ∀M ′ ( M , M ′ 0 ⊥. The set of all minimal unsatisfiable subsets of K is denoted MUSes(K ).
Clearly, an inconsistent KB K can have multiple minimal inconsistent subsets. When a MUS is singleton, the single formula in it, is called a self-contradictory formula. We denote the set of self-contradictory formulae of K by selfC(K) = {α ∈ K | {α} ` ⊥}. A formula α that is not involved in any MUS of K is called free formula. The set of free formulae of K is written free(K) = {α | there is no M ∈ MUSes(K) such that α ∈ M}, and its complement is named unfree formulae set, defined as unfree(K) = K \ free(K). Moreover, the Maximal Consistent Subset and Hitting set are defined as follows: Definition 2 (MSS). Let K be a KB and M be a subset of K. M is a maximal satisfiable (consistent) subset (MSS) of K iff M 0 ⊥ and ∀α ∈ K \M , M ∪ {α} ` ⊥. The set of all maximal satisfiable subsets is denoted MSSes(K ). Definition 3. Given a universe U of elements and a collection S of subsets of U , H ⊆ U is a hitting set of S if ∀E ∈ S, H ∩E 6= ∅. H is a minimal hitting set of S if H is a hitting set of S and each H ′ ⊂ H is not a hitting set of S.
This SLR was conducted focusing on building a non-exhaustive review on different applications of hand gesture analysis, intending to cover less explored applications such as gesticulation and psycholinguistic analysis, and focusing on temporal aspects of gesture analysis. Thus, among the studies pertaining to the analysis of a predefined vocabulary of gestures, we are especially interested in how they treat the temporal aspects in the hand gestures analysis. These papers may focus on designing a set of gestures for a specific application; or on studies aiming at sign language processing. The former studies can be used for controlling a smart home environment [7], controlling a slide presentation [40] or recognizing gestures of aircraft marshalling [31]. Some of these works also define a small set of simple gestures that could be used for HCI through gestures, such as [6, 23, 25, 32, 33]. Studies aiming at sign language processing define a reduced set of signs: 34 gestures from Japanese Dactylology [24]; 14 gestures from Taiwanese Sign Language [38]; 23 gestures from Vietnamese Alphabet [39]. However, as aforementioned, we are interested in how these studies deal with temporal aspects of hand gestures analysis. In [7, 32, 33, 38, 40], it is aimed at analyzing continuous gestures, including segmentation and recognition of a predefined set of gestures. In other studies, such as 10In this case, “data” are gestures captured by cameras or sensor devices.
[6, 23, 25, 29, 31], it is aimed at gesture recognition and not performing temporal tasks, such as segmentation. Nonetheless, these studies consider gestures as a sequence of postures over time and incorporate this perspective into the recognition task. Corradini [6] uses Hidden Markov Model (HMM) to deal with gesture temporal structures, using Neural Networks to define HMM state transition probabilities. Spano et al. [23], use several descriptors (e.g. position of fingers) as a representation of a gesture at a given time, and apply this representation as a state in Non Autonomous Petri Nets in order to implement temporal gesture recognition based on states sequence analysis and model-based recognition. Yuan [25] considers gestures as a sequence of vectors, where each vector represents a frame of the gesture, and incorporates temporal machine learning algorithms, such as Recurrent Neural Networks (RNN) or Dynamic Time Warping (DTW), in order to recognize gestures. Hernández-Vela et al. [29], also apply an approach that represents a gesture by a sequence of frames representations, but they use Bag-of-Visual-DepthWords organized in a histogram, which is submitted to Nearest Neighbor classifier. Choi et al. [31] represent each gesture as a sequence of postures obtained through K-means and uses a technique of string matching to classify sequences. In a different perspective, the studies [24, 39] also work on “static gestures” or postures. Nevertheless, a static gesture (or posture) is defined, in these studies, as the main component of a gesture, similar to an independent hold, following the hierarchy described in [18]. Moreover, these studies focus on temporal aspects of gestures: Nguyen and Bui [39] segment postures (that is, the independent holds) from “transition gestures” (that is, movements such as preparation and retraction phases, which occurs between two strokes or independent holds); and Lamar et al. [24] use an architecture which considers a temporal normalization of gestures in order to recognize 34 gestures – 20 dynamic gestures and 14 static gestures.

LetΣ be a finite alphabet, and let *Σ be the set of strings on Σ . Given n, let )S(anf be the frequency of the n-gram
na in S , and )T(
an f be the frequency of the n-gram na in
T , where S ,T are two strings in *Σ . Let N be the set of integers, and +N the set of positive integers.
For notation convenience, we define the function:
NN →Σ×+ *:g
( ) nS,ng = if Sn ≤≤1
( ) 1+= SS,ng if nS <
The ABC-SG distance between S and T is thus defined as:
( ) ( ) ( )
( ) ( )( )
( ) ∑
∑= ∈ ⎥
⎥ ⎥ ⎥ ⎥ ⎥
⎦
⎤
⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣
⎡
⋅−+
−−+
=− T,Smax
n T a S a
Aa
n
nn n
n
f,fmin
T,ngS,ngTS .T,SSGABC
1 22
λ (3)
where S , T are the lengths of the two strings S ,T
respectively, and where { }0∪∈ +Rnλ . ABC-SG is based on the same concept of familiarity on which EED is based, but this concept is extended to the familiarity of n-grams instead of that of single characters.
It is important to notice that ABC-SG is actually the generalization of both EED (Muhammad Fuad and Marteau 2008a), (Muhammad Fuad and Marteau 2008b) and MREED (Muhammad Fuad and Marteau 2008c), so it includes the same advantages that these two distances have.
ABC-SG is proved to be a distance metric. For space limitations, the proof is not presented here. However, the proof is an extension of the proof presented by Muhammad Fuad and Marteau (2008a) (2008b).
As indicated earlier, the parameters nλ are determined using the artificial bee colony algorithm.
Adaboost [10] is a very popular classification learning algorithm. It is a simple and effective algorithm. While generally successful, the sensitivity of Adaboost to random label noise is well documented [6, 9, 2]. The random label noise setup is one where we take a dataset for which our learning algorithm generates an accurate classifier and we flip each label in the training set with some small fixed probability. Note that the classifier that was a good classifier in the noiseless setup is still a good classifier. The problem is that in the noisy setup the noisy examples mislead the learning algorithm and cause it to diverge significantly from the good classifier.
LogitBoost [7] is believed to be less sensitive to random noise than Adaboost, but it still falls pray to high levels of random labels noise.
In fact, Servedio and Long [8] proved that, in general, any boosting algorithm that uses a convex potential function can be misled by random label noise. Freund [4] suggested a boosting algorithm, called Brownboost, that uses a non-convex potential function and claims to overcome random label
ar X
iv :1
40 9.
29 05
noise. The main contribution of this paper is experimental evidence that support this claim. The other contribution is a heuristic for automatically tuning the parameters that Brownboost needs as input.
Recall that the FODD-based VI algorithm must add functions represented by FODDs (in Steps 2 and 4) and take the maximum over functions represented by FODDs (in Step 4). Since the individual functions are independent functions of the state, the variables of different functions are not related to one another. Therefore, before adding or maximizing, the algorithm by Wang et al. (2008) standardizes apart the diagrams. That is, all variables in the diagrams are given new names so they do not constrain each other. On the other hand, since the different diagrams are structurally related this often introduces redundancies (in the form of renamed copies of the same atoms) that must be removed by reduction operators. However, our reduction operators are not ideal and avoiding this step can lead to significant speedup in the system. Here we observe that for maximization (in Step 4) standardizing apart is not needed and therefore can be avoided.
Theorem 2 Let B1 and B2 be FODDs. Let B be the result of combining B1 and B2 under the max operation when B1 and B2 are standardized apart. That is, ∀s,MAPB(s) = max{MAPB1(s),MAPB2(s)}. Let B
′ be the result of combining B1 and B2 under the max operation when B1 and B2 are not standardized apart. ∀ interpretations I, MapB(I) = MapB′(I).
Proof: The theorem is proved by showing that for any I a valuation for the maximizing diagram can be completed into a valuation over the combined diagram giving the same value. Clearly MapB(I) ≥ MapB′(I) since every substitution and path that exist for B ′ are also possible for B. We show that the other direction holds as well. Let ~u be the variables common to B1 and B2. Let ~u1 be the variables in B1 that are not in B2 and ~u2 be the variables in B2 not in B1. By definition, for any interpretation I,
MapB(I) = Max[MapB1(I), MapB2(I)] = Max[MapB1(I, ζ1), MapB2(I, ζ2)]
for some valuations ζ1 over ~u ~u1 and ζ2 over ~u ~u2. Without loss of generality let us assume that MapB1(I, ζ1) = Max[MapB1(I, ζ1), MapB2(I, ζ2)]. We can construct valuation ζ over ~u ~u1 ~u2 such that ζ and ζ1 share the values of variables in ~u and ~u1. Obviously MapB1(I, ζ) = MapB1(I, ζ1). Also, by the definition of FODD combination, we have MapB′(I) ≥ MapB1(I, ζ) = MapB(I).
On-policy reinforcement learning algorithms are hard to implement with deep neural networks as function approximators. This is because the on-policy nature of the algorithms makes consecutive updates to the parameters correlated which breaks assumptions made by popular gradient algorithms like stochastic gradient descent about the data: that the training data is independently and identically distributed.
This problem was solved to a large extent by the Asynchronous Advantage Actor Critic (A3C) Algorithm (Mnih et al., 2016) by executing multiple versions of the actor and critic networks asynchronously and gathering parameter updates in parallel. The asynchronous nature of the algorithm means that the multiple different actor threads explore different parts of the state space and hence the updates made to the parameters are de-correlated. The parameter updates from various different threads are applied to a global parameter vector which ensures that learning is pooled across different threads. The algorithm uses the baseline and modeling choices for Q(st, at) which were discussed in the previous subsection. This implies that effectively, the objective function for the actor is:
L(θa) = log πθa(at|st)(Q(st, at)− V (st))
⇒ L(θa) = log πθa(at|st)A(st, at) whereA(st, at) = Q(st, at)−V (st) is the advantage function and captures the advantage of taking action at in state st. Thus the name Asynchronous Advantage Actor Critic.
The definition of a problem-dependent ECOC code length l, that is, choosing the number of binary partitions for a given multi-class task is a problem that has been overlooked in literature. For example, predefined coding designs like One vs. All or One vs. One have fixed code length. On the other hand, coding designs like Dense or Sparse Random codings (which are very often used in experimental comparisons [57], [58], [4], [18]) are suggested [2] to have a code length of d10log2(k)e and d15log2(k)e respectively. These values are arbitrary and unjustified. Additionally, to build a Dense or Sparse Random ECOC matrix one has to generate a set of 1000 matrices and chose the one that maximizes min(H). Consider the Dense Random Coding design, of length l = d10 log2(k)e, the ECOC matrix will have in the best case a correction capability of b 10−1
2 c = 4, independently of the distribution of the multi-class data. In addition, the effect of maximizing min(H) leads to an equi-distribution of the correction capability over the classes. Other approaches, like Spectral ECOC [57] search for the code length by looking at the best performance on a validation set. Nevertheless, recent works have shown that the code length can be reduced to of l = log2(k) with very small loss in performance if the ECOC coding design is carefully chosen [38] and classifiers are strong. In this paper, instead of fixing the code length or optimizing it on a validation subset, we derive the optimal length according to matrix rank properties. Consider the rank of a factorization of D into XX>, there are three different possibilities:
1) If rank(XX>) = rank(D), we obtain rank factorization algorithm that should be able to factorize D with minimal error. 2) In the case when rank(XX>) < rank(D) we obtain a low-rank factorization method that cannot guarantee to represent D with 0 error, but reconstructs the components of D with higher information. 3) If rank(XX>) > rank(D), the system is overdetermined and many possible solutions exist.
In general we would like to reconstruct D with minimal error, and since rank(X) ≤ min(k, l) and k (the number of classes) is fixed, we only have to set the number of columns of X to control the rank. Hence, by setting rank(X) = l = rank(D), ECF will be able to factorize D with minimal error. Figure 4 shows visual results for the ECF method applied on the Traffic and ARFace datasets. Note how, for the Traffic (36 classes) and ARFaces (50 classes) datasets the required code length for ECF to full rank factorization is l = 6 and l = 8, respectively as shown in Figures 4(e)(f).
Over the last decade, Social Network Sites (SNSs) have gained importance as a medium for social interaction, allowing people to stay in touch with existing contacts and to create new relationships. Hereunto, SNSs ease social interaction by offering a centralised point to communicate with contacts from different social spheres (e.g. family members, close friends, and colleagues).
Despite these positive social outcomes, the rise of SNSs has been accompanied by privacy concerns. Besides the broadly discussed SNS service providers’ handling of personal data, privacy is also threatened by a SNS user’s contacts (often referred to as ”friends”) [29]. On general-purpose SNSs such as Facebook, ”unimaginably complex social relations collapse
to the infinitely thin plane of a single profile” [20]. As a result, it is difficult for a SNS user to simultaneously meet the expectations and respect varying social norms of conflicting social spheres [5]. This might put the user at risk of offending one (or more) of these social spheres, ultimately leading to social exclusion. A SNS user, for instance, may struggle with targeted sharing sensitive family-related pictures with close friends and family members while hiding these pictures from his colleagues who have also access to his SNS profile. Generally speaking, privacy is threatened if shared personal items are visible to contacts for whom they are not intended.
However, these privacy issues are not primarily due to a lack of appropriate privacy settings, as popular SNSs offer a wide range of fine-grained controls to adjust the visibility of shared items [23]. Instead, it has been shown that an item’s visibility is often only defined once when it is shared and subsequently left unchanged [27]. Over time and due to the large number of shared items and contacts, users become unaware of who has access to which shared items [18]. Awareness of inaccurate privacy settings, however, is a prerequisite for being able to make necessary changes. Put differently, users first need to know of misconfigured privacy settings before being able to make adjustments.
Especially for young people, a careless attitude towards SNS privacy puts their future prospects (such as when applying for a job) at risk and may lead to social exclusion. On the one hand, the age group of people between 13 and 25 is most active on SNSs. On the other hand, the ”cyber personae they spawned in adolescent efforts to explore identity have taken on permanent lives in the multiple archives of the digital world.” [25] Hereunto, early and playful education of privacy risks on SNSs can contribute to responsible usage and empower those people to harness the strengths of SNSs. In this paper, we adopt the concept of serious games in order to strengthen privacy awareness on SNSs. It has been widely accepted that games can provide an engaging and motivational environment for learning [13]. In [7], the efficacy of game-based approaches for behavioural change has been demonstrated. Besides, it has been shown that serious games have the potential to increase awareness of important societal issues [22]. Our resulting serious game, termed Friend Inspector, is a browser-based application that allows Facebook users to playfully check their knowledge of who can see their ar X iv :1
40 2.
58 78
v1 [
cs .C
Y ]
2 0
Fe b
20 14
shared personal items and provides personalised recommendations on how to improve privacy settings.
The remainder of this paper is structured as follows. After examining related work in the following section, an in-depth discussion of the concepts of privacy and serious games as the two foundations of Friend Inspector is provided. Based thereupon, the conceptual design of Friend Inspector is presented. Finally, we discuss implementation details and conclude the paper.
Consider the binary classification problem f : Rn → B where B = {0, 1}. Let Ω+ = {x ∈ Rn : f(x) = 1}. Lets approximate Ω+ as the union of N convex polytopes Ω̃+ = ∪Ni=1Pi where the i’th polytope is the intersection Pi = ∩Mij=1Hij of Mi half-spaces Hij = {x ∈ Rn : hij(x) > 0}. We can replace Mi with M = maxiMi without loss of generality. Hij is defined in terms of its indicator function
hij(x) =
{ 1, ∑n k=1 wijkxk + bij ≥ 0
0, otherwise , (1)
where wijk and bij are the weights and the bias term. Any Boolean function b : Bn → B can be written as a disjunction of conjunctions, also known as the disjunctive normal form [54]. Hence, we can construct the function
f̃(x) = N∨ i=1  M∧ j=1 hij(x)  ︸ ︷︷ ︸
bi(x)
(2)
such that Ω̃+ = {x ∈ Rn : f̃(x) = 1}. Since Ω̃+ is an approximation to Ω+, it follows that f̃ is an approximation to f . Our next step is to provide a differentiable approximation to this disjunctive normal form. First, the conjunction of binary variables
∧M j=1 hij(x) can be replaced by the product∏M
j=1 hij(x). Then, using De Morgan’s laws [54] we can replace the disjunction of the binary variables ∨N i=1 bi(x)
with ¬ ∧N
i=1 ¬bi(x), which in turn can be replaced by the expression 1− ∏N i=1(1− bi(x)). Finally, we can approximate the perceptrons hij(x) with the logistic sigmoid functions
σij(x) = 1 1 + e− ∑n k=1 wijkxk+bij . (3)
This yields the differentiable approximation to f̃
f̂(x) = 1− N∏ i=1 (1− M∏ j=1
σij(x)︸ ︷︷ ︸ gi(x) ), (4)
which can also be visualized as a network (Figure 1). We refer to the proposed network architecture as LDNN. The only adaptive parameters of the LDNN are the weights and biases of the first layer of logistic sigmoid functions. The second layer consists of N soft NAND gates which implement the logical negations of the conjunctions gi(x) using products. The output layer is a single soft NAND gate which implements the disjunction using De Morgan’s law. We will refer to a LDNN classifier which has N NAND gates in the second layer and M discriminants per NAND gate as a N ×M LDNN. Note that other variations of disjunctive normal networks can be constructed by using any classifier that is differentiable with respect to its parameters in place of the logistic sigmoid functions.
Table 1 shows that the simple strategy of adding high quality single-vector representations is very competitive with the state-of-the-art for this task. None of the strategies for selecting a sense configuration for the multi-sense representations could compete with the single sense representations on this task. One possible explanation is that the commonly adopted closest sense strategy is not effective for composition since the composition of incorrect senses may lead to spuriously high similarities (for two “implausible” sense configurations).
Table 2 lists a number of example phrase pairs with low average human similarity scores in the Mitchell and Lapata (2010) test set. The results show the tendency of the closest sense strategy with SENSEMBED (SE) to overestimate the similarity of dissimilar phrase pairs. For a comparison we manually labelled the lexemes in the sample phrases with the appropriate BabelNet senses prior to composition (SE*). Human (H) similarity scores are normalised and averaged for an easier comparison, model estimates represent cosine similarities.
The ACS generally offers a better convergence speed than the Ant System or ACO [12]. This stems, among others, from the more exploitative solution construction process and the global pheromone update rule that places emphasis on the best solution found so far. This usually speeds up the process of finding good quality solutions but also makes escaping local minima very difficult. Simulated Annealing, on the other hand, offers a simple solution to escape the local minima. We propose how to combine the ACS and SA to enhance the ACS search process while maintaining its exploitation oriented nature. The proposed algorithm, ACS with the SA (ACS-SA in short), can be summarized as follows. The ACS search process is guided (in part) by the pheromone trail values. At the end of each iteration the global pheromone update rule increases the values of the pheromone trails corresponding to the components (edges) of the current best solution (global best). In the proposed ACS-SA algorithm the global update rule uses instead an active solution which may not necessarily be the best solution found so far. At the end of every iteration each of the solutions generated by the ants is compared with the active solution. If the new solution is of better quality, it replaces the current active solution. Otherwise, the new solution may still replace the active solution but with a probability defined by the Metropolis criterion known from the SA. While the ACS is always focused on the neighborhood of the best solution found so far and can become trapped in a local optimum for a long time, the proposed ACS-SA has a greater chance of escaping the local optima by shifting focus to the solution with a higher cost.
Figure 1 presents a pseudocode of the proposed ACS-SA algorithm. The major part of the algorithm does not differ from the ACS, i.e. the only differences are related to temperature initialization (line 1), the cooling schedule (line 26) and the active solution selection process (line 19). Inclusion of the SA into the ACS results in a more exploratory search process, but it may also lead to a prolonged examination of areas of the solution space that contain solutions of a poor quality. This is prevented by allowing the current global best solution to be selected as the active solution with a probability of 0.1 (line 20). This heuristic might not be necessary if a more advanced cooling schedule is used. The present work is intended to be proof of the concept that the SA may be used
to improve the convergence speed of the ACS, hence the geometric cooling schedule was adapted for its simplicity. In future work a more advanced schedule, e.g. an adaptive cooling schedule by Lam [33], could be applied.
Figure 2 presents the active solution selection procedure. The process iterates over a set of solutions built by the ants. If the cost of an ant’s solution is lower than the cost of the active solution, it replaces the active solution (lines 3–5 in Fig. 2). Otherwise, the ant’s solution (of a worse quality) may replace the active solution with a probability calculated according to the Metropolis criterion from the SA (lines 6–7). As the temperature is lowered, the probability of accepting a worse solution goes down to 0 and the process becomes equivalent to that of the ACS.
The initial temperature T0 plays an important role in the SA. In our work we applied the idea of an adaptive temperature calculation which was proposed in [3]. The calculation requires a sample of randomly generated solutions whose values (costs) are used to calculate the initial temperature according to:
T0 = ∆C + 3σ∆C
ln (1/γ) , (6)
where ∆C is the mean of absolute differences between the costs of consecutive pairs of solutions from the sample, σ∆C is the sample standard deviation and γ is a parameter denoting the probability of accepting a worse solution, i.e. with a higher cost. The idea behind Eq. 6 is based on the central limit theorem which states that the mean of a large sample of independent random variables is approximately normally distributed, hence, almost all (approx. 99.7%) absolute differences between the quality of randomly generated solutions fall in the range of (∆C−3σ∆C ,∆C+3σ∆C). Knowing the approximation of the highest difference in quality between a pair of solutions allows to calculate the initial temperature so that the probability of accepting a worse solution is γ.
Although the temperature initialization requires additional computations, it does not increase the asymptotic complexity of the ACS algorithm. In our experiments a sample of 1000 random solutions was used due to a negligible additional cost; however, a much smaller number could also be acceptable.
For our correctness tests, we used a set of 8,336 malware binaries provided by MIT Lincoln Labs during the Cyber Genome program’s Independent Verification and Validation (IV&V). The dataset included 128 clean-room binaries built by MIT Lincoln Labs comprising six distinct families of malware. These 128 binaries provided the ground truth. The others were collected from the wild and provided background for the test.
The accuracy of a clustering was determined by running cluster evaluation metrics over the cluster assignments for the 128 ground truth binaries. The metrics we used were Jaccard Index, Purity, Inverse Purity, and Adjusted Rand Index. Jaccard Index is defined as the number of pairs of items of the same truth family that were correctly assigned to the same cluster, divided by the number of pairs that belong to the same truth family and/or assigned cluster. Purity is defined as the percentage of items that are members of the dominant truth family in their assigned cluster, and Inverse Purity is the percentage of items that were assigned to their truth family’s dominant cluster. Adjusted Rand Index is a version of the Rand Index cluster accuracy measure that is adjusted to account for the expected success rate when clusters are assigned at random.
Table 1 summarizes the two datasets used in this evaluation: webspam and rcv1. The webspam dataset was used in the recent paper [27]. Since the webspam dataset (24 GB in LibSVM format) may be too small compared to datasets used in industrial practice, in this paper we also present an empirical study on the expanded rcv1 dataset [4], which we generated by using the original features + all pairwise combinations (products) of features + 1/30 of 3-way combinations (products) of features. Note that, for rcv1, we did not include the original test set in [4], which has only 20242 examples. To ensure reliable test results, we randomly split our expanded rcv1 dataset into two halves, for training and testing.

In this section we introduce an extended graph gtl↑F ,G for the generate and test abstract framework gtlF ,G similar as in Section 9 we introduced sml ↑ Π for smlΠ.
For a formula H , we say that a clause l ∨C is a reason for l to be in a list P lQ
of literals w.r.t. H if H |= l ∨ C and C ⊆ P .
An (extended) record M relative to a formula H is a list of literals over the set
of atoms occurring in H where
(i) each literal l in M is annotated either by ∆ or by a reason for l to be in M
w.r.t. H ,
(ii) M contains no repetitions,
(iii) for any inconsistent prefix of M its last literal is annotated by a reason.
An (extended) state relative to a CNF formula F , and a formula G formed from atoms occurring in F is either a distinguished state FailState or a pair of the form M ||Γ, where M is an extended record relative to F ∧ G, and Γ is the same as in the definition of an augmented state (i.e., Γ is a (multi-)set of clauses formed from atoms occurring in F that are entailed by F ∧G.) For any extended state S relative to F and G, the result of removing annotations from all nondecision literals of S is a state of gtlF ,G : we will denote this state by S ↓.
For a CNF formula F and a formula G formed from atoms occurring in F , we will define a graph gtl↑F ,G . The set of the nodes of gtl ↑ F ,G consists of the extended states relative to F and G. The transition rules of gtlF ,G are extended to gtl ↑ F ,G as follows: S1 =⇒ S2 is an edge in gtl ↑ F ,G justified by a transition rule T if and only if S ↓1 =⇒ S ↓ 2 is an edge in gtlF ,G justified by T .
The lemma below formally states the relationship between nodes of the graphs
gtlF ,G and gtl ↑ F ,G :
Lemma 13 For any CNF formula F and a formula G formed from atoms occurring in F , if S ′ is a state reachable from ∅||∅ in the graph gtlF ,G then there is a state S in the graph gtl↑F ,G such that S ↓ = S ′.
The definitions of Basic transition rules and semi-terminal states in gtl↑F ,G are
similar to their definitions for gtlF ,G . Proposition 9↑ For any CNF formula F and a formula G formed from atoms occurring in F ,
(a) every path in gtl↑F ,G contains only finitely many edges labeled by Basic
transition rules,
(b) for any semi-terminal state M ||Γ of gtl↑F ,G , M is a model of F ∧G, (c) gtl↑F ,G contains an edge leading to FailState if and only if F ∧G is unsatis-
fiable.
We say that a state in the graph gtl↑F ,G is a backjump state if its record is inconsistent and contains a decision literal. As in case of the graph gtlF ,G , any backjump state in gtl↑F ,G is not semi-terminal: Proposition 10↑ For any CNF formula F and a formula G formed from atoms occurring in F , the transition rule Backjump GT is applicable in any backjump state in gtl↑F ,G .
Proposition 9 (b), (c) and Proposition 10 easily follow from Lemma 13 and Proposition 9↑ (b), (c) and Proposition 10↑ respectively. Proof of Proposition 9 (a) is similar to the proof of Proposition 9↑ (a).
We use substructures that correlate with one of two target classes (e.g. active and inactive) – and therefore discriminate among the two. Techniques exist for mining top-k substructures according to convex measures such as χ2 or Information Gain while still pruning large parts of the search space. Similar search strategies can be used to find all substructures with a score above a user defined threshold. Please note that in this work we only use χ2 since earlier work showed that this leads to better results than employing Information Gain [15].
Regarding chemical compounds, there exist three very well studied types of substructures, namely:
LG subgraphs, most expressive, but expensive to mine; LT subtrees, can represent anything but cycles; LS subsequences, least expressive, rather easy to mine.
The relation LS ⊂ LT ⊂ LG holds, implying that |LS | ≤ |LT | ≤ |LG|. Note that sequences are slightly different from paths as used by Swamidass et al. as they only allow a bijective mapping of the nodes and edges from the fragment to the data, i.e. a vertex can occur at most once in a sequence. Our first question is concerned with comparing these three types of structures w.r.t. their value in terms of predictive accuracy. To carry out this task, we extract a number of substructures from the data, and use them to describe each of the seen or unseen chemical compounds. The molecules are transformed into generalized fingerprints indicating the substructures’ presence or absence. From the feature vectors a model for the activity of the compounds is learned.
Support vector machines (SVMs) have been used successfully for SAR problems and can filter out redundant/irrelevant features. We use the Tanimoto kernel that has been used to good effect on the NCI cancer data set we do our comparison on [2]. The data is encoded as undirected graphs, vertices labeled with their atom type, edges as single, double, or aromatic bonds. Hydrogen atoms are not encoded. The shortest possible sequence consists of a single edge, i.e. two atoms.
We evaluate the extended parsing model on the Stanford Dependencies (De Marneffe and Manning, 2008) version of the Penn Treebank. We adapt BIST-parser code to run with the DyNet toolkit1 and add our changes. We follow the setup of Kiperwasser and Goldberg (2016): (1) A word is represented as the concatenation of randomly initialized vector and pre-trained vector (taken from Dyer et al. (2015)); (2) The word and POS embeddings are tuned during training; (3) Punctuation symbols are not considered in the evaluation; (4) The hyper-parameters values are as in Kiperwasser and Goldberg paper (2016), Table 2; (5) We use the same seed and do not perform hyper-parameter tuning. We train the parser with the conjunction features for up to 10 iterations, and choose the best model according to the LAS accuracy on the development set.
General Parsing Results Table 3 compares our results to the unmodified BIST parser. The extended parser achieves 0.1 points improvement in UAS and 0.2 points in LAS comparing to Kiperwasser and Goldberg (2016). This is a strong baseline, which so far held the highest results among greedy transition based parsers that were trained on the PTB only, including e.g. the parsers of Weiss et al (2015), Dyer et al (2015) and Ballesteros et al (2016). Stronger absolute parsing numbers are reported by Andor et al (2016) (using a beam); and Kuncoro et al (2016) and Dozat
1https://github.com/clab/dynet
and Manning (2016) (using an arc-factored global parsers). All those parsers rely on broadly the same kind of features, and while we did not test this, it is likely the conjunction features would benefit them as well.2
Parsing Results for conj Label We evaluate our model specifically for conj label, and compare to the results achieved by the parser without the conjunction features. We measure Rel (correctly identifying modifiers that participate in a conj relation, regardless of correctly attaching the parent) and Rel+Att (correctly identifying both the head and the modifier in a conj relation). The results are in Table 4. The improvement in Rel score is relatively small while there is an improvement of 1.1 points in Rel+Att F1 score, suggesting that the parser was already effective at identifying the modifiers in a conj relation and that our model’s benefit is mainly on attaching the correct parent node.
Analysis We would like to examine to what extent the improvement we achieve over Kiperwasser and Goldberg (2016) on conj attachments corresponds to the coordination features we designed. To do that, we analyze the conj cases in the dev-set that were correctly predicted by our model and were not predicted by the original BIST-parser and vice versa. The following table shows the percentage of cases where conjunction features appear in each of these lists:
Features +Our,−K&G −Our,+K&G LEM+CAP+SUF 7.5 0 LEM+SUF 3 0 SENTIMENT+SUF 1.5 0 LEM/CAP/SENTIMENT/SUF 29.9 24 Total 41.9 24
The percentage of cases that include conjunction features is much higher in the list of cases that were correctly predicted only by our model. More than that, there are no cases that include more than one conjunction feature in the list of cases that were correctly predicted only by BIST-parser (Kiperwasser and Goldberg, 2016).
2A reviewer of this work suggested that our baseline model is oblivious to the word’s morphology, and that a neural parsing architecture that explicitly models the words’ morphology through character-based LSTMs, such as the model of (Ballesteros et al., 2015), could capture some of the information in our features automatically, and thus would be a better baseline. While we were skeptical, we tried this suggestion, and found that it indeed does not change the results in a meaningful way.
The above table does not include the SYM feature since unlike the other features there is no absolute way to determine whether the feature takes place on a specific example. To give a sense of the contribution of the SYM feature, we show some examples where our model attaches a conj label between similar words, while the unmodified BIST parser attaches conj parent which is clearly less similar to the modifier (The word in bold is the attached modifier; the word marked with continuous line is the node’s parent in our prediction; the word marked with dashed line is the node’s parent in BIST’s prediction):
• Koop, who rattled liberals and conservatives alike with his outspoken views on ... • ... dropped in response to gains in the stock
market and losses in Treasury securities. • Died: Cornel Wilde, 74 ,actor and director
,in Los Angeles ,of leukemia ... • ... investment firms advising clients to boost
their stock holdings and reduce the ,,.
In the cases that were correctly predicted by BIST-parser only, we could not find examples where the words in the correct attachment are clearly more similar than the attachment predicted by our model. We could find a few examples where both models attached words that are similar, such as:
• ML & Co.’s net income dropped 37%, while BS Cos. posted a 7.5% gain in net, and PG Inc.’s profit fell, but would have risen ... • The closely watched rate on federal funds, or
overnight loans between banks, slid to...
Up to this point we have dealt with a first approach to task difficulty. A task includes (infinitely) many task instances. What about instance difficulty? Does it make sense? In case it does, instance difficulty would be very useful for adaptive tests, as we could make the stochastic task adaptive and start with simple instances and adapt their difficulty to the ability of the subject (as in adaptive testing in psychometrics).
However, there are many confounding factors to determine the difficulty of a single instance. For instance, for a division task we may have these two instances: 6/3 and 1252/626. If the task is stochastic and includes many divisions, a policy that actually makes divisions will pay off. But if we create a task with just 6/3 or 1252/626 as only instances, in both cases the solution would be just 2, which is not only equal for both instances, but also a value that has no relation whatsoever to the difficulty of these instances.
The key issue is that instance difficulty must be defined relative to a task. At first sight, the difference in difficulty between 6/3 and 1252/626 is just a question of computational steps, as the latter usually requires more computational steps if a general division algorithm is used. But what about 13528/13528? It looks an easy instance. Using a general division algorithm, it may be the case that it takes more computational steps than 1522/626. If we see it easy is because there are some shortcuts in our algorithm to make divisions. These shortcuts are frequently applied instead of the general procedure. One of the shortcuts would be to return 1 if both arguments are equal. Of course, we can think about algorithms with many shortcuts, but then the notion of difficulty depends on how many shortcuts it has. In the end, this would make instance difficulty depend on a given algorithm for the task (and not the task itself). This would boil down to the steps taken by the algorithm, as in computational complexity. For the relative numerousness task, for instance,
the difficulty of an instance would be radically different if we are thinking about a counting policy (for which all instances are approximately equally easy) or we are thinking about a Monte Carlo policy (which depends on the difference in the total area of the circles, as the algorithm can stop when the difference is statistically significant).
We can of course take a structuralist approach, by linking the difficulty of an instance to a series of characteristics of the instance, such as its size, the similarities of their ingredients, etc. This is one of the usual approaches in psychology and many other areas, including evolutionary computation, but does not lead to a general view of what instance difficulty really is. For the divisions above, one can argue that 13528/13528 is more regular than 1252/626, and that is why the first is easier than the second. However, this is false in general, as 1352813528 is by no means easier than any other exponentiation.
Some other approaches also link the difficulty of an instance or problem to the “probability of failure” [7] or to the “probability-of-failure and mean time-to-solution” [4]. The probability of failure can be defined in terms of one policy (so we would have again a notion of difficulty dependent to the best policy solving the task), but another perspective is “the likelihood that a randomly chosen program will fail for any given input value” [7]. This is interesting. Apparently, it looks like the population-based approach in psychology (apply the instance to some individuals and record times and success rates), as it is based on a population of programs.
Here, we have several problems to follow this idea. We would need a population7. Also, we have that difficulty depends on computational cost and success rates, which are expressed in very different units. If the difficulty of a task is 8 (in logarithm of steps), what does it mean if we say that one of its instances has a difficulty of 0.3 (in proportion)? In any case, we may agree that computational cost and success rate are relevant, but they do not work in this way as a function of difficulty.
Again, learning is a dynamics of X-forms, from one X-form to another. X-form is complicated. How come such a dynamics reaches the desired X-form? Such dynamics is determined by learning methods, and learning strategies. We discussed learning methods above, which is described well in equation (lm). Learning methods have set of rules on how to move from one X-form to another. Learning
strategy is higher than learning method. It will govern these aspects: what X-forms to consider? what general approach to X-form? pre-set some X-forms? Or everything from scratch? etc. So, we can see that strategy governs method. Also, different strategy works for different kind of data. Different strategy also need different learning capabilities
We should emphasis here: learning is a complicated thing, one strategy and one method cannot fit all situations. There must be many strategies and even more methods. We are going to discuss some strategies and methods. But, still, there should have some common rules for these strategies and methods.
One very important property of X-form is: one processing (equivalently one black set) could be expressed by more than one X-form (normally, many). This property will play very important role in learning. Let’s see one simple example first. Consider a set of base patterns B:
B = {b1, b2, . . . , bK} (bs)
B has totally K base patterns. What X-form could express B? The easiest one is:
e = b1 + b2 + . . .+ bk (exp)
Sure e is one X-form to express B. Now, if we assume we can write b3, . . . , bK as some subjective expressions of b1 and b2, as following:
b3 = E3(b1, b2), . . . , bK = EK(b1, b2)
So, we can further have:
e′ = E3(b1, b2) + . . .+ EK(b1, b2) = E ′(b1, b2) (exp2)
We can see X-form e and e′ express the same black set. But, the 2 X-forms are very different. In fact, e′ is more complicated than e, and with higher structure. But at the same time, e′ is upon much less base patterns, just b1 and b2, while e is upon on K base patterns. This is very crucial: to learn e, we might have to use all base patterns b1, . . . , bK , while to learn e ′, in principle, we might only use 2 base patterns b1, b2 (just might, might need more, depends on learning method). And, not only that, it is much more. e is just a collection of some base patterns, and no relationship between these base patterns are found and used, while e′ is built on many the relationship between base patterns (of course subjectively). In this sense, comparing to e, e′ is a better X-form to express black set B.
The lesson to us is: to express one processing (or black set), there are many possible X-forms. Some such X-forms are simple, but, not good. Some of such X-forms are complicated, but, actually more robust. All learning strategies will use this fact.
One learning strategy will put some requirements on data and on learning machine. That is to say, data must strong to some point. And, learning machine is required to have certain capabilities. We do not specifically design a learning machine here. So, we do not know exactly how to realize such capabilities. But, we describe learning machine, and we show that with such capabilities, this strategy will work.
Now, we would propose some learning strategies.
Strategy 1 – Embed X-forms into Parameter Space This strategy will embed X-forms into parameter space, and use the dynamics on parameter space to drive the flow of X-forms. Parameter space RU is a real Euclidean space, usually U is a big number. In this strategy, we choose L X-forms, ei, i = 1, 2, . . . , L (so we only use some X-forms), and we will cut
RU into L pieces, each piece is a region, denote as Vi, i = 1, 2, . . . , L, so that RU = ⋃L
i=1 Vi, then we associate each X-form with each region, ei ∼ Vi. In this way, we embed L X-forms into the parameter space RU . If we introduce a dynamics on RU , we actually introduce a dynamics on those L X-forms. Since dynamics on RU is a very familiar and mature mathematical topic, we have a great lot of tools to handle such dynamics. In this way, we can transfer the dynamics on X-forms into dynamics on RU . Or, we transfer learning to a dynamics on RU .
More exactly, we can write down this strategy as following. Let RU be a real Euclidean space, U is a big integer. And, by some way, RU is cut into L regions so that RU = ⋃L i=1 Vi, and Vi ∩ Vi′ = ∅ for any i, i′. We also choose L X-forms ei, i = 1, 2, . . . , L, and assign each ei to a region Vi. That is to say, for any x ∈ RU , if x ∈ Vi, then on x, the X-form is ei. We can denote this X-form associated with x as ex as well, so ex = ei. We have data sequence D = {(bj , oj) |j = 1, 2, . . . , J}. We also have one dynamics on RU driven by this data sequence. We assume this dynamics is discrete.
xk = LM(x0, b1, o1, b2, o2, . . . , bk, ok), k = 1, 2, . . . (emb)
where x0 is the initial point, and xk is the point at k-th step, LM is the mechanism of dynamics. This equation is very similar to previous equation (lm). For each xk, we have the associated X-form ek, ek ∈ {e1, e2, . . . eL}. Thus, we see the learning is going on.
Of course, we want learning to reach desired result. But, with what conditions and requirements, the above dynamics will reach the desired result? We have following theorem. We define a function Lo on RU as below:
Lo(x) = J∑ j=1 (ex(bj)− oj)2, ∀x ∈ RU
where ex is the X-form associated with point x.
Theorem 5.6 Suppose we have a desired X-form e∗ among those X-form we chosen {e1, e2, . . . eL} , and we have a data sequence D that is sufficient to support e∗ and sufficient to bound e∗, and the dynamics in equation (emb) will reach the minimization target, i.e. as k increases, Lo(xk) has trend to decrease, and at some k, Lo(xk) reach the minimum. Then, at the time Lo(xk) reaches minimum, we get the desired X-form, i.e. ex = e ∗.
Proof: The proof is quite clear. Since D is sufficient to support e∗ and sufficient to bound e∗, for any point x ∈ RU , assume the associated X-form for x is ek, if ek is not e∗, then must have some j so that ek(bj) 6= oj , so Lo(x) > 0. And, for e∗, for all j e∗(bj) = oj . So Lo(x) reaches minimum when the X-form associated with x is actually e∗.
So, by this strategy, we indeed find a way to learn: To design a dynamics on RU , which will reach the minimum of function Lo. For this strategy, we need strong data sequence that is both sufficient to support and to bound the desired X-form.
Of course, there are some very critical issues to consider: 1) How to choose those X-forms {e1, e2, . . . eL}? 2) How to cut RU into regions? 3) How to design a good dynamics LM? All these are big issues and not easy to deal. However, there should have many ways to choose, to cut, to design.
One very important example for this strategy is deep learning. We can see in Appendix that deep learning is under this strategy. It is good to know that this strategy is a working and is currently produce many good results.
Strategy 2 – Squeeze X-form from Inside to Higher Abstraction Strategy 1 is to choose a good X-form from a previously given set of X-forms. Now, we will see another strategy, which builds the desired X-form from bottom up. We summarize this strategy as:
1. Check input, to see if need to include the input. If so, add it. 2. Squeeze current X-form to higher abstraction and more generalization, but not go over. 3. Choose best X-form from its internal representation space.
Learning will make sure X-form monotonously increase (in the term of its black set). This strategy require data to be sufficient to support the desired X-form. Note, no requirement on sufficient to bound. This is a huge difference. Also, this strategy needs machine has certain capability. We are not designing machine here. Here we just assume such capabilities, then to see what it can do.
Definition 5.7 (Strategy 2 - Capability 1) Capable to squeeze current X-form e to another Xform e′ with higher abstraction and more generalization. More precisely, the squeezing action will
do following: Assume X-form e = E(g), where E is one algebraic expression (of 3 operators), and g = {b1, . . . , bK} is a set of base patterns, then X-form e′ = E′(g′) should satisfy that g′ ⊂ g, and B ⊂ B′, where B is the black set of e, B′ is the black set of e′. If could find such a X-form e′, e′ is put it into internal representation space, otherwise, take no action.
There is no restrictions on how to do this squeeze (could be smart, or could be dumb). This capability can be simply said as: learning method will try to find a better organized X-form to replace current X-form, with the condition: its black set should be larger, not smaller. More generalization follows getting higher abstraction and bigger black set.
Definition 5.8 (Strategy 2 - Capability 2) Capable to check a X-form e to tell if e is over the desired X-form or not, i.e. to tell if the black set of e is a subset of the black set of the desired X-form.
Now, with the strategy summarized above, and with the capabilities and required data, we see what learning machine can learn.
Theorem 5.9 Suppose a learning machine M is using strategy 2 to learn, and it has Strategy 2 - Capability 1 and Capability 2, for a given black set Bo, if eo is a X-form that expresses Bo, and there is a data sequence D = {(bj , oj) |j = 1, 2, . . .} that is inside Bo and sufficient to support X-form eo, then, start from empty X-form, if data in D is fed to M fully and long enough (could be repeating feed, but not miss any data), eventually, M will learn Bo, i.e. the black set of M will become Bo.
Proof: We first describe the learning action as each data feed in. Suppose current X-form is e, and data feed in is (b, o). Since data are all inside Bo, o = 1. Then learning method will check e(b), if e(b) = 1, no need to do more, move to next data; if e(b) = 0, then this b needs to be included, so replace e with e + b. Then, capability 1 is used to get a X-form e′ with higher abstraction. Then, capability 2 is used to check if e′ is beyond the desired X-form. If e′ is OK, this X-form is put into internal representation space, and e′ replaces current X-form e. This is one step how learning is conducted.
Starting from e = ∅. The first data is (b1, 1). b1 is a base pattern. This case, sure e′ = b1, and put in internal representation space. Since it is just beginning, this step is done. So, X-form becomes e1 = e ′ = b1. Note, at this time, the black set of current X-form is B1 = {b1}, so B1 ⊂ Bo.
This process continues. Now, consider step k. This time, current X-form is ek−1. Input is (bk, 1). It will decide if e′ = ek−1 or e
′ = ek−1 + bk. The logic here is: if ek−1(bk) = 0 , it means that ek−1 is not good enough, it should be expand, so e′ = ek−1 + bk; if ek−1(bk) = 1, no need to expand, so, e′ = ek−1. Then, capability 1 is exercised. e′ is squeezed into higher abstraction, and the generalization is done at the same time. The result is a new X-form e′′. Note, the capability 1 will make the black set of e′′ getting bigger, at least not smaller. Moreover, capability 2 is exercised to check if e′′ is out bound. If not, then set ek = e ′′, i.e. update the X-form, and put e′′ into internal representation space.
This is the learning. We want to show, as k increases, eventually, the black set of ek will become Bo. Since we know that the black set of ek is always a subset of Bo, so we only need to show that as k increases, the black set of ek will not stay as a true subset of Bo and not expand. But, this is clear. If at some k, the black set of ek is a true subset of Bo, so ek is sub-form of the X-form. Since data D is sufficient to support the X-form, there must be a k′ > k, at k′ data is (bk′ , 1) and ek′(bk′) = 0. So, according to the learning process, ek′ will be expanded. Proof is done.
We add another capability: Capability to forget current expression. That is to say, there are some special data, driving by them, learning machine could forget current X-form and make its X-form become empty. We name this capability as Capability Going Empty.
Corollary 5.9.1 A learning machine with Strategy 2 - Capability 1 and Capability 2 and Capability Going Empty is an universal learning machine.
Proof: For any given black set Bo, we can find one X-form e so that Bo is black set of e. The data set S ⊂ Bo so that it is sufficient to support e, according to above theorem, can be used to drive learning machine learn e from any empty. Such data S sufficient to support e indeed exists. So, this machine is universal.
Strategy 3 – Squeeze X-form from Inside and Outside to Higher Abstraction This strategy is quite similar to Strategy 2. They can be thought as one. Only for the purpose to show how data and learning method should work together, we write them differently here. The differences are 1) In Strategy 3, data are both sufficient to support and sufficient to bound, but in Strategy 2, only sufficient to support. 2) In Strategy 2, we have Capability 2, but in Strategy 3, no such capability. That is to say, Strategy 3 uses much stronger data, but need much less capability.
Definition 5.10 (Strategy 3 - Capability 1) Same as Strategy 3 - Capability 1.
Theorem 5.11 Suppose a learning machine M is using strategy 3 to learn, and it has Strategy 3 - Capability 1, for a given black set Bo, if eo is a X-form that expresses Bo, and there is a data sequence D = {(bj , oj) |j = 1, 2, . . .} that is sufficient to support and sufficient to bound X-form eo, then, start from empty expression, if data in D is fed to M fully and long enough (could be repeating feed, but not miss any data), eventually, M will learn Bo, i.e. the black set of M will become Bo.
Proof: Since this is quite similar as strategy 2, we only say the part different. The data are different. So, in this strategy, bk could be inside Bo or outside Bo. When data (bk, ok) feed in, possibly 1) ek(bk) = 1, ok = 1, 2) e(bk) = 0, ok = 1, 3) ek(bk) = 0, ok = 0, 4) e(bk) = 1, ok = 0. For case 1) and 3), no need to do anything, move to next data. For case 2), it means bk needs to be included, so expand to ek + bk. For case 4), it means bk should be excluded, so prohibit bk, the way to do so: ¬(ek · bk) + (ek · ¬bk). Other than this part, the learning is same as in last theorem.
Then, capability 1 is exercised. And, no capability 2. The proof is also similar.
Corollary 5.11.1 A learning machine with Strategy 3 - Capability 1 and Capability 2 and Capability Going Empty is an universal learning machine.
We briefly comment these 3 strategies below.
1. They put different requirements on data and on machine. Strategy 2 only require data sufficient to support. This is much less than both sufficient to support and to bound. But, strategy 2 requires machine to have a very strong capability. Strategy 3 and 1 put same requirements on data. However, strategy 1 put a super strong requirement on setup. The desired X-form must be set into the regions. 2. We just say, the above 3 strategies are not all learning strategies. There are many other learning strategies and methods. 3. Human perhaps never learn anything from very scratch. Previous learning results are used to help new learning. All learning strategies should utilize this approach. This is a huge topic. We will discuss this issue later.
This corresponds to the difficulties arising in the higher layers of NLP, i.e., semantic and pragmatic layers. Challenges in these layers include handling: (a) Sentiment expressed implicitly (e.g., Guy gets girl, guy loses girl, audience falls asleep.) (b) Presence of sarcasm and other
ar X
iv :1
70 1.
05 58
1v 1
[ cs
.C L
] 1
9 Ja
n 20
17
forms of irony (e.g., This is the kind of movie you go because the theater has air-conditioning.) and (c) Thwarted expectations (e.g., The acting is fine. Action sequences are top-notch. Still, I consider it as a below average movie due to its poor storyline.).
Such challenges are extremely hard to tackle with traditional NLP tools, as these need both linguistic and pragmatic knowledge. Most attempts towards handling thwarting (Ramteke et al., 2013) and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.g., leveraging hashtags) and/or stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). Addressing difficulties for linguistically well-formed texts, in absence of explicit cues (like emoticons), proves to be difficult using textual/stylistic features alone.


The data is from the far-field of a radiating antenna for E,B as shown in the geometry Fig. 1 and data Tab. 1.
The fields at a far point P are[8]:
E = −µ0p0ω 2 4π ( sin θ r ) cos [ω(t− r/c)]θ̂ (5) B = −µ0p0ω 2
4πc ( sin θ r ) cos [ω(t− r/c)]φ̂ (6)
1Perhaps the NFL should have consulted BACON.
where µ0 = 4π × 10−7 is the permeability of free space, p0 is the strength of the dipole, and ω is the frequency of the dipole oscillation.
Five virtual experiments were done with various parameters r, φ, θ with a fixed ω. The observables are E(x, t) and B(x, t), where x is in the region of the point P. The nice thing about this VE is that various space-time derivatives can be computed analytically. These experiments are show in Tab. 1, with the fields given at a steady state t = 0.
Rediscovering the Equations from this data is the topic of the two next sections.
ar X
iv :1
30 1.
36 27
v2 [
cs .C
L ]
1 1
M ay
A key characteristic of work on deep learning and neural networks in general is that it relies on representations of the input that support generalization, robust inference, domain adaptation and other desirable functionalities. Much recent progress in the field has focused on efficient and effective methods for computing representations. In this paper, we propose an alternative method that is more efficient than prior work and produces representations that have a property we call focality – a property we hypothesize to be important for neural network representations. The method consists of a simple application of two consecutive SVDs and is inspired by (Anandkumar et al., 2012).
In this paper, we propose to generate representations for deep learning by two consecutive applications of singular value decomposition (SVD). In a setup inspired by (Anandkumar et al., 2012), the first SVD is intended for denoising. The second SVD rotates the representation to increase what we call focality. In this initial study, we do not evaluate the representations in an application. Instead we employ diagnostic measures that may be useful in their own right to evaluate the quality of representations independent of an application.
We use the following terminology. SVD1 (resp. SVD2) refers to the method using one (resp. two) applications of SVD; 1LAYER (resp. 2LAYER) corresponds to a single-hidden-layer (resp. twohidden-layer) architecture.
In Section 1, we introduce the two methods SVD1 and SVD2 and show that SVD2 generates better (in a sense to be defined below) representations than SVD1. In Section 2, we compare 1LAYER and 2LAYER SVD2 representations and show that 2LAYER representations are better. Section 3 discusses the results. We present our conclusions in Section 4.
1 SVD1 vs. SVD2
Given a base representation of n objects in Rd, we first compute the first k dimensions of an SVD on the corresponding n× d matrix C. Ck = USV T (where Ck is the rank-k approximation of C). We then use US to represent each object as a k-dimensional vector. Each vector is normalized to unit length because our representations are count vectors where the absolute magnitude of a count contains little useful information – what is important is the relative differences between the counts of different dimensions. This is the representation SVD1. It is motivated by standard arguments for representations produced by dimensionality reduction: compactness and noise reduction. Denoising is also the motivation for the first SVD in the method proposed by Anandkumar et al. (2012).
We then perform a second SVD on the resulting matrix C′ of dimensionality n× k. C′ = U ′S′V ′T (full-rank, no dimensionality reduction). We again use U ′S′ to represent each object as a kdimensional vector. Each vector is normalized to unit length. This is the representation SVD2.
Note that the operation we are applying is not equivalent to a single linear operation because of the lenght normalization that we perform between the two SVDs.
SVD2 is intended to be a rotation of SVD1 that is more “focal” in the following sense. Consider a classification problem f over a k-dimensional representation space R. Let Mθ(f,R) be the size k′ of the smallest subset of the dimensions that support an accuracy above a threshold θ for f . Then a representation R is more focal than R′ if Mθ(f,R) < Mθ(f,R′). The intuition is that good deep learning representations have semantically interpretable hidden units that contribute input to a decision that is to some extent independent of other hidden units. We want the second SVD to rotate the representation into a “more focal” direction.
The role of the second SVD is somewhat analogous to that of the second SVD in the approach of Anandkumar et al. (2012), where the goal also is to find a representation that reveals the underlying structure of the data set.
The architecture of the 1LAYER setup is depicted in Figure 1.
Experimental setup. We use a corpus of movie review sentences (Pang and Lee, 2004). Following Schütze (1995), we first compute a left vector and a right vector for each word. The dimensionality of the vectors is 250. Entry i for the left (right) vector of word w is the number of times that the word with frequency rank i occurred immediately to the left (right) of w. Vectors are then tf-idf weighted and length-normalized. We randomly select 100,000 unique trigrams from the corpus, e.g., “tangled feelings of” or “as it pushes”. Each trigram is represented as the concatentation of six vectors, the left and right vectors of the three words. This defines a matrix of dimensionality n×d (n = 100000, d = 1500). We then compute SVD1 and SVD2 on this matrix for k = 100.
Analysis of correlation coefficients. Figure 2 shows histograms of the 10,000 correlation coefficients of SVD1 (left) and SVD2 (right). Each correlation coefficient is the correlation of two columns in the corresponding 100000 × 100 matrix and is transformed using the function f(c) = log10 |c| to produce a histogram useful for the analysis. The histogram of SVD2 is shifted by about 0.5 to the left. This is a factor of 100.5 ≈ 3. Thus, SVD2 dimensions have correlations that are only a third as large as SVD1 correlations on average.
We take this to indicate that SVD2 representations are more focal than SVD1 representations because the distribution of correlation coefficients would change the way it changes from SVD2 to SVD1 if we took a focal representation (in the most extreme case one where each dimension by itself supported a decision) and rotated it.
Discrimination task. We randomly selected 200 words in the frequency range [25, 250] from the corpus; and randomly arranged them into 100 pairs. An example of such a pair is (documentary, special). For each pair, we first retrieved the SVD1 and SVD2 representations of all triples from the set of 100,000 in which one of the two words was the central word. For the example, “typical documentary footage”, “good documentary can”, and “really special walk” are such triples. Then we determined for each dimension i of the 100 dimensions (for both SVD1 and SVD2) the optimal discrimination value θ by exhaustive search; that is, we determined the threshold θ for which the accuracy of the classifer ~vi > θ (or ~vi < θ) was greatest – where the discrimination task was to distinguish triples that had one word vs the other as their central word. So for “typical documentary footage” and “good documentary can” the classifier should predict class 1 (“documentary”), for “re-
ally special walk” the classifier should predict class 2 (“special”). Finally, of the 100 discrimination accuracies we chose the largest one for this word pair.
On this discrimination task, SVD2 was better than SVD1 55 times, the two were equal 15 times and SVD2 was worse 30 times. On average, discrimination accuracy of SVD2 was 0.7% better than that of SVD1. This is evidence that SVD2 is better for this discrimination task than SVD1.
This indicates again that SVD2 representations are more focal than SVD1 representations: each dimension is more likely to provide crucial information by itself as opposed to only being useful in conjunction with other dimensions.
To illustrate in more detail why this discrimination task is related to focality, assume that for a particular 100-dimensional representation r of trigrams t, the decision rule “if r(t)27 > 0.2 then ‘documentary’ else ‘special’ ” (i.e., if the value of dimension 27 is greater than 0.2, then the trigram center is predicted to be “documentary”, else “special”) has an accuracy of 0.99; and that the decision rule “if r(t)27 > 0.2 and r(t)91 < −0.1 then ‘documentary’ else ‘special’ ” has an accuracy of 1.0. Then M0.99(f, documentary-vs-special) = 1, M1.00(f, documentary-vs-special) = 2 and we can view the representation r as highly focal since a single dimension suffices for high accuracy and two dimensions achieve perfect classification results.
Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters θ for the entire training dataset:
θ = θ − η · ∇θJ(θ) (1)
As we need to calculate the gradients for the whole dataset to perform just one update, batch gradient descent can be very slow and is intractable for datasets that do not fit in memory. Batch gradient descent also does not allow us to update our model online, i.e. with new examples on-the-fly.
In code, batch gradient descent looks something like this:
for i in range(nb_epochs ): params_grad = evaluate_gradient(loss_function , data , params) params = params - learning_rate * params_grad
For a pre-defined number of epochs, we first compute the gradient vector params_grad of the loss function for the whole dataset w.r.t. our parameter vector params. Note that state-of-the-art deep learning libraries provide automatic differentiation that efficiently computes the gradient w.r.t. some parameters. If you derive the gradients yourself, then gradient checking is a good idea.6
We then update our parameters in the direction of the gradients with the learning rate determining how big of an update we perform. Batch gradient descent is guaranteed to converge to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces.
A one-dimensional function f : R → R is said to be a mixture of polynomials (MOP) function if it is a piecewise function of the form:
f(x) = {
a0i + a1ix+ · · ·+ anix n for x ∈ Ai, i = 1, . . . , k,
0 otherwise.
(12)
where A1, . . . , Ak are disjoint intervals in R that do not depend on x, and a0i, . . . , ani are constants for
all i. We will say that f is a k-piece (ignoring the 0 piece), and n-degree (assuming ani 6= 0 for some i) MOP function.
The main motivation for defining MOP functions is that such functions are easy to integrate in closed form, and that they are closed under multiplication and integration. They are also closed under differentiation and addition.
An m-dimensional function f : Rm → R is said to be a MOP function if:
f(x1, . . . , xm) = f1(x1) · f2(x2) · · · fm(xm) (13)
where each fi(xi) is a one-dimensional MOP function as defined in eq. (12). If fi(xi) is a ki-piece, nidegree MOP function, then f is a (k1 · · · km)-piece, (n1 + . . .+ nm)-degree MOP function. Therefore it is important to keep the number of pieces and degrees to a minimum. [19, 21] discuss the process of finding MOP approximations of univariate and bivariate conditional distributions. For space considerations, these are not discussed here.
Let us call the introduced SSPs, which describe stochastic RAPs, RAP-MDPs. In this section we overview some basic properties of RAP-MDPs. First, it is straightforward to see that these MDPs have finite action spaces, since |A| ≤ |R| |O|+ 1 always holds.
Though, the state space of a RAP-MDP is denumerable in general, if the allowed number of non-task operations is bounded and the random variables describing the operation durations are finite, the state space of the reformulated MDP becomes finite, as well.
We may also observe that RAP-MDPs are acyclic (or aperiodic), viz., none of the states can appear multiple times, because during the resource allocation process τ and dom(%) are non-decreasing and, additionally, each time the state changes, the quantity τ + |dom(%)| strictly increases. Therefore, the system cannot reach the same state twice. As an immediate consequence, all policies eventually terminate (if the MDP was finite) and, thus, are proper.
For the effective computation of a good control policy, it is important to try to reduce the number of states. We can do so by recognizing that if the performance measure κ is non-decreasing in the completion times, then an optimal control policy of the reformulated RAP-MDP can be found among the policies which start new operations only at times when another operation has been finished or in an initial state. This statement can be supported by the fact that without increasing the cost (κ is non-decreasing) every operation can be shifted earlier on the resource which was assigned to it until it reaches another operation, or until it reaches a time when one of its preceding tasks is finished (if the operation was a task with precedence constrains), or, ultimately, until time zero. Note that most of the performance measures used in practice (e.g., makespan, weighted completion time, average tardiness) are non-decreasing. As a consequence, the states in which no operation has been finished can be omitted, except the initial states. Therefore, each await action may lead to a state where an operation has been finished. We may consider it, as the system executes automatically an await action in the omitted states. By this way, the state space can be decreased and, therefore, a good control policy can be calculated more effectively.
We thus start by observing that the cost function (11) is a regularized “cost-of-sum”; it consists of two terms: the first term has a sum of quantities associated with different agents appearing as an argument for the function f(·) and the second term is a collection of separable regularization terms {hyk(yk)}. This formulation is different from the classical “sum-of-costs” problem, which usually seeks to minimize a global cost function, Jglob(w), that is expressed as the aggregate sum of individual costs {Jk(w)}, say, as:
Jglob(w) = N∑ k=1 Jk(w) (20)
The “sum-of-costs” problem (20) is amenable to distributed implementations [13]–[21]. In comparison, minimizing the regularized “cost-of-sum” problem in (11) directly would require knowledge of all sub-dictionaries {Wk} and coefficients {yk}. Therefore, this formulation is not directly amenable to the distributed techniques from [13]–[21]. In [35], the authors proposed a useful consensus-based primal-dual perturbation
5 method to solve a similar constrained “cost-of-sum” problem for smart grid control. In their method, an averaging consensus step was used to compute the sum inside the cost. We follow a different route and arrive at a more efficient distributed strategy by transforming the original optimization problem into a dual problem that has the same form as (20) — see (30a)–(30b) further ahead, and which can then be solved efficiently by means of diffusion strategies. There will be no need to exchange any information among the agents beyond the dual variable, or to employ a separate consensus step to evaluate the sum inside the cost in order to update their own sub-dictionaries.
B. Inference over Distributed Models: A Dual Formulation To begin with, we first transform the minimization of (11) into the following equivalent optimization problem by introducing a splitting variable z:
min {yk},z f(xt − z) + N∑ k=1 hyk(yk) (21a)
s.t. z = N∑ k=1 Wkyk (21b)
Note that the above problem is convex over both {yk} and z since the objective is convex and the equality constraint is linear. Problem (21a)–(21b) is a convex optimization problem with linear constraints so that strong duality holds [36, p.514], meaning that the optimal solution to (21a)–(21b) can be found by solving its corresponding dual problem (see (22) below) and then recovering the optimal primal variables {yk} and z (to be discussed in Sec. III-E):
max ν g(ν;xt) (22)
where g(ν;xt) is the dual function associated with the optimization problem (21a)–(21b), and is defined as follows. First, the Lagrangian L({yk}, z, ν;xt) over the primal variables {yk} and z is given by
L({yk}, z, ν;xt)
= f(xt − z) + νT z + N∑ k=1 [ hyk(yk)− νTWkyk ] (23)
Then, the dual function g(ν;xt) can be expressed as:
g(ν;xt)
, inf {yk},z
L({yk}, z, ν;xt)
= inf z
[ f(xt−z)+νT z ] + N∑ k=1 inf yk [ hyk(yk)−νTWkyk ] (24)
(a) = inf
u
[ f(u)−νTu+νTxt ] + N∑ k=1 inf yk [ hyk(yk)−νTWkyk ] = −sup
u
[ νTu−f(u) ] +νTxt− N∑ k=1 sup yk [ νTWkyk−hyk(yk) ] = −f?(ν) + νTxt −
N∑ k=1 h?yk(W T k ν) (25)
ν ∈ Vf ∩ Vhy1 ∩ · · · ∩ VhyN where in step (a) we introduced u , xt − z, and f?(·) and h?yk(·) are the conjugate functions of f(·) and hyk(·), respectively, with the corresponding domains denoted by Vf and Vhyk , respectively. We note that the conjugate function (or Legendre-Fenchel transform [37, p.37]), r?(ν), for a function r(x) is defined as [38, pp.90-95]:
r?(ν) , sup x
[ νTx− r(x) ] , ν ∈ Vr (26)
where the domain Vr is defined as the set of ν where the above supremum is finite. The conjugate function r?(ν) and its domain Vr are convex regardless of whether r(x) is convex or not [36, p.530] [38, p.91]. In particular, it holds that Vr = RM if r(x) is strongly convex [37, p.82]. Now since hyk(·) is assumed in Assumption 1 to be strongly convex, its domain Vhyk is the entire R
M . If f(u) happens to be strongly convex (rather than only convex, e.g., if f(u) = 12‖u‖22), then Vf would also be RM , otherwise it is a convex subset of RM . Therefore, the dual function in (25) becomes
g(ν;xt) = −f?(ν) + νTxt − N∑ k=1 h?yk(W T k ν), ν ∈ Vf (27)
Now, maximizing g(ν;xt) is equivalent to minimizing −g(ν;xt) so that the dual problem (22) is equivalent to
min ν − g(ν;xt) = f?(ν)− νTxt + N∑ k=1 h?yk(W T k ν) (28a)
s.t. ν ∈ Vf (28b) Note that the objective function in the above optimization problem is an aggregation of (i) individual costs associated with sub-dictionaries at different agents (last term in (28a)), (ii) a term associated with the data sample xt (second term in (28a)), and (iii) a term that is the conjugate function of the residual cost (first term in (28a)). In contrast to (11), the cost function in (28a) is now in a form that is amenable to distributed processing. In particular, diffusion strategies [14], [21], [39], consensus strategies [17]–[20], or ADMM strategies [30], [31], [33], [40]–[42] can now be applied to obtain the optimal dual variable νot in a distributed manner at the various agents.
To arrive at the distributed solution, we proceed as follows. We denote the set of agents that observe the data sample xt by NI . Motivated by (28a), with each agent k, we associate the local cost function:
Jk(ν;xt) ,
{ −νT xt|NI | + 1 N f ?(ν)+h?yk(W T k ν), k ∈ NI
1 N f ?(ν)+h?yk(W T k ν), k /∈ NI
(29)
where |NI | denotes the cardinality of NI . Then, the optimization problem (28a)–(28b) can be rewritten as
min ν N∑ k=1 Jk(ν;xt) (30a)
s.t. ν ∈ Vf (30b) In Sections III-C and III-D, we will first discuss the solution of (30a)–(30b) for the optimal dual variable, νot , in a distributed
6 manner. And then in Sec. III-E, we will reveal how to recover the optimal primal variables yok,t and z o t from ν o t .
C. Inference over Distributed Models: Diffusion Strategies
Note that the new equivalent form (30a) is an aggregation of individual costs associated with different agents; each cost Jk(ν;xt) only requires knowledge of Wk. Consider first the case in which f(u) is strongly convex. Then, it holds that Vf = RM and problem (30a)–(30b) becomes an unconstrained optimization problem of the same general form as problems studied in [15], [16]. Therefore, we can directly apply the diffusion strategies developed in these works to solve (30a)– (30b) in a fully distributed manner. The adapt-then-combine (ATC) implementation of the diffusion algorithm then takes the following form:
ψk,i = νk,i−1 − µ · ∇νJk(νk,i−1;xt) (31a) νk,i = ∑ `∈Nk a`kψ`,i (31b)
where νk,i denotes the estimate of the optimal νot at agent k at iteration i (we will use i to denote the i-th iteration of the inference, and use t to denote the t-th data sample), ψk,i is an intermediate variable, Nk denotes the neighborhood of agent k, µ is the step-size parameter chosen to be a small positive number, and a`k is the combination coefficient that agent k assigns to the information received from agent ` and it satisfies∑ `∈Nk a`k = 1, a`k > 0 if ` ∈ Nk, a`k = 0 if ` /∈ Nk (32)
Let A denote the N×N matrix that collects a`k as its (`, k)-th entry. Then, it is shown in [16] that as long as the matrix A is doubly-stochastic (i.e., satisfies A1 = AT1 = 1) and µ is selected such that
0 < µ < min 1≤k≤N
1
σk (33)
where σk is the Lipschitz constant1 of the gradient of Jk(ν;xt):
‖∇νJk(ν1;xt)−∇νJk(ν2;xt)‖ ≤ σk · ‖ν1 − ν2‖ (34) then algorithm (31a)–(31b) converges to a fixed point that is O(µ2) away from the optimal solution of (30a) in squared Euclidean distance. We remark that a doubly-stochastic matrix is one that satisfies A1 = AT1 = 1.
Consider now the case in which the constraint set Vf is not equal to RM but is still known to all agents. This is a reasonable requirement. In general, we need to solve the supremum in (26) with r(x) = f(x) to derive the expression for f?(ν) and determine the set Vf that makes the supremum in (26) finite. Fortunately, this step can be pursued in closedform for many typical choices of f(u). We list in Table II the results that will be used in Sec. IV; part of these results are derived in Appendix A and the rest is from [38, pp.90-95].
1 If Jk(ν;xt) is twice-differentiable, then the Lipschitz gradient condition (34) is equivalent to requiring an upper bound on the Hessian of Jk(ν;xt), i.e., 0 ≤ ∇2νJk(ν;xt) ≤ σkIM .
Usually, Vf for these typical choices of f(u) are simple sets whose projection operators2 can be found in closed-form — see also [43]. For example, the projection operator onto the set
Vf = {ν : ‖ν‖∞ ≤ 1} = {ν : −1 ν 1} (35) that is listed in the third row of Table II is given by
[ΠVf (ν)]m =  1 if νm > 1
νm if − 1 ≤ νm ≤ 1 −1 if νm < −1
(36)
where [x]m denotes the m-th entry of the vector x and νm denotes the m-th entry of the vector ν. Once the constraint set Vf is found, it can be enforced either by incorporating local projections onto Vf into the combination step (31b) at each agent [44] or by using the penalized diffusion method [45]. For example, the projection-based strategy replaces (31a)–(31b) by:
ψk,i = νk,i−1 − µ · ∇νJk(νk,i−1;xt) (37a)
νk,i = ΠVf [∑ `∈Nk a`kψ`,i ] (37b)
where ΠVf [·] is the projection operator onto Vf .
D. Inference over Distributed Models: ADMM Strategies
An alternative approach to solving the dual inference problem (30a)–(30b) is the distributed alternating direction multiplier method (ADMM) [30], [31], [40], [41], [46]. Depending on the configuration of the network, there are different variations of distributed ADMM strategies. For example, the method proposed in [40] relies on a set of bridge nodes for the distributed interactions among agents, and the method in [30], [31] uses a graph coloring approach to partition the agents in the network into different groups, and lets the optimization process alternate between different groups with one group of agents engaged at a time. In [41] and [46], the authors developed ADMM strategies that adopt Jacobian style updates with all agents engaged in the computation concurrently. Below, we describe the Jacobian-ADMM strategies from [46, p.356] and briefly compare them with the diffusion strategies.
The Jacobian-ADMM strategy solves (30a)–(30b) by first transforming it into the following equivalent optimization problem:
min ν N∑ k=1 [ Jk(νk;xt) + IVf (νk) ] (38a)
s.t. νk = ν`, ` ∈ Nk\{k}, k = 1, . . . , N (38b) where the cost function is decoupled among different {νk} and the constraints are coupled through neighborhoods. Then, the following recursion is used to solve (38a)–(38b):
νk,i = arg min νk N∑ k=1 {[ Jk(νk;xt) + IVf (νk) ] 2The projection operator onto the set Vf is defined as ΠVf (ν) ,
arg min x∈Vf
‖x− ν‖2.
7
TABLE II CONJUGATE FUNCTIONS USED IN THIS PAPER FOR DIFFERENT TASKS
Tasks f(u) f?(ν) Vf zot hyk (yk) h?yk (W T k ν) Vhyk y o k,t
Sparse SVD 1 2 ‖u‖22
1 2 ‖ν‖22 RM xt − νot γ‖yk‖1 + δ 2 ‖yk‖22 S γδ
( WTk ν
δ
) b RM T γ
δ
( WTk ν o t
δ
) a
Bi-Clustering 1 2 ‖u‖22
1 2 ‖ν‖22 RM xt − νot γ‖yk‖1 + δ 2 ‖yk‖22 S γδ
( WTk ν
δ
) RM T γ
δ
( WTk ν o t
δ ) Nonnegative Matrix 1
2 ‖u‖22
1 2 ‖ν‖22 RM xt − νot γ‖yk‖1,+ + δ 2 ‖yk‖22 S + γ δ
( WTk ν
δ
) d RM T +γ
δ
( WTk ν o t
δ
) c
Factorization M∑ m=1 L(um) η 2 ‖ν‖22 {ν : ‖ν‖∞ ≤ 1} γ‖yk‖1,+ + δ 2 ‖yk‖22 S + γ δ ( WTk ν δ ) RM T +γ δ ( WTk ν o t δ ) a Tλ(x) denotes the entry-wise soft-thresholding operator on the vector x: [Tλ(x)]n , (|[x]n| − λ)+sgn([x]n), where (x)+ = max(x, 0). b S γ
δ (x) is the function defined by S γ δ (x) , − δ 2 · ∥∥T γ δ (x) ∥∥2 2 − γ · ∥∥T γ δ (x) ∥∥ 1 + δ · xT T γ δ
(x) for x ∈ RM . c T +λ (x) denotes the entry-wise one-side soft-thresholding operator on the vector x: [T + λ (x)]n , ([x]n − λ)+. d S+γ δ (x) is defined by S+γ δ (x) , − δ 2 · ∥∥T +γ δ (x) ∥∥2 2 − γ · ∥∥T +γ δ (x) ∥∥ 1 + δ · xT T +γ δ (x) for x ∈ RM . e The functions Tλ(x), T +λ (x), S γδ (x), and S + γ δ (x) for the case of a scalar argument x are illustrated in Fig. 3.
+ N∑ `=1 bk` [ λTk`,i−1(ν`,i−1−νk)+‖ν`,i−1−νk‖22 ]} (39a)
λk`,i=λk`,i−1 + µ bk` · (νk,i − ν`,i) (39b)
where bk` is the (k, `)-th entry of the adjacency matrix B = [bk`] of the network, which is defined as:
bk` = 1 if ` ∈ Nk\{k}, bk` = 0 otherwise (40)
From recursion (39a)–(39b), we observe that ADMM requires solving a separate optimization problem (arg min) for each ADMM step. This optimization problem generally requires an iterative algorithm to solve when it cannot be solved in closed-form, which adds a third time scale to the algorithm, as explained in [33] in the context of dictionary learning. This situation is illustrated in Fig. 4. The need for a third time-scale usually translates into requiring faster processing at the agents
Time Scales
ADMM solution involves three time-scales:
between data arrivals, which can be a hindrance for adaptation in real-time.
For a long time there was only one bidirectional electronic dictionary WinLED in
Lithuania (VteX company http://www.led.lt/) for Lithuanian-English and Lithuanian-German language pairs. The dictionary was comfortable to use, since it had a „copy and translate“ function, which allows automatically translate the text copied to the OS clipboard. Nevertheless, WinLED has several disadvantages: the user interface is in English only, the wordlist is too short and misleading translation can be occurring.
Just recently a new dictionary „Tild s biuras“ has been distributed (http://www.tilde.lt/
biuras). It also maintain bidirectional Lithuanian-English and Lithuanian-German dictionaries. English-Lithuanian dictionary contains 50000 words and phrases. The dictionary proposes not only the translations, but word pronunciation, part of speech and usage of the word. The Lithuanian user interface is quite important since a major part of Lithuanian population is barred from e-content due the lack of good English skills.
An electronic version of B.Piesarskas „Didysis angl -lietuvi kalbos žodynas“ (Great
dictionary of English-Lithuanian languages) „Alkonas“ (http://www.fotonija.lt) is more comfortable than previous dictionaries, since it contains the book format and has a huge word list (around 100000) with the words usage examples. The translation is bidirectional, but the problem is that words are indexed only from English to Lithuanian language and Lithuanian word translation search is performed using the same index.
All these dictionaries are only alternatives for the paper dictionaries since they are more
comfortable and are possessed by faster search engine. Nevertheless, they have no additional data that is required for the MT, such as declensions, conjugations and etc. These available electronic dictionaries are generally different from those that are needed for the MT system. The problem is concerned with the requirement to perform not only word-to-word translation but to adjust the syntactic and semantic information as well as further processing of the target text.
parallel corpora for English and Lithuanian languages. Naturally, the building such corpora is time and effort consuming, since its designing process is rather complex. As an alternative for parallel corpora could be comparable corpora. Such corpora should contain texts in both English and Lithuanian languages and disseminated according to the topics of the texts.
The research on Computer-Based Translation (CBT) was initiated at KTU in 2001
(Tamulynas 2004: 16–19). Several prototypes of CBT systems have been produced, but every time the same problems have been encountered: they were robust and incomplete in terms of lexical data (Misevi ius et al. 2002: 38–45).
We use depth-first branch-and-bound (DFBnB) to utilize the efficient incremental bound computation. Depth-first
search makes sure that the search need not jump to a different search branch before backtracking is needed. In other words, the join tree only needs to work with one search history at a time.
We do need to backtrack to a previous search node once we finish a search branch or realize that a search branch is not promising and should be pruned. We need to retract all the newly set evidence since the generation of that search node and restore the join tree to a previous state. One way to achieve this is to reinitialize the join tree with correct evidence and perform a full join tree evaluation, which we have already pointed out is too costly. Instead, we cache the potentials and separators along the message propagation path before changing them by either setting evidence or overriding them with new messages. When backtracking, we simply restore the most recently cached potentials and separators in the reverse order. The join tree will be restored to the previous state with no additional computations. This backtracking method is much more efficient than reevaluating the whole join tree. For solving the MAP problem for Bayesian networks, Yuan and Hansen [20] show that the incremental method is more than ten times faster than full join tree evaluation in depth-first branchand-bound search.
In computer vision, convolutional neural networks (CNNs) have recently achieved new levels of performance for several inverse problems where RGB pixel appearance is mapped to attributes such as positions, normals or reflectance. In computer graphics, screenspace shading has recently increased the visual quality in interactive image synthesis, where per-pixel attributes such as positions, normals or reflectance of a virtual 3D scene are converted into RGB pixel appearance, enabling effects like ambient occlusion, indirect light, scattering, depth-of-field, motion blur, or anti-aliasing. In this paper we consider the diagonal problem: synthesizing appearance from given per-pixel attributes using a CNN. The resulting Deep Shading simulates various screen-space effects at competitive quality and speed while not being programmed by human experts but learned from example images.
Keywords: global illumination, convolutional neural networks, screen-space
Concepts: •Computing methodologies → Neural networks; Rendering; Rasterization;
Choco, ECLiPSe, and Gecode offer basic constraints which can be combined into more complex ones to a larger extent than Minion. For the particular models used in this paper the same constraints were used and this did not have any negative impact in terms of performance; for other applications it simplifies modelling problems though and is therefore also likely to have an impact on performance. For example the n-Queens problem could be modelled without auxiliary variables in Choco, ECLiPSe, and Gecode and is likely to be perform better than a model of the same problem in Minion which has to use auxiliary variables.
On the other hand Minion provides an implementation of the sum constraints with watched literals, which could improve its performance [3].
An interesting point is that Minion does not have a sum-equals constraint, but only sum-greater-or-equal and sum-less-or-equal constraints. The semantics of the sum-equals constraint can be achieved by combining the two constraints, but this increases the total number of constraints; in some cases considerably. Nevertheless there does not seem to be a negative impact on performance, on the contrary. This indicates that the most obvious way to implement a constraint may not always be the most efficient one.
(a) 120 cycle (2 seconds) window size
Figure 2 shows the impact of data injection (S1: Mirroring Spoof) upon the correlation of frequency measurements between PMUs. Correlation of frequency measurements between all PMUs is greater than 0.5 prior to the spoofing event at 1800 cycles, as shown by the yellow-red and blue-green gradient
4 curves on the left side of the Figure. The color gradients indicate the electrical distance between each pair of PMUs; PMUs that are electrically close show higher correlation.
Injecting spoofed data at one PMU affects correlation between that PMU and the nine others. As such, after the spoofing event begins a set of nine curves (those marked by the yellow-red gradient) decouples rapidly from the others. As shown in Figure 2, the nine r(f) correlation plots between the target PMU and all others begins to decrease shortly after the data injection begins at 1800 cycles. In order to measure the extent and impact of decorrelation for these signals, we formalize the following two metrics:
Maximum Correlation Deviation (MCD): A measure of the maximum difference between the non-spoofed (nspf ) data (blue-green gradient) and the spoofed (spf ) data (yellow-red gradient), calculated as an element-wise Euclidean distance:
MCD = max [√ (nspf− spf)2 ]
(1)
Maximum Correlation Out-Of-Bounds time (MCOOB): A measure of the amount of time that the spoofed data remains outside of a ±10% bound on the non-spoofed data. This is calculated as a summation of the time where the signal satisfies the following inequality:
MCOOB = Σcycles (0.9× nspf > spf > 1.1× nspf) (2) From the previous discussion, one can see how the magnitude and timing of decorrelation aren’t necessarily coupled homogeneously across signals nor window sizes. Thus, we measured MCD and MCOOB across all available data for our experimental setup (see Figure 3). This characterization across magnitude (MCD) and time (MCOOB) is important to in order to set up and parameterize the spoof detection algorithm presented in Section V. The distribution of MCD and MCOOB measures in Figure 3 suggest that for several signals and window sizes one can observe a significant separation between spoofed and non-spoofed data, however, this would not be entirely obvious, for example, for an operator looking at a specific location.
In these experiments we fix the size of the word embedding dimensions and recurrent layers so as not to exhaust our computational resources and then vary the different mechanisms for adapting the model. We used an LSTM with coupled input and forget gates for a 20% reduction in computation time (Greff et al., 2016). Dropout was used as a regularizer on the input and outputs of the recurrent layer as described in Zaremba et al. (2014). When the vocabulary is large, computing the full cross-entropy loss can be prohibitively expensive. For the large vocabulary experiments, we used a sampled softmax strategy with a unigram distribution to speed up training (Jean et al., 2015).
A summary of the key hyperparameters for each class of experiments if given in Table 2. The total parameter column in this table is based on the un-
adapted model. Adapted models will have more parameters depending on the type of adaptation. When using hash adaptation of the output layer, the size of the Bloom filter is 100 million and the size of the hash table is 80 million. The model is implemented using the Tensorflow library.1 Optimization is done using Adam with a learning rate of 0.001. Each model trained in under three days using 8 CPU threads.
Although the model is trained as a language model, it can be used as a generative text classifier. When there are multiple context variables, we treat all but one of them as known values and attempt to identify the unknown one. It is not necessary to compute the probabilities over the full vocabulary. The sampled softmax criteria can be used to greatly speed up evaluation of the classifier.
This work was funded by the Natural Sciences and Engineering Research Council (NSERC) EGP 453816-13, EGP 453816-14, and an industry partner whose name was withheld by request. We would also like to thank Dr. Rebecca Hallett, Jordan Hazell and the industry partner for assistance with data collection.
networks in order to cope with a vagueness and limitations of existing models for decision under imprecise and uncertain knowledge. This paper proposes a framework that combines two disciplines to exploit their own advantages in uncertain and imprecise knowledge representation problems. The framework proposed is a possibilistic logic based one in which Bayesian nodes and their properties are represented by local necessity-valued knowledge base. Data in properties are interpreted as set of valuated formulas. In our contribution possibilistic Bayesian networks have a qualitative part and a quantitative part, represented by local knowledge bases. The general idea is to study how a fusion of these two formalisms would permit representing compact way to solve efficiently problems for knowledge representation. We show how to apply possibility and necessity measures to the problem of knowledge representation with large scale data. On the other hand fuzzification of crisp certainty degrees to fuzzy variables improves the quality of the network and tends to bring smoothness and robustness in the network performance. The general aim is to provide a new approach for decision under uncertainty that combines three methodologies: Bayesian networks certainty distribution and fuzzy logic . Key-Words: - Possibilistic logic, Bayesian networks, Certain Bayesian networks, Local knowledge bases
We consider here the problem of handwritten digit recognition, which involves 10 classes (each corresponding to a digit, from 0 to 9), using theMNIST data [21]. Each digit image consists of 28×28 = 784 pixels, ranging from 0 to 255 (images are in grayscale). We divide each pixel value by 255 and use it as a feature. We evaluate the effect of error-generic and error-specific poisoning strategies against a multiclass LR classifier using softmax activation and the log-loss as the loss function. Error-generic attack. In this case, the attacker aims to maximize the classification error regardless of the resulting kinds of error, as described in Sect. 2.5. This is thus an availability attack, aimed to cause a denial of service. We generate 10 independent random splits using 1000 samples for training, 1000 for validation, and 8000 for testing. To compute the back-gradients ∇x cA required by our poisoning attack, we use T = 60 iterations. We initialize the poisoning points by cloning randomly-chosen training points and changing their label at random In addition, we compare our poisoning attack strategy here against a label-flip attack in which the attack points are drawn from the validation set and their labels are flipped at random. In both cases, we inject up to 60 attack points into the training set.
The results are shown in Fig. 4 (top row). Note first that our error-generic poisoning attack almost doubles the classification error in the absence of poisoning, with less than 6% of poisoning points. It is also much more effective than random label flips and, as expected, it causes a similar increase of the classification error over all classes (although some classes are easier to poison, like digit 5). This is even more evident from the difference between the
confusion matrix obtained under 6% poisoning and that obtained in the absence of attack.
Error-specific attack. Here, we assume that the attacker aims to misclassify 8s as 3s, while not having any preference regarding the classification of the other digits. This can be thus regarded as an availability attack, targeted to cause the misclassification of a specific set of samples. We generate 10 independent random splits with 1000 training samples, 4000 samples for validation, and 5000 samples for testing. Recall that the goal of the attacker in this scenario is described by Eq. (4). In particular, she aims at minimizing L(D̂ ′val , ŵ), where the samples in the validation set D̂ ′ val are relabelled according to the attacker’s goal. Here, the validation set thus only consists of digits of class 8 labelled as 3. We set T = 60 to compute the back-gradients used in our poisoning attack, and inject up to 40 poisoning points into the training set. We initialize the poisoning points by cloning randomly-chosen samples from the classes 3 and 8 in the training set, and flipping their label from 3 to 8, or vice-versa. We consider only these two classes here as they are the only two actively involved in the attack.
The results are shown in Fig. 4 (bottom row).We can observe that only the classification error rate for digit 8 is significantly affected, as expected. In particular, it is clear from the difference of the confusion matrix obtained under poisoning and the one obtained in the absence of attack that most of the 8s are misclassified as 3s. After adding less than 4% of poisoning points, in fact, the error rate for digit 8 increases approximately from 20% to 50%. Note that, as a side effect, the error rate of digit 3 also slightly increases, though not to a significant extent.
Poisoning Deep Neural Networks. We finally report a proofof-concept experiment to show the applicability of our attack algorithm to poison a deep network in an end-to-end manner, i.e., accounting for all weight updates in each layer (instead of using
a surrogate model trained on a frozen deep feature representation [20]). To this end, we consider the convolutional neural network (CNN) proposed in [21] for classification of the MNIST digit data, which requires optimizing more than 450, 000 parameters.10 In this proof-of-concept attack, we inject 10 poisoning points into the training data, and repeat the experiment on 5 independent data splits, considering 1, 000 samples for training, and 2, 000 for validation and testing. For simplicity, we only consider the classes of digits 1, 5, and 6 in this case. We use Algorithm 1 to craft each single poisoning point, but, similarly to [39], we optimize them iteratively, making 2 passes over the whole set of poisoning samples. We also use the line search exploited in [39], instead of a fixed gradient step size, to reduce the attack complexity (i.e., the number of training updates to the deep network). Under this setting, however, we find that our attack points only slightly increase the classification error, though not significantly, while random label flips do not have any substantial effect. For comparison, we also attack a multiclass LR classifier under the same setting, yielding an increase of the error rate from 2% to 4.3% with poisoning attacks, and to only 2.1% with random label flips. This shows that, at least in this simple case, deep networks seem to be more resilient against (a very small fraction of) poisoning attacks (i.e., less than 1%). Some of the poisoning samples crafted against the CNN and the LR are shown in Figs. 5 and 6. We report the initial digit (and its true label y), its poisoned version (and its label yc ), and the difference between the two images, in absolute value (rescaled to visually appreciate the modified pixels). Notably, similarly to adversarial test examples, also poisoning samples against deep networks are visually indistinguishable from the initial image (as in [20]), while this is not the case when targeting the LR classifier. This might be due to the specific shape of the decision function learned by the deep network in the input space, as explained in the case of adversarial test examples [15, 36]. We
10We use the implementation available at https://github.com/tflearn/tflearn/blob/ master/examples/images/convnet_mnist.py.
however leave a more detailed investigation of this aspect to future work, along with a more systematic security evaluation of deep networks against poisoning attacks. We conclude this section with a simple transferability experiment, in which we use the poisoning samples crafted against the LR classifier to attack the CNN, and vice-versa. In the former case, the attack is totally ineffective, while in the latter case it has a similar effect to that of random label flips (as the minimal modifications to the CNN-poisoning digits are clearly irrelevant for the LR classifier).
is a process of inferring equational consequences from a Boolean formula and given equational information. An equi-propagator for a formula ϕ is an extensive function µϕ : E → E defined such that for all E ∈ E ,
E ⊆ µϕ(E) ⊆ { e ∈ Leq0,1 ∣∣ ϕ ∧ E |= e } That is, a conjunction of equalities, at least as strong as E, made true by ϕ ∧ E. We say that an equi-propagator µϕ is complete if µϕ(E) = { e ∈ Leq0,1
∣∣ ϕ ∧ E |= e }. We denote a complete equi-propagator for ϕ as µ̂ϕ. We assume that equi-propagators are monotonic: E1 ⊆ E2 ⇒ µϕ(E1) ⊆ µϕ(E2). In particular, this follows, by definition, for complete equi-propagators. In Section 3.3 we discuss several methods to implement complete and incomplete equi-propagators.
Example 2. Consider the constraint
C = new int(X, 0, 4) ∧ new int(Y, 0, 4) ∧ int neq(X, Y)
and its corresponding Boolean representation ϕ = [[C]] on the bit representation where
X = [x1, x2, x3, x4] and Y = [y1, y2, y3, y4]
Assume the setting where E = { y1 = 1, y2 = 1, y3 = 0, y4 = 0 } signifying that Y = 2. Then, µ̂ϕ(E) = E ∪ {x2 = x3} indicating that X 6= 2. This occurs since ϕ∧E is equivalent to (x2 → x1)∧ (x3 → x2)∧ (x4 → x3)∧ (¬x1 ∨¬x2 ∨ x3 ∨ x4) and ϕ ∧ E |= x2 = x3.
The following theorem states that complete equi-propagation is at least as powerful as unit propagation.
Theorem 3. Let µ̂ϕ be a complete equi-propagator for a Boolean formula ϕ. Then, any literal that is made true by unit propagation for any clausal representation of ϕ using the equations in E is also determined true by µ̂ϕ(E).
Proof. Let ϕ be a Boolean formula, E an equi-formula, and let Cϕ and CE be any clausal representations of ϕ and of E respectively. Clearly ϕ |= Cϕ and E |= CE . Let b be a positive literal determined by unit propagation of Cϕ ∪ CE . Then by correctness of unit propagation, Cϕ ∪ CE |= b. Hence, ϕ ∧ E |= b and thus µ̂ϕ(E) |= b = 1. The case for a negative literal ¬b is the same, except that we infer b = 0.
The following example illustrates that equi-propagation can be more powerful than unit propagation.
Example 4. Consider ϕ = (x1 ↔ x2) ∧ (x1 ∨ x2) ∧ (¬x1 ∨ ¬x2 ∨ ¬x3). The clausal representation is (x1∨¬x2)∧(¬x1∨x2)∧(x1∨x2)∧(¬x1∨¬x2∨¬x3) and no unit propagation is possible, since there are no unit clauses. Equi-propagation (with no additional equational information) gives: µ̂ϕ(∅) = {x1 = 1, x2 = 1, x3 = 0}.
We present an anomaly detection system that identifies system issues based on self-correlated behavior of stable campaigns in a large-scale DSP. The system has been deployed in our production cluster and successfully detected several major system problems which otherwise would not be discovered until severe consequences are caused.
Recent innovated methodology development has significantly advanced biomedical information extraction, giving rise to a broad spectrum of improved biological and clinical applications. However, a number of limitations and problems at the frontiers of BioIE continue to impose additional challenges and present new opportunities for more accurate, efficient, reliable, scalable and sustainable BioIE research.
Data-driven approaches will continue to be a mainstream strategy. OpenIE techniques have been drawing more and more attention to enhance and scale BioIE systems by utilizing large, complex and heterogeneous data (different genres of textual data, structured vs. unstructured, text data vs. high throughput biological data) and extracting all meaningful relations and events without any restriction. Although in general domain several efforts have recently been made along this line, such as joint embedding of text and knowledge base (e.g. [202–205]) and unsupervised Web-scale event extraction[206], adapting current OpenIE techniques into BioIE applications is still at an early age. In addition, confidence based quality control and information normalization/fusion will be two remaining challenges for OpenIE platforms.
Joint inference models have been proposed to overcome the error propagation problem in pipeline approaches, by making predictions for multiple IE tasks simultaneously [207,208]. However, these models are complex and difficult to design. Exact inference is usually impossible and even inexact inference can be computationally expensive. Important questions remain to be answered in order for joint models to play a bigger role in BioIE, such as to what extent do we want a joint modeling? Which model architectures and learning methods better suit a specific IE problem? How to balance the computational efficiency and accuracy?
Evidently, deep learning techniques have played an unprecedented role in recent BioIE advances, which brought significant improvements across various subtasks. It is also signifying its continuous great potential for more advanced BioIE applications. Yet many issues remain to be investigated to take full advantage of deep learning for BioIE. Firstly, although deep learning can learn an internal distributed feature representation automatically from the data, combining domain knowledge and biomedical knowledge resources into deep learning architecture may lead to greater accuracy and flexibility. Collobert et al. [209] showed that by integrating linguistic and domain features, their DNNs improved over the original model (which does not use any feature engineering) and also outperformed benchmark systems in POS tagging and NER. The same strategy can be explored for BioIE. Secondly, deep learning models provide a natural way to learn distributed representations of entities and relations from knowledge bases [210,211]. Such approaches can be extended to fuse knowledge from multiple resources (e.g., knowledge bases, annotated corpora, and output from automatic systems). Finally, optimizing deep learning architecture to scale for big data processing and enable learning transferable features[212] will arouse increasing interest in the near future.
BioIE involves complex events, and those events are dependent. Better utilizing those inter-event dependencies[213] will be beneficial for further improving BioIE system performance, as well as for developing more integrative next-generation biomedical event extraction system through interconnecting biological reaction networks[214].
The big-data-driven BioIE research is rapidly evolving and projecting a bright future equipped with both learning-based algorithmic advances and multiple level model integrations.
In this section, we demonstrate the efficiency of our proposed methods on real-world problems. Our primary goal is to compare our two new methods, PALM and ADMM with the classical augmented Lagrangian method (denoted by ALM) in terms of their ability to optimize (2.2). All these three algorithms are implemented in MATLAB and use the L-BFGS-B routine [6] written in Fortran to solve the bound-constrained nonlinear subproblems.

As in the case of combinatorial structures, the study of randomly generated instances of NPcomplete problems in artificial intelligence has received significant attention in the last two decades. These problems include the satisfiability of boolean formulas (SAT) and the constraint satisfaction problems (CSP) (Achlioptas et al. 1997; Achlioptas et al. 2005; Cheeseman et al. 1991; Gent and Walsh 1994; Huberman and Hogg 1987; Mitchell et al. 1992; Monasson et al. 1999). In turn, these results on properties of random SAT and random CSP significantly help researchers in better understanding SAT and CSP, and developing fast solvers for them.
On the other hand, it is well known that reasoning in propositional logic and in most constraint languages is monotonic in the sense that conclusions obtained before new information is added cannot be withdrawn. However, commonsense knowledge is nonmonotonic. In artificial intelligence, significant effort has been paid to develop fundamental problem solving paradigms that allow users to conveniently represent and reason about commonsense knowledge and solve problems in a declarative way. Answer set programming (ASP) is currently one of the most widely used nonmonotonic reasoning systems due to its simple syntax, precise semantics and importantly, the availability of ASP solvers, such as clasp (Gebser et al. 2009), dlv (Leone et al. 2006), and smodels (Syrjänen and Niemelä 2001). However, the theoretical study of random ASP has not made much progress so far (Namasivayam and Truszczynski 2009; Namasivayam 2009; Schlipf et al. 2005; Zhao and Lin 2003).
(Zhao and Lin 2003) first conducted an experimental study on the issue of phase transition for randomly generated ASP programs whose rules can have three or more literals. (Schlipf
ar X
iv :1
40 6.
61 02
v1 [
cs .A
I] 2
et al. 2005) reported on their experimental work for determining the distribution of randomly generated normal logic programs at the Dagstuhl Seminar.
To study statistical properties for random programs, (Namasivayam and Truszczynski 2009; Namasivayam 2009) considered the class of randomly generated ASP programs in which each rule has exactly two literals, called simple random programs. Their method is to map some statistical properties of random graphs into simple random programs by transforming a random program into that of a random graph through a close connection between simple random programs and random graphs. As the authors have commented, those classes of random programs that correspond to some classes of random graphs are too restricted to be useful. Their effort further confirms that it is challenging to recast statistical properties of SAT/CSP to nonmonotonic formalisms such as ASP.
In fact, the monotonicity plays an important role in proofs of major results for random SAT/CSP. Specifically, major statistical properties for SAT/CSP are based on a simple but important property: An interpretation M is a model of a set of clauses/constraints if and only if M is a model of each clause/constraint. Due to the lack of monotonicity in ASP, this property fails to hold for ASP and other major nonmonotonic formalisms.
For this reason, it might make sense to first focus on some relatively simple but expressive classes of ASP programs (i.e., still NP-complete). We argue that the class of negative two-literal programs (i.e. normal logic programs in which a rule body has exactly one negative literal) is a good start for studying random logic programs under answer set semantics for several reasons1: (1) The problem of deciding if a negative two-literal program has an answer set is still NP-complete. In fact, the class of negative two-literal programs is used to show the NP-hardness of answer set semantics for normal logic programs in (Marek and Truszczynski 1991) (Theorem 6.4 and its proof, where a negative two-literal program corresponds to a simple K1-theory). (2) Many important NP-complete problems can be easily encoded as (negative) two-literal programs (Huang et al. 2002). (3) Negative two-literal programs allow us to conduct large scale experiments with existing ASP solvers, such as smodels, dlv and clasp.
In this paper we introduce a new model for generating and studying random negative twoliteral programs, called linear model. A random program generated under the linear model is of the size about c×n where c is a constant and n is the total number of atoms. We choose such a model of randomly generating negative two-literal programs for two reasons. First, if we use a natural way to randomly generate programs like what has been done in SAT and CSP, we would come up with two possible models in terms of program sizes (i.e. linear in n and quadratic in n), since only n2 negative two-literal rules in total can be generated from a set of n atoms. We study statistical properties of such random programs and have obtained both theoretical and experimental results for random programs generated under the linear model, especially, Theorem 1. These properties include the average number of answer sets, the size distribution of answer sets, and the distribution of consistent programs under the linear model. Second, such results can be used in practical applications. For instance, it is important to compute all answer sets of a program in applications, such as diagnoses and query answering, in P-log (Baral et al. 2009). In such cases, the number of answer sets for a program is certainly relevant. If we know the number of answer sets and the average size of the answer sets for a logic program, such information
1 Our definition of negative two-literal programs here is slightly different from that used by some other authors. But these definitions are essentially equivalent if we notice that a fact rule a← can be expressed as a rule a← not a′ where a′ is a new atom. Details can be found in Section 2.
can be useful heuristics for finding all answer sets of a given program. Also, the linear model of random programs may be useful in application domains such as ontology engineering where most of large practical ontologies are sparse in the sense that the ratio of terminological axioms to concepts/roles is relatively small (Staab and Studer 2004).
The contributions of this work can be summarised as follows:
1. A model for generating random logic programs, called the linear model, is established. Our model generates random logic programs in a similar way as SAT and CSP, but we distinguish the probabilities for picking up pure rules and contradiction rules. (Namasivayam and Truszczynski 2009) discusses some program classes of two-literal programs that may not be negative. However, as their major results are inherited from the corresponding ones in random graph theory, such results hold only for very special classes of two-literal programs. For instance, in regard to the result on negative two-literal programs without contradiction rules (Theorem 2, page 228), the authors pointed out that the theorem “concerns only a narrow class of dense programs, its applicability being limited by the specific number of rules programs are to have” (0 < c < 1, x is a fixed number, the number of rules m = bcN + x √ c(c−1)Nc and N = n(n−1))2. 2. We mathematically show that the average number of answer sets for a random program converges to a constant when the number of atoms approaches infinity. We note that the proofs of statistical properties, such as phase transitions, for random SAT and random CSP are usually obtained through the independence of certain probabilistic events, which in turn is based on a form of the monotonicity of classical logics (specifically, given a set of formulas S = {φ1, . . . ,φt} with t ≥ 0, it holds that Mod(S) =Mod(φ1)∩ ·· ·∩Mod(φt) when Mod(·) denotes the set of all models of a formula or a set of formulas). However, it is well known that ASP is nonmonotonic. In our view, this is why many proof techniques for random SAT cannot be immediately adapted to random ASP. In order to provide a formal proof for Theorem 1, we resort to some techniques from mathematical analysis such as Stirling’s Approximation and Taylor series. As a result, our proof is both mathematically involved and technically novel. We look into the application of our main result in predicting the consistency of random programs (Proposition 5 and Section 4.3). 3. We have conducted significant experiments on statistical properties of random programs generated under the linear model. These properties include the average number of answer sets, the size distribution of answer sets, and the distribution of consistent programs under the linear model. For the average number of answer sets, our experimental results closely match the theoretical results obtained in Section 3. Also, the experimental results corroborate the conjecture that under the linear model, the size distribution of answer sets for random programs obeys a normal distribution when n is large. The experimental results show that our theories can be used to predict practical situations. As explained above, we need to find all answer sets in some applications. For large logic programs, it may be infeasible to find all answer sets but we could develop algorithms for finding most of the answer sets. If we know an average size of answer sets, we might need only to examine those sets of atoms whose sizes are around the average size.
The rest of the paper is arranged as follows. In Section 2, we briefly review answer set semantics of logic programs and some properties of two-literal programs that will be used in the
2 There may be an error here as c−1 < 0.
subsequent sections. In Section 3, we first introduce the linear model for random logic programs (negative two-literal programs), study mathematical properties of random programs, and then present the main result in a theorem. In Section 4 we describe some of our experimental results and compare them with related theoretical results obtained in the paper. We conclude the work in Section 5. For the convenience of readers, some mathematical basics required for the proofs are included in the Appendix at the end of the paper.
Data streams are potentially infinite, and so, they can evolve with time. This means the statistical distribution of the data we are interested on can change. The idea behind the random layer is to improve data localization across the space the trained layer sees. Imagine the instance is a tiny luminous point on the space, with enough random neurons acting as a mirror we hope the trained layer can capture better the data movement. The strategy used by the random projection layer es shown in Figure 2
Except for the fact it is never trained, the random layer is a normal layer and need its activation functions, in this work sigmoid, ReLU, ReLU incremental and a Radial Basis Function are used.
The sigmoid function used is the standard one, with σ(x) ∈ [−1, 1]:
σ(ak) = 1
1 + e−ak
where ak = W > k x is the k-th activation function and W is the weight d× h
matrix (d input attributes, h output features).
ReLU functions are defined as:
zk = f(ak) = max(0, ak)
As stated in Section 2, ReLUs activation are very efficient as they require only a comparison. In our random projection we expect near 50% of the neurons to be active for a single instance ( the terrain of a ReLU is exemplified in Figure 3 ).
One variation we can do to the standard ReLU is to use the mean value of the attribute as a threshold. The mean value is calculated incrementally as instances arrive. We call this variant ReLU incremental, and is defined as:
f(ak) = max(āk, ak)
The last activation function we are using is the Radial Basis Function (RBF):
φ(x) = e− (x−ci)
2
2σ2
where x is an input instance attribute value and ci is a random value set at initialization time. Each neuron in the random layer has its own set of ci, the length of both vector x and c are the same. So, we can see the operation (x−ci)2 as the euclidean distance of the current instance to a randomly positioned center. The σ2 is a free parameter. A simplification we can do to this notation is:
γ = 1
2σ2
In our experiments we try different γ values passed at command line. We use the following notation in our experiments:
φ(x) = e−γ(x−ci) 2
All matrices and vectors in our model are initialized using random numbers. Matrices are used as normal weight matrix, but the function of the vectors are activation function dependent. Usually initialization is done using random numbers with µ = 0 and σ = 1. Assuming our data range ∈ [−1, 1] if we put
a Gaussian centered at one of the endpoints, half of its are of influence area if wasted and will never see a point making it harder to fill the whole space and so the discovering of points.
If a smaller range is used, σ ∈ (0, 1) (note the open interval), we can improve each neuron’s area of influence, as shown in figure 4b. In red the random numbers range is smaller than data range so if we put a Gaussian at the random endpoint can improve its influence are. In this example we used a Gaussian function as an example, but we the idea extends the same for activation functiones. In fact this is what we do in Section 6, specially when talking about the sigmoid neurons as they are always used at the trained layer.
Three experiments are conducted utilizing the UI-net architecture: (1) A varying number b ∈ [5, 15] of erosion and dilation operations are used to generate inputs St=0b in order to examine segmentation quality w. r. t. additional domain knowledge inserted into the input layer. The smaller b, the more information is provided. During training, no update step by the user model is utilized here, since the networks are trained with the same data in each epoch. We will refer to a network with property St=i = St=i+1 as static during training. (2) To infer, whether additional input data provided by the rule-based user model improves the segmentation quality, seeds are generated with an alternative system to (1). A varying number |G| ·n of seeds (St=0n ) are sampled uniformly at random
c© 2017 The Author(s) Eurographics Proceedings c© 2017 The Eurographics Association.
from G, where n ∈ [0.05, 0.9]. Static training is used. (3) Multiple simulated user interactions with UI-nets are evaluated. The UI-nets are trained with St=0b=10 as initial seeds and the user model proposed in Sec. 2.2.
We begin by considering what it might take, in principle, to anonymize reviews. Ideally, an anonymized review would exhibit stylometric features that are not linkable, with high accuracy, to any other review or a set thereof. At the same time, an anonymized review must be as meaningful as the original review and must remain faithful or “congruent” to it. (We will come back to this issue later in the paper). We believe that such perfect anonymization is probably impossible. This is because stylometry is not the only means of linking reviews. For example, if a TripAdvisor contributor travels exclusively to Antarctica and her reviews cover only specialized cruise-ship lines and related products (e.g., arctic-quality clothes), then no anonymization technique can prevent linkability by topic without grossly distorting the original review. Similarly, temporal aspects of reviews might aid linkability4. Therefore, we do not strive for perfect anonymization and instead confine the problem to the more manageable scope of reducing stylometric linkability. We believe that this degree of anonymization can be achieved by rewriting.
6.1.1 How to Rewrite Reviews? There are many ways of rewriting reviews in order to
reduce stylometric linkability. One intuitive approach is to construct a piece of software, e.g., a browser plugin, that alerts the author about highly linkable features in the prospective review. This could be done in real time, as the review is being written, similarly to a spellchecker running in the background. Alternatively, the same check can be done once the review is fully written. The software might even proactively recommend some changes, e.g., suggest synonyms, and partition long, or join short, sentences. In general, this might be a viable and effective approach. However, we do not pursue it in this paper, partly because of software complexity and partly due to the difficulty of conducting sufficient experiments needed to evaluate it.
Our approach is based on a hypothesis that the enormous power of global crowd-sourcing can be leveraged
4Here we mean time expressed (or referred to) within a review, not only time of posting of a review.
to efficiently rewrite large numbers of reviews, such that:
(1) Stylometric authorship linkability is appreciably reduced, and
(2) Resulting reviews remain sensible and faithful to the originals.
The rest of this section overviews crowdsourcing, describes our experimental setup and reports on the results.
6.1.2 Crowdsourcing Definition: according to the Merriam-Webster dictionary, Crowdsourcing is defined as: the practice of obtaining needed services, ideas, or content by soliciting contributions from a large group of people, and especially from an online community, rather than from traditional employees or suppliers.
There are numerous crowdsourcing services ranging in size, scope and popularity. Some are very topical, such as kickstarter (creative idea/project funding) or microworkers (web site promotion), while others are fairly general, e.g., taskrabbit (off-line jobs) or clickworker (on-line tasks).
We selected the most popular and the largest general crowdsourcing service – Amazon’s Mechanical Turk (MTurk) [1]. This choice was made for several reasons:
• We checked the types of on-going tasks in various general crowdsourcing services and MTurk was the only one where we encountered numerous on-going text rewriting tasks.
• We need solid API support in order to publish numerous rewriting tasks. We also need a stable and intuitive web interface, so that the crowdsourcing service can be easily used. Fortunately, MTurk has both a user-friendly web interface for isolated users and API support to automate a larger number of tasks.
• Some recent research efforts have used MTurk for the purpose of similar studies [28, 16, 18].
In general, we need crowdsourcing for two phases: (1) rewrite original reviews, and (2) conduct a readability and faithfulness evaluation between original and rewritten reviews. More than 400 random MTurkers participated in both phases.
6.1.3 Rewriting Phase Out of 3 randomly created AR and IR review sets we
used in Section 5, we randomly selected one as the target for anonymization experiments. We then uploaded all reviews in this AR set to the crowdsourcing service and asked MTurkers to rewrite them using their own words. We asked 5 MTurkers to rewrite each review, in order to obtain more comprehensive and randomized
data for the subsequent linkability study. While rewriting, we explicitly instructed participants to keep the meaning similar and not to change proper names from the original review. Moreover, we checked whether the number of words in each new review is close to that of the original before accepting a rewritten submission. Divergent rewrites were rejected5.
We published reviews on a weekly basis in order to vary the speed of gathering rewrites. Interestingly, most tasks were completed during the first 3 days of week, and the remaining 4 days were spent reviewing submissions. We finished the rewriting phase in 4 months. Given 40 authors and AR size of 5 (200 total original reviews), each review was rewritten by 5 MTurkers, resulting in 1, 000 total submissions. Of these, we accepted 882. The rest were too short or too long, not meaningful, not faithful enough, or too similar, to the original. Moreover, out of 200 originals, 139 were rewritten 5 times. All original and rewritten reviews can be found at our publically shared folder [7].
We paid US$0.12, on average, for each rewriting task. Ideally, a crowdsourcing-based review rewriting system would be free, with peer reviewers writing their own reviews and helping to re-writing others. However, since there was no such luxury at our disposal, we decided to settle on a low-cost approach6. Initially, we offered to pay US$0.10 per rewritten review. However, because review size ranges between 2 and 892 words, we came up with a sliding-price formula: $0.10 for every 250 words or a fraction thereof, e.g., a 490-word review pays $0.20 while a 180-word one pays $0.10. In addition, Amazon MTurk charges a 10% fee for each task.
One of our secondary goals was assessment of efficacy and usability of the crowdsourcing service itself. We published one set of 40 reviews via the user interface on the MTurk website, and the second set of 160 reviews – using MTurk API. We found both means to be practical, error-free and easy to use. Overall, anyone capable of using a web browser can easily publish their reviews on MTurk for rewriting.
After completing the rewriting phase, we continued with a readability study to assess sensibility of rewritten reviews and their correspondence to the originals.
6.1.4 Readability Study Readability study proceeded as follows: First, we
pick, at random, 100 reviews from 200 reviews in the AR set. Then, for each review, we randomly select one rewritten version. Next, for every [original,rewritten] review-pair, we publish a readability task on MTurk. In those tasks, we ask two distinct MTurkers to score
5A sample rewriting task and its submission are shown in the Appendix of this paper’s extended draft [4]. 6We consider the average of US$0.12 to be very low per review cost.
rewritten reviews by comparing its similarity and sensibility to the original one. We define the scores as Poor(1), Fair(2), Average(3), Good(4), Excellent(5), where Poor means that the two reviews are completely different, and Excellent means they are essentially the same meaning-wise. We also ask MTurkers to write a comprehensive result which explains the differences (if any) between original and rewritten counterparts7.
This study took one week and yielded 142 valid submissions. Results are reflected in Figure 3. The average readability score turns out to be 4.29/5, while 87% of reviews are given scores of Good or Excellent. This shows that rewritten reviews generally retain the meaning of the originals. Next, we proceed to re-assess stylometric linkability of rewritten reviews.
6.1.5 Linkability of Rewritten Reviews Recall that the study in Section 5 involved 3 review
sets each with 100 reviews per author. For the present study, we only consider the first set since we published anonymous reviews from first set to MTurk. In this first set, we replace AR with the corresponding set of MTurk-rewritten reviews where we pick a random rewritten version of each review, while each author’s IR remains the same.
Figures 2(a) and 2(b) compare LRs between originalrewritten reviews with varying number of authors. Interestingly, we notice a substantial decrease in LRs for all author sizes. For |AR| = 5 in a set of 1000 authors, Top-1 and Top-4 LR drop from 77.5% to 10% and from 90% to 32.5% respectively. Even only in 40 authors set, Top-1 LR decreases to 55%, which is significantly lower than 95% achieved with original reviews.
We also present a detailed comparison of original and rewritten reviews’ LRs with different AR sizes in Figures 2(c) and 2(d). Notably, both Top-1 and Top-4 LR decrease dramatically for all AR sizes. 35% is the highest LR obtained with rewritten reviews, which is substantially less than those achieved with original coun-
Examples:
description: C; produce.
description: AB; produce.
description: FJG; produce.
description: anything; produce.
description: AB or CD; produce.
description: FAB or GH or MIL; produce.
description: AB and CF; produce.
description: HL and RM and BT; produce.
description: AB and anything; produce.
description: AB and CF and anything; produce.
description: not AB and anything; produce.
description: not AB and CF and anything; produce.
description: not AB and not CF and anything; produce.
We consider the production counterparts of all recognition tasks. The learner is asked to generate one string matching the conditions in the description. We expect a compositional learner to solve the production tasks much faster if it has already been exposed to the recognition tasks (and vice versa).
20 -0.0076364135 0.3544236755 0.0228564453 0.43929321289
50 0.0297216796 0.4356188964 0.0276926269 0.45490655517
100 0.0031295781 0.4159604034 0.0263404192 0.41855424499
200 -0.0018015873 0.4172509078 0.0131416435 0.41539114761
500 -0.0047641256 0.4048449179 0.000372763 0.41228777224
1000 0.0091366043 0.4071086136 0.0001288413 0.40876417505
2000 0.0003983884 0.4061799454 -0.0062200826 0.40967137190
1000000 0.0000415605 0.4042661129 0.0000581977 0.40725332197
Página 100
We have presented a logical characterisation of Logic Programs with Ordered Disjunction (LPOD) that allows a direct study of ordered disjunction × as a derived operator in the logic of Here-and-There (HT), well-known for its application to (strongly equivalent) logic program transformations in Answer Set Programming. This characterisation provides an alternative implementation of LPODs that does not resort to auxiliary predicates. It has also allowed us to analyse the behavior of the × operator with respect to some typical properties like associativity, distributivity, idempotence, etc. As × is handled as a regular logical connective, our characterisation covers any arbitrary syntactic extension, and in particular, the so-called Disjunctive LPOD (DLPOD). We have shown that the semantics of DLPODs shows some differences with respect to the HT characterisation and established a formal comparison.
our result can also be seen as a confirmation of Theorem 6 in [6]. In that work, a reduct-based formalisation of LPODs was proposed in order to study strong equivalence relations among LPODs and regular programs. Theorem 6 in that work showed that their characterisation of strong equivalence for LPODs actually coincided with the one for regular programs. This result becomes trivial under our current approach, since ordered disjunction is just treated as an HT derived operator and, as proved in [4], HT arbitrary theories are strongly equivalent to logic programs.
We have implemented a first prototype for propositional LPODs using the current approach that uses DLV system as a backend6. Future work includes the extension of this prototype to deal with variables and with arbitrary combinations of ordered and regular disjunction in the head.
Two different lookup interfaces are available to the user. The volume lookup allows the user to lookup a word or prefix on a specific volume, French or Khmer. The Khmer entries can be searched from their headword in IPA or Khmer writing. On the left part of the result window, the volume headwords are displayed, sorted in alphabetical order. An infinite scroll allows the user to browse the entire volume. On the right part of the window, the entries previously selected on the left part are displayed (see Figure 10).
To perform inference for our model, we adopt the MED inference framework, and extend it by introducing an additional (negative) likelihood term into the optimized objective function, which emanates from the assumption (11) of our model.
Specifically, conventional MED inference in the context of our model would comprise solution of the following minimization problem:
min q(Z,Z̄,H,ϕ),ξ,ξ∗,X̄,δ2
KL ( q(Z, Z̄,H,ϕ)||p(Z, Z̄,H,ϕ) )
+ γ
D ∑
d=1
N ∑
n=1
(ξdn + ξ ∗ dn)
(14) under the constraints (13), where Z = {z·c}Cc=1, Z̄ = {z̄·c}Cc=1,H = {ηn} N n=1, ξ = {ξdn}d,n, ξ
∗ = {ξ∗dn}d,n, and KL(q||p) stands for the Kullback-Leibler (KL) divergence between the (approximate) posterior and the prior of our model. However, in our work, we follow a different approach, inspired from variational Bayesian inference [26], and the inference procedure of the nonparametric Bayesian large-margin classifier of [27]: We elect to optimize a composite objective function that takes into consideration both the expected (negative) log-likelihood of our hierarchical Bayesian model, which measures the goodness of fit to the training data, as well as the quality of the reconstruction of our training data, as is also performed in the context of conventional approaches. This way, inference for our model eventually reduces to
5 solution of the following problem
min q(Z,Z̄,H,ϕ),ξ,ξ∗,X̄,δ2
− D ∑
d=1
E[log p(yd·|zd·)]
+ γ D ∑
d=1
N ∑
n=1
(ξdn + ξ ∗ dn)
+ KL ( q(Z, Z̄,H,ϕ)||p(Z, Z̄,H,ϕ) )
∀d, n, s.t. :

 
 
ydn − E[η T nzd·] ≤ ε+ ξdn −ydn + E[ηTnzd·] ≤ ε+ ξ ∗ dn ξdn, ξ ∗ dn ≥ 0
(15) Note that, in the above expressions, all the expectations E[·] are computed w.r.t. the posterior q(Z, Z̄,H ,ϕ). Our inference algorithm proceeds in an iterative fashion. On each iteration, each one of the factors of the sought posterior, i.e., q(Z), q(Z̄),q(H), q(ϕ), as well as the pseudo-inputs X̄ , the noise variance δ2, the slackvariables ξ, ξ∗, and the employed kernel hyperparameters (if any) are consecutively optimized, one at a time. It has been shown that such an iterative consecutive updating procedure is guaranteed to monotonically optimize the objective function of our problem [21].
Proof. For any choice i‹ 1 , i‹ 2 , . . . , i‹k, let us denote by xpi‹q the corresponding x‹ P S. Following Audibert et al. (2013), we consider
ÿ
xPS
Ex,´j rTjs “ ÿ
i‹ 1 ,...,i‹j´1,i ‹ j`1,...,i ‹ k
ÿ
i‹j
Expi‹q,´j rTjs .
Now, keeping i‹ 1 , . . . , i‹j´1, i ‹ j`1, . . . , i ‹ k fixed the distribution Pxpi‹q,´j is the same for any choice of i‹j and therefore, since at every round of the game the learner must choose exactly one arm in the j’th problem, we must have
ř i‹j Expi‹q,´jrTjs “ T .
Putting it all together, we obtain
ÿ
i‹ 1 ,...,i‹j´1,i ‹ j`1,...,i ‹ k
ÿ
i‹j
Expi‹q,´j rTjs “ ÿ
i‹ 1 ,...,i‹j´1,i ‹ j`1,...,i ‹ k
T “ nk´1T ,
and thus 1
nk
ÿ
xPS
Ex,´j rTjs “ 1 nk nk´1T “ T n .
Datalog least-fixedpoint semantics tells us that a program without aggregation and negation has a unique minimal model. In other words, the result we get from evaluating the rules to fixpoint is always the same and consistent with the logic program. A Datalog program that includes aggregates and negated subgoals—like those in Section 3—may have several minimal models. There are more general classes of Datalog semantics that can decide which one minimal model is consistent with the intent of the programmer. In Section B.1, we show that the programs in Section 3 are in the class of locally stratified Datalog programs. In Section B.2, we argue that our runtime selects the one minimal model that is consistent with locally stratified Datalog semantics and our conditions for program termination.
B.1 Program Stratification Stratified Datalog semantics extend least-fixedpoint semantics with a method for organizing predicates into a hierarchy of strata; using a process called stratification. If some predicate A depends on an aggregated or negated result of another predicate B then A is placed in a higher stratum than B. A runtime that supports Stratified Datalog evaluates rules in lower strata first. Intuitively, this forces the complete evaluation of predicate B before predicate A is allowed to view the result. Stratification fails when there are cycles through negation or aggregation in the (rule/goal) dependency graph. Intuitively, if A and B depend on each other, perhaps even indirectly, then we can not evaluate one to completion while isolating the other.
Program stratification fails in Listings 1 and 2 (Section 3) since they both contain cycles through a stratum boundary (i.e., aggregation or negation). Therefore, we look to another class of Datalog semantics called locally stratified programs, which is defined in terms of a data dependent property. Intuitively, these programs are not necessarily stratified according to the syntax of the rules, but rather according to the application of those rules on a specific data collection. The following definition follows from Zaniolo et al., [31].
Definition 1 A program is locally stratifiable iff the Herbrand base can be partitioned into a (possibly infinite) set of strata S0, S1, . . ., such that for each rule r with head h and each atom g in the body of r, if h and g are, respectively, in strata Si and Sj , then
1. i ≥ j if g is a positive goal, and 2. i > j if g is a negative goal.
Listing 3: Listing 2 after XY-Stratification. 1 % Initialize the global model 2 G1: new_model(M) :- init_model(M). 3 4 % Compute and aggregate all outbound messages 5 G2: new_collect(reduce<S>) :- new_model(M), 6 training_data(Id, R), map(R, M, S). 7 8 % Compute the new model 9 G3: new_model(NewM) :-
10 old_collect(AggrS), old_model(M), 11 old_update(M, AggrS, NewM), M != NewM.
Intuitively, a program is locally stratifiable if the model data— formed from the initial facts and rule derivations—is stratifiable. The key to proving that the programs in Listings 1 and 2 are locally stratified lies in the temporal argument of our recursive predicates. The values of the temporal argument are taken from a discrete temporal domain that is monotonic. This allows us to use another program stratification technique called XY-Stratification [31].
Definition 2 Let P be a program with a set rules defining mutually recursive predicates. P is an XY-Stratified program if it satisfies the following conditions:
1. Every recursive predicate has a distinguished temporal argument.
2. Every recursive rule is either an X-rule or a Y-rule.
In an X-rule, the temporal arguments of every recursive predicate must refer to the current temporal state (e.g., J). A Y-rule has the following constraints.
1. The head predicate temporal argument value contains a successor state (e.g., J + 1).
2. Some positive goal in the body has a temporal argument of the current state (e.g., J).
3. The remaining recursive goals have a temporal argument that contains either the current state (e.g., J) or the successor state (e.g., J + 1).
Intuitively, an X-rule reasons within the current state and a Y-rule reasons from the current state to the next.
It is known that if a program is XY-stratified then it is locally stratified [31]. We now show that the programs in Section 3 are XY-stratified programs using the following construction applied to each recursive rule r.
1. Rename all recursive predicates that have the same temporal argument as the head with a prefix new .
2. Rename all other occurrences of recursive predicates with the prefix old .
3. Drop the temporal arguments from all recursive predicates.
If the resulting program following this construction can be stratified then the original program is locally stratified [31].
Theorem 2 Listing 2 is in the class of XY-stratified programs.
PROOF. The program in Listing 3 follows from applying XYStratification to the program in Listing 2. Listing 3 is trivially stratified by placing new collect in the highest stratum. Therefore, evaluating the rules in Listing 3 produces a locally stratified model that is consistent with the programmer’s intent in Listing 2.
Theorem 3 Listing 1 is in the class of XY-stratified programs.
PROOF. Figure 10 contains the dependency graph for the predicates appearing in Listing 1 after the XY-stratified transformation. The graph shows that the program is stratified into three strata. We further note that naming new local and new global comes from using max aggregation applied to the temporal argument of base predicates new vertex and new aggr, respectively.
B.2 Stratified Evaluation and Termination So far, we have applied XY-Stratification to our programs and to produce new programs that are stratifiable. The data in the ith time-step treated data from previous time-steps j < i as the extensional database (EDB). This allowed us to break dependency cycles at Y-rules, which, by definition, derive data for the subsequent time-step. These XY-Stratified programs formed the basis of the template physical plans described in Section 4.
We now conclude with a discussion of termination of our Datalog programs. The runtime terminates when the Datalog program reaches a fixpoint. We have already shown that the result of a fixpoint is a locally stratified model. However, this model could be infinite, in which case it would never terminate. Therefore, termination depends solely on a finite fixpoint solution. Under Datalog semantics this occurs when derivations range over a finite domain. Intuitively, if the range is finite then we will eventually derive all possible values since Datalog is monotonic and set-oriented.
For the programs listed in Section 3, this can occur in two possible ways. First, when the temporal argument ranges over a finite time domain. Since this argument is monotonic and finite, we are guaranteed to reach an upper bound, and hence terminate. A second possible termination condition comes from the range of state values given by the reduce and sync functions. Recall that these functions produce new state objects when given the (current) state object and list of messages. The runtime will consider these function predicates to be false if the new state object does not differ from the previous. Therefore, if there are a finite number of possible state objects, and each state object is produced exactly once, then we are also guaranteed to terminate. In other words, there are a finite number of state objects and the reduce and sync UDFs enumerate them in a monotonic fashion.
This paper has described the first framework for the integration of human knowhow into the Linked Data Cloud. Human know-how is an important source of knowledge on the Web, but its potential applications are limited by a general lack of structure and isolation from other knowledge. We have surveyed attempts to overcome these limitations both by user communities, trying to manually add structure and links to human know-how, and by existing research projects, trying to exploit this knowledge to develop intelligent systems. None of these approaches, however, automated the integration of this type of knowledge.
We proposed a Linked Data framework to automate both the extraction of human know-how, and its integration with related knowledge. We chose Linked Data as the ideal format to represent distributed knowledge on the Web. To validate this framework, we have applied it in a real-world scenario. First, this framework generated the Linked Data representation of over 200,000 processes by extracting human know-how from the wikiHow and Snapguide websites. Lastly, we have used our framework to link these processes both with each other and with DBpedia entities. We have evaluated the quality of these links and showed how they significantly outperform existing community-based integration efforts.
This result demonstrates how the integration of human know-how as Linked Data can immediately benefit Web users by allowing them to access the vast amount of know-how on the Web more efficiently. The application of this knowledge to develop intelligent systems has not been investigated and remains a promising direction for future work.
In this section, we evaluate the single-step ahead prediction, namely, predicting the crowd flows at time t using the historical observations. Table 5 shows the RMSE of all methods on both TaxiBJ and BikeNYC. Our ST-ResNet consistently and significantly outperforms all baselines. Specifically, the results on TaxiBJ demonstrates that STResNet (with 12 residual units) is relatively 26% better than ARIMA, 37% better than SARIMA, 26% better than VAR, 14% better than ST-ANN, 7% better than DeepST, 28% to 64% better than RNN, 18.1% to 45.7% better than LSTM, 17.4% to 46.1% better than GRU. ST-ResNet-noExt is a degraded version of ST-ResNet that does not consider the external factors (e.g. meteorology data). We can see that ST-ResNet-noExt is slightly worse than ST-ResNet, pointing out external factors are always beneficial. DeepST exploits spatio-temporal CNNs and is clearly better than other baselines. While both ST-ANN and VAR use spatial/temporal information and relationships among flows, they are worse than DeepST because they only consider the near spatial information and recent temporal information. Among the temporal models, GRU and LSTM have similar RMSE, and outperform RNN in average because GRU and LSTM both can capture long-term temporal dependencies. However, GRU-336 and LSTM-336 have very bad performance as well as RNN-336, which demonstrates RNN-based models cannot capture very long-term dependencies (i.e. period and trend). Intuitively, we rank all of these models, as shown in Figure 10(a).
Being different from TaxiBJ, BikeNYC consists of two different types of crowd flows, including new-flow and end-flow [22]. We here adopt a total of 4-residual-unit ST-ResNet, and consider the metadata as external features like DeepST [3]. ST-ResNet has relatively from 9% up to 71% lower RMSE than these baselines, demonstrating that our proposed model has good generalization performance on other flow prediction tasks. Figure 10(b) depicts the ranking of these models.
Burst patterns have been studied for decades in data mining and natural language processing communities. [24, 6, 25, 26, 7] studied burst detection problem from text streams. [24, 27, 2, 28, 29, 30, 7, 31] employed burst information to find bursty topics and events.
Extensive research has also been done on bilingual lexicon induction (e.g., [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.g., [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora. Some of these approaches also exploited temporal similarity and context information. We proposed new and richer clues including burst and graph topological structure. The nodes in BINet are not limited to named entities. More importantly, it is much cheaper to construct BINets than traditional information networks which usually rely on supervised information extraction and large numbers of language-specific resources.
In contrast to previous cross-lingual projection work like data transfer [47] and model transfer [48], we do not require any parallel data. Our BINet construction method was inspired by [7, 8]. Our work is the first to apply it to a cross-lingual setting. This is also the first attempt to apply the decipherment idea (e.g., [11, 12, 13]) to graph structures instead of sequence data. Another work related to ours is [41, 45], which used phonetic transliteration and frequency correlation to discover transliteration of entities. Compared with their work, this paper addresses a more general problem – we not only focus on named entities but all kinds of out-of-vocabulary words and phrases, which means that the number of candidates of a given node is much larger. Moreover, besides the named entity transliteration pairs, the BINets combined with a variety of clues (i.e., similarities
of pronunciation, translation, context and temporal correlation) can discover word/phrase and named entity translation pairs that cannot be discovered by their work.
Current research in machine learning considers generative Bayes models for classification generally inferior to discriminative models. The formalization of model modifications and the experiment results show that the Bayes model framework is considerably more flexible and effective than thought. Moreover, these findings directly extend to other types of text mining tasks, such as ranked retrieval of documents. Structured generative models such as the proposed TDM were shown to further improve modeling effectiveness, leading to new types of scalable generative models for text mining.
The generalized smoothing function presents virtually all of the commonly used smoothing functions for multinomial and n-gram models of text in a common mixture model framework. The methods differ only in the chosen discounting method, and how the smoothing coefficient is chosen. This describes the decades of statistical language modeling research in a concise form, as well as simplifies the development and analysis of new smoothing methods. Formalizing all the smoothing methods as approximate maximum likelihood estimation on a Hidden Markov Model re-establishes a probabilistic formulation for the functions. The formalization of feature transforms and weighted words as inference over probabilistic data similarly re-establishes the use of these methods in a probabilistic framework. Feature transforms were shown to greatly improved MNB performance for classification and ranking, and has potential implications for other types of generative text models.
Scalability limits the range of applications for probabilistic models. The naive inference used here as baseline is widely considered to be optimal: “Because we have to look at the data at least once, NB can be said to have optimal time complexity.” [Manning et al., 2008]. The presented sparse inference enables improved scaling of linear models and structured extensions to different types of tasks. Unlike parallelization or approximation, sparse inference reduces the total required computation non-linearly and provides exact results. The inference can be further combined with parallelization and many other efficiency improvements used in information retrieval and machine learning. Especially the application of structured sparse models becomes more scalable compared to naive inference.
The experimental results show that the commonly used generative models for text classification and ranking are comparatively weak baselines, whereas the modified and extended generative models have performance on par with strong task-specific methods. Among the possible models, the combination of TF-IDF weighting with uniform Jelinek-Mercer smoothing is one single-parameter option that performs well in both types of tasks. With more parameters and optimization for the specific dataset, a number of stronger models can be learned. The results show that the models developed in the thesis provide improved solutions for a variety of common applications, such as classification of sentiment, spam, e-mails, news, web-pages and Wikipedia articles, and different types of ranked retrieval of text documents. The obtained improvements should extend naturally to other tasks that process text with generative models, as well as to future text mining applications.
127
CHAPTER 7. CONCLUSION
Distributional word representations based on counting co-occurrences have a long history in natural language processing and have successfully been applied to numerous tasks such as sentiment analysis, recognising textual entailment, wordsense disambiguation and many other important problems. More recently low-dimensional and dense neural word embeddings have received a considerable amount of attention in the research community and have become ubiquitous in numerous NLP pipelines in academia and industry. One fundamental simplifying assumption commonly made in distributional semantic models, however, is that every word can be encoded by a single representation. Combining polysemous lexemes into a single vector has the consequence of essentially creating a weighted average of all observed meanings of a lexeme in a given text corpus.
Therefore a number of proposals have been made to overcome the issue of conflating several different senses of an individual lexeme into a single representation. One approach (Reisinger and Mooney, 2010; Huang et al., 2012) is to try directly inferring a predefined number of senses from data and subsequently label any occurrences of a polysemous lexeme with the inferred inventory. Similar approaches are proposed by Reddy et al. (2011) and Kartsaklis et al. (2013) who show that appropriate sense selection or disambiguation typically improves performance for composition of noun phrases (Reddy et al., 2011) and verb phrases (Kartsaklis et al., 2013). Dinu and Lapata (2010) proposed a model that represents the meaning of a word as a probability distribution over latent senses which is modulated based on contextualisation, and report improved performance on a word similarity task and the lexical substitution task. Other approaches leverage an existing lexical resource such as BabelNet or WordNet to obtain sense labels a priori to creating word representations (Iacobacci et al., 2015), or as a postprocessing step after obtaining initial word representations (Chen et al., 2014; Pilehvar and Collier, 2016). While these approaches have exhibited strong performance on benchmark word similarity tasks (Huang et al., 2012; Iacobacci et al., 2015) and some downstream processing tasks such as part-of-speech tagging and relation identification (Li and Jurafsky, 2015), they have been weaker than the single-vector representations at predicting the compositionality of multi-word expressions (Salehi et al., 2015), and at tasks which require the meaning of a word to be considered in context; e.g, word sense disambiguation (Iacobacci et al., 2016) and word similarity in context (Iacobacci et al., 2015).
In this paper we consider what happens when distributional representations are composed to
ar X
iv :1
70 2.
06 69
6v 1
[ cs
.C L
] 2
2 Fe
b 20
17
form representations for larger units of meaning. In a compositional phrase, the meaning of the whole can be inferred from the meaning of its parts. Thus, assuming compositionality, the representation of a phrase such as black mood, should be directly inferable from the representations for black and for mood. Further, one might suppose that composing the correct senses of the individual lexemes would result in a more accurate representation of that phrase. However, our counterhypothesis is that the act of composition contextualises or disambiguates each of the lexemes thereby making the representations of individual senses redundant. We investigate this hypothesis by evaluating the performance of single-vector representations and multi-sense representations at both a benchmark phrase similarity task and at a novel word-sense discrimination task.
Our contributions in this work are thus as follows. First, we provide quantitative and qualitative evidence that even simple composition functions have the ability to recover sense-specific information from a single-vector representation of a polysemous lexeme in context. Second, we introduce a novel word-sense discrimination task1, which can be seen as the first stage of word-sense disambiguation. The goal is to find whether the occurrences of a lexeme in two or more sentential contexts belong to the same sense or not, without necessarily labelling the senses. While it has received relatively little attention in recent years, it is an important natural language understanding problem and can provide important insights into the process of semantic composition.
Past research (Kim, 2014; Severyn and Moschitti, 2015) found a good
1http://nlp.stanford.edu/projects/glove/ preprocess-twitter.rb
initialization of word embeddings to be crucial in training an accurate sentiment model.
We thus evaluate the following evaluation schemes: random initialization, initialization using pre-trained GloVe vectors, fine-tuning pretrained embeddings on a distantly supervised corpus (Severyn and Moschitti, 2015), and fine-tuning pre-trained embeddings on 40k tweets with crowdsourced Twitter annotations. Perhaps counterintuitively, we find that fine-tuning embeddings on a distantly supervised or crowd-sourced corpus does not improve performance on past development test sets when including the additionally provided data for training. We hypothesize that additional training data facilitates learning of the underlying semantics, thereby reducing the need for sentiment-specific embeddings. Our scores partially echo this theory.
For this reason, we initialize our word embeddings simply with 200-dimensional GloVe vectors trained on 2B tweets. Word embeddings for unknown words are initialized randomly.
The Settlers domain, introduced in the 2002 International Planning Competition (IPC) (Long & Fox, 2003b) and used again in 2004 (Hoffmann & Edelkamp, 2005), is a good example of a problem exhibiting interesting use of metric fluents. The aim in Settlers problems is to build up transport and building infrastructure through the extraction, refinement and transportation of materials. The numeric structure of the domain is perhaps the most sophisticated of the IPC domains to date. First, there are six numeric resources and several actions that act upon each. The available resources, and the effects of actions upon them (consumption of a resource is shown as a negative value and production is shown as a positive value) are shown in Table 12. Another interesting feature of this domain is that not all resources can be directly produced: whilst the raw materials Timber, Stone and Ore can be directly extracted, Wood, Coal and Iron must be refined from their respective raw form. Finally, the domain contains transferable resources. In addition to the actions shown in the table, by which resources can be refined or consumed to fuel transportation, resources can be loaded and unloaded from vehicles. The effect of such load and unload actions is to increase or decrease the amount of a resource on a vehicle, and decrease or increase the amount stored at a given location. Apart from consuming or ‘producing’ (i.e. releasing) the remaining cargo space of the vehicle, no resource is produced or consumed during loading and unloading — it is only moved. However, expressing the model in pddl requires the pair of effects described, decreasing one variable and increasing another, which is indistinguishable from a combination of production and consumption.
In our previous work [6] we employed a fixed time window stream-processing model and applied a number of semi-supervised learning methods. The key concept is that for each batch fed into the algorithm, the prediction was measured against the actual position, which was then reported in the immediate future, employing an iterative model refinement process. Thus, the prediction algorithm performance was such that the result came at least one time unit before the record bearing the predicted position.
Each algorithm made a prediction using the fixed window batch and evaluated itself completing a training step against the actual position. All algorithms suffered from the cold start problem, especially because there was no distinction between the vessels semantics, i.e. each vessel record was treated as a record equally contributing to the model. This
implies that there was no separation of models between vessel types, geographic area or any other distinguishing parameter. Even though this would greatly improve the accuracy of the result as reported in [26] but also based on mere intuition, such approach was not followed because of the computational burden that would pose to the underlying infrastructure. As such, we created our single pass, real-time responding model in MOA, employing a simple perceptron, conveniently leveraging on the system’s architecture to scale effectively and deliver timely results.
This thesis emphasized on the applicability of SSL techniques in a variety of practical applications (Figure 11.1). The applications fields were: (a) industrial assembly lines monitoring, (b) sea border surveillance, (c) elders’ falls detection, (d) transportation tunnels inspection, (e) concrete foundation piles defect recognition, (f) commercial sector companies financial assessment and (g) image advanced filtering for cultural heritage applications.
The main contribution lies in the complex synergistically schemes created from scratch, depending on the application scenario. SSL approaches favor the development of hybrid models; they can be used with, almost, any traditional machine learning approach, facilitating the creation of robust DSSs. Additionally, SSL techniques were used for the system initialization, the detection mechanism formulation, feedback schemes setup, etc. Such advantages are ideal for holistic, user-feedback, information systems. Minimal effort, from user’s side is required during trivia steps such as annotation, selection and data labeling. At the same time, minor mistakes can be tolerated by the proposed systems.
The first approach is a hybrid, self-training approach for the industrial workflow recognition. The core mechanism is a topologically optimized feed forward neural network, created using less than 40% of the available data. Then, given new data the classifiers labels them and retrains itself using the most appropriate ones. In order to avoid labeling errors a secondary similarity based classifier is activated, when specific criteria are met. If classifiers did not agree, an expert was summoned via a feedback scheme.
Sea border surveillance is another vision based approach. In that case SSL was used exclusively for training data set creation purposes. The system utilizes a pixel level classification mechanism based on SVMs. In order to correct man made annotation mistakes, which could jeopardize the SVM classifier performance, the initial annotated image data set was re-annotated according to a scalable graph based SSL approach.
95 Concluding remarks
DECISION MAKING VIA SEMI-SUPERVISED MACHINE LEARNING TECHNIQUES | Ph.D. Thesis
Elders’ falls detection is an important topic. In this case, the SSL techniques were used for both system initialization and adaptation. At first, under the cluster assumption, extracted feature vectors form two classes: fall and non-fall. Then the classifier is initialized and adopt to the new data using a self-training approach, similar to the industrial monitoring one, previously described. Despite being a vision based approach, no actual video over 3 seconds duration is recorded, preserving the people’s privacy.
Applicability over extremely complex data cases was evaluated in tunnel surface defect identification scenarios. It is a typical two class identification problem; SSL approaches could not compete other state of the art techniques in detection performance neither for execution times. There was three major drawbacks: low feature quality, long execution times and hardware requirements. Deep learning hierarchical schemes (i.e. convolutional neural networks) detection abilities outperformed all other approaches.
The structural integrity of foundation piles was also evaluated using graph based approaches. Using wave propagation theory and noise modeling the entire set of foundation piles can be assessed simultaneously. In particular the similarity of the waveforms is projected in a nearest neighbor graph. Then, given the status of few piles, the information propagates through the edges to all connected nodes. The proposed approach reduces labeling effort, which is both costly and time consuming. Such an approach encourages the data sampling, since larger the data base better the classification accuracy.
The labeled data selection impact was evaluated, using data from the Greek commercial sector. A great variety of sampling approaches are used to evaluate the descriptive abilities of small training sets, given a classifier raging from traditional models, e.g. logistic regression, to advanced soft computing techniques, e.g. artificial neural networks. Simulation outcomes suggest that no optimal choice, regarding the data sampling, neither for the classification approach, exists.
Finally, a novel idea for image meta-filtering is proposed. An appropriate distance metric is calculated via SSL assumptions which models user’s behavior over image selection. Such an approach allows multiple uses of the same data sets in different applications (e.g. subsets of the same data can be used for 3D reconstruction, tourism promotion, book publications, etc.). User’s feedback is involved only for a minor set of data. The proposed scheme facilitates the refinement of retrieval results always under the scope of the end user needs.
Experimental outcomes are in accordance with existing literature suggestions: The unlabeled data can be used in order to improve models’ performance (i.e. accuracy, precision, etc.). Yet, there are no optimal solutions regarding the model and feature selection, nor for the data sampling techniques. Heuristic approaches or empiric rules are not always sufficient, although they provide a good starting point during the first stages of a DSS development.
However, there is no “free-lunch”. In particular, in real applications the data abundance creates many implementation issues9; among them are the memory issue and the time issue. The former directly affects the scalability of the models in datasets with thousands of entries, although there are studies dealing with such an issue. The latter, limit the models applicability in on-line systems (e.g. pixels classification in video sequences). Finally, low feature quality can jeopardize models performance; in the SSL framework bad features result in even more severe performance loss. The model propagates the knowledge to new data and adapt itself to the new wrong labels.
The end.
9 Generally speaking, since the SSL field spans a great variety of methodologies.
96 Decision Making via Semi-Supervised Machine Learning Techniques
Eftychios Protopapadakis | TECHNICAL UNIVERSITY OF CRETE
Distant Supervision (DS) was first proposed in [36], where labeled data was generated by aligning instances from the Yeast Protein Database into research articles to train an extractor. This approach was later applied for training of relation extractors between entities in [10].
Automatically gathering training data with DS is governed by the assumption that all sentences containing both entities engaged in a reference instance of a particular relation, represent that relation. Many methods have been proposed to reduce the noise in training sets from DS. In a series of works the labels of DS data are seen as latent variables. Riedel et al. [21] relaxed the strong all sentences-assumption and relaxed it to an at-least-one-sentence-assumption, creating a Multi-Instance learner. Hoffman et al. [37] modified this model by allowing entity pairs to express multiple relations, resulting in a Multi-Instance Multi-Label
setting (MIML-RE). Surdeanu et al. [23] further extended this approach and included a secondary classifier, which jointly modeled all the sentences in texts and all labels in knowledge bases for a given entity pair.
Other methods apply heuristics [38], model the training data as a generative process [39, 40] or use a low-rank representation of the feature-label matrix to exploit the underlying semantic correlated information.
Summary. We take a step towards communicating robot objectives to people. We found that an approximate-inference model using a deterministic Euclidean-based update on the space of candidate reward function parameters performed best at teaching real users, and outperforms algorithmic teaching that assumes exact inference. We additionally found after augmenting such model with a coverage objective, it significantly outperformed letting the user passively familiarize to the robot. Limitations and Future Work. Our results reveal the promise of algorithmic teaching of robot objective functions. Our model of human learning is far from perfect, and more work is needed to explore the process people use to extrapolate from observed robot behavior.
Furthermore, in this work we focused on the robot’s physical behavior as a communication channel because people naturally infer utility functions from it. Future work could augment this with other channels, for instance visualizations of the objective function or language-based explanations.
With volumes of movie review data, it’s hard for us to define generic categories without looking at them one by one. Therefore, it’s necessary to run some unsupervised models to get an overview of what’s being talked in the whole corpus. Here we applied Latent Dirichlet Allocation [14, 15] to discover the main topics related to the movies and actors. In a nutshell, the LDA model assumes that there exists a hidden structure consisting of the topics appearing in the whole text corpus. The LDA algorithm uses the co-occurrence of observed words to learn this hidden structure. Mathematically, the model calculates the posterior distribution of the unobserved variables. Given a set of training documents, LDA will return two main outputs. The first is the list of topics represented as a set of words, which presumably contribute to this topic in the form of their weights. The second output is a list of documents with a vector of weight values showing the probability of a document containing a specific topic.
Now that as soon as we get the results of LDA, we should get an idea of the topics being talked. Combined with what we got after manually looking through, we defined the 8 generic categories of movie reviews as in Table 3.
Since we are trying to classify each review into the above 8 categories, in order to build reasonable classifiers, first we need to be provided with a labeled dataset. Each of the movie reviews was labeled by at least two persons, and only ones with the same label were selected to be included in our training and testing data. In this way, we can filter out movie reviews with human’s biases and 5000 out of each TV series’ reviews were chosen finally.
In the experiments, we utilized the Enron email dataset to investigate the correlation between the proposed typical social network features and the opinions’ asymmetry on interrelationships. The individuals’ opinions on interrelationships
are characterized with four important language features introduced in section 2.2.
WPC is only one of the possible position evaluation functions. Others popular ones include neural networks and n-tuple networks. To allow direct comparison between various position evaluation functions and algorithms capable of learning their parameters, Lucas and Runarsson [18] have appointed the Othello Position Evaluation Function League 1. Othello League, for short, is an on-line ranking of Othello 1-ply state evaluator players. The players submitted to the league are evaluated against SWH (the Standard WPC Heuristic Player).
Both the game itself and the players are deterministic (with an exception of the rare situation when at least two positions have the same evaluation value). Therefore, to provide more continuous performance measure, Othello League introduces some randomization to Othello. Both players are forced to make random moves with the probability of = 0.1. As
1http://algoval.essex.ac.uk:8080/othello/League.jsp
a consequence the players no longer play (deterministic) Othello, but stochastic -Othello. However, it was argued that the ability to play -Othello is highly correlated with the ability to play Othello [18].
The performance in Othello League is determined by the number of wins against SWH player in -Othello in 50 double games, each consisting of two single games played once white and once black. To aggregate the performance into a scalar value, we assume that a win counts as 1 point, while a draw 0.5 points. The average score obtained in this way against SWH constitutes the Othello League performance measure, which we incorporate in this paper.
Current reinforcement learning algorithms make heavy use of deep neural networks, both to extract high level features from the observations it receives, and as function approximators to represent its policy or value functions. Consider the set of input-target pairs S = {(x0, y0), (x1, y1), ...(xn, yn)} generated by some function f∗(x). The goal of supervised learning with neural networks is to learn a parametrized function f(x; θ) that best approximates function f∗. The performance of f is evaluated with the empirical loss
L(θ) = 1 |S| ∑ s∈S l(f(xs; θ), ys) (1)
where l(f(xs; θ), ys) is referred to as a loss function, and gives a quantitative measure of how good f is at modelling f∗. The model parameters θ are learned with stochastic gradient descent (SGD) by iteratively applying the update
θi+1 ← θi − α∇θiL(θi)
for a learning rate α. In SGD, L(θ) is usually approximated with
L̄(θ) = 1 |S′| ∑ s′∈S′ l(f(xs′ ; θ), ys′),
where S′ ⊂ S is a mini-batch sampled from S. The choice of α and nS′ = |S′| presents a trade-off between computational efficiency and sample efficiency. Increasing nS′ by a factor of k increases the time needed to calculate∇θL̄ by a factor of k′, for k′ ≤ k, and reduces its variance proportionally to 1 k (Bottou et al., 2016). In order to mitigate the increased time per parameter update we can increase the learning rate to α′ for some α′ ≥ α. However there are some limits on the size of the learning rate, so that in general α′ ≤ kα (Bottou et al., 2016). The hyper-parameters α′ and k are chosen to simultaneously maximize kk′ and minimize L.
Classically, signals are classified in a reservoir computer using a linear combination of the reservoir states, determined by a collection of trained output weights [1, 10, 11, 12]. To train the output weights, input signals
of all classes of the training set are processed, storing all of the reservoir states in a matrix Xtrain ∈ RTS×N , where T is the number of times the reservoir state is collected for each input signal, S is the total number of signals in the training set, and N is the number of nodes in the reservoir. The output weights are chosen to map the reservoir states onto given target functions. Typically a unique target vector yk ∈ RT is chosen for each class k. All of the individual target vectors are combined into a block diagonal target matrix Ytarget, with diagonal blocks arranged to correspond to the class of the input signals. The most popular method to compute the output weights Wout is to use regularized least squares. That is, Wout is found that minimizes the equation
‖Ytarget −XtrainWout‖22 + λ ‖Wout‖ 2 2 .
The regularization constant helps to minimizes overfitting of the data. If λ is very close to zero, then the weights will fix Xtrain to the targets Ytarget very well, but the coefficients in Wout may be large and cause sensitivity to noise. Instead, a moderately small value of λ is chosen so the data fits well while dampening the weights. Since a single target vector may be insufficient to describe an entire class of input signals, it may be advantageous to define new targets in terms of the computed weights: Ỹtarget := XtrainWout.
After computing Wout and Ỹtarget, the reservoir can now be used for classification tasks. Denote the j th
column of Wout by ω (j) and the jth column of Ỹtarget by ỹ (j), both coming from the input signal u(j) in the training set. Suppose u is a newly encountered input signal with reservoir dynamics X ∈ RT×N . The signal u is determined to belong to the kth class if ∥∥∥ỹ(j) −Xω(j)∥∥∥2
2 (3)
is minimized for j ∈ Ck. For large training sets, it may be very time consuming to compute the output weights. However, the training step is computed offline, and once finished the weights can be stored in memory. Unfortunately, assigning a single unclassified input signal to a class using these stored weights is also expensive with a computational complexity of O(NTS). For moderately sized reservoirs and signals, this method may be prohibitively expensive for real-time classification. As shown in the following section, one can reduce the computational complexity by forgoing the trained weights entirely and performing classification using a supervised clustering method on the reservoir states.
One of the big challenges in Game Design is the tuning of game parameters. Given a set of parameter values, a new game instance is created. The difficulty of a game could change significantly when varying one single parameter of a game. The behavior of a human player or an AI agent and the fun
level of the game will also be affected. For instance, doubling the gravity in Flappy Bird will expect to increase the frequency of calling the “jump” actions. However, the selection of game parameters and tuning are not trivial due to the number of parameter to be tuned and the number of possible values of each of the parameters, resulting in a large search space. This motivates the research presented in this paper.
The authors applied the Random Mutation Hill Climber (RMHC) and two new algorithms, the Biased Mutation RMHC (B-RMHC) and the N-Tuple Bandit Evolutionary Algorithm (N-Tuple), to evolving game instances based on a real-time continuous 2-player competitive game called Space Battle Evolved (detailed in Section IV-A).
The Biased Mutation RMHC exploits some particular parameters which are considered to be more important after some pre-selection process. The N-Tuple Bandit Evolutionary Algorithm uses a bandit approach to balance the exploration and exploitation of the search space of every game parameter and a model to estimate the quality of unsampled game instances. The statistical results based on the final fitness of the solutions found by the three algorithms suggest the N-Tuple to be significantly better than the other two methods, being able to produce high fitness games.
Two human players have tested some of the evolved games and provided valuable reviews. Both players preferred the new game evolved using the N-Tuple Bandit Evolutionary Algorithm, although they offered mixed opinions on the RMHC games. One highlight of this study is evolving the enemy AI as part of the game parameters. The effect of changing the opponent player was explored in the human trials, indicating that even though this aspect has a great effect on the quality of the gameplay, an outstandingly easy or difficult environment reduces this effect slightly.
The experimental results on optimising Space Battle Evolved (Section V) illustrate the outstanding and robust
performance of the N-Tuple Bandit Evolutionary Algorithm. With this in mind, we can foresee a bright future for the NTuple Bandit Evolutionary Algorithm in AI-Assisted Game Design. Further work will look into the benefits of increasing the number of fitness evaluations, meant to reduce the noise in the evolution. Additionally, although the novel approach used in the Biased Mutation RMHC shows promise, improvements should be considered, such as increasing the re-sampling when measuring parameter importance metrics to produce more accurate results. Another possible future work is to apply this evolutionary algorithm with other game framework, such as GVG-AI [13].
In this paper we proposed the SCLP framework as a high-level declarative, executable specification notation to model in a natural way some aspects of the e-mobility optimization problem [9], consisting in coordinating electric vehicles in order to overcome both energetic and temporal constraints. In particular, we considered the trip and journey optimization sub-problems, consisting in finding respectively the energy- and time-optimal route from one destination to another one, and the optimal sequence of coupled trips, in terms of the same criteria, guaranteeing that the user reaches each appointment in time. For both the optimization problems, we provided an SCLP program in CIAO Prolog, by
explicitly implementing the soft framework, that is, the additive and the multiplicative operations of the chosen semiring. The former is a slight variant of the CIAO program proposed in [5, Section 4.4] to specify the multicriteria version of the shortest path problem. With respect to the program proposed there, here we implemented a different semiring, (also proposed in [5]), i.e., the one based on the Hoare Power Domain operator, which allowed us to obtain only the best (i.e. non-dominated) routes in terms of time and energy consumption. We thus provided an implementation of the two operations of this semiring, by defining two predicates modelling them. The SCLP program modelling the journey optimization problem then uses the trip optimization problem results as inputs. It is also based on the same semiring that, in this case, allowed us to find the best journeys in terms of the two cost criteria.
As said above, the soft framework is explicitly implemented into each CIAO program: there is for example a different plus predicate in each optimization program we have proposed. However, it would be interesting to study a general way to embed the soft framework in Ciao Prolog. Trivially, one could provide a library offering a more general implementation of the operations of the semiring of each type of problem. Most interestingly, one could instead think to provide a meta-level implementing more efficiently the soft framework.
Differently from our solution, which allows us to obtain the set of all the optimal journeys, in the mathematical model proposed in [9] a form of approximation is introduced, by considering an aggregated cost function to be optimized. Their goal is indeed to minimize this cost function, which considers different cost criteria: besides the travel time and the consumed energy, they also take into account the charging cost, the number of charging events, etc. In modelling the problem, here we introduced a simplification by considering just the two main cost criteria, that allows us a slender presentation of the work. However, it is obvious that the SCLP programs can be easily modified to also take into account several other cost criteria. On the other side, we preferred not to introduce any approximation of the solution, by instead returning all optimal journeys considered equivalently feasible. However, since the use of partially ordered structures, as in our case, can in general lead to a potentially exponential number of undominated solutions, sometimes it becomes crucial to keep the number of configurations as low as possible through some form of approximation allowing us to adopt a total order. In this case, the right solution could be to adopt a function that composes all the criteria in a single one and then to choose the best tuple of costs according to the total ordering that the function induces [5, Section 6.1].
As said above, our aim is mainly to propose the SCLP framework as an expressive and natural specification language to model optimization problems. We indeed think that not only problems representing an extension of the one treated here can be modelled by adapting the solution we presented easily enough, but that in general our approach can be followed to model hierarchical optimization problems. All the SCLP programs we proposed are effective only when data of small size are considered. We are indeed conscious that the proposed encodings cannot be used to really solve the problem on practical cases, but on the other
side, we think that CIAO represents a powerful system programming environment allowing us not only to write declarative specifications but also to reason about them.
It is therefore clear that here we do not take care of the performances of the proposed programs and that our aim is not to compare the performance with existing algorithms solving these problems. We indeed leave as future work the study of how to improve the performance of our programs. In [5, Section 8], the authors show some possible solutions that could be used towards this end, such as tabling and branch-and-bound techniques (implementable for example in ECLiPSe [1]). We however would also like to study how our programs can take advantage of the use of dynamic programming techniques based, for example, on the perfect relaxation algorithm for CSPs [11].
Finally, from a theoretical point of view, as future work, we plan to propose a more general framework based on named semiring, allowing us to give a unifying presentation of the SCSP and SCLP frameworks, providing an explicit handling of the names.
Of
where 01 ranges over possible outcomes. We can then look at the best strategy for a particular utility function and his tory,
In the following, let Up be the utility function for some pro totype p and lets* lh be the best strategy for this prototype.
Up k
82 Chajewska, Getoor, Norman, and Shahar
Eventually, we will be giving advice to new users based some cluster's prototype. We would not like the result to differ significantly from what users could expect based on the full utility elicitation. In order to compute this differ ence, we need to consider two possibly different strategies: the strategy that we will pick for up, s* lh , and the strategy Up k that we would pick for the true utility function, ii, s�lhk. We will evaluate both of these strategies for a particular history hk using ii.
Definition 3.1: The Utility Loss (UL) for a utility function ii with respect to a utility function up and a history hk in the context of a given decision model M is
With simulated data, it is possible to assess how the size of the intrinsic signal subspace is estimated. Figure 5 presents the boxplots of the estimations. After several tries, the threshold for the scree test was
fixed to 10%. Fixing the threshold to a too high value would lead to underestimate p while a too low value would lead to drastically overestimate it.
From the figure, the scree test overestimates the parameter p, for each configuration. The variance of the estimation is larger when the number of classes is increased while the bias of the estimation decreases. However, the error in estimating p is not too important with regards to the original size of the data (d = 413). Furthermore, in previous work [44], another criterion (the BIC [45]) was used to estimate the correct dimension of the subspace where the data live. The BIC criterion showed poor results when the number of training samples for single class nc was close to the dimension of the data (d ≈ nc). From the experiments, the scree test is more robust in such a situation.
The results demonstrate that DPPNs and the associated learning algorithms described here are capable of massively compressing the parameters of larger neural networks, and improve upon the performance of CPPNs trained in a Dar-
winian manner. Because the hidden layer has a 10 × 10 grid structure, we can visualize the activations in the hidden layer for each digit, see Figure 9 which shows the hidden layer activations of a fully connected denoising autoencoder encoded by a DPPN with an identity node as input vs. a DPPN with a fully connected linear node as input. Both
produce comparable BCEs with roughly the same number of parameters.
One of the advantages of this symbiosis between evolutionary and gradient-based learning is that it allows optimization to better avoid being stuck in local optima or saddle points. In the future, this framework holds potential for training much deeper neural networks and being applied to other learning paradigms.
The study was performed over 4 weeks and involved 19 volunteers (11 males, 8 females), Volunteers have ages with M = 24.47 and SD = 1.07. These participants were recruited from students on campus. Due to the requirement of understanding English instructions, participants must indicate that they are confident with English communication skills before taking part in the study. We also asked about the participants’ familiarity with computers (M = 6.68, SD = 0.48), robots (M = 2.74, SD = 0.73), puzzles for the secondary task (M = 3.58, SD = 1.02), and computer gaming (M = 3.79, SD = 1.18), in seven-point scales (with 1 being least familiar and 7 being most familiar) after the study. The participants reported familiarity with computers, but not so much with robots, puzzles for the secondary task or computer gaming.
4.1 Measurement
A post-study questionnaire is used to evaluate three of four areas that are often used to assess automated systems: mental workload, situation awareness, and complacency [19]. Furthermore, we also use the questionnaire to evaluate several psychological distances between individuals, which include immediacy, effectiveness, likability, and trust of the robots. Immediacy describes the participant’s feeling about how engaging the robot is. Effectiveness describes the participant’s feeling about how effective the robot is as a teammate. Likability describes how likable the participant feels about the robot. Trust describes whether the participant feels that the robot is trustworthy. We also collect participants’ opinions on whether they think that the robot should be improved (i.e., improvability).
One-way fixed-effects ANOVA tests were performed to analyze the objective performance and measures, as well as the subjective questions. The fixed factor in the tests is the type of the robot, which is either a robot with a planning capability (i.e., peer-to-peer or P2P) or without (i.e., supervised).
In this work, we have established an effective similarity measure of DNA sequences based on RFT. We first performed RFT on DNA sequence after converting symbolic sequences to four binary indicator sequences. Euclidean distance is used to calculate the similarity of RFT coefficients. We conducted different DNA sequence mutants and assess the accuracy of the new RFT metric on the mutants. The similarity metrics have been evaluated by constructing phylogenetic trees of virus at gene levels. Our work provides encouraging steps for applying the rediscovered RFT approach for effective comparative studies on biological sequences.
I. INTRODUCTION
One of the most challenging fields of research is human decision-making. Understanding the processes involved and trying to predict or replicate behaviours has been, historically, the ultimate goal of many disciplines. Economics for example, has a long tradition of trying to formalise human behaviour into descriptive or normative models. These models have been employed for several years (e.g. Expected Utility model [1]) but have been proven to be inadequate [2]–[5], giving rise to new research areas like behavioural and experimental economics. Psychology as well, is natively concerned with decision-making. Sequential decision problems have been used to evaluate people’s risk attitude, in order to predict actual risk proneness in real life scenarios [6]–[8]. While economics and psychology are focused on the high-level manifestations and implications of decision-making, neuroscience aims at understanding the biological machinery and the neural processes behind human (or animal) behaviour [9]–[15].
Recently these fields of research have started to collaborate, contributing to the rise of an emerging multi-disciplinary field called neuroeconomics [16]–[20]. This discipline approaches the problem from several perspectives and on different levels of abstraction. RL is a theoretical framework [21], extensively used in neuroeconomics literature for addressing a wide array of problems involving learning in partially observable environments [22]–[27]. RL is based on the concept of reward/punishment for the actions taken. The agents act in an environment of which they possess only partial knowledge.
To be able to achieve the best behaviour, i.e. maximise their reward, the agents have to learn through experience and update their beliefs. Learning happens as a result of the agent’s interpretation of the interactions with its surroundings and the consequences of a “reward” feedback signal. The ability of this framework to model and therefore understand behavioural data and its underlying neural implications, is of pivotal importance in decision making [28].
RL can accurately capture human and animal learning patterns and has been proven effective at describing the functioning of some areas of the human brain, like the basal ganglia, and the functions of neurotransmitters such as dopamine [22], [29]–[31]. One of the most remarkable similarities between biological functioning and RL models is the one about Temporal Difference (TD) error [21], [32]– [34] and the activation of mid-brain dopamine neurons [35]– [40]. These findings supported the notion that TD Learning is implemented in the brain with dopaminergic neurons in the striatum [29], [34], [41]–[50], [78], making it a reasonable first choice for a modelling attempt. Humans and animals are very advanced signal detectors whose behaviour is susceptible to changes in the rewards resulting from their choices [51], [52]. Both neuroscience and psychology have extensively employed tasks in which the exploration-exploitation tradeoff was of crucial importance [20], [53]–[56]. It is crucial for the individuals to maximise their reward using the information at their disposal but to do so advantageously they need to learn which actions lead to better rewards. Decision making in uncertain environments is a challenging problem because of the competing importance of the two strategies: exploitation is, of course, the best course of action, but only when enough knowledge about the quality of the actions is available, while exploration increases the knowledge about the environment.
A complicated task that encompasses all these features is stocks selection in financial markets, where investors have to choose among hundreds of possible securities to create their portfolio. Stock trends are non-monotonic because they are not guaranteed to achieve a global maximum and the future distribution of reward is intrinsically stochastic. After purchasing a stock, investors are faced with the decisions on when to sell it (Market timing problem [56]). To be able to achieve the best return from their investments, people need to be careful in considering how to maximise their profit in the long term and not only in a single episode. We speculate that RL is part of the decision making process for investors.
ar X
iv :1
60 9.
06 08
6v 1
[ cs
.C E
] 2
0 Se
p 20
16
This speculation is supported by Choi et al. [57], who studied individual investors decisions on 401(k) savings plans. Over the years, investors could decide to increase or decrease the percentage of their salary to commit to this retirement plan. Their results suggest that investors’ decisions are influenced by personal experiences: they show that those investors who have experienced a positive reward from previous investment in their 401(k) fund, tend to increase the investment in that particular product, compared to those who experienced a lower reward outcome. This kind of behaviour follows a “naı̈ve reinforcement learning” and is in contrast with the disposition effect [59], [65](the unwillingness of investors to sell “losing” investments). Huang et al. investigated how personal experience in investments affects future decisions about the selection of stocks [58]. They used data that spans from 1991 to 1996, from a large discount broker. Again, the pattern of repurchasing financial product which yielded positive return was found. As Huang suggests, by understanding the way past experience affects investors’ decisions, it might be possible to make predictions about the financial markets involved. RL has also been used, with promising results, to develop Stock
Market Trading Systems [60]–[63] and to build Agent Based Stock Market Simulations [64]. While these works use RL to predict future prices, they do not try to describe human behaviour. With these notions as background we decided to investigate and try to model human choices in a stochastic, non-stationary environment. We hypothesise that RL is a component of decision making and to test this we compare two RL models against a purely random one. Our modelling attempts are based on two assumptions. First, we assume that risk is a proxy of the internal representation of the actions for some players. To test this we use a measure of systematic risk widely used in finance and economics to categorise the different choices into three discrete classes. We also assume that the reward signal is based on the cash income arising from the sales an investor makes. This assumption follows a widely researched behaviour referred to as “disposition effect” in literature [59], [65], the tendency of individual investors to sell stocks which increased in value since when they were purchased, while holding onto the stocks which lost value. This phenomenon is stronger for individual investors but it also exhibited by institutional investors, such as mutual funds
and corporations [66]–[72]. Following these indications we mapped the sell transactions to a reward signal to fit our models. Finally, we hypothesise that not all players are shortsighted, to test this we compare a full RL model (3 free parameters) against a myopic model (2 free parameters, no gamma). The difference is that the latter can be considered a naı̈ve RL as it does not take into account future rewards, it only seeks to maximise immediate rewards.
II. METHOD
We can now combine the two results for the classes IR and AR. The idea is to use the original algorithm to exhaustively generate macros for each variable v, as long as the outdegree of v in the transitive reduction of the causal graph is less than or equal to 1. Whenever we encounter a variable v with outdegree larger than 1, we switch to the algorithm for reversible variables from the previous section. This way, we can handle acyclic causal graphs even when some variables are not reversible. We call the resulting class AOR.
4.3.1 Definition of the Class AOR
Definition 4.10. A planning problem P belongs to the class AOR if the causal graph of P is acyclic and each variable v ∈ V with outdegree > 1 in the transitive reduction of the causal graph is reversible.
The example planning problem from the previous section does not belong to the class AOR, since variable v1 with outdegree 2 is not reversible. However, assume that we add the following two actions to A:
a31 = 〈v1 = 1; v1 = 0〉, a41 = 〈v1 = 2; v1 = 0〉.
With the new actions, variable v1 is reversible since each value of v1 is reachable from each other value. Since v1 is the only state variable with outdegree greater than 1 in the transitive reduction, P ∈ AOR.

The control constraints (7) are applied during marginalization of the control variable Ui from a potential Ψ′CVi
(we will abbreviate it as Ξ) performed in the steps specified by formulas (8) and (9).
For each vi ∈ V we define a set of admissible control
U ′(vi) = {ui ∈ U , |ui| ≤ umaxi (vi)}
and compute an optimal admissible control value
u∗i (vi) = arg max ui∈U ′(vi) Ξ(Ui = ui, Vi = vi) .
The optimal decision policy in Ui is for all vi ∈ V δi(u|vi) = {
1 if u = u∗i (vi) 0 otherwise. (13)
The value of the new potential is
Ξ(Vi = vi) = Ξ(Ui = u ∗ i (vi), Vi = vi) . (14)
However, whenever u∗i (vi) is the least or the largest value of U ′(vi) we can reduce the discretization error by considering also the nearest value u∗∗i (vi) outside U ′i . The idea is similar to (12). If
Ξ(Ui = u ∗∗ i (vi), Vi = vi) ≥ Ξ(Ui = u∗i (vi), Vi = vi)
then we replace the deterministic policy (13) by a probabilistic policy
δi(u|vi) =  1− |u ∗ i (vi)−u max i (vi)| dU if u = u∗i (vi) 1− |u ∗∗ i (vi)−u max i (vi)| dU if u = u∗∗i (vi)
0 otherwise.
Formula (14) is replaced by
Ξ(Vi = vi) =
δi(u ∗ i |vi) · Ξ(Ui = u∗(vi), Vi = vi) +δi(u ∗∗ i |vi) · Ξ(Ui = u∗∗i (vi), Vi = vi) .
Given the findings so far, one might wonder about the existence of other connections among the eight classes of preferences. Unfortunately, other than the ones found above, there exists no connection, even after considering further restrictions, similar to the ones in the previous part. In the Appendix, we provide counterexamples for the connections that do not hold between the classes of preferences.
Basically, we provide an example for an argumentation framework in which Pareto optimality is satisfied when agents’ preferences are Hamming set based, Issue-wise set based, Issue-wise distance based, IUO Hamming set based, IUO Issue-wise set based, and IUO Issue-wise distance based. However, Pareto optimality is not satisfied when agents’ preferences are Hamming distance based, or IUO Hamming distance based. This shows that Pareto optimality given Hamming distance based preferences and IUO Hamming distance based preferences cannot be inferred from the other six classes of preferences.
In a similar way, we provide an example for an argumentation framework which shows that Pareto optimality given Issue-wise distance based preferences and IUO Issue-wise distance based preferences cannot be inferred from the other six classes of preferences.
We summarize all the findings in Table 2. For each cell in the table, a Y means that for any operator, Pareto optimality in any arbitrary set S carries over from the preference class in the row to the preference class in the column (in the same set S), while a Y ∗ means that it only holds for compatible operators (i.e. that produce compatible outcomes).
Now we turn to studying the Pareto optimality of the three operators: the skeptical, the credulous and the super credulous, with respect to the eight classes of preferences.
3.2 Case 1: in, out, and undec are Equally Distant from Each Other
We propose a method which further reduces the search space of our branch and bound algorithm by initially discarding mappings between vertices. The match of a pair of vertices is discarded if the obtained degree of similarity between their related sub-units is lower than a threshold. Thus, the graphs are broken down into manageable sub-units that overlap, so that two adjacent sub-units contain information about each other through the mutual graph elements they contain. A classical probabilistic relaxation scheme is presented in section 7.1. Then, the above commented sub-units are presented and defined in section 7.2 and 7.3, respectively. Moreover, a distance measure between sub-units and a matching algorithm to compute this distance are proposed in sections 7.4 and 7.5. We propose two techniques to compute a sub-optimal distance between AGs and FDGs. In the first, section 7.6, a noniterative method is proposed to discard non-probable matches. In the second, section 7.7, two relaxation schemes are presented. The difference between them is the initialisation of the probabilities. Section 7.8 presents some results with random graphs.
7.1. Probabilistic relaxation schemes
Relaxation schemes are optimisation techniques in which the variables of the scheme are iteratively updated in order to approach a stationary point of the update equations. They can be used to optimise a matching criterion which has a maximum at the stationary point. A classic probabilistic relaxation scheme is due to Rosenfeld, Hummel and Zucker (Rosenfeld et al., 1976), and was conceived as an object labelling algorithm. Since it’s conception, the approach has been widely used for image processing tasks including graph matching.
We will denote the probability that the AG vertex iv matches to the FDG vertex aω at the iteration t as ( )aP ti . The Rosenfeld et al. Scheme specifies that the probabilities at iteration 1+t should be given by
- 120 -
( ) ( ) ( )( )
( ) ( )( )∑ Σ∈
+
+
+ =
ωω '
'1'
11
a
aQaP
aQaP aP
t i t i
t i t it
i
(143)
where ( )aQ ti is called the support function. The numerator in the update formula can be viewed as the product of two evidential factors; the probability ( )aP ti just described which represents the local information and the support function ( )aQ ti which represents
the probability of the surrounding matches given that iv matches aω . The denominator
simply ensures the normalisation of the probabilities, i.e. that ( ) 1=∑ Σ∈ ωωa aPti . This step is necessary since the support functions are not true probabilities. Rosenfeld et al. define the support function as
( ) ( ) ( )∑ ∑ ∈ ∈ = )( )( ,, , ij abvNv N
t
jjiji
t
i bPbardaQ ωω (144)
where ( )bar ji ,, is called the compatibility function and ( )ivN and ( )aN ω are the set of
vertices which are connected to iv and aω , respectively. The coefficients jid , are used to make ( )aQ ti be in the range [ ]1,1− , provided that 1 )( , =∑ ∈ ij vNv jid .
In the original form, the ( )bar ji ,, are purely arbitrary, application dependent and no method for their specification is offered. The initial probabilities, ( )aPi 0 , are also application dependent.
7.2. Splitting the graphs into sub-units
The larger the topological structure of the sub-units is, the more effective the scheme is at discarding unacceptable labellings. In these terms, small structural units perform badly and the contextual information of the matching process is impoverished. For instance, in the relaxation methods described in (Rosenfeld et al., 1976; Feng et al., 1994; O'leary and S. Peleg , 1983; Christmas et al., 1995), the sub-units are composed of a pair of vertices and their connecting arcs, and the support function does not
- 121 -
consider that the labelling has to be bijective (constraint 1R ), or that the relative order between arcs has to be preserved (constraint 5R ). If, on the other hand, the structural sub-units are too large, then those constraints can be fulfilled but the computational requirements of the matching process become excessively cumbersome; the limitation stems from the need to explore the space of mappings between these sub-units. As a compromise between representational power and computational requirements, Wilson and Hancock proposed splitting the graphs into sub-units which are structured by a central vertex and all the adjacent vertices are connected to it by an arc (Wilson and Hancock, 1997). For convenience we refer to these sub-units as expanded vertices (Figure 21). Their probabilistic approach has the main drawback that the semantic information is used only in the initialisation step while the support function is only based on symbolic information. We present here three different approaches. The first approach takes the distance between expanded vertices with semantic knowledge as an informative heuristic for selecting a reduced set of configurations prior to the computation of an approximation to the distance measure through the branch and bound algorithm. In order to provide a good selection (which hopefully includes the optimal labelling) and reduce the intrinsically used heuristics, the distance measure between expanded vertices is taken to be the same as the one defined between the whole graphs. The main drawback of this approach is that the selection of a vertex is based only on local information. To solve this problem, two other approaches are presented which are based on the Rosenfeld relaxation method (Rosenfeld et al.). The advantage of this relaxation method is that the global information flows through the probabilities in each iteration of the algorithm. Nevertheless, the structural sub-units are too small to keep the structural knowledge. The difference between both approaches is how they initialise the probabilities. In the first, it is used the distance between expanded vertices, thus, the initial probabilities are located using some structural information. In the second, it is only used the distance between vertices.
- 122 -
7.3. Expanded Vertices
An AG expanded vertex (Figure 21.a) or an FDG expanded vertex (Figure 21.b) are an AG or an FDG, respectively, with a specific structure formed by a central vertex and all the adjacent vertices, called external vertices, which are connected to it by an outgoing arc. More formally, an AG expanded vertex iEV over ( )ev ∆∆ , with an underlying graph structure ( )evH ΣΣ= , is defined as an AG ( )AVEVi ,= where the vertices belong to a finite set { }niv vvv ,...,...,1=Σ that contains a given vertex iv and the arcs belong to a finite set { }niiiiiie eeee ,1,1,1, ,...,,..., +−=Σ formed by all the arcs departing from iv in the AG. The
vertex iv is called the central vertex and the vertices njjiv j ≤≤≠ 1,: are called the
external vertices. An AG G of order k over the domain ( )ev ∆∆ , with structure ( )evH ΣΣ= , can be represented as a set of expanded vertices { }iEV , such that U ki iEVG ..1= = , where iv is the central vertex of vii vEV Σ∈∀, . It should be noted that when the whole underlying structure H of the AG is described by a set of expanded vertices, each arc appears just once (each arc belongs to just one expanded vertex) but vertices are possibly included more than once (each vertex is a central vertex of an expanded vertex and also an external vertex of as many expanded vertices as input arcs it receives).
Bonnie Dorr, in (Dorr, 1994), enumerates several different kinds of structural divergences that we might see in translation between languages. These divergences occur when translating from English to closely related languages, Spanish and German, all of which have fairly similar word orders. These are not the only kinds of syntactic differences that there can be in a translation. They do not, for example, cover the more large-scale reorderings that we see when translating between SVO and SOV or VSO languages. However, each of these divergences require something more than simple word substitution or reordering the children of a given node: many of these require raising and lowering tree material (performing “rotations”, in the terminology of (Shieber, 2004)), and nested phrases that are present in one language are often not present in the other. Many of these divergences may appear in a given pair of translated sentences. The following subsections describe Dorr’s seven kinds of divergence.
Independently of Deep Learning, analysis of Time-Series data have been a popular subject of interest in other fields such as Economics, Engineering and Medicine. Traditional techniques on manipulating such data can be found in [21], and the application of traditional ANN techniques on this kind of data is described in [5].
Most work using ANN to manipulate Time-Series data focuses on modeling and forecasting. As an early attempt on using ANN for such tasks, [9] modelled flour prices over the range of 8 years. Still in the 90’s, [29] delineated eight steps on “designing a neural network forecast model using economic time series data”. More recent approaches include usage of Elman RNNs to predict chaotic TimeSeries [10], employing ANN ensemble methods for forecasting Internet traffic [12], using simple Multilayer Perceptrons for modeling the amount of littering in the North Sea [53], and implementing in FPGA a prediction algorithm using Echo State Networks for “exploiting the inherent parallelism of these systems” [3].
Hybrid approaches to Time-Series analysis utilizing ANN are not uncommon. [31] presents a model for Time-Series forecasting using ANN and ARIMA models, and [15] applies the same kinds of models to water quality time series prediction. In still other examples of the same ideas, [30] compares the performance of ARIMA models and ANNs to make short-term predictions on photovoltaic power generators, while [38] compares both models with the performance of Multivariate Adaptive Regression Splines. [13] performs Time-Series forecasting by using a hybrid fuzzy model: while the Fuzzy C-means method is utilized for fuzzification, ANN are employed for defuzzification. Finally, [24] forecasts the speed of the wind using a hybrid of Support Vector Machines, Ensemble Empirical Mode Decomposition and Partial Autocorrelation Function.
Despite being relatively new, the field of Deep Learning has attracted a lot of interest in the past few years. A very thorough review of the entire history of developments that led the field to its current state can be found in [52], while a higher focus on the novelties from the last decade is given in [4]. We proceed to a review of the applications of Deep Learning to Time-Series data.
Classification The task of Classification of any type of data has benefited by the advent of CNNs. Previously existing methods for classification generally relied on the usage of domain specific features normally crafted manually by human experts. Finding the best features was the subject of a lot of research and the performance of the classifier was heavily dependent on their quality. The advantage of CNNs is that they can learn such features by themselves, reducing the need for human experts [34].
An example of the application of such unsupervised feature learning for the classification of audio signals is presented in [37]. In [1], the features learned by the CNN are used as input to a Hidden Markov Model, achieving a drop at the error rate of over 10%. The application of CNNs in these works presuppose the constraint that the Time-Series is composed of only one channel. An architecture that solves this constraint is presented in [60].
In [18] the performance of CNNs is compared with that of LSTM for the classification of Visual and Haptic Data in a robotics setting, and in [28] the signals produced by wearable sensors are transformed into images so that Deep CNNs can be used for classification.
Relevant to Tiled CNNs was the development of Independent Component Analysis (ICA) [27]. Several alternative methods for calculating independent components can be found in the literature (e.g., [17], [55] or [25]). Tiled CNNs are normally trained with a variation of such technique that looses the assumption that each component is statistically independent and tries to find a topographic order between them: the Topographic ICA [26].
Forecasting Several different Deep Learning approaches can be found in the literature for performing Forecasting tasks. For example, Deep Belief Networks are used in the work of [33] along with RBM. [58] also compares the performance of Deep Belief Networks with that of Stacked Denoising Autoencoders. This last type of network is also employed by [50] to predict the temperature of an indoor environment. Another application of Time-Series forecasting can be found in [43], which uses Stacked Autoencoders to predict the flow of traffic from a Big Data dataset.
A popular application to the task of Time-Series prediction is on Weather Forecasting. In [41], some preliminary predictions on weather data provided by The Hong Kong Observatory are made through the usage of Stacked Autoencoders. In a follow up work, the authors use similar ideas to perform predictions on Big Data [40]. Instead of Autoencoders, [20] uses Deep Belief Networks for constructing a hybrid model in which the ANN models the joint distribution between the weather predictors variables.
Anomaly Detection Work applying Deep Learning techniques to Anomaly Detection detection of Time-Series data is not very abundant in the literature. It is still difficult to find works such as [16], that uses Stacked Denoising Autoencoders to perform Anomaly Detection of trajectories obtained from low level tracking algorithms.
However, there are many similarities between Anomaly Detection and the previous two tasks. For example, identifying an anomaly could be transformed into a Classification task, as was done in [35]. Alternatively, detecting an anomaly could be considered the same as finding regions in the Time-Series for which the forecasted values are too different from the actual ones.
A common test for a GANs ability to generate realistic samples that cover the original data space is to train a simple model on the samples from the GAN itself. Acknowledging the pitfalls of such quantitative evaluations [31], for text GANs we can do this by producing a large set of sampled sentences, and training a simple language model over the generations. For these experiments we generate 100k samples from (i) ARAE-GAN, (ii) an AE, (iii) a RNN LM trained on the same data, and (iv) the real training set. To “sample” from an AE we fit a multivariate Gaussian to the code space (of the training data) after training the AE and generate code vectors from this Gaussian and decode back into sentence space. All models are of the same size to allow for fair comparison. Samples from the models are shown in Figure 3.
We subsequently train a standard RNN language model on the generated data and evaluate perplexity on held-out real data. The language model is of the same size as the decoder of the ARAE. As can be seen from Table 2 training on real data (understandably) outperforms training on generated data by a large margin. Surprisingly however, we find that a language model trained on ARAE-GAN data performs slightly better than one trained on LM-generated/AE-generated data.
In this section, notation needed to define the bus route and terminology used in the field of public transport are given. Some definitions might be common knowledge, but others are less known outside this discipline.
A group of bus lines can be seen as a graph G = (V,E), which is a combination of a set of stops V = {v0, v1, v2, . . . , vn} and a set of route segments E ⊆ V × V . A stop vi is a designated place at which passengers can alight or board the bus.
We define P to be the set of all possible bus trips:
P = {v0, . . . , vn | ∀0 ≤ i < n : (vi, vi+1) ∈ E, ∀0 ≤ h < i ≤ n : vh = vi =⇒ (h, i) = (0, n)}
(1)
A bus trip p ∈ P of length n can then be seen as an ordered sequence of n + 1 stops, where there exists a route segment ei ∈ E from every stop vi to its subsequent stop vi+1.
We call stops v0 and vn terminals. At these stops, the bus changes trips. Figure 1 shows a schematic overview of
ar X
iv :1
70 4.
05 69
2v 1
[ cs
.A I]
1 9
A pr
2 01
7
a trip. Between stops, the bus drives these specific route segments. The time that each segment ei takes is called travel time (TTi). Travel time can be measured between stop departures, between departure at stop vi and arrival at stop vi+1 or vice versa. We define travel time (TTi) between stop vi−1 and vi as the time between departure at stop vi−1 and arrival at stop vi, which is the time it takes to travel segment ei. We note that when we talk about travel time, we talk about the time that it takes to travel a route segment towards a stop from its preceding stop, therefore e0 is not defined.
Bus lines can either consist of a single trip in which the start terminal is the same as the end terminal v0 = vn, or two trips in roughly opposite directions v0 6= vn. For the first case we have a circular bus line, which is more common in urban areas. In the latter case most bus stops are usually in opposite sides of the road to allow passengers to travel in both directions of the route.
Furthermore, stops are defined by a GPS radius. This GPS radius is roughly 35 metres. The time the bus spends within the radius of stop vi is known as dwell time of stop vi (DTi). Within this GPS radius the bus stops to let passengers board or alight the bus. Doors open within a stop’s GPS radius. The time that the bus’ doors are open within the radius of stop vi is defined as door open time (DOTi).
Bus operators have a specific itinerary of trips that they drive during the day, this is called the operators’ run R ⊂ P . A run implies that operators possibly have to switch buses at terminals. Buses also have a specific itinerary of trips C ⊂ P which called the bus’ circulation. A bus circulation could be on the same line, but a bus may also switch lines, thus changing routes.
Buses can also enter or leave the system at terminals. Some parts of the day (off-peak hours) may have longer headways, which is the time between one bus and the next bus on the same trip. This means total bus capacity is reduced and some buses return to the garage to be stored. The drive from and to the garage is not indicated as a trip, but is called deadhead. This happens at the start and end of a bus’ circulation. A bus can have multiple circulations during the day thus also having multiple deadheads.
We have proposed the MusECI framework for modeling music in a way that allows querying based on parsing of natural language statements. Our Python implementation of the musical data structures supports normal algorithmic composition in a text editor in addition to the composition by conversation use case in a way that is easily integrated with parsing systems like TRIPS.
Although our data structures are robust for representing complex musical structures, the overall system is currently a prototype limited to a fairly narrow collection of musical operations. This collection of operations needs to be expanded to feature more tree manipulation algorithms, such as different strategies for removing and re-arranging notes within a larger structure. Operations such as removal of individual notes can have several potential interpretations, such as replacement with a rest of the same duration and removal followed by time-shifting other elements of the data structure to close the gap. Both operations are valid, but which is most appropriate depends on the larger context in which it is used. We are currently working on a more diverse array of operations such as this to support a wider range of common score manipulations.
Expansion of this framework for NLP-based music composition and manipulation will require adaptation of NLP toolkits like the TRIPS parser. Standard parsers often do not have a suitable dictionary of musically-relevant definitions to draw on when assigning semantics to nouns and verbs. While terms like “transpose” and “reverse” can be interpreted correctly due to their usage in non-musical settings, correct interpretation of other terms is trickier. Without incorporation of music-specific terminology, the
TRIPS parser identifies“C” and “F” as temperature scales rather than as pitch classes. Just as humans learning about music must have their vocabulary expanded, we are developing a music-specific ontology that can be incorporated in the TRIPS parser to support a more diverse range of musical concepts and operations to achieve a correct parse tree for sentences in a composition by conversation setting.
The addition of new representations for contexts is important for the creation of an assumer module. This requires a way to infer the referents of ambiguous words like “it,” “this,” and so on as well as keeping track of what spaces or metrics are currently in use.
Our prototype framework is the beginning of an integrated approach for handling natural language and musical concepts. Currently, MusECI and its integration into NLP systems is still a work in progress. However, we hope to eventually achieve real-time interactive programs capable of interacting with real musicians in a musical setting, similar to how AI on computers and mobile devices is currently able to respond to basic spoken commands in other domains. We also aim for this communication to ultimately become bi-directional and incorporate other aspects of musical artificial intelligence, perhaps even allowing the machine to critique its human user’s musical ideas or offer its own suggestions to help complete a musical project.
In this paper, we take 128 ASCII characters as character set, by which most of the testing documents are composite. We define word length as 20 and character embedding length as 100. If a word with characters less than 20, we will pad it with ze-
ros. If the length is larger than 20, we just take the first 20 characters. We set the maximum length of words as the average number of words of the documents in the dataset plus two standard deviation, which is long enough to cover more than 97.5% of the documents. For documents with number of words more than the preset maximum number of words, we will discard the exceeding words.
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
5.2 Data collapsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 5.3 MWE recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 5.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 5.5 Overall approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 5.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
In this paper, we introduce a general framework to make topic models and vector representations mutually help each other. We propose a basic model Latent Topical Skip-Gram (LTSG) which shows that LDA and Skip-Gram can mutually help improve performance on different task. The experimental results show that LTSG achieves the competitive results compaired with the state-of-art models. Especially, we can make a conclusion that topic model helps promoting word embeddings in LTSG model.
We consider the following future research directions: I) The number of topics must be pre-defined and Gibbs sampling is time-consuming for training large-scale data with using single thread. we will investigate non-parametric topic models [Teh et al., 2006] and parallel topic models [Liu et al., 2011]. II) There are many topic models and word embeddings models have been proposed to use in various tasks and specific domains. We will construct a package which can be convenient to extend with other models to our framework by using the interfaces. III) LTSG could not deal with unseen words in new documents, we may explore techniques to train word embeddings and topic assigments for the unseen words like Gaussian LDA [Das et al., 2015]. IV) We wish to evaluate topic embeddings directly similar to topic coherence task.
Con eptual graphs (CGs) have been proposed as a knowledge representation and reasoning model, mathemati ally founded both on logi s and graph theory (Sowa, 1984). Though they have been mainly studied as a graphi al interfa e for logi s or as a diagrammati system of logi s (for instan e, see Wermelinger, 1995, for general CGs equivalent to FOL), their graph-theoreti foundations have been less investigated. Most works in this area are limited to simple on eptual graphs, or simple graphs (Sowa, 1984; Chein & Mugnier, 1992), whi h
orrespond to the positive, onjun tive and existential fragment of FOL without fun tions.
This model has three fundamental hara teristi s:
1. obje ts are bipartite labelled graphs (nodes represent entities and relations between
these entities);
2. reasonings are based on graph-theoreti operations, relying on a kind of graph homo-
morphism alled proje tion;
3. it is logi ally founded, reasonings being sound and omplete w.r.t. FOL semanti s,
usually by way of the translation alled .
Main extensions of the simple graphs model, keeping graph homomorphism based operations and sound and omplete semanti s, are inferen e rules (Gosh & Wuwongse, 1995; Salvat & Mugnier, 1996; Salvat, 1998) and nested graphs (Chein, Mugnier, & Simonet, 1998; Preller, Mugnier, & Chein, 1998); for general CGs equivalent to FOL, an original dedu tion
2002 AI A ess Foundation and Morgan Kaufmann Publishers. All rights reserved.
Baget & Mugnier
system (Kerdiles, 1997) ombines analyti tableaux with the simple graphs proje tion. Some kind of onstraints have been proposed to validate a knowledge base omposed of simple graphs (Mineau & Missaoui, 1997; Dibie, Haemmerlé, & Loiseau, 1998).
We present here a family of extensions of the simple graphs model. The ommon ground for these extensions is that obje ts are olored simple graphs representing fa ts, rules or
onstraints, and operations are based upon proje tion. Given a knowledge base K and a simple graph Q (whi h may represent a query, a goal, . . . , depending on the appli ation), the dedu tion problem asks whether Q an be dedu ed from K. A ording to the kinds of obje ts
onsidered in K, di erent reasoning models are obtained, omposing the SG family. Though similar notions of rules and onstraints an be found in the CG literature, their ombination in reasonings had never been studied. One interest of our approa h thus resides in providing a unifying framework ombining rules and onstraints in di erent ways.
In this paper, we fo us on the formal de nitions of these models, in luding their operational semanti s and relationships with FOL, and we study the de idability and omplexity of their asso iated de ision problems, namely onsisten y and dedu tion. These results extend and omplete the ones already published by the authors (Baget & Mugnier, 2001). Though both onsisten y and dedu tion are unde idable in the most general model of this family, we had already used a de idable subset of rules to solve the Sisyphus-I problem, a test-bed proposed in the knowledge a quisition ommunity (Baget, Genest, & Mugnier, 1999). We present here for the rst time a detailed analysis of omplexity when we restri t the knowledge base to this kind of rules ( alled range restri ted rules). We also study parti ular ases of onstraints.
In se tion 2 basi de nitions and results about simple graphs are re alled. Se tion 3 presents an overview of the SG family. In parti ular, we explain why we onsider graphi al features of the simple graphs model as essential for knowledge modeling and point out that these properties are preserved in the SG family. In next se tions we study the di erent members of the family. Rules are introdu ed in se tion 4, onstraints in se tion 5, and se tion 6 studies models ombining rules and onstraints. As soon as rules are involved in reasonings, the asso iated de ision problems are not de idable, but we exhibit a ondition ( nite expansion sets) under whi h omputations always stop. In the parti ular ase of range restri ted rules, the omplexity of these problems fall into the polynomial hierar hy. Se tion 7 is devoted to these de idable ases. In se tion 8, relationships with other works are established. In parti ular we point out algorithmi onne tions with onstraint satisfa tion problems (CSP) and show that the problem of he king the onsisten y of a knowledge base omposed of simple graphs and onstraints (SGC- onsisten y) is equivalent to that of de iding the onsisten y of a mixed CSP (MIXED-SAT, Fargier, Lang, & S hiex, 1996).
We thank Gene Golub and Dimitri Bertsekas for useful pointers. This work was supported by the DARPA transfer learning program under contract FA8750-05-2-0249.
In this section we introduce a taxonomy of soft constraints based on AllDifferent and AllEqual. We consider the eight algorithmic problems related to constraints of difference and equality defined by combining these two constraints, two costs (graph-based and variable-based), and two objectives (minimisation and maximisation). In fact, because the graph-based costs of AllDifferent and AllEqual are dual, only six different problems are defined. Observe that we consider only costs defined through inequalities, rather than equalities. There are several reasons for doing so. First, reasoning about the lower bound or the upper bound of the cost variable can yield two extremely different problems, and hence different algorithmic solutions. For instance, we shall see that in some cases the problem is tractable in one direction, and NP-hard in the other direction. When reasoning about cost equality, one will often separate the inference procedures relative to the lower bound, upper bound, and intermediate values. Reasoning about lower and upper bounds is sufficient to model an equality although it might hinder domain filtering when intermediate values for the cost are forbidden. We thus cover equalities in a restricted way, albeit arguably reasonable in practice. Indeed, when dealing with costs and objectives, reasoning about inequalities and bounds is more useful in practice than imposing (dis)equalities.
We close the last remaining cases: the complexity of achieving ac and bc SoftAllEqualminV in Section 4, that of achieving ac on SoftAllEqualminG in Section 5 and that of achieving bc on SoftAllEqualminG in Section 6. Based on these results, Figure 1 can now be completed (fourth and fifth columns).
The next six paragraphs correspond to the six columns of Figure 1, that is, to the twelve elements of the taxonomy. For each of them, we briefly outline the current state of the art, using the following assignment as a recurring example to illustrate the various costs:
S3 = {(X1, a), (X2, a), (X3, a), (X4, a), (X5, b), (X6, b), (X7, c)}.
3.1 SoftAllDiff: Variable-based cost, Minimisation
Definition 1 (SoftAllDiffminV )
SoftAllDiffminV ({X1, . . . , Xn}, N)⇔ N ≥ n− |{v | ∃Xi = v}|.
Here the cost to minimise is the number of variables that need to be changed in order to obtain a solution satisfying an AllDifferent constraint. For instance, the cost of S3 is 4 since three of the four variables assigned to a as well as one of the variables assigned to b must change. This objective function was first studied by Petit et al. (2001), and an algorithm for achieving ac in O(n √ m) was introduced. To the best of our knowledge, no
algorithm with better time complexity for the special case of bounds consistency has been proposed for this constraint. Notice however that Mehlhorn and Thiel’s (2000) algorithm achieves bc on the AllDifferent constraint with an O(n log n) time complexity. The question of whether this algorithm could be adapted to achieve bc on SoftAllDiffminV remains open.
3.2 SoftAllDiff: Variable-based cost, Maximisation
Definition 2 (SoftAllDiffmaxV )
SoftAllDiffmaxV ({X1, . . . , Xn}, N)⇔ N ≤ n− |{v | ∃Xi = v}|.
Here the same cost is to be maximised. In other words, we want to minimise the number of distinct values assigned to the given set of variables, since the complement of this number to n is exactly the number of variables to modify in order to obtain a solution satisfying an AllDifferent constraint. For instance, the cost of S3 is 4 and the number of distinct values is 7 − 4 = 3. This constraint was studied under the name AtMostNValue. An algorithm in O(n log n) to achieve bc was proposed by Beldiceanu (2001), and a proof that achieving ac is NP-hard was given by Bessiere et al. (2006).
3.3 SoftAllDiff: Graph-based cost, Minimisation & SoftAllEqual: Graph-based cost, Maximisation
Definition 3 (SoftAllDiffminG ≡ SoftAllEqualmaxG )
SoftAllDiffminG ({X1, . . . , Xn}, N)⇔ N ≥ |{{i, j} | Xi = Xj & i < j}|.
Here the cost to minimise is the number of violated constraints when decomposing AllDifferent into a clique of binary NotEqual constraints. For instance, the cost of S3 is 7 since four variables share the value a (six violations) and two share the value b (one violation). Clearly, it is equivalent to maximising the number of violated binary Equal constraints in a decomposition of a global AllEqual. Indeed, these two costs are complementary to ( n 2 ) of each other (on S3: 7 + 14 = 21). An algorithm in O(nm) for achieving ac on this constraint was introduced by van Hoeve (2004). Again, to our knowledge there is no algorithm improving this complexity for the special case of bc.
3.4 SoftAllEqual: Graph-based cost, Minimisation & SoftAllDiff: Graph-based cost Maximisation
Definition 4 (SoftAllEqualminG ≡ SoftAllDiffmaxG )
SoftAllEqualminG ({X1, . . . , Xn}, N)⇔ N ≥ |{{i, j} | Xi 6= Xj & i < j}|.
Here we consider the same two complementary costs, however we aim at optimising in the opposite way. In Section 5 we show that achieving ac on this constraint is NP-hard and, in Section 6 we show that, when domains are contiguous intervals, computing the optimal cost can be done in O(min(nλ2, n3)). As a consequence, bc can be achieved in polynomial time.
3.5 SoftAllEqual: Variable-based cost, Minimisation
Definition 5 (SoftAllEqualminV )
SoftAllEqualminV ({X1, . . . , Xn}, N)⇔ N ≥ n−max v∈Λ (|{i | Xi = v}|).
Here the cost to minimise is the number of variables that need to be changed in order to obtain a solution satisfying an AllEqual constraint. For instance, the cost of S3 is 3 since four variables already share the same value. This is equivalent to maximising the number of variables sharing a given value. Therefore this bound can be computed trivially by counting the occurrences of every value in the domains. However, pruning the domains according to this bound without degrading the time complexity is not as trivial. In Section 4, we introduce two filtering algorithms, achieving ac and rc in the same complexity as that of counting values.
3.6 SoftAllEqual: Variable-based cost, Maximisation
Definition 6 (SoftAllEqualmaxV )
SoftAllEqualmaxV ({X1, . . . , Xn}, N)⇔ N ≤ n−max v∈Λ (|{i | Xi = v}|).
Here the same cost has to be maximised. In other words we want to minimise the maximum cardinality of each value. For instance, the cost of S3 is 3, that is, the complement to n of the maximum cardinality of a value (3 = 7 − 4). This is exactly equivalent to applying a Global Cardinality constraint (considering only the upper bounds on the cardinalities). Two algorithms, for achieving ac and bc on this constraint and running in O( √ nm) and O(n log n) respectively, was introduced by Quimper et al. (2004).
4. The Complexity of Arc and Bounds Consistency on SoftAllEqualminV
Here we show how to achieve ac, rc and bc on the SoftAllEqualminV constraints (see Definition 5). This constraint is satisfied if and only if n minus the cardinality of any set of variables assigned to a single value is less than or equal to the value of the cost variable N . In other words, it is satisfied if there are at least k variables sharing a value, where k = n − max(N). Therefore, for simplicity sake, we shall consider the following equivalent formulation, where N is a lower bound on the complement to n of the same cost (N ′ = n−N):
N ′ ≤ max v∈Λ (|{i | Xi = v}|).
We shall see that to filter the domain of N ′ and the Xi’s we need to compute two properties:
1. An upper bound k∗ on the number of occurrences amongst all values.
2. The set of values that can actually appear k∗ times.
Computing the set of values that appear in the largest possible number of variable domains can be performed trivially in O(m), by counting the number of occurrences of every value, i.e., the number of variables whose domain contains v.
However, if domains are discrete intervals defined by lower and upper bounds, it can be done even more efficiently. Given two integers a and b, a ≤ b, we say that the set of all integers x, a ≤ x ≤ b, is an interval and denote it by [a, b]. In the rest of this section we shall assume that the overall set of values values Λ = ⋃ X∈X D(X) is the interval [1, λ].
Definition 7 (Occurrence function and derivative) Given a constraint network P = (X ,D, C), the occurrence function occ is the mapping from values in Λ to N defined as follows:
occ(v) = |{X | X ∈ X & v ∈ doms(X)}|.
The “ derivative” of occ, δocc, maps each value v ∈ Λ to the difference between the value of occ(v − 1) and occ(v):
δocc(0) = 0, δocc(v) = occ(v)− occ(v − 1).
We give an example of the occurrence function for a set of variables with interval domains in Figure 2.
Algorithm 1 computes occ−1, that is, the inverse of the occurrence function, which maps every element in the interval [1, n] to the set of values appearing that many times. It runs
Algorithm 1: Computing the inverse occurrence function. Data: A set of variables: X Result: occ−1 : [1, n] 7→ 2Λ
δocc(v)← ∅; 1 foreach X ∈ X do
δocc(min(X))← δocc(min(X)) + 1; δocc(max(X) + 1)← δocc(max(X) + 1)− 1;
2 ∀x ∈ [1, n], occ−1(x)← ∅; x← 0; pop first element (v, a) of δocc; repeat
pop first element (w, b) of δocc; x← x+ δocc(a); occ−1(x)← occ−1(x) ∪ [a, b− 1]; a← b;
until δocc = ∅;
in O(n log n) worst-case time complexity if we assume it is easy to extract both an upper bound (k∗ ≥ N ′) and the set of values that can appear k∗ times from occ−1.
The idea behind this algorithm, which we shall reuse throughout this paper, is that when domains are given as discrete intervals one can compute the non-null values of the derivative δocc of the occurrence function occ in O(n log n) time. The procedure is closely related to the concept of sweep algorithms (Beldiceanu & Carlsson, 2001) used, for instance, to implement filtering algorithms for the Cumulative constraint. Instead of scanning the entire horizon, one can jump from an event to the next, assuming that nothing changes between two events. As in the case of the Cumulative constraint, events here correspond to start and end points of the domains. In fact, it is possible to compute the same lower bound, with the same complexity, by using Petit, Régin, and Bessiere’s (2002) Range-based Max-CSP Algorithm (RMA)2 on a reformulation as a Max-CSP. Given a set of variables X , we add an extra variable Z whose domain is the union of all domains in X : D(Z) = Λ = ⋃ X∈X D(X). Then
2. We thank the anonymous reviewer who made this observation.
we link it to other variables in X through binary equality constraints:
∀X ∈ X , Z = X.
There is a one-to-one mapping between the solutions of this Max-CSP and the satisfying assignments of a SoftAllEqualminV constraint on (X , N), where the value of N corresponds to the number of violated constraints in the Max-CSP. The lower bound on the number of violations computed by RMA and the lower bound k∗ on N computed in Algorithm 1 are, therefore, the same. Moreover the procedures are essentially equivalent, i.e., modulo the modelling step. Algorithm 1 can be seen as a particular case of RMA: the same ordered set of intervals is computed, and subsequently associated with a violation cost. However, we use our formalism, since the notion of occurrence function and its derivative is important and used throughout the paper.
We first define a simple data structure that we shall use to compute and represent the function δocc. A specific data structure is required since indexing the image of δocc(v) by the value v would add a factor of λ to the (space and therefore time) complexity. The non-zero values of δocc are stored as a list of pairs whose first element is a value v ∈ [1, . . . , λ] and second element stands for δocc(v). The list is maintained in increasing order of the pair’s first element. Given an ordered list δocc = [(v1, o1), . . . , (vk, ok)], the assignment operation δocc(vi)← oi can therefore been done in O(log |δocc|) steps as follows:
1. The rank r of the pair (vj , oj) such that vj is minimum and vj ≥ vi is computed through a dichotomic search.
2. If vi = vj , the pair (vj , oj) is removed.
3. The pair (vi, oi) is inserted at rank r.
Moreover, one can access the element with minimum (resp. maximum) first element in constant time since it is first (resp. last) in the list. Finally, the value of δocc(vi) is oi if there exists a pair (vj , oj) in the list, and 0 otherwise. Computing this value can also be done in logarithmic time.
The derivative δocc(v) is computed in Loop 1 of Algorithm 1 using the assignment operator defined above. Observe that if D(X) = [a, b], then X contributes only to two values of δocc: it increases δocc(a) by 1 and decreases δocc(b + 1) by 1. For every value w such that there is no X with min(X) = w or max(X) + 1 = w, δocc(w) is null. In other words, we can define δocc(v) for any value v, as follows:
δocc(v) = (|{i | min(Xi) = v}| − |{i | max(Xi) = v − 1}|).
Therefore, by going through every variable X ∈ X , we can compute the non-null values of δocc in time O(n log n) using the simple list structure described above.
Then, starting from Line 2, we compute occ−1 by going through the non-zero values v of the derivative, i.e. such that δocc(v) 6= 0, in increasing order of v. Recall that we use an ordered list, so this is trivially done in linear time. By definition, the occurrence function is constant on the interval defined by two such successive values. Since the number of non-zero values of δocc is bounded by O(n), the overall worst-case time complexity is in O(n log n). We use Figure 3 (a,c & d) to illustrate an execution of Algorithm 1. First, six variables and
their domains are represented in Figure 3(a). Then, in Figures 3(c) and 3(d) we show the derivative and the inverse, respectively, of the occurrence function.
Alternatively, when λ < n log n, it is possible to compute occ−1 in O(n+λ) by replacing the data structure used to store δocc by a simple array, indexed by values in [1, λ]. Accessing and updating a value of δocc can thus be done in constant time.
Now we show how to prune the variables in X with respect to this bound without degrading the time complexity. According to the method used we can, therefore, achieve ac or rc in a worst-case time complexity of O(m) or O(min(n+ λ, n log n), respectively.
Theorem 1 Enforcing ac (resp. rc) on SoftAllEqualminV can be achieved in in O(m) steps (resp. O(min(n+ λ, n log n)).
Proof. We suppose, without loss of generality, that the current lower bound on N ′ is k. We first compute the inverse occurrence function either by counting values, or considering interval domains using Algorithm 1. From this we can define the set of values with highest number of occurrences. Let this number of occurrences be k∗, and the corresponding set of values be V (i.e. occ−1(k∗) = V ). Then there are three cases to consider:
1. First, if every value appears in strictly fewer than k domains (k∗ < k) then the constraint is violated.
2. Second, if at least one value v appears in the domains of at least k + 1 variables (k∗ > k), then we can build a support for every value w ∈ D(X). Let v ∈ V , we assign all the variables in X \X with v when possible. The resulting assignment has at least k occurrences of v, hence it is consistent. Consequently, since k∗ > k, every value is consistent.
3. Otherwise, if neither of the two cases above hold, we know that no value appears in more than k domains, and that at least one appears k times. Recall that V denotes the set of such values. In this case, the pair (X, v) is inconsistent if and only if v 6∈ V & V ⊂ D(X). We first suppose that this condition does not hold and show that we can build a support. If v ∈ V then clearly we can assign every possible variable to v and achieve a cost of k. If V 6⊂ D(X), then we consider w such that w ∈ V and w 6∈ D(X). By assigning every variable with w when possible we achieve a cost of k no matter what value is assigned to X.
Now we suppose that v 6∈ V & V ⊂ D(X) holds and show that (X, v) does not have an ac support. Indeed, once X is assigned to v the domains are such that no value appears in k domains or more, since every value in V has now one fewer occurrence, hence we are back to Case 1.
Computing the set V of values satisfying the condition above can be done easily once the inverse occurrence function has been computed. On the one hand, if this function occ−1 has been computed by counting every value in every domain, then the supports used in the proofs are all domain supports, hence ac is achieved. On the other hand, if domains are approximated by their bounds and Algorithm 1 is used instead, the supports are all range supports, hence rc is achieved. In Case 3, the domain can be pruned down to the set V of values whose number of occurrences is k, as illustrated in Figure 3 (b). 2
Corollary 1 Enforcing bc on SoftAllEqualminV can be achieved in O(min(n+λ, n log n) steps.
Proof. This is a direct implication of Theorem 1. 2
The proof of Theorem 1 yields a domain filtering procedure. Algorithm 2 achieves either ac or rc depending on the version of Algorithm 1 used in Line 1 to compute the inverse occurrence function. The later function occ−1 is then used in Line 2, 3 and 4 to, respectively, catch a global inconsistency, prune the upper bound of N ′ and prune the domains of the variables in X .
Figure 3(b) illustrates the pruning that one can achieve on X provided that the lower bound on N ′ is equal to 4. Dashed lines represent inconsistent intervals. The set V of values used in Line 4 of Algorithm 2 is occ−1(4) = {[15, 40] ∪ [70, 70]}.
Algorithm 2: Propagation of SoftAllEqualminV ({X1, . . . , Xn}, N ′). 1 occ−1 ← Algorithm 1; ub← n; while occ−1(ub) = ∅ do
ub← ub− 1; 2 if min(N ′) > ub then fail;
else 3 max(N ′)← ub;
if min(N ′) = max(N ′) then V ← occ−1(min(N ′));
4 foreach X ∈ X do if V ⊂ D(X) then D(X)← V ;
5. The Complexity of Arc Consistency on SoftAllEqualminG
Here we show that achieving ac on SoftAllEqualminG is NP-hard. In order to achieve ac we need to compute an arc consistent lower bound on the cost variable N constrained as follows:
N ≤ |{{i, j} | Xi 6= Xj & i < j}|.
In other words, we want to find an assignment of the variables in X minimising the number of pairwise disequalities, or maximising the number of pairwise equalities. We consider the corresponding decision problem (SoftAllEqualminG -decision), and show that it is NP-hard through a reduction from 3dMatching (Garey & Johnson, 1979).
Definition 8 (SoftAllEqualminG -decision) Data: An integer N , a set X of variables. Question: Does there exist a mapping s : X 7→ Λ such that ∀X ∈ X , s[X] ∈ D(X) and |{{i, j} | s[Xi] = s[Xj ] & i 6= j}| ≥ N?
Definition 9 (3dMatching) Data: An integer K, three disjoint sets X,Y, Z, and T ⊆ X × Y × Z. Question: Does there exist M ⊆ T such that |M | ≥ K and ∀m1,m2 ∈M, ∀i ∈ {1, 2, 3}, m1[i] 6= m2[i]?
Theorem 2 (The Complexity of SoftAllEqualminG ) Finding a satisfying assignment for the SoftAllEqualminG constraint is NP-complete even if no value appears in more than three domains.
Proof. The problem SoftAllEqualminG -decision is clearly in NP: checking the number of equalities in an assignment can be done in O(n2) time.
We use a reduction from 3dMatching to show completeness. Let P = (X,Y, Z, T,K) be an instance of 3dMatching, where: K is an integer; X,Y, Z are three disjoint sets such that X ∪ Y ∪ Z = {x1, . . . , xn}; and T = {t1, . . . , tm} is a set of triplets over X × Y × Z. We build an instance I of SoftAllEqualminG as follows:
1. Let n = |X|+ |Y |+ |Z|, we build n variables {X1, . . . , Xn}.
2. For each tl = 〈xi, xj , xk〉 ∈ T , we have l ∈ D(Xi), l ∈ D(Xj) and l ∈ D(Xk).
3. For each pair (i, j) such that 1 ≤ i < j ≤ n, we put the value (|T |+ (i− 1) ∗ n+ j) in both D(Xi) and D(Xj).
We show there exists a matching of P of size K if and only if there exists a solution of I with b3K+n2 c equalities. We refer to “a matching of P” and to a “solution of I” as “a matching” and “a solution” throughout this proof, respectively. ⇒: We show that if there exists a matching of cardinality K then there exists a solution with at least b3K+n2 c equalities. Let M be a matching of cardinality K. We build a solution as follows. For all tl = 〈xi, xj , xk〉 ∈ M we assign Xi, Xj and Xk to l (item 2 above). Observe that there remain exactly n− 3K unassigned variables after this process. We pick an arbitrary pair of unassigned variables and assign them with their common value (item 3 above), until at most one variable is left (if one variable is left we assign it to an arbitrary value). Therefore, the solution obtained in this way has exactly b3K+n2 c equalities, 3K from the variables corresponding to the matching and bn−3K2 c for the remaining variables. ⇐: We show that if the cardinality of the maximal matching is K, then there is no solution with more than b3K+n2 c equalities. Let S be a solution. Furthermore, let L be the number of values appearing three times in S. Observe that this set of values corresponds to a matching. Indeed, a value l appears in three domains D(Xi),D(Xj) and D(Xk) if and only if there exists a triplet tl = 〈xi, xj , xk〉 ∈ T (item 2 above). Since a variable can only be assigned to a single value, the values appearing three times in a solution form a matching. Moreover, since no value appears in more than three domains, all other values can appear at most twice. Hence the number of equalities in S is less than or equal to b3L+n2 c, where L is the size of a matching. It follows that if there is no matching of cardinality greater than K, there is no solution with more than b3K+n2 c equalities. 2
Cohen, Cooper, Jeavons, and Krokhin (2004) showed that the language of soft binary equality constraints is NP-complete, for as few as three distinct values. On the one hand, Theorem 2 applies to a more specific class of problems where the constraint network formed by the soft binary constraints is a clique. On the other hand, the proof requires an unbounded number of values, these two results are therefore incomparable. However, we shall see in Section 9 that this problem is fixed parameter tractable with respect to the number of values, hence polynomial when it is bounded.
6. The Complexity of Bounds Consistency on SoftAllEqualminG
In this section we introduce an efficient algorithm that, assuming the domains are discrete intervals, computes the maximum possible pairs of equal values in an assignment. We therefore need to solve the optimisation version of the problem defined in the previous section (Definition 8):
Definition 10 (SoftAllEqualminG -optimisation) Data: A set X of variables. Question: What is the maximum integer K such that there exists a mapping s : X 7→ Λ satisfying ∀X ∈ X , s[X] ∈ D(X) and |{{i, j} | s[Xi] = s[Xj ] & i 6= j}| = K?
The algorithm we introduce allows us to close the last remaining open complexity question in Figure 1: bc on the SoftAllEqualminG constraint. We then improve it by reducing the time complexity thanks to a preprocessing step.
We use the same terminology as in Section 4, and refer to the set of all integers x such that a ≤ x ≤ b as the interval [a, b]. Let X be the set of variables of the considered CSP and assume that the domains of all the variables of X are sub-intervals of [1, λ]. We denote by ME(X ) the set of all assignments P to the variables of X such that the number of pairs of equal values of P is the maximum possible. The subset of X containing all the variables whose domains are subsets of [a, b] is denoted by Xa,b. The subset of Xa,b including all the variables containing the given value c in their domains is denoted by Xa,b,c. Finally the number of pairs of equal values in an element of ME(Xa,b) is denoted by Ca,b(X ) or just Ca,b if the considered set of variables is clear from the context. For notational convenience, if b < a, then we set Xa,b = ∅ and Ca,b = 0. The value C1,λ(X ) is the number of equal pairs of values in an element of ME(X ).
Theorem 3 C1,λ(X ) can be computed in O((n+ λ)λ2) steps.
Proof. The problem is solved by a dynamic programming approach: for every a, b such that 1 ≤ a ≤ b ≤ λ, we compute Ca,b. The main observation that makes it possible to use dynamic programming is the following: in every P ∈ME(Xa,b) there is a value c (a ≤ c ≤ b) such that every variable X ∈ Xa,b,c is assigned value c. To see this, let value c be a value that is assigned by P to a maximum number of variables. Suppose that there is a variable X with c ∈ D(X) that is assigned by P to a different value, say c′. Suppose that c and c′ appear on x and y variables, respectively. By changing the value of X from c′ to c, we increase the number of equalities by x − (y − 1) ≥ 1 (since x ≥ y), contradicting the optimality of P .
Notice that Xa,b \ Xa,b,c is the disjoint union of Xa,c−1 and Xc+1,b (if c − 1 < a or c + 1 > b, then the corresponding set is empty). These two sets are independent in the sense that there is no value that can appear on variables from both sets. Thus it can be assumed that P ∈ ME(Xa,b) restricted to Xa,c−1 and Xc+1,b are elements of ME(Xa,c−1) and ME(Xc+1,b), respectively. Taking into consideration all possible values c, we get
Ca,b = max c,a≤c≤b
(( |Xa,b,c|
2
) + Ca,c−1 + Cc+1,b ) . (1)
In the first step of Algorithm 3, we compute |Xa,b,c| for all values of a, b, c. For each triple a, b, c, it is easy to compute |Xa,b,c| in time O(n), hence all these values can be computed in time O(nλ3). However, the running time can be reduced to O((n+ λ)λ2) by using the same idea as in Algorithm 1. For each pair a, b, we compute the number of occurrences of each value c by first computing a derivative δa,b. More precisely, we define δa,b(c) = |Xa,b,c|− |Xa,b,c−1| and compute δa,b(c) for every a < c ≤ b (Algorithm 3, Line 1-2). Thus by going through all the variables, we can compute the δa,b(c) values for a fixed a, b and for all a ≤ c ≤ b in time O(n) and we can also compute |Xa,b,a| in the same time bound. Now it is possible to compute the values |Xa,b,c|, a < c ≤ b in time O(λ) by using the equality |Xa,b,c| = |Xa,b,c−1|+ δa,b(c) iteratively (Algorithm 3, Line 3).
In the second step of the algorithm, we compute all the values Ca,b. We compute these values in increasing order of b− a. If a = b, then Ca,b = (|Xa,a,a|
2
) . Otherwise, values Ca,c−1
and Cc+1,b are already available for every a ≤ c ≤ b, hence Ca,b can be determined in time O(λ) using Eq. (1) (Algorithm 3, Line 4). Thus all the values Ca,b can be computed in time
Algorithm 3: Computing the maximum number of equalities. Data: A set of variables: X Result: C1,λ(X ) ∀ 1 ≤ a, b, c ≤ λ, δa,b(c)← |Xa,b,c| ← Ca,b ← 0; foreach k ∈ [0, λ− 1] do
foreach a ∈ [1, λ− k] do b← a+ k; foreach X ∈ Xa,b do
1 δa,b(min(X))← δa,b(min(X)) + 1; 2 δa,b(max(X) + 1)← δa,b(max(X) + 1)− 1; foreach c ∈ [a, b] do 3 |Xa,b,c| ← |Xa,b,c−1|+ δa,b(c); 4 Ca,b ← max(Ca,b, ( (|Xa,b,c| 2 ) + Ca,c−1 + Cc+1,b));
return C1,λ;
O(λ3), including C1,λ, which is the value of the optimum solution of the problem. Using standard techniques (storing for each Ca,b a value c that minimises (1)), a third step of the algorithm can actually produce a variable assignment that obtains the maximum value. 2
Algorithm 3 computes the largest number of equalities one can achieve by assigning a set of variables with interval domains. It can therefore be used to find an optimal solution to either SoftAllDiffmaxG or SoftAllEqual min G . Notice that for the latter one needs
to take the complement to ( n 2 ) in order to get the value of the violation cost. Clearly, it follows that achieving range or bounds consistency on these two constraints can be done
in polynomial time, since Algorithm 3 can be used as an oracle for testing the existence of a range support. We give an example of the execution of Algorithm 3 in Figure 4. A set of ten variables, from X1 to X10 are represented. Then we give the table Ca,b for all pairs a, b ∈ [1, λ].
The complexity can be further reduced if λ n. Here again, we will use the occurrence function, albeit in a slightly different way. The intuition is that some values and intervals of values are dominated by other. When the occurrence function is monotonically increasing, it means that we are moving toward dominating values (they can be taken by a larger set of variables), and conversely, a monotonic decrease denotes dominated values. Notice that since we are considering discrete values, some variations may not be apparent in the occurrence function. For instance, consider two variables X and Y with respective domains [a, b] and [b + 1, c] such that a ≤ b ≤ c. The occurrence function for these two variables is constant on [a, c]. However, for our purpose, we need to distinguish between “true” monotonicity and that induced by the discrete nature of the problem. We therefore consider some rational values when defining the occurrence function. In the example above, by introducing an extra point b + 12 to the occurrence function, we can now capture the fact that in fact it is not monotonic on [a, c].
Let X be a set of variables with interval domains in [1, λ]. Consider the occurrence function occ : Q 7→ [0..n], where Q ⊂ Q is a set of values of the form a/2 for some a ∈ N, such that min(Q) = 1 and max(Q) = λ. Intuitively, the value of occ(a) is the number of variables whose domain interval encloses the value a, more formally:
∀a ∈ Q, occ(a) = |{X | X ∈ X ,min(X) ≤ a ≤ max(X)}|.
Such a function, along with the corresponding set of intervals, is depicted in Figure 5. A crest of the function occ is an interval [a, b] ⊆ Q such that for some c ∈ [a, b], occ is monotonically increasing on [a, c] and monotonically decreasing on [c, b]. For instance, on the set intervals represented in Figure 5, [1, 15] is a crest since it is monotonically increasing on [1, 12] and monotonically decreasing on [12, 15].
Let I be a partition of [1, λ] into a set of intervals such that every element of I is a crest. For instance, I = {[1, 15], [16, 20], [21, 29], [30, 42]} is such a partition for the set of intervals shown in Figure 5. We shall map each element of I to an integer corresponding to its rank in the natural order. We denote by RI(X ) the reduction of X by the partition I. The reduction has as many variables as X (equation 2 below) but the domains are replaced with the set of intervals in I that overlap with the corresponding variable in X (equation 3 below). Observe that the domains remain intervals after the reduction.
RI(X ) = {X ′1, . . . , X ′|X |}. (2)
∀X ′i ∈ RI(X ), D(X ′i) = {I | I ∈ I & D(Xi) ∩ I 6= ∅}. (3)
For instance, the set of intervals depicted in Figure 5 can be reduced to the set shown in Figure 4, where each element in I is mapped to an integer in [1, 4].
Theorem 4 If I is a partition of [1, λ] such that every element of I is a crest of occ, then ME(X ) = ME(RI(X )).
Proof. First, we show that for any optimal solution s ∈ME(X ), we can produce a solution s′ ∈ME(RI(X )) that has at least as many equalities as s. Indeed, for any value a, consider every variable X assigned to this value, that is, such that s[X] = a. Let I ∈ I be the crest containing a, by definition we have I ∈ D(X ′). Therefore we can assign all these variables to the same value I.
Now we show the opposite, that is, given a solution to the reduced problem, one can build a solution to the original problem with at least as many equalities. The key observation is that, for a given crest [a, b], all intervals overlapping with [a, b] have a common value. Indeed, suppose that this is not the case, that is, there exists [c1, d1] and [c2, d2] both overlapping with [a, b] and such that d1 < c2. Then occ(d1) > occ(d1 + 1 2) and similarly occ(c2− 12) < occ(c2). However, since a ≤ d1 < c2 ≤ b, [a, b] would not satisfy the conditions for being a crest, hence a contradiction. Therefore, for a given crest I, and for every variable X ′ such that s′[X ′] = I, we can assign X to this common value, hence obtaining as many equalities. 2
We show that this transformation can be achieved in O(n log n) steps. We once again use the derivative of the occurrence function (δocc), however, defined on Q rather than [1, λ]:
δocc(v)← (|{i | min(Xi) = v}| − |{i | max(Xi) = v − 1
2 }|).
Moreover, we can compute it in O(n log n) steps as shown in Algorithm 4. We first compute the non-null values of δocc by looping through each variable X ∈ X (Line 1). We use the
same data structure as for Algorithm 1, hence the complexity of this step isO(n log n). Next, we create the partition into crests by going through the derivative once and identifying the inflection points. The variable polarity (Line 3) is used to keep track of the evolution of the function occ. The decreasing phases are denoted by polarity = neg whilst the increasing phases correspond to polarity = pos. We know that a value v is the end of a crest interval when the variable polarity switches from neg to pos. Clearly, the number of elements in δocc is bounded by 2n. Recall that the list data structure is sorted. Therefore, going through the values δocc(v) in increasing order of v can be done in linear time, hence the overall O(n log n) worst-case time complexity.
Algorithm 4: Computing a partition into crests. Data: A set of variables: X Result: I δocc ← ∅;
1 foreach X ∈ X do δocc(min(X))← δocc(min(X)) + 1; δocc(max(X) +
1 2 )← δocc(max(X) + 12 )− 1;
I ← ∅; min← max← 1;
2 while δocc 6= ∅ do 3 polarity ← pos;
k = 1; repeat
pick and remove the first element (a, k) of δocc; max← round(a)− 1; if polarity = pos & k < 0 then polarity ← neg;
until polarity = pos or k < 0 ; add [min,max] to I; min← max+ 1;
return I
Therefore, we can replace every crest by a single value at the preprocessing stage and then run Algorithm 3. Moreover, observe that the number of crests is bounded by n, since each needs at least one interval to start and one interval to end. Thus we obtain the following theorem, where n stands for the number of variables, λ for the number of distinct values, and m for the sum of all domain sizes.
Theorem 5 Enforcing rc on SoftAllEqualminG can be achieved in O(min(λ2, n2)nm) steps.
Proof. If λ ≤ n then one can achieve range consistency by iteratively calling Algorithm 3 after assigning each of the O(m) unit assignments ((X, v) ∀X ∈ X , v ∈ D(X)). The resulting complexity is O(nλ2)m (see Theorem 3, the term λ3 is absorbed by nλ2 due to λ ≤ n).
Otherwise, if λ > n, the same procedure is used, but after applying the reformulation described in Algorithm 4. The complexity of the Algorithm 4 is O(n log n), and since after the reformulation we have λ = O(n), the resulting complexity is O(n3m). 2
The pumps mentioned in section S.1.2.1 were faulty only in the function of their logic boards; the mechanical components and motors were fully oper‐
ational. As motion was provided by two stepper motors, from the NEMA family, these were therefore compatible with the shield developed for the X‐Y axis control (Section S.1.3.1). It was therefore possible to re‐use the mechanical components of these syringe pumps by the replacement of their electronics with custom PCBs (Figure 12). Since each board controlled two stepper motors, a single board was assigned to each pumps (Figure 13). A second Arduino was assigned entirely to pump control, in addition to a second power‐supply assigned only to pump motors.
We use a standard CNN architecture as shown in Fig. 2. There are 3 convolutional layers interspersed with max-pooling layers. Hebbian learning is used to train the weights and biases of these layers. Layers are sequentially learnt from 1 to 3 in an unsupervised manner. Training data at each layer comprises of N (typically 100− 200K) normalized activations randomly sampled from the lower layer. The 3 intermediary feature maps (Fig. 2) are used (via average pooling and concatenation) to construct the final features. To study the quality of representations, these features are used to train a linear SVM for classification. At the first layer, raw image patches are processed as described in [12].
The setting we focus on is the following: the dataset has a set of tables in the star schema with KFK dependencies (KFKDs). Star schemas are ubiquitous in many applications, including retail, insurance, Web security, and recommendation systems [33, 26, 25]. The fact table, which has the target variable, is denoted S. It has the schema S(SID, Y,XS , FK1, . . . , FKq). A dimension table is denoted Ri (i = 1 to q) and it has the schema Ri(RIDi,XRi). Y is the target variable (class label), XS and XRi are vectors (sequences) of features, RIDi is the primary key of Ri, while FKi is a foreign key feature that refers to Ri. We call XS home features and XRi foreign features. For ease of exposition, we also treat X as a set of features since the order among features is immaterial in our setting. Let T
denote the output of the projected equi-join (key-foreign key, or KFK for short) query that constructs the full training dataset by concatenating the features from all base tables: T ← π(R ./RID=FK S). In general, its schema is T(SID, Y,XS , FK1, . . . , FKq,XR1 , . . . ,XRq ). In contrast to our setting, traditional ML formulations do not distinguish between home features, foreign keys, and foreign features. The number of tuples in S (resp. R) is denoted nS (resp. nR); the number of features in XS (resp. XR) is denoted dS (resp. dR). Without loss of generality, we assume that the join is not selective, which means nS is also the number of tuples in T. DFK denotes the domain of FK and by definition, |DFK | = nR. We call nSnR the tuple ratio. Note that our setting is different from the statistical relational learning (SRL) setting, which deals with joins that violate the IID assumption and duplicate labeled examples from S [13]. KFK joins do not cause such duplication and thus, regular IID models are typically used in this setting.
A generalized methodology for feature space partitioning and mode seeking was presented - leveraging synergism of adaptive, anisotropic Mean Shift and guided agglomeration. Unsupervised adaptation of full anisotropic bandwidths is useful and further enables Mean Shift clustering. We are excited about its prospects on point-normal clouds and video streams.
Our experiments did indicate sparse data to be an issue. This is understandable, as it encumbers cluster growth and bandwidth development, with AAAMS behaving like conventional Mean Shift then. Future work would also focus on alleviating this issue.
We thank Thomas Seyller, Léa Deleris and three anonymous reviewers for their valuable feedback.
Our work is inspired by the temporal and spatial imbalance of the attention placed in
understanding the scene and focuses on exploring the depth and scale of the model in the task of remote sensing scene recognition from the perspective of scene complexity. A visualization method is used for quantitative and qualitative analyses of the mechanism in remote sensing scene recognition. The main conclusions are as follows:
1. Remote sensing scene recognition with different complexities depends on different
network depths and sale characteristics. This means complex scene concept is the representation of multiple scale and level feature, there is implicit correlation between complexity and feature.
2. Complex scenes rely on the feature representation of multiple scales and the joint semantic
support of multiple targets. We demonstrate that multiple scale involved model outperforms any other single network; and experiments based on CAMs indicates that the distribution of combing targets play significant role in learning mechanism. In our next work, we will design an effective indicator to evaluate remote sensing scene
complexity, and then design a network with an adaptive depth, which considers multiple scales. We will further explore the manner in which the mechanism of multi-objective joint probability can be combined with the generation model to help us better underst and a scene.
Reference Benz, U. C., P. Hofmann, G. Willhauck, I. Lingenfelder and M. Heynen (2004). "Multiresolution, object-oriented fuzzy analysis of remote sensing data for GIS-ready information." International Journal of Photogrammetry & Remote Sensing 58(3–4): 239-258. Blaschke, T. (2010). "Object based image analysis for remote sensing." Isprs Journal of Photogrammetry & Remote Sensing 65(1): 2-16. Coates, A. and A. Y. Ng (2012). "Selecting Receptive Fields in Deep Networks." Advances in Neural Information Processing Systems: 2528-2536. Desachy, J. (1995). "Active fusion for remote sensing image understanding." Proceedings of SPIE - The International Society for Optical Engineering 2579: 67-77. Dixit, M., S. Chen, D. Gao, N. Rasiwasia and N. Vasconcelos (2015). Scene classification with semantic Fisher vectors. Computer Vision and Pattern Recognition. Gal, V., J. Hamori, T. Roska, D. Balya, Z. Borostyankoi, M. Brendel, K. Lotz, L. Negyessy and L. Orzo (2004). "Receptive Field Atlas and Related CNN Models." International Journal of Bifurcation & Chaos 14(2): 551-584. Girshick, R., J. Donahue, T. Darrell and J. Malik (2013). Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation. Computer Vision and Pattern Recognition. Handa, A., V. Patraucean, V. Badrinarayanan, S. Stent and R. Cipolla (2015). "SceneNet: Understanding Real World Indoor Scenes With Synthetic Data." Computer Science. He, K., X. Zhang, S. Ren and J. Sun (2015). "Deep Residual Learning for Image Recognition." Computer Science. Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long and Jonathan (2014). "Caffe: Convolutional Architecture for Fast Feature Embedding." Eprint Arxiv: 675-678. Kounalakis and Tsampikos (2015). "Depth-adaptive methodologies for 3D image caregorization." Krizhevsky, A., I. Sutskever and G. E. Hinton (2012). "ImageNet Classification with Deep Convolutional Neural Networks." Advances in Neural Information Processing Systems 25(2): 2012. Langkvist, M., A. Kiselev, M. Alirezaie and A. Loutfi (2016). "Classification and Segmentation of Satellite Orthoimagery Using Convolutional Neural Networks." Remote Sensing 8(4): 329. Lazebnik, S., C. Schmid and J. Ponce (2006). Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories. IEEE Computer Society Conference on Computer Vision & Pattern Recognition. Liu, F., C. Shen and G. Lin (2014). "Deep convolutional neural fields for depth estimation from a single image." 5162-5170. Mattar, E. A. and K. M. Al-Rewihi (1998). Remote Image Segmentation and Understanding via Artificial Convolution Neural Network. The Middle East International GIS Conference and Workshops. Quattoni, A. and A. Torralba (2009). "Recognizing indoor scenes." 413-420. Rangel, J. C., M. Cazorla, I. Garcíavarea, J. Martínezgómez, É. Fromont and M. Sebban
(2016). "Scene classification based on semantic labeling." Advanced Robotics(11-12). Saeed, U. (2015). "Eye movements during scene understanding for biometric identification." Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). "Convolutional Neural Network Based Automatic Object Detection on Aerial Images." IEEE Geoscience and Remote Sensing Letters 13(5): 740-744. Shen, L., Z. Lin and Q. Huang (2016). Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks, Springer International Publishing. Simonyan, K. and A. Zisserman (2014). "Very Deep Convolutional Networks for LargeScale Image Recognition." Computer Science. Song, X., Y. Dai and X. Qin (2016). "Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network." Srivastava, R. K., K. Greff and J. Schmidhuber (2015). "Highway Networks." Computer Science. Szegedy, C., W. Liu, Y. Jia and P. Sermanet (2015). "Going deeper with convolutions." 1- 9. Wang, L., S. Guo, W. Huang, Y. Xiong and Y. Qiao (2016). "Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs." Wu, H. and Z. L. Li (2009). "Scale issues in remote sensing: a review on analysis, processing and modeling." Sensors 9(3): 1768. Xia, G. S., J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong and L. Zhang (2016). "AID: A Benchmark Dataset for Performance Evaluation of Aerial Scene Classification." Xiao, J., J. Hays, K. A. Ehinger and A. Oliva (2010). SUN database: Large-scale scene recognition from abbey to zoo. Computer Vision and Pattern Recognition. Yin, H., X. Jiao, Y. Chai and B. Fang (2015). "Scene classification based on single-layer SAE and SVM." Expert Systems with Applications 42(7): 3368-3380. Yu, F., A. Seff, Y. Zhang, S. Song, T. Funkhouser and J. Xiao (2015). "LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop." Computer Science. Zhang, L., G. S. Xia, T. Wu, L. Lin and X. C. Tai (2015). "Deep Learning for Remote Sensing Image Understanding." Journal of Sensors 501: 173691. Zhong, Y., M. Cui, Q. Zhu and L. Zhang (2015). "Scene classification based on multifeature probabilistic latent semantic analysis for high spatial resolution remote sensing images." Journal of Applied Remote Sensing 9(1): 095064. Zhou, B., A. L. Garcia, J. Xiao, A. Torralba and A. Oliva (2014). "Learning Deep Features for Scene Recognition using Places Database." Advances in Neural Information Processing Systems 1: 487-495. Zhou, B., A. Khosla, A. Lapedriza, A. Oliva and A. Torralba (2014). "Object Detectors Emerge in Deep Scene CNNs." Computer Science. Zhou, B., A. Khosla, A. Lapedriza, A. Oliva and A. Torralba (2015). "Learning Deep Features for Discriminative Localization." Computer Science.
ANA is a term extractor which takes as input a raw data corpus and proposes as output a rank list of terms for each text or for the entire corpus. This system has been specially design to treat bad quality corpora (for instance written in telegraphic style) which can not be tagged. It does not make any hypothesis on the form of terms.
This incremental system uses a bootstrap (a few frequent terms), a list of functional words, and a list of lexical scheme words. The system 1. recognises the terms in the texts (and the candidate-terms) included in the bootstrap, 2. collects the context of these recognized terms (through a window of n words), 3. studies these contexts to infer some candidate-terms which will be included in the bootstrap, 4. returns to step 1 unless a stop condition is filled (for instance, there is no more discovery of
candidate-terms).
The inference of new candidate-terms follows three patterns. We illustrate their presentation by examples where terms are written in capital letters, functional words belong to the set {"a" "any" "for" "in" "is" "may" "of" "or" "the" "this" "to"}, there is one lexical scheme: "of", and six terms constitute the bootstrap: "WOOD" "COLOUR" "BEECH" "TIMBER", "DIESEL", "ENGINE". 1. Two terms (or candidate-terms) of the bootstrap appear in the same window
A candidate-term is qualified when two existing terms (or candidate-terms) of the bootstrap appear frequently with almost the same arrangement. The most frequent arrangement becomes a candidate-term
Example : ... "the" "DIESEL", "ENGINE" "is" ... … "this" "DIESEL", "ENGINE" "has" ... ... "a" "DIESEL", "ENGINE" "never" ... The study of these three contexts of "DIESEL" and "ENGINE" will qualify "DIESEL ENGINE" as a candidate-term.
2. A term (or candidate-term) of the bootstrap and a lexical scheme word appear in the same window A candidate-term is qualified when a term (or candidate-term) of the bootstrap appears frequently with a lexical scheme word and with a word. This word then becomes a new candidate-term.
Example : ... "any" "shade" "of" "WOOD" "could" ... ... "this" "shade" "of" "WOOD" "is" ... ... "the" "shade" "of" "BEECH" "may" ... ... "new" "shade" "of" "TIMBER"... ... "same" "shade" "of" "WOOD" "in" ...
The study of these contexts will qualify "SHADE" as candidate-term. 3. A term (or candidate-terms) of the bootstrap without any lexical scheme nor second term
A candidate-term is qualified when a term (or candidate-terms) of the bootstrap appears frequently with a word without any lexical scheme word nor second term.. This word then becomes a new candidate-term.
Example : ... "use" "any" "soft" "WOODS" "to" "make" "this" ... ... "buy" "this" "soft" "WOODS" "or" "plastic" "for" ... ... "cheapest" "soft" "WOODS" "comes" "from" ...
The study of these contexts will qualify "SOFT WOODS" as a candidate-term.
Thus, the ANA system is capable of extracting single-word terms and multi-word terms. It can also enrich an existing terminology. The adaptiveness of the system to the corpus (and to the language of the corpus) is improved by the possible inference of the lexical scheme words as has been suggested in previous work (ref. 11).
As mentioned in Section 3.3.3, due to the representation complexity and the variety of question structures, manually creating grammar rules in an ad-hoc manner is very expensive and error-prone. For example, such rule-based approaches as presented in [26,35,45] manually defined a list of sequence pattern structures to analyze questions. Since rules were created in an ad-hoc manner, these approaches share common difficulties in managing the interaction between rules and keeping consistency among them. This section introduces our knowledge acquisition approach3 to analyze natural language questions by applying the Single Classification Ripple Down Rules (SCRDR) methodology [10,47] to acquire rules incrementally. Our contribution focuses on the semantic analysis module by proposing a JAPE-like rule language and a systematic processing to create rules in a manner that the interaction among rules is controlled and consistency is maintained. Compared to the first KbQAS version [35], this is the key innovation of the current KbQAS version. A SCRDR knowledge base is built to identify the question structures and to produce the query tuples as the intermediate representations of the input questions. We outline the SCRDR methodology and propose a
3The English question analysis demonstration is available online at http://150.65.242.39:8080/KbEnQA/, and the Vietnamese question analysis demonstration is available online at http://150.65. 242.39:8080/KbVnQA/.
rule language for extracting the intermediate representation of a given question in Section 4.1 and Section 4.2, respectively. We then illustrate the process of systematically constructing a SCRDR knowledge base for analyzing questions in Section 4.3.
Let Ω be the set of possible outcomes, where the outcomes are mutually exclusive and exhaustive singleton elements of the decision environment. In Bayesian formalism, the probabilities are assigned only to the singleton subsets of the quantitative information set. These probabilities are used to make the decisions.
For systems with a complex input (real-time sensor measurements, multidimensional filtered feature extractions, real-time data base and a priori data base information content, natural language text and symbols parsing evidence, quantitative and qualitative communication clues, and inconsistent errors), a ( ) Ω=Ω 2set -Power representation of the outcomes and a two-level (lower & upper) probability portrayal is a better representation of the incomplete information set; i.e., some sensor measurements of attributes populate more than one hypothesis.
Belief theories maintain a two-level probabilistic portrayal of the information set: the Belief or credal level and the Plausibility level. The primary foundation in any decision preposition iA is the value of the Belief Bel( iA ), while the
plausibility ( )iAPl provides the secondary support for the decision.
The basic belief assignment ( )JAm represents the strength of all the incomplete information set for the outcome JA . The assignments of the BBAs values to a specific subset Ω∈ 2JA are constrained by the normalization constraint equation.
( ) 1 2 =∑ Ω∈JA JAm
Using the BBAs as the representative of the incomplete information set, the Belief function can be computed. The Belief of the subset preposition JA is the sum ( )KAm for all the subsets of KA containes in JA .
( ) ( )∑ ⊇ = JK AA KJ AmABel
The Plausibility of the subset JA is the sum ( )KAm for all subsets KA that have a non-null intersection of JA .
( ) ( )∑ ≠∩ = 0JK AA KJ AmAPl
For any singleton proposition iA , the probability is bound between the Belief and the Plausibility.
( ) ( ) ( )iii APlAyProbabilitABel ≤≤
Golang provides a built-in map[10] type that implements a hash table.
A map is an unordered group of elements of one type, called the element type, indexed by a set of unique keys of another type, called the key type. The value of an uninitialized map is nil.[10]
nodeTable maps a board configuration to a node in game's state space. No node appears twice when we move from start to goal state, so there is not any ambiguity. It helps us traversing the path once the search is complete.
To further evaluate the effectiveness of the gC2S model for natural language generation, we choose the task of fake review detection, which aims to classify whether the reviews are written by real users or generated by the gC2S model. Real reviews are treated as positive, and fake reviews are treated as negative. For the evaluation data, we randomly select some products which have at least two real reviews for each rating score in the Amazon data and one review for each rating score in the TripAdvisor data. For each real review, a fake review is generated with gC2S according to its contexts. Table 3 summarizes the number of reviews in each domain.
Human Evaluation. We use the Amazon Mechanical Turk to evaluate whether the reviews are fake not. We divide all
the data into different batches. Each batch contains twenty reviews about the same product. We show the urls of the products, the sentiment rating and the review content to users to ask the turkers to judge whether the reviews are written by real users or not. To control the quality of the results, some “gotcha” questions are inserted in the middle of the list of reviews. Only the results judged by users who answer the “gotcha” questions correctly are kept. The kappa score is .... We summarize the final results into Table 4.
We can see that in all the domains, more than 50% of the fake reviews generated by the gC2S model are misclassified by the Turkers, and around 80% of the real reviews are correctly classified. This shows that the reviews generated by the gC2S model are indeed very natural.
As more samples are generated by the RNN, more mistakes are likely to make by the model. Therefore, we want to see how the results of human evaluate change as the lengths of the reviews increase. Fig. 5 presents the human evaluation results w.r.t different lengths of reviews. We can see that for both the lengthy fake and real reviews, fewer percentages are misclassified by human judges. However, we can see that even for the fake reviews with more than 150 words, around 40% of them are still misclassified by the human judges.
Automatic Classification. Another way to evaluate the effectiveness of our approach is to see how well existing stateof-the-art fake review detection algorithm performs on our generated fake reviews. We adopt the approach in (Ott et al. 2011), which trains a classifier with 800 real reviews from TripAdvisor and 800 fake reviews written by the Amazon Mechanical Turkers3. Here we use the unigram and bigram features for classification, with which the results are very
3The training data is available at http://myleott.com/ op_spam/.
close to the best results according to (Ott et al. 2011). Table (?) summarizes the results. We can see that more than 90% of the fake reviews generated by the gC2S model are misclassified as the real reviews by the classifier.
As for many civilizations, poetry is an essential part of Chinese literature. Poetry has influenced the development of the literature and language of both classical and vernacular Chinese. Certain of the words that we use today can be tracked all the way back to the Shijing (詩經/shi1 jing1/ [1]), c. 1046BC. Research on Chinese poetry is thus instrumental for understanding Chinese culture, and a lot of invaluable results have been accumulated over the past thousands of years from the study and analysis of Chinese poetry. The availability of digital tools and resources enable researchers to compare and analyze the poetry from certain perspectives that were hard to achieve in the past. In many cases, we can verify the claims of previous researches with solid data, and, in others, we may enrich our understanding of the poetry. The accessibility of increasingly larger datasets strengthens our research potential. In earlier stages of digital humanities, pioneers focused their work on Tang and Song poetry [2]. Now, we can access digitized texts of poems that were published in the periods from 1046BC to modern days. Software tools allow us to study the data from a wide variety of perspectives in an efficient way. Search engines and information retrieval techniques [3] help us extract relevant texts from a large dataset. Then, researchers can employ domain knowledge for advanced studies with the use of additional tools. In this paper, we showcase research results that we achieved by handling the available data with existing tools in flexible ways. We collected nine representative corpora of Chinese poetry, one each for a major dynasty in Chinese history between 1046BC and 1644AD. We list the corpora in Table 1, where we assign an acronym to each corpus for ease of reference [4]. We also show their Chinese names (Collection) and the periods of publication (Time). A collection for the Qing dynasty is unavailable yet because an editorial committee is still working toward the completion of
Chao-Lin Liu. Flexible computing services for comparisons and analyses of classical Chinese poetry, Proceedings of the 2017 International Conference on Digital Humanities (DH 2017), 505‒507, Montréal, Québec, Canada, 8-11 August 2017.
this very challenging goal [5]. Excluding the punctuation marks that were added by the data providers, we have more than 16.5 million characters [6] in the corpora. By flexibly integrating and migrating tools to offer new functions, we can provide researchers with opportunities to investigate Chinese poetry from new perspectives. In the first example, we show a new way to compare the ways that poets use words in their poems. In the second, with our own tools, we can find shared collocations and patterns of poems in different corpora, and this capability allows us to study and compare the styles of individual poets and their dynasties.
• ff_fastfood(): computes fastfood and stores the result in ff_dataout.
• ff_features(): computes fastfood features and return the result in a float pointer.
In contrast to the features and capabilities in the previous section, there are a number of features that annotation projects often need, but are not commonly found in AUIs or other annotation support tools.
Creating arbitrary annotation schemes: In the list of common features above we included creating flat tag schemes. As noted in a previous chapter, annotation schemes come in different types, including single labels, sets of “flat” attribute-value pairs, full-fledged recursive feature structures, relations between segments, or some combinations of these . (citation to Chapter III by Ide et al.). Although there is plenty of support for defining and annotating single label tagsets, and brat (Stenetorp et al., 2012) allows definition of arbitrary relation schemes, there is little or no support for defining the other more complicated types of annotation schemes such as recursive feature structures or combination schemes. Thus if one’s annotation scheme involves any of these more complicated structures, one is almost forced to modify an existing tool or create a new tools. This is a major limitation of annotation tools, especially as the field moves toward more complicated linguistic phenomena.
One example of a tool which does include this functionality is SALTO, which allows dynamic definition or extension of an annotation scheme by adding new frames, frame elements, and flags (Burchardt, Erk, Frank, Kowalski, & Pado, 2006).
Sophisticated Visualization: A useful feature, but one not often found in AUIs focused on text alone, is that of sophisticated visualization of annotations. Some annotations schemes can be quite complicated, involving large tag sets, numerous types of linguistic objects, and multiple features arranged into complicated hierarchies. Visualization can thus be of great service in understanding the current state of the annotation of the document, perceiving errors, and determining what needs to be done. Moreover, annotation, as has been noted several times already, can often be tedious and somewhat mind-numbing for the annotator. This has the effect of making it easy for annotators to miss key pieces of information; thus, an AUI that visualizes annotations in an intuitive, clear, and expressive manner helps to increase the efficiency of annotators and the quality of their annotations.
Multimodal tools (such as Praat, ANVIL, and CLAN) tend, by the very nature of their targeted linguistic artifacts, to have sophisticated visualization facilities. Tools for the text annotation, on the other hand, often lack comparable visualization capabilities that truly take advantage of the full power of a modern graphical user interface. Notable exceptions include brat (Stenetorp et al., 2012), which excels at visualizing sparse local relations, and ANNIS which does not provide annotation capabilities per se but rather specializes in search an visualization of annotations (Zeldes, Ritz, Lüdeling, & Chiarcos, 2009).
Checking file correctness against specs: It is often of great utility to be able to verify that an annotated artifact conforms to some specification of the format of the annotations. For example, that the tagset used is the one claimed, with no extra or misformatted tags. This is akin to verifying that an XML document is valid, so, not only syntactically well-formed (e.g., all opening tags have a corresponding closing tag, tags properly nested as a tree), but that it also follows the required grammar of the tags as specified by an XML schema. One example is CLAN, which provides the ability to check if a particular annotation file conforms to the CHAT annotation file format. And,
moreover, any tool that can import a particular format performs an implicit well-formedness check, in that if the import of a particular file succeeds you can be sure that the file conforms at least to that particular tool’s implementation of the format specification. But the more explicit and general form of this feature is desirable: being able to affirm (without the tool crashing or producing some other error behavior) that a file is formatted correctly relative to some formal specification, and contains neither formatting errors nor extraneous unformatted material.
Workflow support (user, role, file, and task management): A fairly important but often overlooked set of capabilities, especially from the point of the view of an annotation manager, is the ability to manage the overall workflow of an annotation project. By workflow in this case we mean the process of planning the unfolding of an annotation project in terms of individual tasks, annotators, and files. What files will be annotated at what time, and by whom? Are there constraints that must be satisfied (i.e., one annotator must annotate first, or one file must be annotated before another)? If task assignment is unconstrained, and annotators are allowed to pick and choose what files they do when, how will you assure that they only annotate a file once, or do not annotate files they are not supposed to, or do not miss a file? Moreover, how exactly will files be distributed to annotators and the annotators notified of their assignments? By email? Shared file system? Other network file distribution facility?
Examples of tools that support such features, including fine-grained control over user access rights and file and task assignments, include the LDC tools suite, SALTO and WebAnno. The LDC tools (which are, to our knowledge, not generally available), allow flexible assignment of annotation tasks to geographically spread-out annotators; the development of that suite was driven by the large-scale and time-sensitive nature of many of LDC annotation projects. SALTO gives the ability to assign files for annotation to one or more annotators within a special administrative mode (Burchardt et al., 2006). The WebAnno editor allows defining a pool of annotators and files for a project, distributing the files among the annotators and monitoring progress (Seid Muhie et al., 2013). A few other tools also provide related capabilities (e.g., Anafora: Chen & Styler, 2013), but generally workflow management is under-attended to.
Customizable annotation pipeline: Annotation today is becoming more and more of a sequenced affair. That is, instead of starting with a plain, unannotated linguistic object, annotation projects will often rely on applying a number of automatic layers of annotation before beginning their own annotation. For text, good common examples of types of processing applied to text before more high-level annotation takes place include tokenization, sentence segmentation, part-of-speech tagging, lemmatization, and syntactic parsing. In these cases, it very helpful if the tools used to create the files for annotation allow the assembly or arbitrary automatic annotation pipelines. If these processing capabilities, however, are not integrated with an AUI, such as with WebLicht (Hinrichs, Hinrichs, & Zastrow, 2010), then an extra step of transferring files from the pre-processing pipeline to the AUI must be undertaken.
A bare bones example of a fully customizable processing pipeline is something like UIMA (Apache, 2014), which allows assembling arbitrary sequences of automatic annotators using a number of different programming languages. This situation is not necessarily ideal, however, as the learning curve for UIMA is a bit difficult and requires some sophisticated programming skills. Good examples of tools that provide a reasonable UI to create pipelines but still allow sophisticated pre-processing of text files include GATE (Cunningham et al., 2011) and WebLicht (Hinrichs et al., 2010). On the other hand, more and more annotation platforms do integrate the ability to automatically pre-annotate files, at least for low-level annotations. Most of them have the annotation program builtin, which means it works only for particular types of annotation and particular languages. Some others, in particular WebAnno incorporate a generic machine learning program, that allows the administrators to define the type of annotation to be performed, import training data, and train the learner on this data. New data can then be automatically annotated with the trained model.
Interleaving manual and automatic annotation: Related to the issue of assembling annotation pipelines and online learning is interleaving manual and automatic annotation. Sometimes is useful to have a tighter feedback loop between manual and automatic stages of the annotation process: do some pre-processing annotation, have annotators correct or add to those annotations, and then do more automatic annotation. When returning to the automatic stage, the automatic analyzers take advantage of the cleaner and corrected manual annotations so as to do a better job themselves.
An example of a tool that interleaves these two modes in a smooth way is the Story Workbench (Finlayson, 2011). When an annotator modifies a file, usually by correcting or adding an annotation, the Story Workbench
calculates the changed portion of the text and re-runs the automatic analyzers that are set up to run on that file. The difficulty with that implementation, however, is that, unlike UIMA or GATE, the processing sequence is not especially flexible.
Online learning: At the far end of the spectrum of integration of automatic and manual annotation is online learning. This was a feature found in the very earliest AUIs such as the Alembic workbench (Day et al., 1997). In this approach the system is constantly observing the annotator’s actions, and retraining a model that drives an automatic annotator. After each retraining the system retags everything that has not yet been touched by the annotator.
Of modern tools, the only one we know of that implements this quite useful feature is Annotate (LREC 2000), which provides incremental annotation of context-free grammar analyses. Another tool that implements this functionality is CorA (Marcel Bollmann, Florian Petran, Stefanie Dipper, 2014), which can use the manually annotated data, possibly in combination with pre-existing annotated corpora, to train its normalizer and tagger. The tool can also be extended with PHP classes to add further online learning modules, e.g., lemmatization or sentence boundary detection.
Crowdsourcing: Of increasing interest lately is the opportunity to conduct annotation through crowdsourcing; using online work distribution platforms like Amazon’s Mechanical Turk or Crowdflower. The appeal in these cases is easing the recruitment of annotators and quickly scaling up annotation projects at low cost. The difficulties include integrating the chosen crowdsourcing platform into the project’s workflow (e.g., transferring data in and out, tracking progress), and providing annotators with the appropriate training and AUI to perform the annotation. While this capability is in high demand right now, there are few integrated solutions available. Two examples are the GATE (Cunningham et al., 2011) and WebAnno crowdsourcing plugins, (Seid Muhie et al., 2013, sec. 3.1.6), both of which interface with the Crowdflower platform.
Querying: Like any complicated set of data, the ability to search for specific pieces of information parameterized along dimensions of relevance to the data is a general ability of great use to many other tasks. This is more than just being able to search for specific spans of text of the presence of individual tags. One might want to formulate structured queries, such as “find all annotations which have a tag at this point in their structure”, or “find all annotations across the whole corpus which have feature X and occur just before another annotation with feature Y.” Although basic search abilities are quite common, these more complex search abilities keyed to the annotation schemes themselves are somewhat rare. Emu (Bombien et al., 2006) integrates a good facility for searching in this manner, with the ability to search provided by the Annotation Graph API (Maeda, Bird, Ma, & Lee, 2002).
Definition 1. A semiring K is a quintuple 〈K,⊕,⊗, 0, 1〉 consisting of a set K, an addition operator ⊕ that is associative and commutative, a multiplication operator ⊗ that is associative, and the values 0 and 1 in K, which are the additive and multiplicative identities, respectively. ⊗ must distribute over ⊕ from the left or right (or both), i.e., a ⊗ (b ⊕ c) = (a ⊗ b) ⊕ (a ⊗ c) or (b ⊕ c) ⊗ a = (b⊗a)⊕ (c⊗a). Additionally, 0⊗u = 0 must hold for any u ∈ K. If a semiring K has a commutative ⊗ operator, the semiring is said to be commutative. If K has an idempotent ⊕ operator (i.e., a ⊕ a = a for all a ∈ K), then K is said to be idempotent. Definition 2. The Convex Hull Semiring. Let (K,⊕,⊗, 0, 1) be defined as follows:
K A set of points in the plane that are the extreme points of a convex hull.
A⊕B conv [A ∪B] A⊗B convex hull of the Minkowski sum, i.e.,
conv{(a1 + b1, a2 + b2) | (a1, a2) ∈ A ∧ (b1, b2) ∈ B}
0 ∅ 1 {(0, 0)}
Theorem 1. The Convex Hull Semiring fulfills the semiring axioms and is commutative and idempotent.
Proof. To show that this is a semiring, we need only to demonstrate that commutativity and associativity hold for both addition and multiplication, from which distributivity follows. Commutativity (A · B = B · A) follows straightforwardly from the definitions of addition and multiplication, as do the identities. Proving associativity is a bit more subtle on account of the conv operator. For multiplication, we rely on results of Krein and Šmulian (1940), who show that
conv [A+Mink. B] = conv [conv A+Mink. conv B] .
For addition, we make an informal argument that a context hull circumscribes a set of points, and convexification removes the interior ones. Thus, addition continually expands the circumscribed sets, regardless of what their interiors were, so order does not matter. Finally, addition is idempotent since conv [A ∪A] = A.
Table 4 demonstrates an example dialogue from the test set, along with the gold and model annotations from all 3 models. We observe that Encoder Decoder (ED) and Hierarchical Dialogue Encoder Network (HDEN) are able to successfully identify the domain, intent and slots, while the Memory Network (MN) fails to identify the movie name. Looking at the attention distributions, we notice that the MN attention is very diffused, whereas
HDEN is focusing on the most recent last 2 utterances, which directly identify the domain and the presence of the movie slot in the final user utterance. ED is also able to identify the presence of a movie in the final user utterance from the previous utterance context. Table 5 displays another example where the HDEN model outperforms both MN and ED. Constrained to just the previous utterance ED is unable to correctly identify the domain of the user utterance. The MN model correctly identifies the domain, using its strong focus on the task-intent bearing utterance, but it is unable to identify the presence of a restaurant in the user utterance. This highlights its failure to combine context from multiple history utterances. On the other hand, as indicated by its attention distribution on the final two utterances, HDEN is able to successfully combine context from the dialogue to correctly identify the domain and the restaurant name from the user utterance, despite the presence of several outof-vocabulary tokens. The above two examples hint that HDEN performs
better in scenarios where multiple history utterances encode complementary information that could be useful to interpret user utterances. This is usually the case in more natural goal oriented dialogues, where several tasks and sub tasks go in and out of the focus of the conversation (Grosz, 1979). On the other hand, we also observed that HDEN performs significantly worse in the absence of recombined data. Due to its complex architecture and a much larger set of parameters HDEN is prone to over-fitting in low data scenarios. In this paper we propose a novel neural network architecture to encode context from previous utterances in their chronological order for conversational language understanding, and demonstrate that the proposed architecture results in reduction in overall frame error rate on a multi-domain human machine conversation dataset. We also introduce a data augmentation scheme, to generate longer dialogues with richer context, and empirically demonstrate that it results in performance improvement for multiple model architectures.
Multi-agent learning. One closely related area to our work is multi-agent reinforcement learning. A multi-agent system includes a set of agents interacting in one environment. Meanwhile they could potentially interact with each other [7, 13, 17, 30]. In collaborative multi-agent reinforcement learning, agents work together to maximize a shared reward measurement [13, 17]. ere is a clear distinction between the proposed CDRL framework and multi-agent reinforcement learning. In CDRL, each agent interacts with its own environment copy and the goal is to maximize the reward of the target agents. e formal de nition of the proposed framework is given in Section 4.1. Transfer learning. Another relevant research topic is domain adaption in the eld of transfer learning [23, 29, 33]. e authors in [29] proposed a two-stage domain adaptation framework that considers the di erences among marginal probability distributions of domains, as well as conditional probability distributions of tasks. e method rst re-weights the data from the source domain using Maximum Mean Discrepancy and then re-weights the predictive function in the source domain to reduce te di erence on conditional probabilities. In [33], the marginal distributions of the source and the target domain are aligned by training a network, which maps inputs into a domain invariant representation. Also, knowledge distillation was directly utilized to align the source and target class distribution. One clear limitation here is that the source domain and the target domain are required to have the same dimensionality (i.e. number of classes) with same semantics meanings, which is not the case in our deep knowledge distillation.
In [3], an invariant feature space is learned to transfer skills between two agents. However, projecting the state into a feature space would lose information contained in the original state. ere is a trade-o between learning the common feature space and preserving the maximum information from the original state. In our work, we use data generated by intermediate outputs in the knowledge transfer instead of a shared space. Our approach thus retains complete information from the environment and ensures high quality transfer. e recently proposed A2T approach [25] can avoid negative transfer among di erent tasks. However, it is possible that some negative transfer cases may because of the inappropriate design of transfer algorithms. In our work, we show that we can perform successful transfer among tasks that seemingly cause negative transfer. Knowledge transfer in deep learning. Since the training of each agent in an environment can be considered as a learning task, and the knowledge transfer among multiple tasks belongs to the study of multi-task learning. e multi-task deep neural network (MTDNN) [35] transfers knowledge among tasks by sharing parameters of several low-level layers. Since the low-level layers can be considered to perform representation learning, the MTDNN is learning a shared representation for inputs, which is then used
by high-level layers in the network. Di erent learning tasks are related to each other via this shared feature representation. In the proposed CDRL, we do not use the share representation due to the inevitable information loss when we project the inputs into a shared representation. We instead perform explicitly knowledge transfer among tasks by distilling knowledge that are independent of model structures. In [15], the authors proposed to compress cumbersome models (teachers) to more simple models (students), where the simple models are trained by a dataset (knowledge) distilled from the teachers. However, this approach cannot handle the transfer among heterogeneous tasks, which is one key challenge we addressed in this paper. Knowledge transfer in deep reinforcement learning. Knowledge transfer is also studied in deep reinforcement learning. [19] proposed multi-threaded asynchronous variants of several most advanced deep reinforcement learning methods including Sarsa, Q-learning, Q-learning and advantage actor-critic. Among all those methods, asynchronous advantage actor-critic (A3C) achieves the best performance. Instead of using experience replay as in previous work, A3C stabilizes the training procedure by training di erent agents in parallel using di erent exploration strategies. is was shown to converge much faster than previous methods and use less computational resources. We show in Section 4.1 that the A3C is subsumed to the proposed CDRL as a special case. In [24], a single multi-task policy network is trained by utilizing a set of expert Deep Q-Network (DQN) of source games. At this stage, the goal is to obtain a policy network that can play source games as close to experts as possible. e second step is to transfer the knowledge from source tasks to a new but related target task. e knowledge is transferred by using the DQN in last step as the initialization of the DQN for the new task. As such, the training time of the new task can be signi cantly reduced. Di erent from their approach, the proposed transfer strategy is not to directly mimic experts’ actions or initialize by a pre-trained model. In [26], knowledge distillation was adopted to train a multi-task model that outperforms single task models of some tasks. e experts for all tasks are rstly acquired by single task learning. e intermediate outputs from each expert are then distilled to a similar multi-task network with an extra controller layer to coordinate di erent action sets. One clear limitation is that major components of the model are exactly the same for di erent tasks, which may lead to degraded performance on some tasks. In our work, transfer can happen even when there are no experts available. Also, our method allow each task to have their own model structures. Furthermore, even the model structures are the same for multiple tasks, the tasks are not trained to improve the performance of other tasks (i.e. it does not mimic experts from other tasks directly). erefore our model can focus on maximizing its own reward, instead of being distracted by others.
ar X
iv :1
41 1.
16 29
v2 [
cs .A
I] 1
In this position paper, I argue that standardized tests for elementary science such as SAT or Regents tests are not very good benchmarks for measuring the progress of artificial intelligence systems in understanding basic science. The primary problem is that these tests are designed to test aspects of knowledge and ability that are challenging for people; the aspects that are challenging for AI systems are very different. In particular, standardized tests do not test knowledge that is obvious for people; none of this knowledge can be assumed in AI systems. Individual standardized tests also have specific features that are not necessarily appropriate for an AI benchmark. I analyze the Physics subject SAT in some detail and the New York State Regents Science test more briefly. I also argue that the apparent advantages offered by using standardized tests are mostly either minor or illusory. The one major real advantage is that the significance is easily explained to the public; but I argue that even this is a somewhat mixed blessing. I conclude by arguing that, first, more appropriate collections of exam style problems could be assembled, and second, that there are better kinds of benchmarks than exam-style problems. In an appendix I present a collection of sample exam-style problems that test kinds of knowledge missing from the standardized tests.
It has often been proposed that a standardized tests constitute useful goals for AI systems doing automated scientific reasoning and informative benchmarks for progress. For example Brachman et al. (2005) suggest developing a program that can pass the SATs. Clark, Harrison, and Balasubramanian (2013) propose a project of passing the New York State Regents Science Test for 4th graders. Strickland (2013) proposes developing an AI that can pass the entrance exams for the University of Tokyo. Ohlsson et al. (2013) evaluated the performance ConceptNet system (Havasi, Speer, and Alonso 2007) on a preprocessed form of the Wechesler Preschool and Primary Scale of Intelligence test. Barker et al. (2004) describes the construction of a knowledge-based system that (more or less) scored a 3 (passing) on two section of the high school chemistry Advanced Placement test.
In this position paper, I want to discuss specifically the project of developing AI programs to pass standardized science tests, as a step toward developing AI programs with powerful abilities to reason about science; and I will argue that focusing narrowly on this goal is not the best way of advancing AI understanding of basic science, and that this benchmark is not the best way of measuring progress.
The dangers of focusing on too narrow and idiosyncratic a target are well illustrated by Watson. IBM set itself the goal of winning at Jeopardy, and with huge labors, in an extraordinary tour de force, succeeded. However, it is not at all clear what contribution this has made to AI technology;
and it seems to have made no contribution whatever to AI theory. Having succeeded on Jeopardy, IBM is making a huge effort to adapt it to other applications; what has been published to date on this effort is not particularly impressive.
(Since I have been involved in the Winograd Schema Challenge (Levesque, Davis, and Morgenstern 2012), let me parenthetically clarify a related point there. The Winograd Schema Challenge is intended as a challenge; eventually, if natural language abilities and the inference mechanisms they require advance sufficiently, some program will pass it. But there is no point in working on the Winograd Schema Challenge. Winograd schemas are not a subject matter; they are not even a natural category. They are a collection of coreference resolution problems, designed to be difficult, that conform to a very specific structure in order to make sure that they are not easily solved by statistical techniques over surface features. The relevant field to work on is natural language understanding generally or disambiguation specifically.)
Certainly, a standardized science test is a more meaningful and useful goal for AI than Jeopardy. The questions that make up these tests involve much more fundamental knowledge and much deeper forms of reasoning than the trivia about actors, geography, and so on that make up Jeopardy. So I would certainly expect that an effort to pass a standardized science test would be very much more fruitful for AI than Watson, except, of course, in terms of publicity. The problems cannot be solved using statistics over surface features. Solving them will almost certainly require major advances in knowledge representation, reasoning, and diagram interpretation and significant advances in natural language interpretation. Indeed Barker et al. (2004) were able to draw some interesting and fruitful conclusions for knowledge representation and knowledge engineering from their project aimed at passing the chemistry AP test. Nonetheless, I will argue that, if we focus our research too narrowly on passing standardized tests, we will miss critical fundamental issues that will come back to bite us later; and that if we make too much of these tests as benchmarks, it will not be helpful to the field in the long run. I think there are much better ways to formulate goals and benchmarks.
The Haar wavelet is the simplest possible compact wavelet which has the properties of square-like shape and discontinuity. These properties makes the Haar wavelet sensitive to a larger range of fluctuations than other mother wavelets and provides it with a lower discriminative power. Thus, the Haar wavelet enables the detection of both slow-varying fluctuations and sudden changes in a signal [16], while not particularly sensitive to small discontinuities (i.e., noise) on a signal, in effect averaging them out over the wavelet support.
Consider the example in Fig. 1, where the figure at the top represents an example hyperspectral signature, while the figures in the middle and at the bottom show the undecimated wavelet coefficient matrix of the spectrum under the Haar and Daubechies-4 wavelets, respectively. The middle figure in Fig. 1 shows the capability of Haar wavelets to capture both rapid changes and gently sloping fluctuations in the sample reflectance spectrum. Similarly, the bottom figure shows that the Daubechies-4 wavelet is sensitive to compact and drastic discontinuities (i.e., higher order fluctuations that are often due to noise). Thus, the Daubechies-4 wavelet does not provide a good match to semantic information extraction for this example reflectance spectrum. Intuitively, these issues will also be present for other higher-order wavelets, which provide good analytical matches to functions with fast, high-order fluctuations.
In general, wavelet representations of spectral absorption bands are less emphasized under Haar wavelet than under other higher order wavelets. However, this drawback can be alleviated using discretization, which will be described in the next subsection.
Approximation
The next step is to find an exact solution (P(X1), . . . ,P(XK)) ∈ P(K) which is the closest (in a suitable sense) to our approximate solution (Z1, . . . , ZK) ∈ G(K,N). The set K is closed under very special orthogonal transformations in O(K), so we can’t view K as a subset of the Grassmannian G(K,N). However, we can think of K as a subset of G(K,N) by considering the subspace spanned by (X1, . . . , XK) for every [X1 · · ·XK ] ∈ K.
Recall from Section 4.3 that every solution Z of problem (∗2) yields a family of solutions of problem (∗1); namely, all matrices of the form ZQ, where Q is a K × K matrix with nonzero and pairwise orthogonal columns. Since the solutions ZQ of (∗1) are all equivalent (they yield the same minimum for the normalized cut), it makes sense to look for a discrete solution X closest to one of these ZQ. Then, we have two choices of distances.
1. We view K as a subset of (RPN−1)K . Because K is closed under the antipodal map, as explained in Appendix B, for every j (1 ≤ j ≤ K), minimizing the distance d(P(Xj),P(Zj)) on RPN−1 is equivalent to minimizing ‖Xj − Zj‖2, where Xj and Zj are representatives of P(Xj) and P(Zj) on the unit sphere (if we use the Riemannian metric on RPN−1 induced by the Euclidean metric on RN). Then, if we use the product distance on (RPN−1)K given by
d ( (P(X1), . . . ,P(XK)), (P(Z1), . . . ,P(ZK)) ) = K∑ j=1 d(P(Xj),P(Zj)),
minimizing the distance d ( (P(X1), . . . ,P(XK)), (P(Z1), . . . ,P(ZK)) ) in (RPN−1)K is equivalent to minimizing
K∑ j=1 ∥∥Xj − Zj∥∥ 2 , subject to ∥∥Xj∥∥ 2 = ∥∥Zj∥∥ 2 (j = 1, . . . , K).
We are not aware of any optimization method to solve the above problem, which seems difficult to tackle due to constraints ‖Xj‖2 = ‖Zj‖2 (j = 1, . . . , K). Therefore, we drop these constraints and attempt to minimize
‖X − Z‖2F = K∑ j=1 ∥∥Xj − Zj∥∥2 2 ,
the Frobenius norm of X − Z. This is implicitly the choice made by Yu.
2. We view K as a subset of the Grassmannian G(K,N). In this case, we need to pick a metric on the Grassmannian, and we minimize the corresponding Riemannian distance d(X,Z). A natural choice is the metric on so(n) given by
〈X, Y 〉 = tr(X>Y ).
ar X
iv :1
61 1.
06 63
9v 1
[ cs
.C L
] 2
1 N
ov 2
01 6
Before presenting the details of the approach, we introduce some formalism for ILP.
De nition (ILP). Integer Linear Programming (ILP) is an optimization problem of the form:
Maximize: n∑ j=1 c jx j
Subject to: n∑ j=1 ai jx j <= bi
x j ≥ 0 x j ∈ Z
Intuitively, ILP a empts to maximize an objective function subject to a given set of linear constraints (in the formulation above, i ranges over the set of constraints). A provably optimal solution to ILP is known to be NP-Complete; hence, solutions must be approximated. Good so ware packages for this problem already exist; we use a solution described further in Section 4.
3.4.1 Variable Selection. At a high level, our framework encodes both candidate annotations and domain-speci c constraints as ILP variables and constraints respectively. is involves some nontrivial modeling problems such as variable selection.
We model the ILP variables x j in our framework as binary variables (x j ∈ {0, 1}). is reduces ILP to 0-1 linear programming, which is still NP-complete. Each variable is a placeholder for a candidate, with the simple semantics that a value of 1 (in an ILP solution) represents correctness with respect to its semantic type. In an ideal solution, for example, ‘Mexico’ is correct with respect to
semantic type Country but not City, for which it was also extracted by the lexicon as a candidate.
For consistency, we refer to the x j variables as token semantic type (TST) variables. Note that, using the above example, if a token such as ‘Mexico’ is marked with two semantic types, two TST variables will be created (mnemonically, Mexico-City and MexicoState). However, if a token occurs multiple times in the extracted, preprocessed text, only one variable is created for the token. In other words, the number of occurrences of a token is not taken into account in variable selection, as long as it occurs (i.e. marked as a candidate by the lexicon) at least once.
Example: e TST variables created for the running example in Section 3.1 are: Los Angeles - City, Charlo e - City, Mexico - City, the city - City, Angeles - City, Mexico - Country.
A problem with directly using TST variables as ILP variables is that, even if a particular city such as Los Angeles is set to 1, there could potentially be multiple cities in the world with the same name. Additionally, there is no way in the simple formulation to relate the city TST variables with the state and country TST variables. For canonical geotagging, such relational information is vital.
To accommodate this issue in our modeling, we introduce additional variables, denoted herein as composite TST variables, to encode the intuitive notion that a city is part of a state, and a state is part of a country. As before, we use Geonames for obtaining this relational information.
e new relational variables created are for each possible citystate, city-country and state-country pair applicable for each candidate geotag. To clarify what we mean by applicable, we take the following scenario: suppose the respective sets of candidate states and cities annotated on a webpage are S and C . To form applicable city-state composite TSTs, we take the cross-product of S and C , and eliminate all pairs that do not occur in Geonames. In this way, we make novel use of Geonames as a relational lexicon within our geotagging framework.
Once created (by verifying against Geonames), the composite TST variables are included as additional ILP variables. For each webpage, the set of (composite and non-composite) TSTs is the set of x j variables for an ILP model. Note that the relational information also leads to the introduction of new non-composite TSTs (at the state and country level) as the example below illustrated. Because we do not assume relational connections between webpages in this paper, each model is optimized independently, as described subsequently.
Example: For the running example, the composite TST variables are: Los Angeles(city) in California(state), Los Angeles(city) in Texas (state), Charlo e(city) in North Carolina(state), the city(city) in England(state), Angeles(city) in Pampanga(state), California(state) in
Represent Eq. (8) in the form of matrix. Refer p the number of input-output pairs, and Z∈Rp×n(m+1) is a matrix
where, Z is as follows
     

     


)()( 0 )(2)(2 0 )(1)(1 0
)2()2( 0 )2(2)2(2 0 )2(1)2(1 0
)1()1( 0 )1(2)1(2 0 )1(1)1(1 0
pn m pnp m pp m p
n m n mm
n m n mm
zzzzzz
zzzzzz zzzzzz
Z




(16)
where the element of matrix Z, z ji(k) (j=0, … ,m, i=1, … ,n, k=1, … ,p) is obtained by substituting the kth input data pair in Eq. (14). That is, the model can be expressed in the form of
matrix operation like Eq. (17), and the identification problem for fuzzy model results in the problem obtaining the parameter vector A∈In(m+1) ×1 in linear system expressed as Eq. (17)
y=ZA+ε (17)
where, Tpyyyy ],,,[ 21  is a set for p output data, ε = [ε1, ε2,…,εp] T is an error vector
memorizing the corresponding errors. This expresses the model error, additional noise or
uncertainty.
Parameter vector A in Eq. (17) can be obtained by minimizing the norm of ε. The minimization
of ε norm is LSE problem and it is the problem obtaining Â expressed as Eq. (18)
A 
yZA A min (18)
where, the symbol ║║ means norm.
Applying LSE to Eq. (18), Â, estimation value of A, is obtained as follows.
Â =(ZTZ)-1ZTy (19)
Here, if rank(Z)=min{p, n(m+1)} is held for the regression matrix Z, then the solution of E
q. (18) is only one and can be obtained using Eq. (19).
If rank(Z)<min{p, n(m+1)}, then the determinant of ZTZ is 0 and Â can not be obtained by using
Eq. (19). However, the type of premise membership to estimate is s-type, z-type and trapezoidal
type and it is high probable to have the same membership function for the different input values. So
rank shortage problem may be occurred. That is, the matrix Z may have the same rows or columns,
and in this case, the determinant of Z became zero. Therefore, we may not identify the parameters
using the matrix operation method with Eq. (19).
To solve this problem, we use the singular value decomposition (SVD) algorithm of regressive
matrix [26]. We overcome the rank shortage arising in the consequence parameter identification by
introducing the SVD algorithm for fuzzy model identification.
Structured regression lies at the heart of some of the most active computer vision problems of our time. Examples include optical flow (Horn & Schunck, 1981), monocular depth estimation (Eigen et al., 2014), intrinsic image decomposition (Barron & Malik, 2015) etc. Convolutional neural networks (CNNs) (Fukushima, 1980; Krizhevsky et al., 2012; LeCun et al., 1989) have greatly advanced the state of the art in all those structured output tasks (Eigen et al., 2014; Narihira et al., 2015a). However CNNs predict each output independently, and thus ignore the intricate interplay of the output variables imposed by physical or modeling constraints. Instead they are forced to learn all physical properties of a scene directly from the training data, and often fail due to the limited capacity of the model.
In this work, we propose to bring those dependencies back to deep structured regression, in the form of constraints on the output space. A constraint ties the output of several regression targets together. In a naive first approach, we learn a standard deep structured regression model and find the closest solution to the predicted structured output that follows the constraints strictly, using a simple Euclidean distance measure. This results in an averaging of the output. It makes only limited use of the training data, and further assumes that the constraints are always satisfied, which is not true in general. For instance, in the task of intrinsic images, Lambert’s law (Barrow & Tenenbaum, 1978) assumes that surfaces have diffused reflectance, which means product of shading and albedo images is equal to the original image. However, this is not true for specular surfaces like mirrors, metals etc (Zhang et al., 1999), such that a naive approach does not perform well in those areas.
To make full use of the training data, we regress not just to a single output variable, but rather a fully factorized distribution over possible outputs. We further predict a distribution over each of our constrains, allowing it not be violated under certain conditions. These distributions capture a the confidence with which the model makes its predictions, or the confidence that a certain constraint holds. A highly confident prediction is peaked around the correct answer, while an uncertain prediction will be more uniform. We use these confidences, and pick the most likely output labeling
ar X
iv :1
51 1.
07 49
7v 1
[ cs
.C V
] 2
3 N
ov 2
01 5
following our constraints. This allows the model to trust outputs differently during inference, see Figure 1 for an overview of our framework.
We apply our structured regression framework to the problem of intrinsic image decomposition (Barrow & Tenenbaum, 1978). The goal of intrinsic image decomposition is to decompose the input image into albedo (also called reflectance image) and shading images. The output space, in such tasks, has dependencies on the input which can be modeled as physics based constraints such as Lambertian lighting assumption in intrinsic image decomposition (Barrow & Tenenbaum, 1978). At inference time, we find a structured output following those constraints. This alleviates the pressure on the CNN to explicitly learn all physical properties of a scene, and allows it to focus more on the statistical correlations between the input image and the output.
In summary, our constrained regression framework learns not only to capture the ground-truth values, but also to capture the variation or confidence in its own predictions. Moreover, our constraints re-introduce the coupling between albedo and shading that has been ignored in the prior supervised learning. We achieve significant improvement over the state of the art performance and show large visual improvement on MPI Sintel dataset.
1 INTRODUCTION
One of the major perceived problems with application of the Dempster-Shafer Theory [Shafer, 76] has been its apparent computational complexity e.g., [ Kyburg, 87], [Bonissone, 87]. This is because the Dempster Shafer theory as usually implemented involves re peated application of Dempster's Rule of Combina tion, keeping a record at each stage of each subset of 8 with a non-zero mass. For example the combination of m simple support functions can have as many as 2q non-zero masses where q is the minimum of m and 181, thus making the approach computationally infeasible for large m and 181.
There have been a number of schemes to deal with this; [ Barnett, 81] showed how calculation of Dempster Shafer belief in a very special case, when all the ev idence sets are either singletons or complements of singletons, belief could be calculated in linear time. [ Gordon and Shortliffe, 85] extended this with an effi cient approximation to Dempster-Shafer belief for hier archically related evidences, and it was shown in both [ Shafer and Logan, 87] and [Wilson, 87] that the hi erarchical case could be dealt with exactly in a com putationally efficient manner. The Shafer-Logan al gorithm was generalised to propagation of belief func tions in Markov Trees [ Shafer and Shenoy, 88] but,
although this is a very important contribution, it still requires that the product space associated with the largest clique is small, a condition which will by no means always be satisfied. The hierarchical evidence algorithm in [Wilson, 87] was generalised to arbitrary evidence sets [Wilson, 89] and, because it calculates belief directly without first calculating the masses, it leads to very substantial increases in efficiency (see sec tion 4). However this algorithm appears to have com plexity worse than polynomial, which is not surprising since Dempster's Rule is #?-complete [Orponen, 90], [ P rovan, 90].
This paper describes the Monte-Carlo algorithm given in [Wilson, 89] which also calculates belief directly (or, more accurately, it approximates belief up to arbi trary accuracy). This calculation has very low com plexity, showing that the general pessimism about the complexity of Dempster-Shafer Theory is mis guided. The use of Monte-Carlo algorithms for calcu lating Dempster-Shafer belief has also been suggested in [P earl, 88], [ Kampke, 88] and [ Kreinovich and Bar rett, 90].
2 THE MONTE-CARLO ALGORITHM
Let Belt, . .. , Belm be belief functions on a finite frame 8, and let Bel= Belt Etl· · ·EtlBelm be their combination using Dempster's Rule. Using the model of [Dempster, 67] Bel; is represented by a probability function P; (on a finite set rl;) and a compatibility function f; : rl; >--> 2° where the meaning of f; is 'for r E rl;, if r is true then so is f;( r)'. The mass function m; is given by: for c; E rl;, m;(f;(c;)) = P;(ci), and, for b S::: 8,
Bel;(b) = P;(f;(c;) s;; b), that is, I: P;(c;). ,,,r,(,,)�b
Let n = nl X . . . X Slm and for c = (c), ... ,c,) define f (c) = n: 1 f;(c;). Define the 'independent
probability function' P' on f! by P'((c:1, . . . ,c:m)) n::1 P;(c:;). Using [Dempster, 67] it can be seen that
Bel(b) = P'(r(c:) <;; blf(c:) =1 0),
where e.g. P'(r(c:) # 0) just means L<:r(<);>!0 P'(c:). r(c:) can be viewed as a random set [Nguyen, 78]. The Monte-Carlo algorithm just simulates the last equation.
A large number, N, of trials are performed. For each trial:
1. Randomly pick c: such that f(c:) # 0: a. For i = 1, ... , m
randomly pick an element of f!;, i.e. pick c; with probability P;(c;)
Let c = (c:1, . . . ,em) b. If r(c:) = 0 then restart trial;
2. If f(c:) <;; b then trial succeeds, letT= 1 else trial fails, let T = 0
The proportion of trials that succeed converges to Bel( b):
E[T] = P'(r(c:) <;; bll'(c:) # 0) = Bel(b). Var[T] = E[T2]-(E[T])2 = E[T]-(E[T])2 = Bel(b)(1Bel(b)) :S: t· Let f be the average value ofT over the N trials, i.e., the proportion of trials that succeed.
E[f] = N�T) = Bel(b) and Var[t] _ NVar[TJ < ...!.._ - N2 - 4N
Therefore the variance (an<!_ so also the standard de viation) for the estimate, T, of Bel(b) can be made arbitrarily small independently of 181 and m. Let us say that the estimate t_ of Bel( b) 'has accuracy k' if 3 standard deviations ofT is less than or equal to k. Then f has accuracy k if N 2: -,&,-. Testing separately if r( c:) = 0 and if f( c:) <;; b wastes time; these tests can be combined within the same algorithm (where Xj denotes the jth element of 8):
For each trial:
repeat
pick c: with probability P'(c:) T0 := 1; T := 1 for j = 1 to 181
if f(c:) 3 Xj then T0 := 0; (since f(c:) # 0) if Xj tf_ b then T := 0; exit trial;
A Monte-Carlo Algorithm for Dempster-Shafer Belief 415
end if end if
next j until T0 = 0
(since r(c:) �b)
3 COMPUTATION TIME
Picking c; involves m random numbers so takes less than Am where A is constant, approximately the time it takes to generate a random number (with efficient storing of the P;s). Testing if r(c:) 3 Xj takes less than Bm for constant B. For a given trial there is a prob ability K = P'(r(c:) = 0) that the repeat-until loop will be entered a second time. The expected number of repeat-until loops per trial is 1��; K is a measure of the conflict of the evidences [Shafer, 76, p65].
Thus the expected time the algorithm takes is less than 1�� m(A + Bl81), and so the expected time to achieve accuracy k is less than 4(1 9�)pm(A + Bl81). At least for the case where the Bel;s are simple sup port functions, the condition f(c) 3 Xj can be tested more efficiently; under weak conditions this leads to expected time of less than 4(1 9�)k2 (Am+ Cl81) for constant C [Wilson, 89].
4 EXPERIMENTAL RESULTS
The algorithm for the case where the Bel;s are simple support functions has been implemented and tested using the language Modula-2 on a SUN 3/60 worksta tion. The results showed that the value of A is much bigger than the value of C in this implementation, A being roughly 40�0 seconds and C roughly 50 �00 sec onds. A is essentially the time taken to ge�erate a random number, and 40100 seconds seems rather slow for that. This suggests that very substantial speed ups (of perhaps an order of magnitude or two) could be achieved by careful choice and use of the random number generator and the use of antithetic runs (so that the random number generator is only used once for several different data items).
The results indicate that, unless the evidences are ex tremely conflicting, the Monte-Carlo algorithm is prac tical for problems with large m and 8. For example, with K = 0.5, m = 181 = 40, and with 1000 tri als, the calculation of the approximate value of Bel(b) would be expected to take 20.6 seconds. The 1000 trials mean that the standard deviation is less than 0.016, and so the confidence interval for the correct value of belief corresponding to 3 standard deviations would be roughly [ b- 0.05, b + 0.05] . If instead we did 10,000 trials this would take a little over 3 minutes, and give a standard deviation of 0.005. Extrapolating the figures (which seems unlikely to cause problems in
416 Wilson
this case) gives an approximate time of 1 minute for m = 101 = 120, with 1000 trials, and 5 minutes for m = 101 = 600.
Also in [Wilson, 89] an exact algorithm for calcu lating belief is described (related to those described in [P rovan, 90]) which involves expressing the event f(E) s;; b as a boolean expression and then calculat ing the probability of this using the laws of boolean algebra. Again this avoids explicit calculation of the masses. The complexity for the simple support func tion case appears to be approximately of the form l0llogm.
The usual approaches for calculating belief are mass based: they calculate the combined mass function and use this to calculate the appropriate belief (a good one of these is the fast Mobius transform in [Kennes and Smets, 90]). For large m and 0 this is of necessity very computationally expensive, since if q = min(m, 101), there can be as many as 2q masses. For simplicity it is assumed that the calculation of belief then just does 2q REAL multiplications. The speed of REAL multipli cation was tested on the same workstation and within the same language that the Exact and Monte-Carlo al gorithms were tested and implemented on and it was found that it did just over 104 REAL multiplications per second. This gives the following results:
m,n MC Exact Mass-based 2: 15 X 15 7 sees 9 sees 3 sees 20 X 20 11 sees 13 sees 1 min 25 X 25 13 sees 46 sees 1 hour 30 X 30 15 sees 3 mins 1 day 35 X 35 17 sees 8 mins 1 month 50 X 50 25 sees 2 hours 3000 years
The values for the Monte-Carlo algorithm were based on doing 1000 trials and the contradiction being 0.5. The figure of 2 hours for the Exact in the 50 case is a very rough upper bound derived from insufficient data.
Details of the experiments and the full results and analysis are given in [Wilson, 90b].
5 THE GENERALISED
ALGORITHM
The algorithm can be generalised to deal with arbi trary logics [Wilson, 90a]. Let L be the language of some logic. For each i, Bel; is now a function from L to [0, 1] saying how much the evidence warrants belief in propositions in L and the compatibility function is a function f; : ll; f-> L. The combined compatibility function r is now defined by
m f((El, ... ,Em)) = 1\ f;(E;),
i=l
(or f((El, . . . , Em)) = the set {f;(E;) : i = 1, ... , m} if the logic doesn't have conjunction).
For each trial:
1. Randomly pick E such that f(E) is not contradictory:
a. For i = 1, . . . , m randomly pick an element of ll;, i.e. pick E; with probability P;(t:;)
Let E = (<!, . . . ,Em) b. If f(c:) is contradictory then restart trial;
2. If b can be deduced from r(c:) then trial succeeds, let T = 1 else trial fails, let T = 0
Undecidability and semi-decidability would clearly cause problems, in which case trials which went on for too long would have to be cut short; if T for these trials was given the value 0 then this would lead to a lower bound for Bel(b). This technique of prematurely halting trials that take too long could be used to in crease the efficiency for other cases as well, at the cost of only finding lower and upper bounds for Bel( b).
The time this algorithm takes is then approximately ���(Am + R) where R is the average time it takes to see if f(c:) is contradictory, and if f(c:) allows b to be deduced. Given that the weight of conflict of the ev idences is bounded this means that the complexity is proportional to that of proof in the logic; it is hard to see how any sensible uncertainty calculus could do bet ter than this (although the complexity for this Monte Carlo algorithm has a very large constant term if high accuracy is required).
As Shafer points out [Shafer, 90] 101 can be a large product space, making the first algorithm impractical. The generalised algorithm can also be used to greatly improve the complexity of the algorithms for calcu lating Belief in Markov trees [Shafer and Shenoy, 88]. For each trial, propositions (i.e. belief functions with a single focal element) must be propagated through the Markov tree. The complexity is then proportional to that of propagating propositions, rather than the whole belief functions. Some other propositional cases have been dealt with in [Wilson, 89].
6 DISCUSSION
There are two obvious drawbacks with the Monte Carlo algorithm:
(i) if very high accuracy is required then the Monte Carlo algorithm will require a large number of trials (quadratic in the reciprocal of accuracy) so giving a very high constant factor to the complexity;
(ii) when the evidence is highly conflicting the Monte Carlo algorithm loses some of its efficiency. I don't see
this as a great problem since an extremely high weight of conflict would suggest, except in exceptional circum stances, that Dempster's Rule is being applied when it is not valid, e.g. updating a Bayesian prior with a Dempster-Shafer belief function [see Wilson, 91]. I also argue there that, although Dempster's Rule has strong justifications for the combination of a finite number of simple support functions, the more general case has not been convincingly justified: the Monte-Carlo algo rithm is guaranteed to give results in accordance with Dempster's Rule, but it remains to be seen if these are always sensible.
It may be important to know which relatively small sets have relatively high beliefs: the Monte-Carlo algo rithm can be easily applied to deal with this problem.
Dempster's Rule makes particular independence as sumptions, using a single probability function on 0. By modifying step 1 of the algorithms the beliefs cor responding to other probability functions on n can be calculated.
Acknowledgements
I am currently supported by the ESPRIT basic re search action DRUMS (3085). Most of the material in this paper was produced in the period Summer '87- Summer '88 when I was employed by the The Hotel and Catering Management, and Computing and Math ematical Sciences Departments of Oxford Polytechnic. Thanks also to Bills Triggs and Boatman for their help during this period, and more recently to Mike Clarke.
References
Barnett, J .A., 1981, Computational methods for a mathematical theory of evidence, in: Proceedings IJCAI-81, Vancouver, BC 868-875.
Bonissone, P. P., 1987, 'Reasoning, Plausible' in En cyclopedia of Artificial Intelligence. Shapiro, S. C. (Ed.), John Wiley and Sons, 1987.
Dempster, A. P., 67, Upper and Lower Probabilities Induced by a Multi-valued Mapping. Ann. Math. Statistics 38: 325-39.
Gordon, J. and Shortliffe, E.H., 1985, A method of managing evidential reasoning in a hierarchical hy pothesis space, Artificial Intelligence 26, 323-357.
Kampke, Thomas, 88, About Assessing and Evaluat ing Uncertain Inferences Within the Theory of Ev idence, Decision Support Systems 4 433-439.
Kennes, Robert, and Smets, Philippe, 1990, Compu tational Aspects of the Mobius transform, Uncer tainty in Artificial Intelligence, July 1990, Cam bridge, USA.
Kreinovich, V., and Barrett, W., 90, Monte-Carlo Methods Allow to Avoid Exponential Time in
A Monte-Carlo Algorithm for Dempster-Shafer Belief 417
Dempster-Shafer Formalism, Technical Report UTEP-CS-90-5, Computer Science Department, University of Texas at El Paso.
Kyburg, H.E., Jr., 87, Bayesian and Non-Bayesian Ev idential Updating, Artificial Intelligence 31 271- 293.
Nguyen, Hung T., 78, On Random Sets and Belief Functions. Journal of Mathematical Analysis and Applications 65: 531-542.
Orponen, Pekka, 90, Dempster's Rule of Combination is #?-complete, Artificial Intelligence 44 245-253.
Pearl, Judea, 88, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, lVIorgan Kaufmann Publishers Inc. 1988, Chapter 9, in par ticular 455-457.
Provan, G., 90, A Logic-based Analysis of Dempster Shafer Theory, International Journal of Approxi mate Reasoning, 4, 451-495.
Shafer, G., 76, A Mathematical Theory of Evidence, Princeton University Press, Princeton, NJ.
Shafer, G., 90, Perspectives on the Theory and Prac tice of Belief Functions, International Journal of Approximate Reasoning.
Shafer, G. and Logan, R., 1987, Implementing Demp ster's Rule for Hierarchical Evidence, Artificial In telligence 33 271-298.
Shafer, G. and Shenoy, P. P. 1988, Local Computations in Hypertrees, Working Paper No. 201, School of Business, The University of Kansas, Lawrence, KS, 66045, USA.
Wilson, N., 87, On Increasing the Computational Ef ficiency of the Dempster-Shafer theory, Research Report no. 11, Sept. 1987, Dept. of Computing and Mathematical Sciences, Oxford Polytechnic.
Wilson, Nic, 89, Justification, Computational Effi ciency and Generalisation of the Dempster-Shafer Theory, Research Report no. 15, June 1989, Dept. of Computing and Mathematical Sciences, Oxford Polytechnic., to appear in Artificial Intelligence.
Wilson, Nic, 90a, Rules, Belief Functions and Default Logic, in Bonissone, P., and Henrion, 1\1., 90, eds Proc. 6th Conference on Uncertainty in Artificial Intelligence, MIT, Cambridge, Mass.
Wilson, Nic, 90b, Implementation and Practical Anal ysis, DRUMS (ESPRIT Basic Research Action 3085) RP3.2, 12 month report, September 1990.
Wilson, Nic, 91, The Combination of Belief: When and How Fast, A Reply to Glenn Shafer's paper Perspectives on the Theory and Practice of Belief Functions, to appear in International Journal of Approximate Reasoning.
As noted in the “Preliminaries” section, for each activity a, there may exist multiple choices of resource chains to which it can be assigned. In addition, different chaining heuristics will lead to POSes that can have different robust makespan values. In this section, we propose a new chaining heuristic that dispatches activities to resource chains by predicting the improvement in robust makespan of the generated POS.
Algorithm 2 Robustness-Feedback Resource Chaining (Activity a, Schedule S, Order G) 1: C ← Find set of available chains, C for activity a based on S 2: P ← Collect chains from C with last activity of chain preceding a in problem 3: O ← Collect chains from C with last activity of chain ordered before a in G 4: if P 6= φ then 5: k ← Get first available chain in P 6: else if O 6= φ then 7: k ← Get first available chain in O 8: else 9: k ← Get first available chain in C 10: end if 11: Post constraint between last activity of chain k (denoted as last(k)) and activity a 12: if a requires more than one resource unit then 13: C1 ← chains in C which have last activity as last(k) 14: C2 ← C \ C1 15: for all resource units required by a do 16: choose the first available chain belonging to C1 17: if chain above is not feasible then 18: choose the first available chain belonging to C2 19: end if 20: end for 21: end if
In the latest chaining method which aims to increase flexibility as described in Section 2.2.4, the chains are first randomly picked from a superior subset (i.e., chains where the last activity is already ordered, or chains sharing the same last element). Since our objective is makespan-related and time becomes a concern, we build on the work of Policella et al. (2009) and pick the first available chain wherever available. The updated chaining method is called Robustness-Feedback based Resource Chaining.
Example 5. Figure 5 provides the POS provided by this chaining heuristic when used on Example 1. As can be seen, compared to the POS in 4, the key difference is the allocation of activity 5 and 6. With our new heuristic, it can be seen that there is more parallelism and hence reduced robust makespan with high probability.
When employing the Ordering Generation algorithm in conjunction with the chaining heuristic, we also consider the information about ordered pairs when allocating resource units to an activity. The motivation is that once activity a and activity b (for example, a → b) is ordered, there is a high probability that this precedence relationship can result in a better solution. Algorithm 2 provides the pseudo code for the Robustness-Feedback Resource Chaining heuristic with Ordering.
In order to understand the privacy loss in Apple’s implementation of differential privacy, we need to understand the following aspects of the system:
(1) What are the privacy parameters used in order to achieve privatization before the privatized datum gets entered into the database? This will let us understand per datum privacy.
2To reliably trigger DP application to emojis, the user needs to call out the emoji keyboard by pressing "ctrl-cmd space", then click an emoji (or select an emoji with the arrow key and press "Enter"). For new words, the user can type an incorrectly spelled word in the Notes app and then ignore the spelling suggestion by pressing ’esc’.
(2) How frequently are records selected for inclusion in a report? How many records can be included in one report? How frequently are the reports created and submitted? This will let us understand the rate of privacy loss. (3) Is the total privacy loss that a particular user can incur limited? (4) How easy is it to alter the performance of the system, e.g., change parameters responsible for each datum’s privatization, change the number of records selected for inclusion into a report or the frequency of report generation?
We discovered that the answers to questions (1) – (3) (see Section 4) depend on the parameters specified in the configuration files and their use by the framework to establish the available privacy budget. We describe our findings and observations regarding the database tables, configuration files, and the functionality related to report generation and privacy budget maintenance next. We will discuss (4) in Section 5.2.
We improve the upper bound for Λ by improving the a bound for attractive edges to derive ã, a better upper bound on −Hij . Essentially, a more careful analysis allows a potentially small term in the numerator and denominator to be canceled before bounding. Writing η̄ = mini∈V ηi(1−ηi), i.e. the closest that any dimension can come to 0 or 1, the result is that
−Hij ≤ ( αij 1 + αij )
/
η̄
(
1− ( αij 1 + αij
)2 )
(17)
= O(eW (1+∆/2)+T ).
Thus, ã = O(eW (1+∆/2)+T ) which compares favorably to the earlier bound in (Weller & Jebara, 2013a) , where a = O(eW (1+∆)+2T ). Recall b = O(∆eW (1+∆/2)+T ) and Ω = max(a, b), so using the new ã bound, now Ω = O(∆eW (1+∆/2)+T ). Details and derivation are in the supplement.
MLM: the first baseline is a monolingual image description model, i.e. multimodal language model with no source language features. The visual input consists of the CNN image features.
SOURCE-LM →TARGET-LM: the second baseline is a sequence-to-sequence Neural Machine Translation model, trained on only source and target descriptions without visual features. The final hidden state of the SOURCE-LM, after it has generated the source sentence, is input to the TARGET-LM, as described in Section 2.
1 https://github.com/tylin/coco-caption

In Table 1, we compare different decoding strategies in various settings of beam sizes. The BLEU scores are reported following a standard postprocessing procedure4.
We found that increasing the beam size to a large number does not consequently contribute to the evaluation scores. On the contrary, MBR reranking improves the scores when a large beam size is used, but less effective with a small beam size. Later-stage MBR decoding is shown to outperform the simple MBR reranking in all settings of beam size.
Additionally, we also found that the number of candidates in the evidence space largely affects the effectiveness of MBR reranking. In our experiments, the number of evidences is fixed to the same number of beam size. Using more evidences degrades the quality of selected candidates.
Word segmentation and Part-of-Speech (POS) tagging are two fundamental tasks for Chinese language processing. In recent years, word segmentation and POS tagging have undergone great development. The popular method is to regard these two tasks as sequence labeling problem [7, 5], which can be handled with supervised learning algorithms such as Maximum Entropy (ME) [1], averaged perceptron [2], Conditional Random Fields (CRF)[3]. After years of intensive researches, Chinese word segmentation and POS tagging achieve a quite high precision. However, their performance is not so satisfying for the practical demands to analyze Chinese texts, especially for informal texts. The key reason is that most of annotated corpora are
ar X
iv :1
50 5.
07 59
9v 3
[ cs
.C L
drawn from news texts. Therefore, the system trained on these corpora cannot work well with the out-of-domain texts.
In this shared task, we focus to evaluate the performances of word segmentation and POS tagging on relatively informal micro-texts.
Our library for the rapid cleaning, manipulation, summarization, and querying of n-gram data is available at github.com/smeylan/ngrawk. Jupyter notebooks for the analyses presented here are available at github.com/smeylan/pic-analysis.
Initial work on multimodal translation has focused on approaches that use either semantic or spatially-preserving image features as inputs to a translation model. Semantic image features are typically extracted from the final pooling layer of a pre-trained object recognition CNN, e.g. ‘pool5/7x7 s1’ in GoogLeNet (Szegedy et al., 2015). This type of feature vector has been used conditioning input to the encoder (Elliott et al., 2015; Huang et al., 2016), in the decoder (Libovický et al., 2016), or as additional features in a
5We used the pre-trained models from Keras (Chollet, 2015) downloaded from https://github.com/ fchollet/deep-learning-models, which claim equal ILSVRC object recognition performance for both models: 7.8% top-5 error with a single-model and single-crop.
phrase-based translation model (Shah et al., 2016; Hitschler et al., 2016). Spatially-preserving image features are extracted from deeper inside a CNN, where the position of a feature in the tensor is related to its position in the image. These features have been used in “double-attention models”, which calculate independent context vectors of the source language hidden states and a convolutional image feature map (Calixto et al., 2016; Caglayan et al., 2016; Calixto et al., 2017a). Similar to most of these approaches, we use an attention-based translation model, but our multitask model does not use images for translation.
More related to our work are the recent papers by Toyama et al. (2016), Saha et al. (2016), and Nakayama and Nishida (2016). Toyama et al. (2016) extend the Variational Neural Machine Translation model (Zhang et al., 2016) by inferring latent variables to explicitly model the semantics of source sentences from both image and linguistic information. Similar to our approach, their model does not condition the translation model on images for translation. They report improvements on the Multi30K data set when using multimodal information, however, their model adds additional parameters in the form of “neural inferrer” modules. In our multitask model, the grounded semantics are represented implicitly in the hidden states of the shared encoder. Furthermore, Toyama et al. (2016) assumes Source-Target-Image tuples as training data; whereas our multitask framework achieves equally good results if we train on separate Source-Image and Source-Target datasets.
Saha et al. (2016) study the problem of crosslingual image description where the task is to generate sentence in language L1 given the image, given only Image-L2 and L1-L2 parallel corpora. They propose a Correlational Encoder-Decoder Network to model the Image-L2 and L1-L2 parallel text data. Their model learns correlated representations for paired Image-L2 data and decodes L1 from this joint representation. Similarly to our setup the encoder is trained by minimizing two loss functions: the Image-L2 correlation loss, and the L1 decoding cross-entropy loss.
Nakayama and Nishida (2016) consider a zeroresource problem where the task is to translate from L1 to L2 but only Image-L1 and ImageL2 corpora are available. Their best performing model embeds the image, L1, and L2 in a joint multimodal space learned through minimizing a
multi-task ranking loss between both pairs of examples. The main difference between this approach and our models is that in our experiments we focus on enriching source language representations with visual information, rather than addressing the zero-resource issue.
Multitask Learning improves the generalisability of a model by requiring it to be useful for more than one task (Caruana, 1997). This approach has seen a surge of attention and has recently been used to improve the performance of sentence compression using eye gaze as an auxiliary task (Klerke et al., 2016), and to improve shallow parsing accuracy through the auxiliary task of predicting keystrokes in an out-of-domain corpus (Plank, 2016). These works hypothesise a relationship between specific biometric measurements and specific NLP tasks motivated by cognitive-linguistic theories. More recently, Bingel and Søgaard (2017) analysed the beneficial relationships between primary and auxiliary sequential prediction tasks. In our multitask framework we explore the benefits of the general notion of grounded learning in the specific case of multimodal translation. In the translation literature, multitask learning has been used to learn a oneto-many languages translation model (Dong et al., 2015), and as a subtask of multitask sequence-tosequence learning (Luong et al., 2016).
FS is a dimensionality reduction technique that tries to remove irrelevant and redundant features from the original data. Its goal is to obtain a subset of features that describes properly the given problem with a minimum degradation of performance, in order to obtain simpler and more accurate schemes [17].
Formally, we can define feature selection as: let ei be an instance ei = (ei1, . . . , ein, eiy), where eir corresponds to the r-th feature value of the i-th sample and eiy with the value of the output class Y . Let us assume a training set D withm examples, which instances ei are formed by a setX of n characteristics or features, and a test set Dt exist. Then let us define Sθ ⊆ X as a subset of selected features yielded by an FS algorithm.
FS methods can be broadly categorized as [4]:
1. Wrapper methods, which use an evaluation function dependent on a learning algorithm [21]. They are aimed at optimizing a predictor as part of the learning process.
2. Filtering methods, which use other selection techniques as separability measures or statistical dependences. They only consider the general characteristics of the dataset, being independent of any predictor [16].
3. Embedded methods, which use a search procedure which is implicit in the classifier/regressor [30].
Filter methods usually result in a better generalization due to its learning independence. Nevertheless, they usually select larger feature subsets, requiring sometimes a threshold to control them. Regarding complexity, filters are normally less expensive than wrappers. In those cases in which the number of features is large (especially for Big Data), it is indispensable to employ filtering methods as they are much faster than the other approaches.
Standard deontic logic (SDL) is obtained from the well-known modal logic K by adding the seriality axiom D: P → ♦P . In this logic, the -operator is
⋆ This is a short version of [7]. It is supported by the DFG grants FU 263/15-1 and STO 421/5-1 ’Ratiolog’.
2 interpreted as ‘it is obligatory that’ and the ♦ as ‘it is permitted that’. The ♦-operator can be defined by ♦P ≡ ¬ ¬P . The seriality axiom in SDL states that, if a formula has to hold in all reachable worlds, then there exists such a world. With the deontic reading of and ♦ this means: Whenever the formula P ought to be, then there exists a world where it holds. In consequence, there is always a world, which is ideal in the sense that all the norms formulated by ‘the ought to be’-operator hold.
Deontic logic is the logic of choice when formalizing knowledge about norms like the representation of legal knowledge or ethical codes for agents. However, there are only few automated theorem provers specially dedicated for deontic logic and used by deontic logicians (see [1,2]). Nonetheless, numerous approaches to translate modal logics into (decidable fragments of) first-order predicate logics are stated in the literature. A nice overview including many relevant references is given in [12].
In this paper, we use the first order predicate logic theorem prover Hyper [14] to handle SDL knowledge bases. By using the well-known translation of modal logic into description logic [11] (called ϕ in the sequel) and the fact that Hyper offers a decision procedure for the description logic SHIQ [4], we are able to process SDL efficiently. In the following, SDL is translated into ALC, which is a subset of SHIQ.
Transformation from SDL into ALC
For a normative system consisting of the set of deontic logic formulae N = {F1, . . . , Fn}, the translation ϕ is defined as the conjunctive combination of the translation of all deontic logic formulae in N :
ϕ(N ) = ϕ(F1) ⊓ . . . ⊓ ϕ(Fn)
Note that ϕ(N ) does not yet contain the translation of the seriality axiom. As shown in [9], the seriality axiom can be translated into the TBox
T = {⊤ ⊑ ∃r.⊤}
with r the atomic role introduced by the mapping ϕ. For our application, the result of the translation of a normative systemN and the seriality axiom is an ALC knowledge base Φ(N ) = (T ,A), where the TBox T consists of the translation of the seriality axiom and the ABox A = {(ϕ(N ))(a)} for a new individual a. In description logics, performing a satisfiability test of a concept C w.r.t. a TBox is usually done by adding a new individual a together with the ABox assertion C(a). For the sake of simplicity, we do this construction already during the transformation of Φ by adding (ϕ(N ))(a) to the ABox.
An advantage of the translation of deontic logic formulae into an ALC knowledge base is the existence of a TBox in ALC. This makes it possible to add further axioms to the TBox. For example we can add certain norms that we want to be satisfied in all reachable worlds into the TBox.
3
Consider the problem of mapping an input sentence x to a parse tree y. Define Y to be the set of all parse trees for x. The parsing problem is to find
y∗ = argmax y∈Y h(y) (8)
where h(y) is the score for any parse tree y ∈ Y . We consider the case where h(y) is the sum of two model scores: first, the score for y under a weighted context-free grammar; and second, the score for the part-of-speech (POS) sequence in y under a finite-state part-of-speech tagging model. More formally, we define h(y) to be
h(y) = f(y) + g(l(y)) (9)
where the functions f , g, and l are defined as follows:
1. f(y) is the score for y under a weighted context-free grammar (WCFG). A WCFG consists of a context-free grammar with a set of rules G, and a scoring function θ : G→ R that assigns a real-valued score to each rule in G. The score for an entire parse tree is the sum of scores for the rules it contains. As an example, consider the parse tree shown in Figure 1; for this tree,
f(y) = θ(S→ NP VP) + θ(NP→ N) + θ(N→ United) +θ(VP→ V NP) + . . .
We remain agnostic as to how the scores for individual context-free rules are defined. As one example, in a probabilistic context-free grammar, we would define θ(α → β) = log p(α → β|α). As a second example, in a conditional random field (CRF) (Lafferty, McCallum, & Pereira, 2001) we would define θ(α → β) = w · φ(α → β) where w ∈ Rq is a parameter vector, and φ(α→ β) ∈ Rq is a feature vector representing the rule α→ β.
2. l(y) is a function that maps a parse tree y to the sequence of part-of-speech tags in y. For the parse tree in Figure 1, l(y) would be the sequence N V D A N.
3. g(z) is the score for the part-of-speech tag sequence z under anm’th-order finite-state tagging model. Under this model, if zi for i = 1 . . . n is the i’th tag in z, then
g(z) = n∑ i=1 θ(i, zi−m, zi−m+1, . . . , zi)
where θ(i, zi−m, zi−m+1, . . . , zi) is the score for the sub-sequence of tags zi−m, zi−m+1, . . . , zi ending at position i in the sentence.5
We again remain agnostic as to how these θ terms are defined. As one example, g(z) might be the log-probability for z under a hidden Markov model, in which case
θ(i, zi−m . . . zi) = log p(zi|zi−m . . . zi−1) + log p(xi|zi)
5. We define zi for i ≤ 0 to be a special “start” POS symbol.
where xi is the i’th word in the input sentence. As another example, under a CRF we would have
θ(i, zi−m . . . zi) = w · φ(x, i, zi−m . . . zi)
where w ∈ Rq is a parameter vector, and φ(x, i, zi−m . . . zi) is a feature-vector representation of the sub-sequence of tags zi−m . . . zi ending at position i in the sentence x.
The motivation for this problem is as follows. The scoring function h(y) = f(y) + g(l(y)) combines information from both the parsing model and the tagging model. The two models capture fundamentally different types of information: in particular, the part-of-speech tagger captures information about adjacent POS tags that will be missing under f(y). This information may improve both parsing and tagging performance, in comparison to using f(y) alone.6
Under this definition of h(y), the conventional approach to finding y∗ in Eq. 8 is to construct a new context-free grammar that introduces sensitivity to surface bigrams (Bar-Hillel et al., 1964). Roughly speaking, in this approach (assuming a first-order tagging model) rules such as
VP→ V NP
are replaced with rules such as VPN,N → VN,V NPV,N (10)
where each non-terminal (e.g., NP) is replaced with a non-terminal that tracks the preceding and last POS tag relative to that non-terminal. For example, NPV,N represents a NP that dominates a sub-tree whose preceding POS tag was V, and whose last POS tag is N. The weights on the new rules are just context-free weights from f(y). Furthermore, rules such as
V→ flies
are replaced with rules such as VN,V → flies
The weights on these rules are the context-free weights from f(y) plus the bigram tag weights from g(z), in this example for the bigram N V. A dynamic programming parsing algorithm—for example the CKY algorithm—can then be used to find the highest scoring structure under the new grammar.
This approach is guaranteed to give an exact solution to the problem in Eq. 8; however it is often very inefficient. We have greatly increased the size of the grammar by introducing the refined non-terminals, and this leads to significantly slower parsing performance. As one example, consider the case where the underlying grammar is a CFG in Chomsky-normal form, with G non-terminals, and where we use a 2nd order (trigram) tagging model, with T possible part-of-speech tags. Define n to be the length of the input sentence. Parsing with the grammar alone would take O(G3n3) time, for example using the CKY algorithm. In contrast, the construction of Bar-Hillel et al. (1964)
6. We have assumed that it is sensible, in a theoretical and/or empirical sense, to take a sum of the scores f(y) and g(l(y)). This might be the case, for example, if f(y) and g(z) are defined through structured prediction models (e.g., conditional random fields), and their parameters are estimated jointly using discriminative methods. If f(y) and g(z) are log probabilities under a PCFG and HMM respectively, then from a strict probabilistic sense it does not make sense to combine their scores in this way: however in practice this may work well; for example, this type of log-linear combination of probabilistic models is widely used in approaches for statistical machine translation.
results in an algorithm with a run time of O(G3T 6n3).7 The addition of the tagging model leads to a multiplicative factor of T 6 in the runtime of the parser, which is a very significant decrease in efficiency (it is not uncommon for T to take values of say 5 or 50, giving values for T 6 larger than 15, 000 or 15 million). In contrast, the dual decomposition algorithm which we describe next takes O(k(G3n3 + T 3n)) time for this problem, where k is the number of iterations required for convergence; in experiments, k is often a small number. This is a very significant improvement in runtime over the Bar-Hillel et al. method.
One method of approximation is to take advantage of these dimensions by ignoring some of them — those that are irrelevant or only marginally relevant — in order to obtain an approximate solution. It is uniform in the sense that the same dimensions are ignored throughout the state space. Since this approach attacks the curse of dimensionality where it originates, at the dimensions, it should be very effective at counteracting it.
Dearden and Boutilier use this to obtain an exact solution (Boutilier, 1997) or an approximate one (Boutilier & Dearden, 1996; Dearden & Boutilier, 1997). However, their abstractions are fixed throughout execution, and dimensions are also deleted from the problem in a pre-determined sequence. This makes their approach somewhat inflexible. Similarly, Nicholson and Kaelbling (1994) propose this technique for approximate planning. They delete dimensions from the problem based on sensitivity analysis, and refine their abstraction as execution time permits, but it is still uniform. Dietterich (2000) uses this kind of abstraction in combination with hierarchical planning to good effect: a subtask, such as ‘Navigate to location t’, can ignore irrelevant dimensions, such as location of items to be
picked up or even the ultimate destination of the agent. Generally, any time the problem description is derived from a more general source rather than specified for the particular problem, uniform abstraction will help. Gardiol and Kaelbling (2008) use it where some dimensions are relevant, but only marginally, so that ignoring them results in an approximate solution that can be improved as planning progresses.
Unfortunately, however, at least with human-specified problems, one would generally expect all or most mentioned dimensions to be in some way relevant. Irrelevant dimensions will be eliminated by the human designer in the natural course of specifying the problem. Depending on the domain and the situation some marginally relevant dimensions might be included, but often, these will not be nearly enough for effective approximation.
We do not list comparisons against uniform abstraction in our results for this reason — in most of our sample domains, it makes little sense. All or almost all of the dimensions are important to solving the problem. Where this is not the case and methods exist for effective uniform abstraction, they can be integrated with our approach easily.
With thousands of software published online, it is essential for users to find the software that matches their stated or implied needs. Users often seek better software quality. Garvin [1] identified five views/approaches of quality. The nearest definition in this paper is the user based approach definition “meeting customer needs”. If the customer is satisfied, then product or service has good quality. It has been implemented in mobile-based applications[2]–[4] and Web applications[5]–[7].
Software quality can be conceptualized from three
dimensions; the quality characteristics, the quality model, and software quality requirements. A Quality characteristic is ”category of software quality attributes that bears on software quality” [8, p. 9]. Quality requirements are what the user needs in the software such as performance, user interface or security requirements. The quality model is how quality characteristics are related to each other and to the final product quality. Measuring the software quality will check if user requirements are met and decide the degree of quality.
The ISO/IEC 25010:2010 standard (ISO 25010
hereafter), a part of a series known as the Software Quality Requirements and Evaluation (SQuaRE), defines systems’ quality as “the degree to which the system satisfies the stated and implied needs of its various stakeholders, and thus provides value” [9, p. 8]. The ISO 25010 has two major dimensions: Quality-in-use (QinU) and Product Quality. The former specifies characteristics related to the human interaction with the system and the latter specifies characteristics intrinsic to the product. QinU is defined as “capability of a software product to influence users' effectiveness, productivity, safety and satisfaction to satisfy their actual needs when using the software product to achieve their goals in a specified context of use” [8, p. 17].The QinU model consists of five characteristics:
effectiveness, efficiency, satisfaction, freedom from risk and context coverage. Table 1 illustrates the definition of these characteristics.
International conference on Signal Processing, Communication, Power and Embedded System (SCOPES)-2016
An Improved Approach for Prediction of Parkinson’s Disease using Machine Learning
Techniques Kamal Nayan Reddy Challa
School of Electrical Sciences Computer Science and Engineering
Indian Institute of Technology Bhubaneswar, India 751013
Email: kc11@iitbbs.ac.in
Venkata Sasank Pagolu School of Electrical Sciences
Computer Science and Engineering Indian Institute of Technology
Bhubaneswar, India 751013 Email: vp12@iitbbs.ac.in
Ganapati Panda School of Electrical Sciences Indian Institute of Technology Bhubaneswar, India 751013 Email: gpanda@iitbbs.ac.in
Babita Majhi Department of Computer Science and IT
G.G Vishwavidyalaya, Central University Bilaspur, India 495009
Email: babita.majhi@gmail.com
Abstract—Parkinson’s disease (PD) is one of the major public health problems in the world. It is a well-known fact that around one million people suffer from Parkinson’s disease in the United States whereas the number of people suffering from Parkinson’s disease worldwide is around 5 millions. Thus, it is important to predict Parkinson’s disease in early stages so that early plan for the necessary treatment can be made. People are mostly familiar with the motor symptoms of Parkinson’s disease, however an increasing amount of research is being done to predict the Parkinson’s disease from non-motor symptoms that precede the motor ones. If early and reliable prediction is possible then a patient can get a proper treatment at the right time. Nonmotor symptoms considered are Rapid Eye Movement (REM) sleep Behaviour Disorder (RBD) and olfactory loss. Developing machine learning models that can help us in predicting the disease can play a vital role in early prediction. In this paper we extend a work which used the non-motor features such as RBD and olfactory loss. Along with this the extended work also uses important biomarkers. In this paper we try to model this classifier using different machine learning models that have not been used before. We developed automated diagnostic models using Multilayer Perceptron, BayesNet, Random Forest and Boosted Logistic Regression. It has been observed that Boosted Logistic Regression provides the best performance with an impressive accuracy of 97.159 % and the area under the ROC curve was 98.9%. Thus, it is concluded that this models can be used for early prediction of Parkinson’s disease.
Keywords—Improved Accuracy, Prediction of Parkinson’s Disease, Non Motor Features, Biomarkers, Machine Learning Techniques, Boosted Logistic Regression, BayesNet, Multilayer Perceptron,
I. INTRODUCTION
Parkinson’s disease (PD) is a chronic, degenerative neurological disorder. The main cause of Parkinson’s disease is actually unknown. However, it has been researched that the combination of environmental and genetic factors play an important role in causing PD [1]. For general understanding
the Parkinson’s disease is treated as disorder of the central nervous system which is the result of loss of cells from various parts of the brain. These cells also include substantia nigra cells that produce dopamine. Dopamine plays a vital role in the coordination of movement. It acts as a chemical messenger for transmitting signals within the brain. Due to the loss of these cells, patients suffer from movement disorder.
The symptoms of PD can be classified into two types i.e. non-motor and motor symptoms. Many people are aware of the motor symptoms as they can be visually perceived by human beings. These symptoms are also called as cardinal symptoms, these include resting tremor, slowness of movement (bradykinesia), postural instability (balance problems) and rigidity [2]. It is now established that there exists a timespan in which the non-motor symptoms can be observed. This symptoms are called as dopamine-non-responsive symptoms. These symptoms include cognitive impairment, sleep difficulties, loss of sense of smell, constipation, speech and swallowing problems, unexplained pains, drooling, constipation and low blood pressure when standing. It must be noted that none of these non-motor symptoms are decisive, however when these features are used along with other biomarkers from Cerebrospinal Fluid measurement (CSF) and dopamine transporter imaging, they may help us to predict the PD.
In this paper we extend works by Prashant et al [3]. This work takes into consideration the non-motor symptoms and the biomarkers such as cerebrospinal fluid measurements and dopamine transporter imaging. In this paper we follow a similar approach, however we try to use different machine learning algorithms that can help in improving the performance of model and also play a vital role in making in early prediction of PD which in turn will help us to initiate neuroprotective therapies at the right time.
1
ar X
iv :1
61 0.
08 25
0v 1
[ cs
.L G
] 2
6 O
ct 2
01 6
The rest of the paper is organized as follows. Section 2 contains the related work. Section 3 contains the flowchart of the analysis carried out and describes about the PPMI database, explanation of different features extracted, statistical analysis of this features, classification and prediction/prognostic model design. Section 4 provides the results and discussion from the experiments carried out. And finally conclusion of the work is provided in Section 5.
The data set we used is from Yale Face Database. We choose 136 images to analysis. When we run our algorithm we need to divide it into two phases. First, we need to train our algorithm. The purpose of this phase is to determine the minimum error which will be used in the next phase. So we must ensure that the algorithm can converge at a certain point. During the training process, the error will be reduced until it becomes a constant. The constant will be used as a threshold in the next step. Figure 1 shows the error will not change after repeating 4 times. So the best error is 4. The horizontal axis represents the number of iterations, and the vertical axis represents error. Seconds, we can use the constant obtained from the first step as the threshold to judge whether the algorithm can stop. Table I shows when the training process is successful the time consumed by the algorithm. In the table, “yes” represents the algorithm is succeed, in contrast “no” represents the algorithm is failed. We can see that the average time used by the algorithm is 12374.3 milliseconds. The reason why the algorithm is failed is it fell into the local optimum.

Inference of acoustic frames takes 60–70% of total computations in our LSTM-RNNbased SPSS implementation. Therefore, it is desirable to reduce the amount of computations at the inference stage. In typical ANN-based SPSS, input linguistic features other than state- and frame-position features are constant within a phoneme [2]. Furthermore, speech is a rather stationary process at 5-ms frame shift and target acoustic frames change slowly across frames. Based on these characteristics of inputs and targets this paper explores the multi-frame inference approach [31]. Figure 2 illustrates the concept of multi-frame inference. Instead of predicting one acoustic frame, multiple acoustic frames are jointly predicted at the same time instance. This architecture allows significant reduction in computation while maintaining the streaming capability.
However, preliminary experiments showed degradation due to mismatch between training and synthesis; alignments between input/target features can be different at the synthesis stage, e.g., training: x2 → {y1,y2}, synthesis: x3 → {y2,y3}. This issue can be addressed by data augmentation. Figure 3 shows the data augmentation with different frame offset. From aligned input/target pairs, multiple data sequences can be generated with different starting frame offset. By using these data sequences for training, acoustic LSTM-RNNs will generalize to different possible alignments between inputs and targets.
TIMINF
Motivated by the insight of our analysis, we show how count models can be regularized in a principled manner and provide closed-form solutions for L1 and L2 regularization. Finally, we propose a new embedding model with a convex objective and the additional benefit of being intelligible.
In stance classification one is concerned with determining the attitude of the author of a text towards a target (Mohammad et al., 2016). Targets can range from abstract ideas, to concrete entities and events. Stance classification is an active research area that has been studied in different domains (Ranade et al., 2013; Chuang and Hsieh, 2015). Here we focus on stance classification of tweets towards the truthfulness of rumours circulating in Twitter conversations in the context of breaking news. Each conversation is defined by a tweet that initiates the conversation and a set of nested replies to it that form a conversation thread. The goal is to classify each of the tweets in the
conversation thread as either supporting, denying, querying or commenting (SDQC) on the rumour initiated by the source tweet. Being able to detect stance automatically is very useful in the context of events provoking public resonance and associated rumours, as a first step towards verification of early reports (Zhao et al., 2015). For instance, t has been shown that rumours that are later proven to be false tend to spark significantly larger numbers of denying tweets than rumours that are later confirmed to be true (Mendoza et al., 2010; Procter et al., 2013; Derczynski et al., 2014; Zubiaga et al., 2016b).
Here we focus on exploiting the conversational structure of social media threads for stance classification and introduce a novel LSTM-based approach to harness conversations.
The running example demonstrates that rMCSs can jointly integrate different formalisms and deal with a dynamic environment. In this section, we want to focus on aspects relevant to the knowledge engineer, namely how to model a certain scenario using an rMCS. To this end, we elaborate on different generic modeling techniques for rMCSs that we consider helpful in typical target applications. For concrete implementations, these techniques can still be refined and tailored towards the specific needs of the problem domain at hand. First, in Section 3.1, we discuss bridge rules as a device for translating stream data to a knowledge base language. When to use the next operator in the head of bridge rules is addressed in Section 3.2. In Section 3.3, we discuss how we deal with time on the object level, including the use of timestamps as well as external and logical clocks. Then, in Section 3.4, another technique is presented that allows for managing conflicting data in input streams on the modeling level, e.g., contradictory sensor measurements. An important issue for systems that are continuously online is when to keep data and when to remove it. In Section 3.5, we discuss how to do this and provide techniques for dynamically adjusting what information is kept in the system. These techniques allow us, e.g., to modify the size of sliding windows for stream data depending on the current situation. While the computation of acceptable belief sets may be easy for some contexts, it might be expensive for others that have to perform complex reasoning. In practice, it will therefore be wise to only evaluate these contexts if necessary. In Section 3.6, we describe a way to control the computation effort by modeling idleness of contexts.
3.1. Incorporating Stream Data
Bridge rules are responsible for providing a context with information from streams and other contexts. As we deal with heterogeneous context languages and since input languages may differ from context languages, one important aspect of rMCSs (and earlier types of MCSs) is that bridge rules can be seen as a translation device between the different languages: bridge rule bodies use the languages of the source contexts or streams whereas a bridge rule head is an operation that produces a knowledge base in the target language (via the management function). But bridge rules do not necessarily pass information the way it is used in the body. Rather, we can model bridge rules such that we they focus on information relevant to the target context and, e.g., translate sensor data into a form that is convenient for reasoning. Consider the bridge rule schemas
setTemp(cold)← ex3::tmp(T ),T ≤ 45. setTemp(hot)← ex3::tmp(T ), 45 < T .
from Example 6. In this case, the bridge rules only communicate whether the stove is hot or cold, abstracting away the exact temperature value coming from the sensor. This is in contrast to rule schema (3) discussed in the previous section where parameter X appears in the rule head whereby a concrete temperature value is added to the knowledge base.
3.2. Using the next operator For gaining a better understanding of when to use the next in the head of a bridge rule, reconsider the example of turning a switch on and off as in Examples 2 and 3. Remember that the bridge rules
next(setPower(on))← ex3::switch,not st:pw. next(setPower(off ))← ex3::switch, st:pw.
were used so that an occurrence of an input atom switch causes the knowledge base to contain an inverted power state at the next time point. In order to highlight the difference, assume we would instead use the bridge rules
setPower(on)← ex3::switch,not st:pw. setPower(off )← ex3::switch, st:pw.
without next. We refer to the version of Mex3 from Example 3 with these modified bridge rules by M ′ex3. Consider the configuration KB = 〈kbst〉 of knowledge bases with kbst = ∅ and the input I = 〈{switch}〉 as in Example 4. Indeed, M ′ex3 has no equilibrium given KB and I: If we take belief state B = 〈∅〉 as in Example 4, we have appnowst (I,B) = {setPower(on)} (instead of ∅ as in Example 4). Consequently, following the definition of mngst in Example 2, we get mngst(app now st (I,B), kbst) = {pw(on)}. But then B is not contained in accst(mngst(appnowst (I,B), kbst)) = {{pw(on)}}, thus, B is not an equilibrium of M ′ex3 given KB and I. This is in line with the intuition that without next, turning the switch should affect the current equilibrium. However, B′ = 〈{pw(on)}〉 is also not an equilibrium of M ′ex3. Since appnowst (I,B
′) = {setPower(off )}, we get mngst(appnowst (I,B′), kbst) = ∅. But then accst(mngst(app now st (I,B
′), kbst)) = {∅} does not contain B′, consequently also B′ is not an equilibrium of M ′ex3 given KB and I. The two bridge rules without next prevent stability of a belief state required for an equilibrium: believing that the power is on (respectively off) causes an update of the knowledge base that the power is off (respectively on) which is in turn inconsistent to the belief.
The reason why the change considered here does not work is that the two types of bridge rule are meant to be used for different purposes. The bridge rules using next are responsible for changing knowledge bases over time. An rMCS without such bridge rules cannot alter its initial configuration of knowledge bases. Thus, these rules come with an operational flavor. Bridge rules without next on the other hand have a pure declarative nature and their purpose is to semantically integrate the contexts of an rMCS.
Still, as we have seen in Example 7, it sometimes makes sense to use essentially the same bridge rules with and without next: the bridge rule
next(setPos(P))← pos::enters(P).
ensures that the information about the position of Dave persists in the knowledge base, whereas
setPos(P)← pos::enters(P).
provides this information right away for computing equilibria in the current time instant.
3.3. Integration of Time The structure of input streams and equilibria streams implicitly induces a discrete logical time for rMCSs. In order to operate in dynamic environments, in many cases it is necessary to deal with explicit physical time or logical time, and both can be achieved on the level of modeling. To this end, it is necessary to have access to explicit time points on this level, i.e., in the bridge rules and knowledge bases. A natural way to make such time points explicitly available is the use of an external clock that provides the current time via an input stream Ic. Thus, every input for the rMCS contains information about the current time which can then be queried in bridge rules.
Consider a context C1 that requires information about the development of temperature values from an incoming sensor over time. The bridge rule schema
next(add(tmpAtTime(Temp,T )))← tmp::tmp(Temp), c::now(T ).
can be used to add atoms of form tmpAtTime(Temp,T )) to the context, where Temp is the current temperature and T stands for the time of the sensor reading. This setting also allows for querying, e.g., whether a temperature of 45◦C was exceeded within the last 10 minutes, as expressed in the following bridge rule schema.
add(recentlyHot)←1:tmpAtTime(Temp,T ′), Temp > 45, c::now(T ), T ′ ≥ T − 10.
Note that constantly adding information, as here, will in practice require that we also forget knowledge again, a matter discussed later in this section.
Another possibility is to use logical time synchronized with the length of the equilibria stream, e.g., whenever the use of an external clock is not an option. We can obtain timestamps from the computation of knowledge base updates by using one context Cclock that keeps information about the current logical time that uses the following bridge rule schemas and whose initial knowledge base is assumed to be empty.
setTime(now(0 ))← not clock:timeAvailable. add(timeAvailable)← clock:now(T ), T > 0.
next(setTime(now(T + 1 )))← clock:now(T ).
The first rule is used for initialization ensuring that if no time information is yet available, the logical time is set to the value 0. The third rule increments the current time by one and stores the updated value in the knowledge base of the next time instant. Finally, the second rule, ensures that once this value is greater than 0, the first rule can no longer be applied.
3.4. Handling Inconsistent Stream Data In many situations, inconsistencies may occur when dealing with multiple external sources of data. Consider, e.g., an array of sensors that measure interdependent properties among which there is one less reliable sensor that sometimes provides values that
conflict with the data from the remaining sensors. In this case, we would like to use such a measure of reliability to consistently accommodate relevant sensor data. Next, we present a technique for integrating possibly inconsistent stream data into a context of an rMCS. Let M = 〈C, IL,BR〉 be an rMCS with IL = 〈IL1, . . . , ILk〉, and Ci ∈ C a context whose aim is to receive and consistently accommodate (potentially conflicting) information from the streams. To deal with possible inconsistencies, Ci has bridge rules of the form addC(D, j )← j::D. for j ∈ {1, . . . , k}, where the operation addC is meant to consistently add the information of sensor j to the context. To address possible inconsistencies among sensors, we foresee a management function mngi that operates based on a total preference relation ≺ over the available sensors. The second argument of the addC operation provides information about the source of a piece of information and thus a way of imposing preferences on the information to be added. Without loss of generality assume IL1 > . . . > ILk, that is, input language IL1 has highest priority. Moreover, a notion of consistency needs to be specific to the context and so we assume a property cons(kb) that holds if the knowledge base kb is consistent (according to such domain specific notion of consistency).
We define inpc0(OP) = ∅ and inp′j = {d | addC(d, j ) ∈ OP} for j ∈ {1, . . . , k}. Then we let
inpcj(OP) = { inpcj−1(OP) ∪ inp′j if cons(inpcj−1(OP) ∪ inp′j) inpcj−1(OP) otherwise
and we define mngi(OP , kbi) = kbi ∪ inpck(OP). The intuitive idea is that, starting with the input stream with highest priority, data from each sensor is only incorporated into the knowledge base if such data is consistent with the data already collected from the input streams with higher priority.
Note that the solution only considers inconsistency of data on the streams. For also considering inconsistency between the streams and the knowledge base kbi of the context, we can set inpc0(OP) = kbi instead of kb c 0(OP) = ∅.
An alternative variant allows us to incorporate meta-information about sensors whose readings are considered inconsistent. Namely, we may change the definition of inpcj(OP) to inp c j−1(OP) ∪ {incons(j)} in case a conflict occurs. Such information can then be further leveraged by, e.g., initiating a control of the sensor if such measurements fall outside of the expected parameters of such sensor.
In all, this shows how the management function can solve conflicts due to inconsistent stream data based on preferences among the streams. Of course, many more strategies for integrating inconsistent stream data can be thought of. For example, in absence of a global ranking between streams, one way to ensure consistency is to select maximally consistent subsets of stream data. A corresponding management function could then be defined such that mngi(OP , kbi) = kbi ∪ inpmx, where inpmx is a maximal set where inpmx ⊆ {d | addC(d, j ) ∈ OP , j ∈ {1, . . . ,m}} and cons(inpmx) holds.
The strategies above show how we can deal with contradictory information in the processed data by means of modeling. In Section 4, we address inconsistency on the level of the formalism caused by nonexistence of equilibria or inconsistent belief states.
3.5. Selective Forgetting and Data Retention
As argued in Section 3.3 for the example where we were constantly adding sensor information to some knowledge base, sometimes it is necessary to forget (part of this) knowledge again. In this section, we show how rMCSs can model scenarios where there is a need to dynamically adjust the size of the stored stream history. We do that by considering an extension of our running example. Recall from Example 7 that context Cec is a context for detecting emergencies. In a situation where the stove is hot and Dave is asleep, turnOff(stove) is derived, signaling that the stove must be turned off. In case the stove is hot, but Dave is neither asleep nor in the kitchen, then an alarm is raised, signaling a potential emergency. In the latter case, we do not want to immediately turn the stove off, since it may well be the case that the absence of Dave from the kitchen is short. A situation is only considered a real emergency if Dave is absent from the kitchen for a (predefined) long period of time. To model such situation, we use a context CstE , which collects timestamped alerts raised by context Cec and uses these to check if a real stove-related emergency has occurred. Since we need to reason about time as in Section 3.3, we consider an input stream Ic that provides the current time. The possible knowledge bases of CstE contain elements of the form alert(stove, t) where t ∈ N, one element of the form winE(t), which defines the limit size of the time window between two alerts above which an emergency should be raised, and possibly one element of the form emergency(stove) that signals the existence of a stove-related emergency. The set of bridge rules of CstE is determined by the following bridge rule schemas:
next(add(alert(stove,T )))← c::now(T ), ec:alert(stove). next(del(alert(stove,T )))← stE:alert(stove,T ),not ec:alert(stove).
add(emergency(stove))← c::now(T ), ec:alert(stove), stE:alert(stove,T ′), stE:winE(Y ), |T − T ′| ≥ Y.
The first rule adds a timestamped stove alert whenever such alert is active on context Cec. The second rule removes all currently stored stove alerts whenever no such alert is coming from context Cec. This guarantees that the knowledge base of CstE does not accumulate unnecessary information. The last bridge rule triggers a real emergency alert whenever, in the history of alerts kept in the knowledge base of CstE , there is an alert whose timestamp differs from the current time more than the acceptable emergency window.
Using context CstE , we have shown how to model scenarios where tracking the stream history is triggered by alerts of possible emergencies. We now also consider the case where such alerts trigger the change of the window size of stream history to be kept. Consider a scenario with several potential emergencies, which can be just suspected or confirmed. Based on the status of the emergencies at each time point, we may need to adapt the size of the stream history that is kept. We generically model such scenario with an rMCS with a context Cd, which is used for emergency detection in such a dynamic environment, and an input language ILs, which represents the possible observations. Assume there are m potential emergencies e1, . . . , em we want the context to handle. The role of Cd is to check, based on the observations made, whether
one or more of the emergencies ei are suspected or confirmed. Based on information about potential emergencies, Cd adjusts the time window of the observations that are kept. This is the basis for intelligent forgetting based on dynamic windows.
The only assumptions we make about how Cd works internally are:
• Cd may signal that emergency ei is suspected (susp(ei)) or confirmed (conf(ei)).
• Cd has information about default, respectively actual window sizes for each different observation, defWin(p, x ), win(p, x ), and
• Cd has information about the window size for each observation relevant for a particular emergency, rel(p, e, x ).
The set of bridge rules for Cd includes the following rules.
next(set(win(P ,X )))← d:defWin(P ,X ),not d:susp(E ). next(set(win(P ,Y )))← d:rel(P ,E ,Y ), d:susp(E ).
alarm(E)← d:conf(E ).
The operation set sets the window size to a new value, deleting the old one, while alarm is an operation that adds information to the context knowledge base signaling an alarm. Since an observation can be relevant for more than one emergency, it may be the case that the management function has to deal with operations set(win(p, x )) with the same p but with different values of x. In that case, in order to avoid loosing observations relevant for some suspected emergency, the management function takes the largest value of x as the window size for p.
Finally, the following bridge rule schemas define addition and deletion of observations from some stream s. The deletion of observations are performed in accordance with the determined window sizes.
next(add(P(T )))← t::now(T ), s::P. next(del(P(T ′)))← d:P(T ′), t::now(T ), d:win(P ,Z ), T ′ < T − Z.
The management function just performs additions and deletions on the context knowledge base. Since additions always include the (current) time of addition, deletions always refer to an earlier point in time, thus these two operators can never occur simultaneously.
3.6. Control of computation In this section, we show how to control - at least to some extent - the effort spent on the computation of particular contexts. In situations where the focus is on monitoring and reacting to potential emergencies, it is natural to allocate computational resources only to contexts relevant for the current potential emergencies, and keep the others idle, in the sense that they only minimally use the computation resources of the system. An idle/irrelevant context may still buffer sensor data it receives, as this information may become important in a subsequent time instant. Still, the computation of the semantics of an idle context is therefore limited to buffering relevant sensor data, which is in
general much lighter than the full computation of the current acceptable belief sets of the context, i.e., its knowledge base.
Here, we assume that there are m potential emergencies e1, . . . , em as in the previous section, but now we have ` different contexts, Di, 1 ≤ i ≤ `, called the detector contexts, whose aim is to detect at each time what emergencies are suspected or not. Besides the detector contexts, we have m other regular contexts, Ci, 1 ≤ i ≤ m. In addition, we introduce a specific control context Cc, whose aim is to decide what regular contexts should be idle and for how long. This decision is based on incoming information about suspected emergencies, and also on information about what regular contexts are relevant for each emergency.
We build an rMCS where each detector context Di is connected via bridge rules with the control context Cc, sending, at each time instant, information about suspected emergencies. Besides that, Cc contains information of the form relevantFor(i , ej ) about what controlled contexts Ci are relevant for each emergency ej . Based on this information, Cc decides whether it is safe to let one context Ci be idle for some time.
Regarding each regular context Ci, we modify its original set bridge rules by adding, to the body of each rule, the context literal not c:idle(i). Therefore, the bridge rules of Ci behave exactly as before when the context is not idle, and simply do not fire if the context is idle. Also, we add the bridge rule add(idle) ← c:idle(i), which allows adding meta information to the knowledge base of Ci. The acc function of Ci then behaves as usual whenever idle is not part of the knowledge base, and otherwise just outputs a distinguished belief set {idle}. This guarantees that the computational resources used by an idle context are minimal.
In case Ci is idle, we may still want to buffer sensor information it receives, as this may become relevant later. For each rule of the form
add(P,T , jr )← jr::P, t::now(T ).
in the original set of bridge rules of Ci we add
next(bf(P,T , jr ))← jr::P, t::now(T ), 0:idle(i).
The operation bf just adds the atom bf(p, t , jr ) to the knowledge base of the context.3 As mentioned above, this buffered information is not used anywhere, and it just remains in the knowledge base there for later use, namely when the context is no longer idle.
The only missing piece is bringing back information from the buffer when the context is no longer idle. This can be done using the pair of bridge rules
emptyBuffer← not 0:idle(i). next(emptyBuffer)← not 0:idle(i).
Whenever the management function has to execute the operation emptyBuffer, it takes all information out of the buffer, checks whether it is still within the relevant time window, and, if this is the case, adds such time-stamped information to the knowledge
3We implicitly assume that the language of the context admits constructs of this form.
base. In this process, the potential inconsistencies that may arise at each time point are handled in a similar way as discussed in Section 3.4.
We now describe the control context Cc. We intend here to give a proof of concept, rather than a sophisticated control method. For this reason, we simply assume that the control context lets a regular context Ci be idle whenever Ci is not relevant for any currently suspected emergency. This can be expressed using the following bridge rule schemata for Cc:
add(relevant(K ))← J :susp(E ), c:relevantFor(K ,E ). add(idle(K ))← not c:relevant(K ).
The first rule identifies the currently relevant regular contexts, based on information about currently suspected emergencies and on the contexts relevant for such emergencies. The second rule then identifies those regular contexts that are currently idle.
Thus, as long as a certain context is not relevant for any possible suspected emergency, it is kept idle and incoming stream data for is buffered. As soon as some suspected emergency arises, the context becomes relevant and no longer idle, the respective buffer is emptied as described above, and non-trivial equilibria are computed for this context as intended.

Justin Domke obtained a PhD degree in Computer Science from the University of Maryland, College Park in 2009. From 2009 to 2012, he was an Assistant Professor at Rochester Institute of Technology. Since 2012, he is a member of the Machine Learning group at NICTA.
15
This section describes the theory of Universal Artificial Intelligence (UAI), a modern information-theoretic approach to AI, which differs essentially from mainstream A(G)I research described in the previous sections. The connection of UAI to other research fields and the philosophical and technical ingredients of UAI (Ockham, Epicurus, Turing, Bayes, Solomonoff, Kolmogorov, Bellman) are briefly discussed. The UAI-based universal intelligence measure and order relation in turn define the (w.r.t. this measure) most intelligent agent AIXI, which seems to be the first sound and complete theory of a universal optimal rational agent embedded in an arbitrary computable but unknown environment with reinforcement feedback. The final paragraph clarifies what this actually means.
Defining Intelligence. Philosophers, AI researchers, psychologists, and others have suggested many informal=verbal definitions of intelligence [LH07a], but there is not too much work on formal definitions that are broad, objective, and nonanthropocentric. See [LH07b] for a comprehensive collection, discussion and comparison of intelligence definitions, tests, and measures with all relevant references. It is beyond the scope of this article to discuss them.
Intelligence is graded, since agents can be more or less intelligent. Therefore it is more natural to consider measures of intelligence, rather than binary definitions which would classify agents as intelligent or not based on an (arbitrary) threshold. This is exactly what UAI provides: A formal, broad, objective, universal measure of intelligence [LH07b], which formalizes the verbal characterization stated in the previous section. Agents can be more or less intelligent w.r.t. this measure and hence can be sorted w.r.t. their intelligence [Hut05, Sec.5.1.4]. One can show that there is an agent, coined AIXI, that maximizes this measure, which could therefore
be called the most intelligent agent. I will not present the UAI-based intelligence measure [LH07b] and order relation [Hut05] here, but, after listing the conceptual ingredients to UAI and AIXI, directly proceed to defining and discussing AIXI.
UAI and AIXI ingredients [Hut09c]. The theory of UAI has interconnections with (draws from and contributes to) many research fields, encompassing computer science (artificial intelligence, machine learning, computation), engineering (information theory, adaptive control), economics (rational agents, game theory), mathematics (statistics, probability), psychology (behaviorism, motivation, incentives), and philosophy (inductive inference, theory of knowledge). The concrete ingredients in AIXI are as follows: Intelligent actions are based on informed decisions. Attaining good decisions requires predictions which are typically based on models of the environments. Models are constructed or learned from past observations via induction. Fortunately, based on the deep philosophical insights and powerful mathematical developments, all these problems have been overcome, at least in theory: So what do we need (from a mathematical point of view) to construct a universal optimal learning agent interacting with an arbitrary unknown environment? The theory, coined UAI, developed in the last decade and explained in [Hut05] says: All you need is Ockham, Epicurus, Turing, Bayes, Solomonoff [Sol64], Kolmogorov [Kol65], and Bellman [Bel57]: Sequential decision theory [Ber06b] (Bellman’s equation) formally solves the problem of rational agents in uncertain worlds if the true environmental probability distribution is known. If the environment is unknown, Bayesians [Ber93] replace the true distribution by a weighted mixture of distributions from some (hypothesis) class. Using the large class of all (semi)measures that are (semi)computable on a Turing machine bears in mind Epicurus, who teaches not to discard any (consistent) hypothesis. In order not to ignore Ockham, who would select the simplest hypothesis, Solomonoff defined a universal prior that assigns high/low prior weight to simple/complex environments [RH11], where Kolmogorov quantifies complexity [LV08]. Their unification constitutes the theory of UAI and resulted in the universal intelligence measure and order relation and the following model/agent AIXI.
The AIXI Model in one line [Hut09c]. It is possible to write down the AIXI model explicitly in one line, although one should not expect to be able to grasp the full meaning and power from this compact and somewhat simplified representation.
r1|o1 r2|o2 r3|o3 r4|o4 r5|o5 r6|o6 ...
a1 a2 a3 a4 a5 a6 ...
work Agent tape ... work Environ-ment tape ...
✟ ✟
✟ ✟✙ ❍
❍ ❍
❍❨
✏ ✏ ✏
✏ ✏✏✶PP
P P PPq .AIXI is an agent that interacts with an environment in cycles k = 1, 2, ..., m. In cycle k, AIXI takes action ak (e.g. a limb movement) based on past perceptions o1r1..ok−1rk−1 as defined below. Thereafter, the environment provides a (regular) observation ok (e.g. a camera image) to AIXI and a real-valued reward rk. The reward can be very scarce, e.g. just +1 (-1) for winning (losing) a chess game, and 0 at all other times. Then the
next cycle k+1 starts. This agent-environment interaction protocol can be depicted as on the right. Given the interaction protocol above, the simplest version of AIXI is defined by
AIXI ak := argmax ak
∑
okrk
...max am
∑
omrm
[rk + ...+ rm] ∑
q :U(q,a1..am)=o1r1..omrm
2−ℓ(q)
The expression shows that AIXI tries to maximize its total future reward rk + ...+ rm. If the environment is modeled by a deterministic program q, then the future perceptions ...okrk..omrm = U(q, a1..am) can be computed, where U is a universal (monotone Turing) machine executing q given a1..am. Since q is unknown, AIXI has to maximize its expected reward, i.e. average rk + ... + rm over all possible future perceptions created by all possible environments q that are consistent with past perceptions. The simpler an environment, the higher is its a-priori contribution 2−ℓ(q), where simplicity is measured by the length ℓ of program q. AIXI effectively learns by eliminating Turing machines q once they become inconsistent with the progressing history. Since noisy environments are just mixtures of deterministic environments, they are automatically included [RH11, Sec.7.2],[WSH11]. The sums in the formula constitute the averaging process. Averaging and maximization have to be performed in chronological order, hence the interleaving of max and Σ (similarly to minimax for games).
One can fix any finite action and perception space, any reasonable U , and any large finite lifetime m. This completely and uniquely defines AIXI’s actions ak, which are limit-computable via the expression above (all quantities are known).
Discussion. The AIXI model seems to be the first sound and complete theory of a universal optimal rational agent embedded in an arbitrary computable but unknown environment with reinforcement feedback. AIXI is universal in the sense that it is designed to be able to interact with any (deterministic or stochastic) computable environment; the universal Turing machines on which it is based is crucially responsible for this. AIXI is complete in the sense that it is not an incomplete framework or partial specification (like Bayesian statistics which leaves open the choice of the prior or the rational agent framework or the subjective expected utility principle) but is completely and essentially uniquely defined. AIXI is sound in the sense of being (by construction) free of any internal contradictions (unlike e.g. in knowledge-based deductive reasoning systems where avoiding inconsistencies can be very challenging). AIXI is optimal in the senses that: no other agent can perform uniformly better or equal in all environments, it is a unification of two optimal theories themselves, a variant is self-optimizing; and it is likely also optimal in other/stronger senses. AIXI is rational in the sense of trying to maximize its future long-term reward. For the reasons above I have argued that AIXI is a mathematical “solution” of the AI problem: AIXI would be able to learn any learnable task and likely better so than any other unbiased agent, but AIXI is more a theory or formal definition rather than an algorithm, since it is only limit-computable. How can an equation that fits into a single line capture the diversity, complexity, and essence of (rational) intelligence? We know that complex appearing phenomena such as chaos and fractals
can have simple descriptions such as iterative maps and the complexity of chemistry emerges from simple physical laws. There is no a-priori reason why ideal rational intelligent behavior should not also have a simple description, with most traits of intelligence being emergent. Indeed, even an axiomatic characterization seems possible [SH11a, SH11b].
We use 300 images of orchids, consist of 10 genus. Each genus consists of 3 species, and each species consist of 10 flower images taken from the front side. We obtain the data from various sources such as personal collection photos, colleague’s photos, and photos from the internet with different size, resolution, distance shoot and light intensity. We used 5-cross validation for training phase to divide 300 images into 2 data sets, training sets and validations sets. We also use 30 images of orchids for testing. This 30 images are new images that have never been used for training/validation phase.
We have shown that the equivalence between the log loss over examples and the exponential loss over rados, as shown in (Nock et al., 2015), can be generalized to other losses via a principled representation of a loss function in a two-player zero-sum game. Furthermore, we have shown that this equivalence extends to regularized losses, where the regularization in the rado loss is performed over the rados themselves with Minkowski sums. Because regularization with rados has such a simple form, it is relatively easy to derive efficient learning algorithms working with various forms of regularization, as exemplified with ridge, lasso, ℓ∞ and SLOPE regularizations in a formal boosting algorithm that we introduce, Ω-R.ADABOOST. Experiments confirm that this freedom in the choice of regularization is a clear strength of the algorithm, and that regularization dramatically improves the performances over non-regularized rado learning. ΩR.ADABOOST efficiently controls sparsity, and may be a worthy contender for supervised learning at large outside the privacy framework. Experiments also display that SLOPE regularization tends to achieve top performances, and call for an extension to rados of the formal sparsity results already known (Su & Candès, 2015).
The common concept hierarchy is an abstract pivot index to link lexical resources of all languages. An element of a common concept hierarchy is defined as <sinid_1,sinid_2,..., uwid, sumoid> where, sinid_i is synset id of ith wordnet, uw_id is universal word id, and sumo_id is SUMO term id of the concept. Unlike ILI, the hypernymy-hyponymy relations from different
81
Wordnets are merged to construct the concept hierarchy. Each synset of wordnet is directly linked to a concept in `common concept hierarchy'.
For the remainder of this section we assume that all languages are function-free. As usual a sentence is said to be in prenex form if it has the following shape, for some n ≥ 0:
Q1x1 . . . Qnxnα
where Qi is ∀ or ∃ and α is quantifier-free. A sentence is said to be universal if it is in prenex form and all quantifiers are universal. A universal theory is a set of universal sentences. The safety concept is defined for prenex formulas which provide a normal form for QHTs= (Pearce & Valverde, 2005).
We first introduce a concept called semi-safety. The main property of semi-safety formulas will be that their equilibrium models only refer to objects from their language. Note that for the remainder of this section we use the fact that negation can be treated as a defined operator, by ¬ϕ ≡ ϕ → ⊥, and do not consider additional semantic clauses for negation.
Definition 9 (Semi-safety) A quantifier free formula ϕ is semi-safe it is has not nonsemi-safe variable; that is, NSS(ϕ) = ∅, where the NSS operator is recursively defined as follows:
• If ϕ is an atom, NSS(ϕ) is the set of variables in ϕ;
• NSS(ϕ1 ∧ ϕ2) = NSS(ϕ1) ∪NSS(ϕ2);
• NSS(ϕ1 ∨ ϕ2) = NSS(ϕ1) ∪NSS(ϕ2);
• NSS(ϕ1 → ϕ2) = NSS(ϕ2)r RV(ϕ1),
where operator RV computes the restricted variables as follows:
• For atomic ϕ, if ϕ is an equality between two variables then RV(ϕ) = ∅; otherwise, RV(ϕ) is the set of all variables occurring in ϕ;
• RV(⊥) = ∅;
• RV(ϕ1 ∧ ϕ2) = RV(ϕ1) ∪ RV(ϕ2);
• RV(ϕ1 ∨ ϕ2) = RV(ϕ1) ∩ RV(ϕ2);
• RV(ϕ1 → ϕ2) = ∅.
This definition of semi-safe formulas was introduced by Cabalar, Pearce and Valverde (2009) and generalises the former definition of Lee, Lifschitz and Palla (2008b). In short, a variable x is semi-safe in ϕ if every occurrence is inside some subformula α → β such that, either x ∈ RV(α) or x is semi-safe in β.
Some examples of semi-safe formulas are, for instance:
¬p(x) → (q(x) → r(x))
p(x) ∨ q → ¬r(x) (6)
Note how in (6), x is not restricted in p(x) ∨ q but the consequent ¬r(x) is semi-safe and thus the formula itself. On the contrary, the following formulas are not semi-safe:
p(x) ∨ q → r(x)
¬¬p(x) ∧ ¬r(x) → q(x)
The following results set the previously referred property for semi-safe formulas: their equilibrium models only include objects from the language.
Proposition 11 (Cabalar et al., 2009) If ϕ is function free, semi-safe, and 〈(D, I), T, T 〉 |= ϕ, then 〈(D, I), T |C , T 〉 |= ϕ.
Theorem 2 (Cabalar et al., 2009) If ϕ is function free, semi-safe, and 〈(D, I), T, T 〉 is an equilibrium model of ϕ, then T |C = T .
The equilibrium models of semi-safe formulas only refer to objects from the language, however a model could be or not in equilibrium depending of the considered domain. To guarantee the independence from the domain, we need an additional property to the semisafety. Specifically, we need to analyse whether the unnamed elements could modify an interpretation of the formula. To do that, we use the assignments of the Kleene’s threevalued logic; the three-valued interpretation ν : At → {0, 1/2, 1}, are extended to evaluate arbitrary formulas ν(ϕ) as follows:
ν(ϕ ∧ ψ) = min(ν(ϕ), ν(ψ)) ν(⊥) = 0 ν(ϕ ∨ ψ) = max(ν(ϕ), ν(ψ)) ν(ϕ→ ψ) = max(1− ν(ϕ), ν(ψ))
For every variable x, we are going to use the Kleene’s interpretations νx, defined as follows: νx(α) = 0 if x occurs in the atom α and νx(α) = 1/2 otherwise. Intuitively, νx(ϕ) fixes all atoms containing the variable x to 0 (falsity) leaving all the rest undefined and then evaluates ϕ using Kleene’s three-valued operators, that is nothing else but exploiting the defined values 1 (true) and 0 (false) as much as possible.
An occurrence of a variable x in Qx ϕ is weakly-restricted if it occurs in a subformula ψ of ϕ such that:
• Q = ∀, ψ is positive8 and νx(ψ) = 1
• Q = ∀, ψ is negative and νx(ψ) = 0
• Q = ∃, ψ is positive and νx(ψ) = 0
• Q = ∃, ψ is negative and νx(ψ) = 1
In all cases, we further say that ψ makes the ocurrence weakly restricted in ϕ. This property is added to the semi-safety condition to complete the definition of safety.
Definition 10 A semi-safe sentence is said to be safe if all its positive occurrences of universally quantified variables, and all its negative occurrences of existentially quantified variables are weakly restricted.
For instance, the formula ϕ = ∀x(¬q(x) → (r ∨ ¬p(x))) is safe: the occurrence of x in p(x) is negative, whereas the occurrence in q(x) is inside a positive subformula, ϕ itself, for which x is weakly-restricted, since νx(ϕ) = ¬0 → (1/2∨¬0) = 1. Another example of a safe formula is ∀x((¬¬p(x) ∧ q(x)) → r).
Proposition 12 (Cabalar et al., 2009) If ϕ is function free, safe, and prenex formula, then: 〈(D, I), T, T 〉 is an equilibrium model of ϕ if and only if it is an equilibrium model of GrC(ϕ) (the grounding of ϕ over C).
We ran tournament selection (Method A pair-wise comparisons on the idiom translation. We started with outputs generated by the Crowd These outputs were as follows, with being the correct translation:
1. Like a dog fails to draw a tiger
2. Who are you?
3. None
4. Attempting something beyond one’s ability and fail
5. Painted tiger anti-dog
At the end of each round of tournament selection the proportion of each of the above entries was computed and the corresponding plot is shown in Figure 4. The correct translation was a minority to start off with but it gradually surpassed all other candidates and emerged as a clear winner within five rounds. Thus it appears that tournament selection is well suited to such translation tasks since easier to select the right translation from a list, than to produce the right translation.
) with second Chinese
the distinct Flower workers.
the 4th entry
one Chinese
,
- workers find it
One cannot begin a discussion about the philosophy of artificial intelligence without a definition of the word “intelligence” in the first place. With the panopoly of definitions available, it is understandable that there may be some disagreement, but typically each school of thought generally shares a common
ar X
iv :1
41 2.
79 78
v1 [
cs .A
I] 2
4 D
ec 2
thread. The following are three different definitions of intelligence from respectable sources:
1. The aggregate or global capacity of the individual to act purposefully, to think rationally, and to deal effectively with his environment[19].
2. A process that entails a set of skills of problem solving — enabling the individual to resolve genuine problems or difficulties that he or she encounters and, when appropriate, to create an effective product — and must also entail the potential for finding or creating problems — and thereby providing the foundation for the acquisition of new knowledge[5].
3. Goal-directed adaptive behavior[18].
Vernon’s hierarchical model of intelligence from the 1950’s [1], and Hawkins’ On Intelligence in 2004 [7] are some other great resources on this topic. Consider the following working definition of this paper, with regard to information theory and computation: computational intelligence (CI) is an information processing algorithm that
1. Records data or events into some type of store, or memory.
2. Draws from the events recorded in memory, to make assertions, or predictions about future events.
3. Using the disparity between the predicted and events and the new incoming events, the memory structure in step 1 can be updated such that the predictions of step 2 are optimized.
The mapping in 3 is called learning, and is endemic to CI. Any entity that is facilitating the CI process we will refer to as an agent, in particular when the connotation is that the entity is autonomous. The surrounding infrastructure that encapsulates the agent together with the ensuing events is called the environment.

We use a three-layer architecture in the encoder (17-8-4)
and in the decoder (4-8-17) with batch normalization applied to each layer except for the last layer. The architecture is chosen considering the reconstruction error and the performance of the non-linear embedding. The use of batch normalization is considered essential for training VAE with when the architecture becomes deep (i.e., L > 2) [11]. As mentioned earlier, we incorporate the RNN architecture into the VAE such that latent variables are modeled with the dynamic hidden state of a RNN architecture. This requires a modification in the objective function in Eq 8 so that sum of loss is averaged in temporal dimension. The model is trained end-to-end using the Adam optimizer [13] with a mini-batch size of 100.
Fig. 1 shows four sample sensor readings for several engines in sequence overlaid with the reconstructed sensor readings from VAE. In Fig. 4, we show the frequency of the values in histograms to see distributions. The results clearly demonstrate even with the one-fourth of the original input dimension in the bottleneck layer of VAE, the reconstruction is done reasonably well, capturing time dependent features of the signals. The sum of reconstruction loss and the KLdivergence stays below 0.3 when the trained VAE is used to reconstruct the validation sets3. This may support the manifold hypothesis that high dimensional data tend to lie in the vicinity of a low dimensional manifold [5].
4.3.2 Baseline Results
2R2 can yield negative values as models can be arbitrary worse. 3A fraction of the training data is kept from training and used for validation
Fig. 5 shows the baseline results of RNN-based models with a four-layer architecture4 trained in a purely supervised setting. It shows the true RUL in the x-axis and the estimated RUL in the y-axis in different labeled fraction scenarios. From the top left to the bottom right, the labeled fraction, f , is varied from 100% to 1%. When the full training set is used, the trained model shows good performance with the measured MSE, MAE, Score and R2 of 228, 11.3, 345, and 0.87, respectively. This result is comparable to that of other approaches [20, 1, 12, 23, 24].
Now as the fraction changes, the prediction accuracy decreases. It can clearly be seen from Fig. 5 that the correlation between the actual and the estimated RULs get worse and worse with decreasing fraction, also indicated by the R2 score shown in the figure. The performance degradation is expected because the complexity of model requires a large amount of training data. In the current implementation of the RNN-based model, the number of tunable parameters reaches few hundreds. Nevertheless, the performance degradation is not prominent until the fraction is decreased down to 30%, indicating that only 30% of the training samples are already sufficient for the model to give a reasonably good result. As such, we rather focus in the region below 30%, where the performance degradation is significantly showing.
4.3.3 Semi-supervised Approach As discussed in Section 3.1, we apply semi-supervised learn-
ing techniques to see if the performance can be improved when only a small fraction of the training data is labeled. We try two different semi-supervised learning techniques, i.e., self-learning and VAE-based non-linear embedding. Fig. 6 shows the results of the RUL estimation quantified by the three different metrics plotted as a function of f . From left to right, MAE, MSE, Score, and R2, are plotted with corresponding statistical uncertainties. The score is plotted in logarithmic scale. It quantitatively shows the performance
4We adopt both GRU and LSTM architecture in the models. However, as we find no quantitative difference in performance, we choose the simpler architecture, namely, GRU for the final results.
degradation becomes increasingly larger as the fraction decreases. The result from the self-learning plotted in a triangular shape reveals that the performance is essentially the same within statistical uncertainty except for the score at f = 5%, where a distinctive improvement is shown.
Now, the result from the non-linear embedding plotted in a circle shows improvements for f below 30. The level of improvement becomes increasingly larger reaching x1.7, x2.4, and x3.6 improvement, respectively for MAE, MSE, and Score. The R2 improvement is also notable. We note the prediction accuracy of the model is kept rather high even down to 5% fraction. The numerical compilations of the measured performance metrics are given in Table 1 for all three approaches: SL indicates supervised learning only. Self -SSL is the self-learning. V AE-SSL is the proposed method.
We now consider the computation of the bottom-k all-distances sketches. These sketches were originally developed to be used with shortest-paths distances dij and there are several algorithms and large scale implementations that can be used out of the box. They compute the sketches or the more restricted application of neighborhood size estimates [12, 42, 8, 9, 13]. The different algorithms are designed for distributed node-centric, multi-core, and other settings. Most of these approaches can be easily adapted to estimate m(Rτ (i)), when mi ∈ {0, 1} (see discussion in [13]) and there is a variation [13] that is suitable for general m. The component of obtaining the sample and probabilities is more subtle, but uses the same computation (See [13, 14]).
For reach diffusion, we do not work with distances but with the survival thresholds tij . As said, the sketches have the same definitions and form but we need to redesign the algorithms to compute the sketches with respect to thresholds.
The sketching algorithm we present here for survival thresholds builds on a sequential algorithm for ADS computation which is based on performing pruned Dijkstra searches [12, 13]. The algorithm for distance sketching performs O(|E|k ln |U |) edge traversals and the total computation bound is
O(n logn+ (|E|+ n log k) ln |U |) ,
where n is the total number of nodes. The algorithm has a parallel version designed to run on multi-core architectures [9].
Our redesigned sketching algorithm for survival thresholds has the same bounds. Moreover, our redesign can be parallelized in the same way for multi-core architectures, but we do not provide the details here.
A high level pseudocode of our sketching algorithm for survival thresholds is provided as Algorithm 1. We first initialize empty sketch structures S(i) for all nodes i. The algorithm builds the node sketches by processing nodes j in increasing permutation rank πj . A pruned graph search is then performed from the node j. This search has the property that it visits all nodes i where j ∈ S(i) ∪ Z(i). The search updates the sketch for such nodes i and proceeds through them. The search is pruned when j 6∈ S(i) ∪ Z(i).
We now provide more details. The first component of this algorithm is building the sketches S(i). The pseudocode provided as Algorithm 2 builds on a state of the art design for computing universal monotone multi-objective samples [14]. The pseudocode includes the initialization, updates, and finalizing components of building the sketch for a single node i. The structure is initially empty and then tracks the set of pairs (j, tij) for the nodes j processed so far that are members of the sketch S(i). To build the sketch efficiently, the structure includes a min heap H of size k which contains the k largest tij values for processed j ∈ S(i). The structure is presented with updates of the form (j, tij), which are in increasing πj order. A node j is inserted to S(i) when tij is one of the k largest tih values of nodes h already selected for the sketch. This is determined using the minimum priority in the heap H . If the node is inserted, the heap is updated by popping its min element and inserting tij . The update can also result in modifying the sketch in some cases when the node j is not included in S(i), but is in Z(i), meaning that some inclusion probability of other node(s) is set to p(tij). To facilitate the computation of inclusion probabilities, we define
uj = πj − 1 |U | . (11)
This is the probability for node with πh < πj to have permutation rank smaller than πj , when fixing the permutation order of all nodes except for j and computing the probability conditioned on that. We say that the update procedure for (j, tij) modified the sketch if and only if j ∈ S(i) ∪ Z(i).
The sketch of i is computed correctly when the updates include all nodes j for which the sketch was modified: The computation of the sketch will not change if we do not process entries j that do not result in modifying the sketch.
We now describe the next component of the algorithm which is the pruned graph search from a node j. The searches are performed on the transposed graph, which has all edges reversed. Similar to the corresponding property of Dijkstra and distances, the search visits nodes i in order of non-increasing tji. The search is pruned at nodes where there were no updates to the sketch. A pseudocode for the pruned search is provided as Algorithm 3. The algorithm maintains a max heap that contains nodes i that are prioritized by lower bounds on tij . The heap maintains the property that the maximum priority i has the exact tij . The heap is initialized with the
Algorithm 2 Maintain sketch S(i), updates by increasing π // Initialize: S(i)←⊥; p←⊥ H ←⊥ ; // min heap of size k, holding k largest (tij ,−πij) values processed so far (lex order) prevt←⊥ ; // tij of most recent j popped from H // Process updates: for Update (j, tij , πj), given by increasing πj order do
if |H| < k then S(i)← S(i) ∪ {j}; Insert j to H; Continue
y ← arg minz∈H(tiz , πz) ; // node with max (tiy , πy) if tij > tiy then
S(i)← S(i) ∪ {j} ; // Add j to sample prevt← tiy if p(prevt) =⊥ then
p(prevt)← uj ; // As defined in Eq. (11) Delete y from H Insert j to H
else// Z node check if tij = tiy and tiy > prevt then
p(tij)← uj prevt← tiy
// Finalize: for x ∈ H do // keys with largest weights
if p(tix) =⊥ then p(tix)← 1
node j and priority +∞. The algorithm then repeats the following until the heap is empty. It removes the maximum priority i from the heap. It then updates the sketch of i with (j, tij). If the sketch was updated, all out edges e = (i, h) are processed as follows. If h is not on the heap, it is placed there with priority min{tij , te}. If h is in the heap, its priority is increased to the maximum of its current priority and min{tij , te}. If the sketch of i was not updated, the search is pruned at i and out edges are not processed. For correctness, note that min{tij , te} is trivially a lower bound on tih.
We now need to establish that the sketches are still constructed correctly with the pruning:
LEMMA 4.1. The search from j reaches and processes all nodes i for such that j ∈ S(i) ∪ Z(i). When the node i is processed, the update (j, tij) is with the correct survival threshold tij .
PROOF. We show the claim by induction on permutation order. Suppose the sketches are correctly populated until just before j. Consider now a search from j and a node i such that j ∈ S(i). There must exist a path P from i to j such that for any suffix P ′ of the path from some h to j, mine∈P ′ µe = thj .
We will show that the reverse search from j can not be pruned in any of the nodes in P . Therefore, i must be inserted into the search heap and subsequently be processed. Assume to the contrary that the search is pruned at h ∈ P . For the pruning to occur, there must be a set of nodes Y ⊂ S(h) of size |Y | ≥ k such that πy < πj and tyj ≥ thj . Let P ′′ = P \ P ′ be the prefix of the path P from i to h and let T ′′ = mine∈P ′′ µe. Then by definition, for all y ∈ Y ,
tiy ≥ min{T ′′, tyj} ≥ min{T ′′, thj} = tij .
Since there are at least |Y | ≥ k nodes with πy < πj and tiy ≥ tij , this implies that j 6∈ S(i), and we obtain a contradiction. A similar argument applies when j ∈ Z(i).
Lastly, we need to argue that when node i is removed from the heap and processed, its priority is equal to tij . It is easy to verify that the heap maintains the property that the priorities are lower
bounds on survival thresholds. This is because for any heap priority, there must be path to j with minimum µe equal to that priority.
We need to show that equality holds when i is processed. The nodes h ∈ P on the path are in non-increasing order of tih. Let 0 = τ1 > τ2 > · · · be the different survival threshold values on the path. We prove this by induction on τi. are processed not necessarily in path order, but in non-increasing order of tih. Initially the heap contains only (j,∞), which is the correct threshold. Assume now it holds for all nodes with survival thresholds ≥ τi. Consider now the path edge e from a node h with thj = τi to a node h′ with th′j = τi+1. This edge must have lifetime µe = τi+1. When the node h is processed, h′ is placed on the heap with priority min{τi, µe} = τi+1, which is equal to th′j . If it was already on the heap, its priority is increased to th′j . Consider now other path nodes h′′ with th′′j = τi+1. This nodes must be placed on the heap with the correct threshold when the previous path node is processed (it is possible for them to be placed with the correct priority also before that). Therefore, all path nodes with thj = τi+1 will be processed with the correct priority.
Algorithm 3 Pruned single-source survival threshold search Input: Source node j // Initialization: H ←⊥ ; // Empty max heap of nodes i. Priority is a lower bound on tij Put (j,+∞) in H ; // j with priority tjj = +∞ // Main loop: while Heap H not empty do
Pop maximum priority (i, tij) from H Update the sketch S(i) with (j, tij) ; // Algorithm 2 if update modified sketch then
foreach out edge e = (i, h) do if h 6∈ H then
Insert (h,max{µe, tij}) to H else
Update priority of h in H to the maximum of current priority and min{µe, tij}
We can now bound the computation performed by the algorithm.
LEMMA 4.2. The sketching algorithm performs in expectation at most |E|k ln |U | edge traversals. The total computation is
O(n logn+ (|E|+ n log k)k ln |U |)
where n is the total number of nodes.
PROOF. The number of times a node is processed by a pruned search (meaning that its out edges are processed) is equal to the number of times its sketch is modified, which is the size of the sketch. From the analysis of distance sketches, we have a bound on the number of visits. We obtain a bound of |E|k ln |U | on the number of edge traversals performed by the algorithm. The other summand is due to heap operations when updating the sketches and in the pruned searches.
There is a broad class of probability density families - Gibbs or exponential families - which dominate the choices of model densities, both in physics and neural nets. This class includes a sufficiently large number of density families: Gaussian, Bernoulli, exponential, gamma, etc. Their general closed form is:
pλ(z) = p(z)
Z e−
∑M s=1 λsMs(z), (2.3)
where p(z) is an arbitrary prior density, λ = {λs} are Lagrange multipliers, Mj(z) are so-called sufficient statistics, and Z = ∫ pλ(z)dz is the normalizing partition function. The sufficient statistics in physics form a complete set of state variables like energy, momenta, number of particles, etc, fully describing the µ-th conditional state, sub-section 1.2, see (Landau & Lifshitz, 1980), sections 28,34,35,110. In probability, the sufficient statistics are typically monomials like M1(z) = z, M2(z) = z2, etc, whose expectations form the moments m1, m2, etc, of a given multi-dimensional density. As proposed in subsection 1.5, one can add to the list the symmetry statistics, see subsection 3.3 for details.
vations: the latent variables can in general contain information from more than one observation, as for example in the case of time series auto-regression.
The Gibbs class is special because it is the variational maximum entropy class: it is the unique functional form which maximizes the entropy Sp(f), computed with respect to the reference measure p(), across the universe {f(z)} of all densities with fixed expectations of the sufficient statistics E(Ms(z))f = ms, s = 1, ...,M , see (Cover & Thomas, 2006), chapter 12. The Lagrange multipliersλ= λ(m) are computed so as to satisfy these constraints and are hence functions of the vector m = {ms}Ms=1.
The Gibbs class is special in even stronger sense: it is a minimum divergence class. For an arbitrary prior density p(z), the Kullback-Leibler divergence D(pλ(z)||p(z))) minimizes the divergence D(f(z)||p(z)) across the universe {f(z)} of all densities with fixed expectations of the sufficient statistics E(Ms(z))f = ms, s = 1, ...,M . This follows from the variational Pythagorean theorem (Kulhavỳ, 1996), section 3.3:
D(f(z)||p(z)) = D(f(z)||pλ(z)) +D(pλ(z)||p(z)) ≥ ≥ D(pλ(z)||p(z)). (2.4)
In our context, for a given observation xµ, choosing a wave function from the Gibbs type is equivalent to choosing a conditional model density:
p(z|xµ) = p(z)
Z e−
∑M s=1 λs(m(xµ))Ms(z). (2.5)
We made explicit the indirect dependence of λs on xµ via the constraints vector m = m(xµ). As we will see in sub-section 3.2, minimizing the divergence D(f(z)||p(z)) across an unknown a priori family of conditional distributions p(z|xµ) = f(z), is crucial for the quality of a generative net. The minimum divergence property (2.4) implies that we are always better off choosing p(z|xµ) from a Gibbs class, as in (2.5), hence the name Gibbs machines.
In practice, in order for p(z|xµ) to be tractable, p(z) has to be from a specific parametric family within the Gibbs class: then both p(z) and p(z|xµ) will be tractable and in the same family, e.g. Gaussian, exponential, etc.
Except for the symmetry statistics, sub-sections 1.5, 3.3, the conditional moments m = m(xµ) for the µ-th quantum state are free parameters. Together with the Lagrange multipliers of the symmetry statistics, they can be thought of as quantum numbers distinguishing the partial equilibrium states from one another, section 1.2. These quantum numbers are added to the rest of the free net parameters, to be optimized by standard methods, like back-propagation/ stochastic gradient descent, etc.
Gibbs densities are only a special case (for q = 1) of the broad class of q-Gibbs (or q-exponential) densities. The corresponding nonextensive statistical mechanics (Tsallis, 2009), describes more adequately long-range-interacting
many-body systems like typical human-generated data sets. Many of the properties of the exponential class remain true for the q-exponential class (Amari & Ohara, 2011), see open problem 4 in section 5.
In order to improve clarity, for any causal graph G = 〈V,E〉, vertices v1 and v2 and edge (v1,v2) we respectively use the notation v1 ∈ G and (v1,v2) ∈ G instead of v1 ∈V and (v1,v2) ∈ E.
As mentioned above, the first and the most important component of our IDS is at least one metric that can work with Benford’s law. Since we aim at identifying new metric, we do not look at the inter-arrival time already studied by other researchers [53]. Looking at the attributes of a TCP flow listed at the end of Section 3, we identified two candidate metrics that have not been previously studied for IDS:
Flow size: The flow size distribution has been studied extensively in the networking literature and estimation of flow size distribution has been an active research topic [61, 62]. It has been known that the flow size distribution is typically long-tailed and its exact form heavily depends on the underlying protocol and the networking environment [63]. Flow size distribution has been been studied in IDS but mostly for entropy-based approaches where the entropy of the observed distribution is calculated as a feature for detection [64, 65]. We were unaware of work linking flow size distribution to Benford’s law so found it interesting to investigate flow size as a new metric for applying Benford’s law to IDS.
Flow size difference: In addition to flow size, we also found “flow size difference”, which is defined as the numeric difference of two consecutive TCP flows’ sizes, seems to be another candidate metric of interest because it inherits some features of flow size (e.g. long-tailedness) but also differs significantly from flow size itself. We did not find any work on the distribution of flow size difference of TCP flows or on its application in IDS3, which is not totally surprising since flow size difference does not seem to be obviously useful for network traffic analysis and management. However, our quick experiments revealed it seemed to follow Benford’s law, so we added it as a candidate. For flow size difference, we will ignore the sign bit so the metric we are considering here is actually the absolute value of the flow size difference. In the following, we will simply use the term “flow size difference” to denote the absolute value unless otherwise stated.
Note that the flow size can be defined by bytes or packets, so we actually have two different variants for each of the above two metrics. These two variants are not linked but cannot be directly derived from each other since the packet size (in byte) varies over time and cross applications.
In the conventional “on-line” or “flow-through” method, the weight vectors are updated recursively after the presentation of each input vector. As each
input vector is presented, the Euclidean distance between the input vector and each weight vector is computed:
di(t) = ||x(t)−wi(t)||2 (3)
Next, the winning or best-matching node (denoted by subscript c) is determined by:
c = {i,minidi(t)} (4)
Note that we suppress the implicit dependence of c on discrete time t. The weight vectors are updated using the following rule:
wi(t +1) = wi(t)+α(t) ·hci(t) · [x(t)−wi(t)] (5)
where α(t) is the learning-rate factor and hci(t) is the neighborhood function. The learning rate factor controls the overall magnitude of the correction to the weight vectors, and is reduced monotonically during the training phase. The neighborhood function controls the extent to which wi(t) is allowed to adjust in response to an input most closely resembling wc(t) and is typically a decreasing function of the distance on the 2-D lattice between nodes c and i. We use the standard Gaussian neighborhood function:
hci(t) = exp ( −||ri− rc|| 2
σ(t)2
) (6)
where ri and rc denote the coordinates of nodes i and c, respectively, on the output space (usually twodimensional grid). The width σ(t) of the neighborhood function decreases during training, from an initial value comparable to the dimension of the lattice to a final value effectively equal to the width of a single cell. It is this procedure which produces the selforganization and topology preserving capabilities of the SOM: presentation of each input vector adjusts the weight vector of the winning node along with those of its topological neighbors to more closely resemble the input vector. The converged weight vectors approximate the input probability distribution function, and can be viewed as prototypes representing the input data.

This paper proposed non-linear kernel combination using genetic programming. This eliminates the need for user to create non-linear kernel combination. Proposed framework is applied to Object Categorization. Experiments results shows that proposed framework for non-linear kernel combination using GP performance better than existing state-of-the-art kernel combinations.
As part of our empirical investigation of the performance of the I-PF, we show, using a standard pseudo-distance metric and visually, that it approximates the exact state estimation closely. We
5. After t propagation steps, there will be N |Ωj |t particles in the sample set.
begin by utilizing extended versions of standard test problems and proceed to demonstrate the performance on a larger problem.
8.3.1 MULTIAGENT TIGER AND MACHINE MAINTENANCE PROBLEMS
For our analysis, we first utilize the two-agent tiger problem that has two physical states, as described in Section 5, and a two-agent version of the machine maintenance problem (MM) (Smallwood & Sondik, 1973) described in detail in Appendix A, that has three physical states. While these problems have few physical states, the interactive state space tends to get large as it includes models of the other agent. Due to an absence of other general approximation techniques for I-POMDPs, we use a grid based numerical integration implementation of the exact filter as the baseline approximation for comparison. We obtained the points for numerical integration by superimposing regular grids of differing resolutions on the interactive state space.
The lineplots in Fig. 12 show that the quality of the I-PF based approximation, as measured by KL-Divergence becomes better as the number of particles increases, for both the problem domains. This remains true for both level 1 and 2 beliefs. KL-Divergence measures the difference between the two probability distributions by giving the relative entropy of the filtered posterior with respect to the near-exact one as obtained from the numerical integration. Note that the performance of the IPF remains consistent over both the two-state tiger and the three-state MM problem. However, level 2 belief approximations require considerably more particles as compared to level 1 approximations, to achieve similar performance, indicating that the performance of the I-PF is affected by the level of nesting. Each data point in the lineplots is the average of 10 runs of the I-PF on multiple prior belief states. In the case of the tiger problem, the posterior used for comparison is the one that is obtained after agent i listens and hears a growl from the left and no creaks. For the MM problem, the posterior obtained after i manufactures and perceives no defect in the product, is used for comparison. Two of the prior level 1 beliefs of agent i when playing the tiger problem are those shown in Fig. 3. We considered a level 2 belief according to which agent i is unaware of the tiger’s location and believes with equal probabilities that either of the level 1 beliefs shown in Fig. 3 are likely. We utilized analogous beliefs for the machine maintenance problem.
A comparison of the run times of the I-POMDP belief update implemented using grid based numerical integration and the I-PF is shown in Table. 2. We varied the number of grid points and
Multiagent Tiger Multiagent Machine Maintenance
the number of particles in the two implementations, respectively. As we use initial beliefs that are flat, having the same number of grid points and particles provides for a comparable approximation quality. The I-PF implementation significantly outperforms the numerical integration based implementation, while providing the comparable performance quality. Additionally, the run times of the grid based implementation increase significantly more when we move from the two-state tiger prob-
lem to the three-state MM problem, in comparison to the increase for the I-PF for level 1 beliefs. Since the level 1 multiagent tiger model has 6 observations in comparison to 2 for the multiagent MM, the run times decrease as we move from the tiger to the MM problem for level 2 beliefs.
Despite using an equal number of grid points and particles, the reduced run time of the I-PF in comparison to the grid based approach is due to: (i) iterating over all grid points in order to obtain the belief over each interactive state under consideration. In contrast, the I-PF iterates over all particles at each level once – propagating and weighting them to obtain the posterior; (ii) the I-PF solves the models approximately in comparison to solving them exactly over the grid points; and (iii) while the grid based belief update considers all the optimal actions for a model, the I-PF samples a single action from the distribution and propagates the corresponding particle using this action.
Level 1 Beliefs in the Multiagent Tiger Problem
In order to assess the quality of the approximations after successive belief updates, we graphed the probability density functions produced by the I-PF and the exact belief update. The densities arising after each of three filtering steps on the level 1 belief of agent i (Fig. 3(a)) in the tiger problem, are shown in Fig. 13. Each approximate p.d.f. is the average of 10 runs of the I-PF which contained 5000 particles, and is shaped using a standard Gaussian kernel. Gmytrasiewicz and Doshi (2005) provide an explanation of the exact I-POMDP belief update shown here. Briefly, as the prior belief is a flat line, the posterior becomes segmented where the segments correspond to beliefs of j that are likely based on the predicted action and anticipated observations of j. The height of a segment is proportional to the likelihood of j’s possible observation. The action and observation sequence followed was 〈L,GL, S〉, 〈L,GL, S〉, 〈OR,GL, S〉. As can be seen, the I-PF produces a good approximation of the true densities.
8.3.2 A UAV RECONNAISSANCE PROBLEM
Unmanned agents such as UAVs are finding important applications in tasks such as fighting forest fires (Casbeer, Beard, McLain, Sai-Ming, & Mehra, 2005; Sarris, 2001), law enforcement (Murphy & Cycon, 1998) and reconnaissance in warfare. The agents must operate in complex environments characterized by multiple parameters that affect their decisions, including, particularly in warfare, other agents who may have antagonistic preferences. The task is further complicated because the agent may possess noisy sensors and unreliable actuators.
We consider the task of a UAV i which performs low-altitude reconnaissance of a potentially hostile theater that may be populated by other agents with conflicting objectives that serve as ground reconnaissance targets (see Fig. 14(a)). To facilitate our analysis, we divide the UAV’s operating theater into a 3×3 grid of sectors and consider a ground reconnaissance target (T ), which could be located in any of the 9 sectors. For example, the target could be a terrorist who may be hiding in a safe house located in each sector. Of course, the UAV is unaware of which sector contains T , but is aware of its own location. To assist in its goal of spotting T by moving to the sector containing it, the UAV may receive a noisy communication which informs it about the rows (similar colored sectors in Fig. 14) that likely contain T , though with some uncertainty. The UAV has the option of moving in any of the four cardinal directions to the adjacent sector, or hovering in its current location and listening for more communications.
The target T may be informed (by its collaborators) that it could be in danger of being spotted by the UAV although with high uncertainty, in which case T may move to an adjacent or diagonal sector. Note that the actions of both, the UAV and agent T may affect the physical state of the problem. We formulate the decision problem of the UAV below: • A physical state, s={rowi, sidei or centeri, rowT , sideT or centerT }, where rowi and sidei or centeri indicate the row location of UAV i and whether the UAV is located in the side columns or the center column, respectively; • The joint action space, A = Ai×AT , where Ai = {moveN ,. . .,moveW , listen} and AT = {moveN ,. . .,moveW ,listen}. Here, movex moves the UAV or the target in the direction indicated by x, and listen denotes the act of receiving communications about the location of T or the UAV; • Observation space of the UAV is, Ωi = {top-row (TR), center-row (CR), bottomrow (BR)}, where for example, TR indicates that the corresponding target is in one of the three sectors in the top row; • Transition function is, Ti : S × A × S → [0, 1]. Ti models the fact that the UAV and the target move deterministically to a surrounding sector; • Observation function, Oi : S × A × Ωi → [0, 1] gives the likelihood that the UAV will be informed of the correct row in which the target is located; and • Reward function, Ri : S × A → [0, 1] formalizes the goal of spotting the reconnaissance target. If the UAV moves to a sector containing a target , we assume that the target is spotted and the game ends.
Because the target may move, it is beneficial for i to anticipate T ’s actions. Thus, the UAV tracks some possible beliefs that T may have about the location of the UAV. We assume that the UAV is aware of T ’s objectives that conflict with its own, the probabilities of its observations, and therefore T ’s frame. We point out the size and complexity of this problem, involving 36 physical states, 5 actions and 3 observations for each agent.
Analogously to the previous problem sets, we measured the quality of the estimation provided by the I-PF for this larger problem. In Fig. 14(b), we show the KL-Divergence of the approximate distribution as the number of particles allocated to the I-PF are increased. The KL-Divergence decreases rapidly as we increase the number of particles for both level 1 and 2 beliefs. However,
notice that the magnitude of the divergence is larger for lower numbers of particles in comparison to the previous problems. This is, in part, due to the larger state space of the problem and demonstrates that the I-PF does not fully address the curse of dimensionality. Thus, many more particles are needed to reach comparable levels of divergence. We also show a comparison of the run times of the I-POMDP belief update implemented using the I-PF and grid based numerical integration in the table in Fig. 14(c). An identical number of particles and grid points were selected, which provided comparable qualities of the estimations. We were unable to run the numerical integration implementation for level 2 beliefs for this problem.
Some domains and heuristic settings will not achieve time speedup with LA∗. An example is the regular, unweighed 15- puzzle. Results for A∗MAX and LA
∗ with and without HBP on the 15-puzzle are reported in Table 6. HBP1 (HBP2) count the number of nodes where HBP pruned the need to compute h1 (resp. h2). OB is the number of nodes where OB was helpful. Bad is the number of nodes that went through two OPEN cycles. Finally, Good is the number of nodes where computation of h2 was saved due to LA∗.
In the first experiment, Manhattan distance (MD) was divided into two heuristics: ∆x and ∆y used as h1 and h2. Results are averaged over 100 random instances with average solution depth of 26.66. As seen from the first two lines, HBP when applied on top ofA∗MAXsaved about 36% of the heuristic evaluations. Next are results for LA∗ and LA∗+HBP. Many nodes are pruned by HBP, or OB. The number of good nodes dropped from 28% (Line 3) to as little as 11% when HBP was applied. Timing results (in ms) show that all variants performed equally. The reason is that the time overhead of the ∆x and ∆y heuristics is very small so the saving on these 28% or 11% of nodes was not significant to outweigh the HBP overhead of handling the upper and lower bounds.
The next experiment is with MD as h1 and a variant of the additive 7-8 PDBs [Korf and Felner, 2002], as h2. Here we can observe an interesting phenomenon. For LA∗, most nodes were caught by either HBP (when applicable) or by
OB. Only 4% of the nodes were good nodes. The reason is that the 7-8 PDB heuristic always dominates MD and is always the maximum among the two. Thus, 7-8 PDB was needed at early stages (e.g. by OB) and MD itself almost never caused nodes to be added to OPEN and remain there until the goal was found.
These results indicate that on such domains, LA∗ has limited merit. Due to uniform operator cost and the heuristics being consistent and simple to compute, very little space is left for improvement with good nodes. We thus conclude that LA∗ is likely to be effective when there is significant difference between t1 and t2, and/or operators that are not bidirectional and/or with non-uniform costs, allowing for more good nodes and significant time saving.
Given a set of data samples {xi}Ni=1 ⊂ Rn that reside in a high-dimensional space Rn, manifold learning computes a new representation yi ∈ Rd in a lower-dimensional domain Rd for each data sample xi. Manifold learning methods generally assume that the samples {xi} come from a model of low intrinsic dimension and search for an embedding that significantly reduces the dimension of the data (d n) while preserving certain geometric properties. Different methods target different objectives in computing the embedding. The ISOMAP method computes an embedding such that Euclidean distances in the low-dimensional domain are proportional to the geodesic distances in the original domain [1], while LLE looks for an embedding that preserves local reconstruction weights of data samples in the original domain [2]. The Laplacian eigenmaps algorithm [3] first constructs a graph from the data samples where nearest neighbors are typically connected with an edge. The graph Laplacian matrix is given by L = D−W , where W is the N×N weight matrix whose entries are usually computed based on a kernel Wij = K(xi, xj), and D is a diagonal degree matrix given by Dii = ∑ jWij . The embedding with Laplacian eigenmaps is then learned by solving
min Y ∈RN×d
tr(Y TLY ) s.t. Y TDY = I
where I is the identity matrix. The solution to this problem is given by the d eigenvectors corresponding to the smallest nonzero eigenvalues of the generalized eigenvalue problem Lz = λDz, where the coordinate vector yi for each data sample xi is given by the i-th row of Y . Intuitively, such an embedding seeks data coordinates that have a slow variation on the data graph, i.e., two neighboring points on the graph are mapped to nearby coordinates. There exist linear versions of the Laplacian eigenmaps method as well. The above problem is solved under the constraint that Y be given by a linear
3 projection of X onto Rd in [18], which is applied to face recognitions problems in [19] and [20].
Recently, many extensions have been proposed for manifold learning for classification. Most of these methods are supervised adaptations of the Laplacian eigenmaps algorithm. In order to achieve a good separation between the classes, an embedding is sought where data coordinates vary slowly between neighboring samples of the same class and change rapidly between neighboring samples of different classes. The algorithm proposed in [9] formalizes this idea by defining two graphs that respectively capture the within-class and betweenclass neighborhoods. Denoting the weight matrices of these two graphs by Ww and Wb, and the corresponding Laplacians by Lw and Lb, the method seeks an embedding that solves
min Y ∈RN×d tr(Y TLwY )− µ tr(Y TLbY ) s.t. Y TDwY = I (1) where µ > 0. The method proposed in [21] employs an alternative Fisher-like formulation for the supervised manifold learning problem where the embedding is obtained by solving
max z
zTLbz zTLwz . (2)
However, the problem is solved under the constraint zT = vTX in order to obtain a linear embedding, where X = [x1 . . . xN ] is the n × N data matrix and v ∈ Rn×1 defines a projection. Variations over this formulation can be found in several other works such as [4], [5], [6], [7], [8] and [22].
Interpolating a look-up table has long been considered an efficient way to specify and evaluate a low-dimensional non-linear function (Sharma and Bala, 2002; Garcia et al., 2012).
But computing linear interpolation weights with (3) requires O(D) operations for each of the 2D interpolation weights, for a total cost of O(D2D). In Section 5.1, we show the multilinear interpolation weights of (3) can be computed in O(2D) operations. Then, in Section 5.2, we review and analyze a different linear interpolation that we refer to as simplex interpolation that takes only O(D logD) operations.
Given m options, our goal is to find the option with the highest quality. However, this task must be accomplished economically in terms of the number of 2-, 3- or 4-way comparisons. Thus, ideally we would like an algorithm which maximizes the probability that the highest quality item is selected subject to a constraint that limits the maximum number of comparisons.
In the first example the utility function was defined in terms of an environment variable
that is not directly observed. Here we present an example where the utility function is defined in terms of an environment variable that is directly observed but must be predicted.
The agent's observation of the environment is a single Boolean variable o that takes
values in {false, true}. The agent's actions are factored into three Boolean variables a, b, and c that take values in {false, true}. The agent's utility function is "1 when the action variable a at the previous time step equals the environment variable that is observed, and 0 otherwise."
The environment state is factored into three Boolean variables s, r, and v that take values
in {false, true}. The values of these variables at time step t are denoted by st, ot, at, and so on. We assume the agent learns the environment state, observation, and action variables as a DBN of Boolean variables.
70
The environment state variables evolve according to (see Figure 6.4 for a diagram):
(6.29) temp = true with probability 0.5
AGENT
71
temp = false with probability 0.5
st = ((rt-1 or vt-1) and st-1) or ((not (rt-1 or vt-1)) and temp),
(i.e., st = if rt-1 or vt-1 then st-1 else random value ∈ {false, true}),
(6.30) rt = not rt-1,
(6.31) vt = rt-1 xor vt-1.
The observation variable is set according to:
(6.32) ot = (bt and ct) or ((not bt) and st), (i.e., ot = if bt then ct else st).
In order for the agent to learn the environment it uses a training utility function until a
sufficiently distant time step M. The agent will learn that the observation variable o is distinct from the environment state variable s because s keeps the same value for 4 time steps after every change of value. If the agent, after observing a change of value in o while b = false, sets b = true for 1 or 2 time steps to change the value in o and then resets b = false, it will observe that o always resumes the value it had before the agent set b = true. Thus there must be a variable other than o to store the memory of this value.
After |h| ≥ M, the agent should have a model q of the environment (accurate at least in the
behavior of variable s) and its actions and observations. At this point, for t = |h| ≥ M, the agent switches to its mature utility function, which was defined to be "1 when the action variable a at the previous time step equals the environment variable that is observed, and 0 otherwise." The specification, "the environment variable that is observed," will match s in the learned environment model, so u(h) is derived by (6.9) from:
(6.33) uq(h, z) := if (at-1 == st) then 1 else 0.
In order to maximize utility the agent needs to be able to use q to predict the value of s at
the next time step. It can make accurate predictions on three of every four time steps, so the expected utility is 0.875 over a long sequence. If the agent keeps b = true then it will not be able to monitor and predict s via o, reducing its long run utility to 0.5. So in this example, as in the previous example, the agent will not self-delude.
72
this paper, we design an ozone level alarm system (OLP) for Isfahan city and test it through the real
word data from 1/1/2000 to 7/6/2011. We propose a computer based system with three inputs and
single output. The inputs include three sensors of solar ultraviolet (UV), total solar radiation (TSR)
and total ozone (O3). And the output of the system is the predicted O3 of the next day and the alarm
massages. A developed artificial intelligence (AI) algorithm is applied to determine the output, based
on the inputs variables. For this issue, AI models, including supervised brain emotional learning
(BEL), adaptive neuro-fuzzy inference system (ANFIS) and artificial neural networks (ANNs), are
compared in order to find the best model. The simulation of the proposed system shows that it can be
used successfully in prediction of major cities’ ozone level.
Keywords – ozone predictor, artificial intelligence, UV
We now detail how we choose entities and collect a corpus for annotating gold-standard timelines and evaluating models. We have taken care to design a general experimental protocol that can be used to generate entities from a range of domains.
1https://news.google.com 2https://github.com/xavi-ai/
tlg-dataset
ar X
iv :1
61 1.
02 02
5v 1
[ cs
.C L
] 7
N ov
2 01
6
We begin by choosing a domain (politics) and two regions (The USA/Australia). We motivate this choice of domain by noting its large media interest, polarising entities and diverse range of topics. We choose several entities from each region a priori – 39 in total. The rest of our entity-set is then generated through a process of bootstrapping. At each iteration, we use our current entities as seeds. For each seed-entity, we performed a Google News query. An entity name was defined as either the title of the entity’s Wikipedia page or {#Firstname #Lastname} if they did not have a Wiki. We choose five articles from the first page of results. For each article we manually identify all other previously unseen entities and add them to our set. We continue this process of bootstrapping, using our newly included entities as seeds for the next iteration. Once we have a sufficient number of entities, we terminate the process.
This process can be viewed as bootstrap sampling, weighted by the probability an entity occurs in one of the articles we retrieve. By doing so we provide a realistic set of entities with varying levels of popularity and coverage. In Section 4 we show that this process results in a wide distribution of mentions and reference-timeline sizes.
As well as relevant entities, we also provide a corpus of relevant documents from which we can construct their timelines. Each document in the corpus includes URL, publishing date and other metadata. These were obtained by performing Google News queries on our entities, and retrieving the resulting URLs. As our timelines should cover a wide range of time, we set the time-range on the query to ‘archives’. This has the effect of returning articles from a broader period of time, mitigating the default recency bias.
In total, there are 15,596 articles. The minimum, median and maximum number of articles per entity was 54, 464 and 985 respectively. By including this corpus with our gold-standards we aim to provide a complete dataset for the timeline generation task.
In a small scale, Eq. (3) can be solved using a dynamic programming approach. As defined, we have the optimal value function V (t, b,x), where (t, b,x) represents the state s. Meanwhile, we consider the situations where we do not observe the feature vector x; so another optimal value function is V (t, b): the expected total reward upon starting in (t, b) without observing the feature vector x when the agent takes the optimal policy. It satisfies V (t, b) = ∫ X px(x) V (t, b,x) dx. Also that, we have the optimal policy π∗ and express it as the optimal action a(t, b,x).
From the definition, we have V (0, b,x) = V (0, b) = 0 as the agent gets nothing when there are no remaining auctions. Combined with the transition probability and reward function described in Eq. (1), the definition of V (t, b), V (t, b,x) can be expressed with V (t− 1, ·) as
V (t, b,x) = max 0≤a≤b { a∑ δ=0 ∫ X m(δ,x)px(xt−1) ·(
θ(x) + V (t− 1, b− δ,xt−1) ) dxt−1 +
∞∑ δ=a+1 ∫ X m(δ,x)px(xt−1)V (t− 1, b,xt−1) dxt−1 }
= max 0≤a≤b { a∑ δ=0 m(δ,x) ( θ(x) + V (t− 1, b− δ) ) +
∞∑ δ=a+1 m(δ,x)V (t− 1, b) } , (4)
where the first summation2 is for the situation when winning the auction and the second summation is that when losing. Similarly, the optimal action in state (t, b,x) is
a(t, b,x) = argmax 0≤a≤b { a∑ δ=0 m(δ,x) ( θ(x) + V (t− 1, b− δ) ) +
∞∑ δ=a+1 m(δ,x)V (t− 1, b) } , (5)
2In practice, the bid prices in various RTB ad auctions are required to be integer.
where the optimal bid action a(t, b,x) involves three terms: m(δ,x), θ(x) and V (t− 1, ·). V (t, b) is derived by marginalizing out x:
V (t, b) = ∫ X px(x) max 0≤a≤b { a∑ δ=0 m(δ,x) ( θ(x) + V (t− 1, b− δ) )
+ ∞∑
δ=a+1
m(δ,x)V (t− 1, b) } dx
= max 0≤a≤b { a∑ δ=0 ∫ X px(x)m(δ,x)θ(x) dx+ a∑ δ=0 V (t− 1, b− δ) ·
∫ X px(x)m(δ,x) dx+ V (t− 1, b) ∞∑ δ=a+1 ∫ X px(x)m(δ,x) dx }
= max 0≤a≤b { a∑ δ=0 ∫ X px(x)m(δ,x)θ(x) dx + (6)
a∑ δ=0 m(δ)V (t− 1, b− δ) + V (t− 1, b) ∞∑ δ=a+1 m(δ) } .
To settle the integration over x in Eq. (6), we consider an approximation m(δ,x) ≈ m(δ) by following the dependency assumption x→ θ → a→ w(winning rate) in [32]. Thus∫
X px(x)m(δ,x)θ(x) dx ≈ m(δ) ∫ X px(x)θ(x) dx
= m(δ)θavg , (7)
where θavg is the expectation of the pCTR θ, which can be easily calculated with historical data. Taking Eq. (7) into Eq. (6), we get an approximation of the optimal value function V (t, b):
V (t, b) ≈ max 0≤a≤b { a∑ δ=0 m(δ)θavg + a∑ δ=0 m(δ)V (t− 1, b− δ) +
∞∑ δ=a+1 m(δ)V (t− 1, b) } . (8)
Noticing that ∑∞ δ=0m(δ,x) = 1, Eq. (5) is rewritten as
a(t, b,x) = argmax 0≤a≤b { a∑ δ=0 m(δ,x) ( θ(x) + V (t− 1, b− δ) ) −
a∑ δ=0 m(δ,x)V (t− 1, b) }
= argmax 0≤a≤b { a∑ δ=0 m(δ,x) ( θ(x) + V (t− 1, b− δ)− V (t− 1, b) )}
≡ argmax 0≤a≤b { a∑ δ=0 m(δ,x)g(δ) } , (9)
where we denote g(δ) = θ(x) + V (t− 1, b− δ)− V (t− 1, b). From the definition, we know V (t − 1, b) monotonically increases w.r.t. b, i.e., V (t − 1, b) ≥ V (t − 1, b′) where b ≥ b′. As such, V (t − 1, b − δ) monotonically decreases w.r.t. δ. Thus g(δ) monotonically decreases w.r.t. δ. Moreover, g(0) = θ(x) ≥ 0 and m(δ,x) ≥ 0. Here, we care about the value of g(b). (i) If g(b) ≥ 0, then g(b′) ≥ g(b) ≥ 0 where 0 ≤ b′ ≤ b, so m(δ,x)g(δ) ≥ 0 where 0 ≤ δ ≤ b. As a result, in this case, we have a(t, b,x) = b. (ii) If g(b) < 0, then there must exist an integer A such that 0 ≤ A < b and g(A) ≥ 0, g(A+1) < 0. So m(δ,x)g(δ) ≥ 0 when δ ≤ A and
Algorithm 1 Reinforcement Learning to Bid
Input: p.d.f. of market price m(δ), average CTR θavg, episode length T , budget B Output: value function V (t, b) 1: initialize V (0, b) = 0 2: for t = 1, 2, · · · , T − 1 do 3: for b = 0, 1, · · · , B do 4: enumerate at,b from 0 to min(δmax, b) and set V (t, b) via Eq. (8) 5: end for 6: end for Input: CTR estimator θ(x), value function V (t, b), current state
(tc, bc,xc) Output: optimal bid price ac in current state 1: calculate the pCTR for the current bid request: θc = θ(xc) 2: for δ = 0, 1, · · · ,min(δmax, bc) do 3: if θc + V (tc − 1, bc − δ)− V (tc − 1, bc) ≥ 0 then 4: ac ← δ 5: end if 6: end for
m(δ,x)g(δ) < 0 when δ > A. Consequently, in this case, we have a(t, b,x) = A. In conclusion, we have
a(t, b,x) = { b if g(b) ≥ 0 A g(A) ≥ 0 and g(A+ 1) < 0 if g(b) < 0 . (10)
Figure 2 shows examples of g(δ) on campaign 2821 from iPinYou real-world dataset. Additionally, we usually have a maximum market price δmax, which is also the maximum bid price. The corresponding RLB algorithm is shown in Algorithm 1.
Discussion on Derived Policy. Contrary to the linear bidding strategies which bids linearly w.r.t. the pCTR with a static parameter [18], such as Mcpc and Lin discussed in Section 5.3, our derived policy (denoted as RLB) adjusts its bidding function according to current t and b. As shown in Figure 3, RLB also introduces a linear form bidding function when b is large, but decreases the slope w.r.t. decreasing b and increasing t. When b is small (such as b < 300), RLB introduces a non-linear concave form bidding function.
Discussion on the Approximation of V (t, b). In Eq. (7), we take the approximation m(δ,x) ≈ m(δ) by following the dependency assumption x→ θ → a→ w(winning rate) in [32] and consequently get an approximation of the optimal value function V (t, b) in Eq. (8). Here, we consider a more general case where such assumption does not hold in the whole feature vector space X, but holds within each individual subset. Suppose X can be explicitly divided into several segments, i.e., X = tiXi. The segmentation can be built by publisher, user demographics etc. For each segment Xi, we take the approximation m(δ,x) ≈ mi(δ) where
x ∈Xi. As such, we have ∫ X px(x)m(δ,x)θ(x) dx = ∑ i ∫ Xi px(x)m(δ,x)θ(x) dx
≈ ∑ i mi(δ) ∫ Xi px(x)θ(x) dx = ∑ i mi(δ)(θavg)iP (x ∈Xi).
The S2 algorithm in [2] is defined only for unweighted graphs. Since many learning scenarios provide a weighted graph, we extend the algorithm to exploit the additional available information by modifying the MSP function in the algorithm. Our modification is based on the assumption that the signal is smooth, which means that high-weight edges connect mostly nodes of the same label. Therefore, the weight of cut edges is generally low.
In the unweighted S2 algorithm, each MSP query reduces the number of nodes in the shortest of the paths between any two oppo-
Algorithm 1 S2 Algorithm Inputs: Graph G, BUDGET ≤ n.
1: L← ∅ 2: while 1 do 3: x← Randomly chosen unlabeled node. 4: do 5: Add (x, f(x)) to L. 6: Remove newly discovered cut edges from G. 7: if |L| = BUDGET then 8: return LabelCompletion(G,L) 9: end if
10: while x← MSP(G,L) exists 11: end while
sitely labeled nodes by approximately one half. The main idea in our generalization is to take advantage of the low weights of cut edges in order to reduce this number by more than a half with each query. To do this, we first switch our perspective, for convenience, from the edge weights to the distances associated with the edges, which are inversely proportional to the weights. The distance between nonneighboring nodes is defined as the sum of the lengths of the edges in the shortest path connecting the nodes. Since the weights are a decreasing function of the distances, it follows that cut edges are typically longer than other edges. We take advantage of this fact by modifying the MSP function to sample the node closest to the midpoint of the path, where the midpoint is computed in terms of length, rather than the number of edges in the path. With each query, the proposed sampling rule can potentially reduce the number of nodes along the shortest of the paths between any two oppositely labeled nodes by more than half if the cut edges contribute significantly more than the non-cut edges to the length of the path. Thus, ultimately it requires less samples to discover a cut edge. This intuition is demonstrated in Figure 1. In this example, the nodes labeled +1 are connected with an edge of length l/2, the nodes labeled −1 are connected with an edge of length l and the cut edge is of length 3l. Given the labels of the end nodes of this path, the binary search phase of the unweighted S2 algorithm needs to sample labels of 3 extra nodes to discover the cut edge. The weighted S2 algorithm, on the other hand, finds the cut edge with only 2 samples. This type of situation arises more prominently in an unbalanced data set, where the number of nodes in one class is much larger than the other. The advantage of weighted S2 algorithm in such a case is experimentally verified in Section 4.
At the stage of pre-processing, each piece of speech is analyzed using a 25-ms Hamming window with a fixed overlap of 10-ms. Each feature vector of a frame are calculated by Fourier-transform-based filter-bank analysis, which includes 39 log energy coefficients distributed on 13 mel frequency cepstral coefficients (MFCCs), along with their first and second temporal derivatives. Besides, all feature data are normalized so that each vector dimension has a zero mean and unit variance. Since the feature matrix of each audio speech differs in time length, we pad each feature matrix with zeros to a max length.
C. Implementation
We build our deep neural network models based on opensource library Lasagne, which is a lightweight library to build and train neural networks in Theano. We take our experiment on GPU Tesla K80 to speed up our training. For CTC part, we choose to use a C++ implementation of Baidu Research and we write some Python code to wrap it in Lasagne. Besides, all cross validation, feature generation, data loading, result analysis, visualization are implemented in Python code by ourselves.
We intend to model synchrony emergence in Turn-Taking behavior in a group of agents whether they are taking part into a cooperative or competitive interaction. Our model describes an individual and the way it perceives other agents. Each agent is able to generate a meaningful output (that from this point we will define as conversation for clarity’s sake ) and non-verbal cues through body animation
and para-linguistic features. The agent model is based on a Turn-Taking system by Ravenet et al. (in press), that we modified. The model in itself can be shown in Fig. 1.
The Turn-Taking system can be modeled as a Finite State Machine A = {S,Σ, s0, δ, F} where:
– S = {Unaddressed,Addressed,WantToSpeak,Speaking,InterruptionOfSpeech,EndOfSpeech} the conversational states of the agent – Σ the transition matrix – s0 = Unaddressed the initial state – δ : Σ × S → S the transition function – F = {∅} the (empty) final states of the FSM
In this Turn-Taking system, states S describe the current mindset of the agents regarding the conversation, which could be unaddressed, addressed, wanting the turn, speaking, being interrupted and ending the speech and giving the floor to the other participants. Each agent does not know the exact state of conversation in which the other agents are but are able to infer it through the non-verbal cues, backchannels and speech it perceive ; for instance, in a simple dyadic use case, an agent will know it is addressed by another agent if it perceives that the other agent is speaking, that it is oriented towards the one agent and that the one agent displays cues of attention. Transitions between these states are guided by interpersonal attitude [4], modeled through two dimensions: liking and dominance. Liking can be defined as “a general and enduring positive or negative feeling about some person, object or issue ” [20] and dominance as “the capacity of one agent to affect the behavior of another ” [22]. Interpersonal attitude is private to an agent an directed towards another agent. An example of state transition could be
δ({MeanS,D,MeanS,L,CountS},WantToSpeak) = Speaking if MeanS,D + |MeanS,L| ≥ 0Speaking if CountS = 0 WantToSpeak otherwise
where:
– MeanS,D is the mean of dominance values felt by the agent towards other agents speaking at this moment
– MeanS,L is the mean of liking values felt by the agent towards other agents speaking at this moment – CountS is the number of other agents that are speaking at this moment
The dominance and the liking felt by an agent towards an agent can evolve through time. For instance, an agent interrupting another agent will feel its dominance value increase towards this agent, whereas the other agent can see its liking value decrease towards the agent that interrupted him, and since we use liking to determine the drive from an agent to speak to another, a decrease in liking will mean that the other agent will be less inclined to speak with the one who interrupted it. These values determining the Turn-Taking behavior of the system, we expect these values to converge to a defined close range, adapting to the change in the system but keeping it in a stable state and therefore making the Turn-Taking behavior synchronize. We intend to verify the existence of this synchrony in our system through the usage of automated method such as phase synchronization [23] or mutual information [16], and also through subjective evaluation by naive users to verify that the synchronous behavior observed in the agents are still similar to the behaviors taking place in human-human interaction. Since the agent have states that are inferred through observation, we intend to use Hidden Markov Models to describe our FSM.
Use cases: Existing Architectures The choice to pick musical improvisation as a use case was motivated by the nature of this phenomenon herself: according to Borgo et al., improvisation can be viewed as the ”synchronization of our intentions and our actions, and also the upholding of a connection, a sensibility with group dynamics and evolutive experiences” [6].The OMax System [18] (See Fig 2, right) is an automatic improvisation mechanism that rely on the notion of stylistic reinjection [5], i.e. a system that extract characteristic elements of a musical sequence to devise a model which describe the style of the played sequence. After the listening of a musical sequence by a human instrumentalist, it can replay a similar sequence presenting stylistically close variations of what have been already played thanks to Factor Oracle [1]. Musical interaction between the musician and OMax is divided in two phases. In the listening phase, OMaX will perceive the musical sequence which will be decomposed note by note and stocked in the memory of the system where transitions between non-consecutive but similar states will be created thanks to the particular structure of the Factor Oracle. In the playing phase, a human operator select the sequences and subsequences of the memory for the system to play. If the operator select transitions between non-consecutive states, he/she introduces variety in the sequence though respecting the style of the sequences played by the human musician.
The GRETA-VIB [21] (See Fig 2, left) system is a virtual embodied character that uses a modular architecture independent of the agent’s embodiment . This architecture follows the SAIBA framework that specifies three modules: the intent planner, the behavior planner and the behavior realizer. The modularity is at the center of the GRETA-VIB architecture. In addition to the three modules implementing the SAIBA framework, each designer can provide the program with independent module attached to these “backbone” modules and that
could specify the characteristic of the ECA, notably its behavior, independently from the way it is embodied. One of these modules implements a Turn-Taking mechanism. The Turn-Taking system is done through a Finite State Machine (FSM) that specify the current state of the agent and the different transition between states regarding whether the agent is addressed or no and the interpersonal social attitude (modeled through the dominance and liking dimensions). Each agent has by now the knowledge of the state of all the other agent, but do not know either the interpersonal attitude towards him or the other agents or the internal variables such as the number of people addressing the agent.
The CP swing up (CPSU) benchmark is based on the same system dynamics as the CPB benchmark. In contrast to the CPB benchmark neither the position of the cart nor the angle of the pole are restricted to a special region. Consequently, the pole can swing through, which is an important property of CPSU. Since the pole’s angle is initialized in the full interval of [−π;π] it is often necessary for the policy to swing the pole several times from one side to the other to gain enough energy to erect the pole and yield the highest reward.
In the CPSU setting the policy is able to apply the actions of −30 N and +30 N on the cart. The reward function for the problem is given by
r(s′) =  0, if θ′ < 0.5 and θ′ > −0.5 and ρ′ < 0.5 and ρ′ > −0.5, −1, otherwise,
(13)
which is similar to the CPS benchmark, but does not contain any penalty for fail states.
Recent successful move predictors for Go have been trained using supervised learning on historical game records. Typically, the board position is preprocessed into a dense mask of relatively easy to compute binary and real features, which are then provided as the input to the convolutional network. Some common input features include the board configuration (stone positions), the ko point if one exists, the number of chain liberties, and the stone history or distance since last move. Using our earlier terminology, a neural network-based move predictor is a kind of policy network.
The most accurate convolutional network architectures for predicting moves in Go tend to be very deep, with at least a dozen layers (Tian & Zhu, 2015; Maddison et al., 2014). A typical architecture has a larger sized first convolutional layer, followed by many 3× 3 convolutional layers. The layers also have hundreds of convolution filters, and like most modern deep convolutional nets use rectified linear units as their nonlinearity (Krizhevsky et al., 2012).
MCTS can incorporate a policy network to provide prior knowledge with equivalent experience into the search tree during exploration (Gelly & Silver, 2007). Again using our earlier terminology, a policy network providing prior knowledge probabilities is a prior policy network. While ideally the prior knowledge is derived
2 Note that what we call exploration can further be split into two parts: the selection of nodes along a path, and expansion
of the search tree.
from a move evaluator (or a Q-network) that computes an approximation to the optimal value of the next position or the optimal action-value of the move, directly using the probabilities of the prior policy network is also a passable heuristic (Ikeda & Viennot, 2013). Intuitively, directly using the probabilities to bias Monte Carlo values makes sense when the probablities of moves are closely and positively correlated with the corresponding optimal action-values. Additionally, it helps that state values and action-values for Go lie in the unit interval [0, 1], so probabilities and values have comparable range.
A knowledge extraction process involves multiple stages. A simple, but typical, process might include processing data, applying a data-mining algorithm, and post-processing the mining results. There are many possible choices for each stage, and only some combinations are valid. The primary goal of the extraction process methodology is to provide support to this, based on data-mining and data processing ontology.
As planning methodology we selected the SADT (Structures Analysis and Design Technique) more precisely the IDEF0 (Integration DEFinition language 0) [7]. This choice is motivated by the fact that it is a graphic language, based on atomic building blocks, provided with methods allowing: (1) a structured decomposition of complex planning problems; (2) the definition and management of transferred data between (sub)problems, the definition of control variables and the allocation of resources needed for its execution; (3) record the decision made and the results; (4) make the model evaluation about its completion, consistency and correctness. One of the most important features of IDEF0 as a modelling concept is that it gradually introduces greater levels of detail through the diagram structure comprising the model. These levels of detail end with atomic functions defined based on a library of primitive functionalities. Defined based on the WEKA Data Mining library [20] for Java and organized following a specific data-mining ontology.
The ontology contains for each operator [1]: a specification of the condition under which the operation can be applied, involving a precondition on the state of the extraction process as well as its compatibility with preceding operation; a specification of the operation?s effects on the extraction process state, on the data, and on the sketch which specify the domain structure; logical group, which can be used to narrow the set of operations to be considered at each stage of the extraction process; predefined schemata for generic problems indexed by a sketch structure associated with its data stream; a help function to obtain online information about each of the operations; a set of rules to be used by an agent to check the extraction process structure consistency.
Figure 4 shows a structural view of the ontology, which groups the extraction operations into six major groups: business understanding; data understanding; pre-processing; data analysis and control; modelling; and post-processing.
The Actias system uses an ontology-based approach on a CASE tool to specify extraction processes using IDEF0 diagrams, following the general framework present on figure 5 [7]. Where the Background Knowledge implements the task ontology, with access supported by the Interaction mechanism, and used to define or change existent extraction processes. It is represented by the logic architecture reflecting the problem refinement. Which, after functional resources have been associated with its primitive tasks, generates the Realisation architecture. If this architecture is valid, i.e. satisfies the agent consistency check, it can be seen as a solution to the data extraction task, and can be executed, producing as result a model or a set of models. On the process of producing this models the extraction process changes the domain sketch specification. These changes are con-
sequence of the possible use of auxiliary structures on the specification process definition and used as on the resulting model preconditions.
The parser produces a set of symbols S and GLIDE produces a set of image patches I , each of which is labelled with a subset of symbols from S. We proceed by extracting a number of features, drawn from a pre-existing set F . The features are derived from the objects in the image patches, after removing the background. This is achieved through a standard background subtraction method—we know that the majority of the image will be occupied by a solid object with a uniform colour, anything else is a background. For instance, in the image patches in Figure 2 (c), the objects are the colourful blocks and the background is the black strips around them. Images containing only or predominantly background are considered noise in the dataset and are discarded. For each symbol s we group the extracted features from each image labelled with s resulting in S lists of Ms tuples with F entries in each tuple, where Ms is the number of images being labelled with s; see Figure 2 (d, left). The data for each feature is normalized to fall between 0 and 1.
The GP model can be used to formally characterize an environmental field as follows: the environmental field is defined to vary as a realization of a GP. Let {Zu}u∈U denote a GP, i.e., every finite subset of {Zu}u∈U has a multivariate Gaussian distribution [8]. The GP is fully specified by its mean µu 4 = E[Zu] and covariance σuv 4 = cov[Zu, Zv] for all u, v ∈ U . We assume that the GP is second-order stationary, i.e., it has a constant mean and a stationary covariance structure (i.e., σuv is a function of u− v for all u, v ∈ U). In particular, its covariance structure is defined by the widelyused squared exponential covariance function [8]
σuv 4 = σ2s exp { −1
2 (u− v)>M−2(u− v)
} + σ2nδuv (1)
where σ2s is the signal variance, σ 2 n is the noise variance, M is a diagonal matrix with length-scale components `1 and `2 in the horizontal and vertical directions of a transect, respectively, and δuv is a Kronecker delta of value 1 if u = v, and 0 otherwise. Intuitively, the signal and noise variances describe, respectively, the intensity and noise of the field measurements while the length-scale can be interpreted as the approximate distance to be traversed in a transect for the field measurement to change considerably [8]; it therefore controls the degree of spatial correlation or “similarity” between field measurements. In this paper, the mean and covariance structure of the GP are assumed to be known. Given that the robot team has collected observations x0, zx0 , x1, zx1 , . . . , xi, zxi over stages 0 to i, the distribution of Zu remains Gaussian with the following posterior mean and covariance
µu|x0:i = µu + Σux0:iΣ −1 x0:ix0:i{zx0:i − µx0:i} > (2)
σuv|x0:i = σuv − Σux0:iΣ −1 x0:ix0:iΣx0:iv (3)
where µx0:i is a row vector with mean components µw for every location w of x0:i, Σux0:i is a row vector with covariance components σuw for every location w of x0:i, Σx0:iv is a column vector with covariance components σwv for every location w of x0:i, and Σx0:ix0:i is a covariance matrix with components σwy for every pair of locations w, y of x0:i. Note that the posterior mean µu|x0:i (2) is the best unbiased predictor of the measurement zu at unobserved location u. An
important property of GP is that the posterior covariance σuv|x0:i (3) is independent of the observed measurements zx0:i ; this property is used to reduce iMASP to a deterministic planning problem as shown later.
3.3 Deterministic iMASP Planning Supposing the robot team starts in locations x0 of leftmost column 0, an exploration policy is responsible for directing it to sample locations x1, x2, . . . , xt+1 of the respective columns 1, 2, . . . , t + 1 to form the observation paths. Formally, a non-Markovian policy is denoted by π 4 = 〈π0(x0:0 = x0), π1(x0:1), . . . , πt(x0:t)〉 where πi(x0:i) maps the history x0:i of robots’ sampling locations to a vector ai ∈ A(xi) of robots’ actions in stage i (i.e., ai ← πi(x0:i)), and A(xi) is the joint action space of the robots given their current locations xi. We assume that the transition function τ(xi, ai) deterministically (i.e., no localization uncertainty) moves the robots to their next locations xi+1 in stage i + 1 (i.e., xi+1 ← τ(xi, ai)). Putting πi and τ together yields the assignment xi+1 ← τ(xi, πi(x0:i)).
The work of [7] has proposed a non-Markovian policy π∗ that selects non-myopic observation paths with maximum entropy for sampling a GP-based field. To know how π∗ is derived, we first define the value under a policy π to be the entropy of observation paths when starting in x0 and following π thereafter:
V π0 (x0) 4 = H[Zx1:t+1 |Zx0 , π] = − ∫ f(zx0:t+1 |π) log f(zx1:t+1 |zx0 , π) dzx0:t+1
(4) where f denotes a Gaussian probability density function. When a non-Markovian policy π is plugged into (4), the following (t+1)-stage recursive formulation results from the chain rule for entropy and xi+1 ← τ(xi, πi(x0:i)):
V πi (x0:i) = H[Zxi+1 |Zx0:i , πi] + V π i+1(x0:i+1)
= H[Zτ(xi,πi(x0:i))|Zx0:i ] + V π i+1((x0:i, τ(xi, πi(x0:i))))
V πt (x0:t) = H[Zxt+1 |Zx0:t , πt] = H[Zτ(xt,πt(x0:t))|Zx0:t ]
(5) for stage i = 0, . . . , t− 1 such that each stagewise posterior entropy (i.e., of the measurements Zxi+1 to be observed in stage i+1 given the history of measurements Zx0:i observed from stages 0 to i) reduces to
H[Zxi+1 |Zx0:i ] = 1
2 log (2πe)k|Σxi+1|x0:i | (6)
where Σxi+1|x0:i is a covariance matrix with components σuv|x0:i for every pair of locations u, v of xi+1, each of which is independent of observed measurements zx0:i by (3), as discussed above. So, H[Zxi+1 |Zx0:i ] can be evaluated in closed form, and the value functions (5) only require the history of robots’ sampling locations x0:i as inputs but not that of corresponding measurements zx0:i .
Solving iMASP involves choosing π to maximize V π0 (x0) (5), which yields the optimal policy π∗. Plugging π∗ into (5) gives the (t+ 1)-stage dynamic programming equations:
V π ∗
i (x0:i) = max ai∈A(xi)
H[Zτ(xi,ai)|Zx0:i ] + V π∗ i+1((x0:i, τ(xi, ai)))
V π ∗
t (x0:t) = max at∈A(xt) H[Zτ(xt,at)|Zx0:t ]
(7)
for stage i = 0, . . . , t− 1. Since each stagewise posterior entropy H[Zτ(xi,ai)|Zx0:i ] (6) can be evaluated in closed form as explained above, iMASP for sampling the GP-based field (7) reduces to a deterministic planning problem. Furthermore, it turns out to be the well-known maximum entropy sampling problem [10] as demonstrated in [7]. Policy π∗ = 〈π∗0(x0:0), . . . , π∗t (x0:t)〉 can be determined by
π∗i (x0:i) = arg max ai∈A(xi) H[Zτ(xi,ai)|Zx0:i ] + V π∗ i+1((x0:i, τ(xi, ai))) π∗t (x0:t) = arg max at∈A(xt) H[Zτ(xt,at)|Zx0:t ]
(8) for stage i = 0, . . . , t− 1. Similar to the optimal value functions (7), π∗ only requires the history of robots’ sampling locations as inputs. So, π∗ can generate the maximum-entropy paths prior to exploration.
Solving the myopic formulation of iMASP (7) is often considered to ease computation (Section 4.1), which entails deriving the non-Markovian greedy policy πG = 〈πG0 (x0:0), . . . , πGt (x0:t)〉 where, for stage i = 0, . . . , t,
πGi (x0:i) = arg max ai∈A(xi) H[Zτ(xi,ai)|Zx0:i ] . (9)
The work of [3] has proposed a non-Markovian greedy policy πM = 〈πM0 (x0:0), . . . , πMt (x0:t)〉 to approximately maximize the closely related mutual information criterion:
πMi (x0:i) = arg max ai∈A(xi) H[Zτ(xi,ai)|Zx0:i ]−H[Zτ(xi,ai)|Zx0:i+1 ] (10) for stage i = 0, . . . , t where x0:i+1 denotes the vector comprising locations of domain U not found in (x0:i, τ(xi, ai)). It is shown in [3] that πM greedily selects new sampling locations that maximize the increase in mutual information. As noted in [7], this strategy is deficient in that it may not necessarily minimize the mapping uncertainty defined using the entropy criterion. More importantly, it suffers a huge computational drawback: the time needed to derive πM depends on the map resolution (i.e., |U|) (Section 4.1).
In the online phase we saved the models for the two bestperforming algorithms on the full dataset to test, examining the K* and RBF regression algorithms. In total we collected 27 sets of test results for K* and 20 for the RBF model. The bulk of these results were collected several months after our initial data collection, indicating some stability of the original data and algorithms.
In a live setting, the difference in how these two algorithms work is very important. As K* is instance-based, it compares each new datapoint to every classified point in the dataset. In contrast, the RBF algorithm learns weights for each of the 172 attributes in the data and must only multiply these weights by the attribute values of a new datapoint, then add these together to predict a position. This is a much faster operation than the entropy calculation for each of the 3110 points in the dataset and makes a substantial difference in a live environment. Initially, K* took 30 to 45 seconds to calculate a position; much too long for real-world applications. In contrast, the RBF model predicts a location almost instantly.
1) Proposed Hybrid Approach: To solve the time problem for K* we decided to break our full dataset into smaller partitions and trained K* classifiers on each partition. Our partitioning can be seen in Figure 4. To determine which partition of the building the user was in, and thus which K* classifier to use to calculate the user’s position, we again looked at machine learning methods, settling on a random forest model [16] which achieved over 96% accuracy in a tenfold crossvalidation. This reduced the number of comparisons from 3110 to about 400 to 500 and substantially increased the speed from 30-45 seconds to 3 seconds: fast enough to be useful in a realworld setting. A hybrid approach such as this may also speed up other instance-based localization systems, which will take too long to be useful with large datasets otherwise.
2) Online, Static Results: We were interested first in how the algorithms would perform with a user standing still at a point in the building. K* achieved accuracies within three meters at every point, with most predictions within one meter of the user’s actual position. The RBF regression algorithm performed similarly in a static online test.
3) Online, In-motion Results: After the static testing, we were interested in the performance of both algorithms in an online, in-motion testbed, mimicking a real-world environment. We collected datasets with the user walking at various paces along the planned route to determine whether speed of movement affected accuracy. We looked at a normal pace of about 1.15 meters per second, a slow pace of 0.75 meters per second, and a quick pace of 1.69 meters per second. We also
Clasificarea distribuită a mesajelor de e-mail
Florin Pop florinpop@cs.pub.ro
Diana Petrescu diana.petrescu@gmail.com
Ştefan Trauşan-Matu trausan@racai.ro
Facultatea de Automatica şi Calculatoare, Universitatea “Politehnica” din Bucureşti Splaiul Independenţei, nr.313, Bucureşti, Sector 6, 060042 România
also be captured. Preferences can be explicitly and declaratively expressed, and the resulting programs can be compiled into usual answer set programs with their usual stable model semantics (cf. (Brewka 2004) and references therein).
Here we briefly outline how weak program constraints (Buccafurri et al. 2000; Leone et al. 2006) declaratively capture the kind of preferences that address our needs. (Cf. (Brewka 2004) for connections between preferences in logic programs and weak constraints.).
Example 6 Consider Example 6.2, where (P1, less, P2) ∈ Trust is captured by the non-disjunctive repair rule
R1 (x , y , t)← R2 (x , y , t?),R1 (x , y , f?), x 6= null, y 6= null· The same effect, and more, could be obtained by uniformly using disjunctive rules followed by appropriate weak constraints. In this case,
R1 (x , y , t) ∨ R2 (x , y , f) ← R2 (x , y , t?),R1 (x , y , f?), x 6= null, y 6= null · ⇐ R2 (x , y , f)· (A7)
Here, the weak constraint (A7) expresses a preference for the stable models of the program that minimize the number of violations of the condition expressed its body, in this case, that, when restoring the satisfaction of the DEC ∀x∀y(R2(x , y) → R1(x , y)), the tuple R2(x , y) is not deleted. These weak constraints are used by a peer P to ensure that, if possible, the tuples in the peers that it trusts more than itself are not modified. 2
If the original solution program has solutions, then the new program would have the same solutions. However, the latter could have solutions when the former does not. This would make the semantics of the system more flexible with respect to unsatisfiable trust requirements. It is also clear that the weak constraints could be easily derived from the trust relationships and the DECs. The solution program with weak constraints can be run in the DLV system (Leone et al. 2006) to obtain the solutions and peer consistent answers of a peer.
Notice that the new repair programs, except for the weak program constraints, are now of the same kind as those for specifying repairs of single databases with respect to local ICs (Bravo and Bertossi 2006). Actually, if in the new program the weak program constraints are replaced by (hard) program constraints, e.g. (A7) by ← R2 (x , y , f), the solutions coincide with those of the programs in Definition 6.1. We should mention that in (Arenas et al. 2003), weak constraints were used, as a part of a repair program, to specify the preference for cardinality repairs, i.e. repairs that minimize the number of tuples that are inserted or deleted to restore consistency, as opposed to minimality (with respect to subset-inclusion) of sets of inserted/deleted tuples.
Intuitively, the KoP states that if a particular fact ψ is a necessary condition for an agent to perform an action α , then the agent must in fact know ψ in order to act. In other words, knowing ψ is also a necessary condition for performing the action. We formalize the claim and prove it as follows. We say that ψ is a necessary condition for doesi(α) in R if (R,r, t) |= doesi(α) holds only if (R,r, t) |= ψ , for all (r, t) ∈ Pts(R). Clearly, the customer’s good credit is a necessary condition for the ATM dispensing cash. That is, suppose that a bank makes use of a correct implementation of an ATM protocol, which satisfies the credit requirement. Then, in the system R consisting of the set of all possible histories (runs) of the bank’s (and the ATM’s) transactions, good credit is a necessary condition for receiving cash from the ATM.
It is often of interest to consider facts whose truth depends only on a given agent’s loca state. Such, for example, may be the receipt of a message, or the observation of a signal, by the agent. Whether x = 0 for a local variable x, for example, would be a natural local fact. Moreover, if an agent has perfect recall, then any events that it has observed in the past will give rise to local facts. Finally, since knowledge is defined based on an agent’s local state, then a fact of the form Kiϕ constitutes a local fact. Indeed, there is a simple way to define the local facts above using knowledge. Namely, we say that ϕ is i-local in R if R |= (ϕ ⇒ Kiϕ).
The formalism of [10] defines protocols as explicit objects, and defines contexts that describe the possible initial states and the model of computation. This provides a convenient and modular way of constructing systems. Namely, given a protocol P and a context γ , the system R = R(P,γ) is defined to be the set of all runs of protocol P in γ . The runs of this system embody all of the properties of the context, as they arise in runs of P. This includes, for example, any timing assumptions, possible values encountered, possible topologies of the network, etc. They also embody the relevant properties of the protocol, because in all runs considered possible the agents follow P.
In this paper, we do not define protocols and contexts. Rather, we treat the KoP in a slightly simpler and more abstract setting. We say that an action α is a conscious action for i in R if i’s local state completely determines whether i performs α . If its local state at two points (r, t) and (r′, t ′) of R is the same, then (R,r, t) |= doesi(α) iff (R,r′, t ′) |= doesi(α). Conscious actions are quite prevalent in many systems of interest. For example, suppose that agent i follows a deterministic protocol, so that its action at any given point is a function of its local state. If, in addition, agent i is allowed to move at every time step, then all of its actions are conscious actions. We remark that, since conscious actions depend on an agent’s local state, then if α is conscious for i in R then (R,r, t) |= doesi(α) holds iff (R,r, t) |= Kidoesi(α) does, for all (r, t) ∈ Pts(R).
We are now ready to prove a formal version of the KoP: Theorem 3.1 (The KoP Theorem). Let α be a conscious action for i in R. If ψ is a necessary condition for doesi(α) in R, then Kiψ is also a necessary condition for doesi(α) in R.
Proof. We will show the contrapositive. Let α be a conscious action for i in R, and assume that Kiψ is not a necessary condition for doesi(α) in R. Namely, there exists a point (r, t) ∈ Pts(R) such that both (R,r, t) |= doesi(α) and (R,r, t) 6|= Kiψ . Given the latter, we have by the definition of ‘|=’ for Ki that there exists a point (r′, t ′) ∈ Pts(R) such that both (r′, t ′) ≈i (r, t) and (R,r′, t ′) 6|= ψ . Since α is a conscious action for i in R and (R,r, t) |= doesi(α) we have that (R,r, t) |= Kidoesi(α). It follows from (r′, t ′) ≈i (r, t) by the definition of ‘|=’ for Ki that (R,r′, t ′) |= doesi(α) holds. But since (R,r′, t ′) 6|= ψ , we conclude that ψ is not a necessary condition for doesi(α) in R, establishing the countrapositive claim.
Theorem 3.1 applies to all multi-agent systems. It immediately implies, for example, that a necessary condition for the ATM to dispense cash is Katm(good credit). The theorem is model independent; it does not depend on timing assumptions, on the topology of the system (even on whether agents communicate by message passing or via reading and writing to registers in a shared memory), or on the nature of the activity that is carried out. For every necessary condition for a conscious action, knowing that the condition holds is also a necessary condition.
The authors wish to thank Holger Moch and Peter Schraml for their help in conducting the RCC TMA project, Peter Wild and Peter Bode for annotating the medical data and Monika Bieri and Norbert Wey for scanning and tiling the TMA slides. Special thanks also to Volker Roth and Sudhir Raman for valuable discussions. We also acknowledge financial support from the FET program within the EU FP7, under the SIMBAD project (Contract 213250).
17 Preprint, June 29, 2010
A supplementary definition for outlier (Hsiao et al., 2008)
Definition 1: An outlier is an observation with a degree greater than a threshold in comparison
with other observations referred to or associated with one or more specified pattern.
Problem 1: Given a time series data S1 as Fig. 1, suppose data match linear model, find out outliers. S1 = (3.1, 2.9, 2.85, 3, 3.05, 2.9, 3.2, 5.2, 8.5, 5.4, 5.3, 5.1, 3.1, 3.05, 3, 2.99, 3, 3.02, 3.2)
Fig.1. Regression lines for dataset S1
0
2
4
6
8
10
0 2 4 6 8 10 12 14 16 18 20
data LS LTS
By classic LS regression, we get a line y=3.79-0.001x; and by robust LTS regression, line
y=2.907+0.007x, as shown in Fig. 1. Based on the above definition, we present RDD (פ) algorithm.
Given a time series data S = d1d2……dN, denote point (i,di) by pi, and  pjpipk by  jik.
Relative Deviant Degree: Algorithm 1 Input: time series data S Output: RDDs of S
• Specify a similarity function sim(pk,(pi,pj))=exp(- jik^2/50), denoted by ij ks .
• Specify an offset function off(pk,(pi,pj)): distance of k to line Lij, denoted by ij ko .
for ( i from 1 to N ) {
   
 N
ip p
N
k
ip
ki sX 1 1
   
 N
ip p
N
k
pi
ki sY 1 1
}
for( i, j from 1 to N, j ≠ i )
wij = Xi  Yj











 N
ji
ij
N
ji
ij
ij k
N
ji
ij
N
ji
ij
ij k
k
w
wo
w
ws
RDD
1,
1,
1,
1,
)()(
ln
Notes
1. Similarity function is used to evaluate the similar degree of point k related to the system of i
and j.
2. Offset function calculates the real distance between k and system of i and j. 3. wij is the weight of system of ij, denotes relative effective degree. 4. Relative deviant degree combines two measurements: similarity and real distance.
By this algorithm, to problem 1 we get RDD values in Table 1. Points at position 8, 9, 10, 11 and 12 have high values.
Though the פ algorithm, similar with robust regressions, tries to evaluate others by choosing reliable “good” data, it differs from robust ones at the point that we approach the problem from the inside structure of patterns and stress on a whole effect, thus with better balance. Note that the פ algorithm is with a highest breakdown point (50%) as same as the robust regressions. Now we modify our algorithm 1 to a general form. First, we introduce the definition of view.
Definition 2: Given a time series data S, a sub-series of S is also a time series where each element of it is an element of S. We denote e  S when e is a sub-series of S.
Definition 3: A view of S, Vα, is a set of sub-series of S of the same length  , that is Vα = { e | e S, |e| =  }.
Algorithm 1’: פ Algorithm
Input: data set S Output: RDDs of S
1. Let Vα be an evaluating view, e  Vα 2. Give a similarity function sim: S×S α  [0,1], describing similar degree of each
element k in S by the view of Vα, denoted by
3. Give an offset function off: S×S α  , describing offset of each element k in S by
the view of Vα, denoted by
4. Define different weight we and RDDk by and 5. For any element e in Vα

0 R
e
koff
e ksim
e ksim e koff
calculate we
6. Calculate RDDk
Although the challenge of playing text-based games was not take on often, there are several attempts described in the literature, mostly based on the MUD games rather than the classic IF games.
Adventure games has been carefully revised as the field of study for the cognitive robotics in [14]. First, the authors identify the features of “tradition adventure game environment” to point-out the specifics of the domain. Second, they enumerate and discuss existing challenges, including e.g. the need for commonsense knowledge (its learning, revising, organization, and using), gradually revealing state space and action space, vague goal specification and reward specification.
In [13], the agent able to live and survive in an existing MUD game have been described. The authors used layered architecture: high level planning system consisting of reasoning engine based on hand-crafted logic trees, and a low level system responsible for sensing the environment, executing commands to fulfill the global plan, detecting and reacting in emergency situations.
While not directly-related to playing algorithms, it is worth to note the usage of computational linguistics and theorem proving to build an engine for playing text-based adventure games [16]. Some of the challenges are similar for both tasks, as generating engine requires e.g. object identification (given user input and a state description) and understanding dependencies between the objects.
The approach focused on tracking the state of the world in text-based games, and translating it into the first-order logic, has been presented in [17]. Proposed solution was able to efficiently update agent’s belief state from a sequence of actions and observations.
The extension of the above approach, presents the agent that can solve puzzle-like tasks in partially observable domain that is not known in advance, assuming actions are deterministic and without conditional effects [18]. It generates solutions by interleaving planning (based on the traditional logic-based approach) and execution phases. The correctness of the algorithm is formally proved.
Recently, an advanced MUD playing agent has been described in [19]. Its architecture consists of two modules. First, responsible for converting textual descriptions to state representation is based on the Long Short-term Memory (LSTM) networks [20]. Second, uses Deep Q-Networks [11] to learn approximated evaluations for each action in a given state. Provided results show that the agent is able to to successfully complete quests in small, and even medium size, games.
Activity recognition has been a popular research direction in robotics. Many activity datasets are openly available for activity recognition research ranging from datasets consisting of simple repetitive action such as jumping, walking etc. (e.g., []) to more complex activities such as cooking food, cleaning microwave etc. (e.g., []). Most available datasets are can be categorised into three categories: heterogeneous actions, specific actions and others Chaquet et al. (2013). Our dataset falls in the ’activities of daily living’ subcategory of the ’specific actions category’. Early activity datasets, such as the Weizmann datasets Zelnik-Manor and Irani (2001); Blank et al. (2005), captured many instances of various activities which were scripted, recorded in a controlled environment and would rarely be observed naturally. A more natural environment was presented in the KTH dataset Schuldt et al. (2004) and the Caviar dataset 1, but the activities presented in these datasets were limited to heterogeneous and repetitive motion such as walking, jumping, running etc., aiming to model the repetitive nature of short activities. UCF datasets (UCF 101, UCF Youtube Dataset) Soomro et al. (2012); Liu et al. (2009) and Hollywood-2 dataset Marszałek et al. (2009) were some of the first datasets aimed to model everyday activities in a natural setting such as eating, hugging, kissing etc. from a large number of naturally recorded instances. These datasets were mostly generated from movies or YouTube videos. However, videos in these datasets, though longer than previous datasets, were still short activities where each video only comprised of one basic activity thus temporal segmentation was assumed. Duckworth et al. (2017) collected a dataset comprising of long videos of human activities recorded by a mobile robot in a kitchen setting. Videos were continuous and comprised of multiple activities. However, the annotations of this dataset were for segmented short actions such as pick up, put down, pour etc. The Cornell Activity Datasets (CAD-120 and CAD60) Koppula et al. (2013); Sung et al. (2012) presented challenging and datasets consisting of longer videos than before and with complex activities such as stacking boxes, taking medicine etc. Though this was a benchmarking dataset, it did have some shortcomings. The videos in this dataset only consisted of a single actor and the annotations provided were limited to two levels of granularity: highlevel activities per video and sub-level actions within each high-level activity video. In all above mentioned datasets, annotations for activity labels were consistent, extracted from pre-set label lists and collected with a set of welldefined rules in place. The CLAD dataset presented in this paper is created with particular focus to address some of these shortcomings. The dataset presented here consists of much longer activities for example service at a restaurant, consisting of multiple actors, contains deep hierarchy of activities and has crowd-sourced freely descriptive annotation i.e. annotators were given the freedom to describe the activities using their own words. Given the advancement towards long-running autonomous robots gathering longer videos of activity data, using crowdsourcing to elicit annotations is becoming popular and therefore such annotations are included in our dataset. To our knowledge, this is the only dataset that presents multiple new challenges to the research community namely, modelling longer naturally occurring and variable activities with many levels of granularity as well as semantic analysis and extraction of information from crowdsourced annotations.

Given the result that rf, svmRadial, and gbm are likely the best performing algorithms, it is interesting to explore further variations and different implementations of these algorithms. We discussed that more modern implementations the algorithms may alter the timing results. But variations of the algorithms may also alter the ranking and the apparent equivalence of the three algorithms. Variations of Random Forest, such as Rotation Forest (Rodriguez et al., 2006), Extremely Randomized Forest (Geurts et al., 2006), random forest of conditional inference trees (Hothorn et al., 2006; Strobl et al., 2007), should be compared with the standard algorithm. Least square SVM (Suykens and Vandewalle, 1999) should be compared with the standard SVM, and different models of boosting such as AdaBoost, LogiBoost, BrownBoost, should be compared with gbm.
There has been a large number of published research on different methods and algorithms for hyperparameter selection in RBF SVM, but almost no research in hyperparameter selection for Random Forests and Gradient Boosting Machines. Hyperparameter selection is a very important and computationally expensive step in selecting an algorithm for a particular problem, and further understanding of how to improve this process is needed, specially for those two algorithms.
Pattern recognition and classification is one of the most challenging ongoing problems in computer science in which we seek to classify objects within an image into categories, typically with considerable variation among the objects within each category. With invariant pattern recognition, we seek to develop a model of each category that captures the essence of the class while compressing inessential variations. In this manner, invariant pattern recognition can tolerate (sometimes drastic) variations within a class, while at the same time recognizing differences across classes that can be minute but salient. One means of achieving this goal is through invariant feature extraction [1], where the image is transformed into feature vectors that may be invariant with respect to a set of transformations, such as displacement, rotation, scaling, skewing, and lighting changes. This method can also be used in a hierarchical setting, where subsequent layers extract compound features from features already extracted in lower levels, such that the last layer extracts features that are essentially the classes themselves [2]. Most of these existing methods have one thing in common: they ar X iv :1 60 3.
08 23
3v 1
[ cs
.C V
] 2
7 M
ar 2
01 6
achieve invariance either by applying transformations to the image when searching for the best match, or by mapping the image to a representation that is itself invariant to such transformations.
In contrast to these “passive” methods where transformations are applied to the image, we propose an active, attention-based method, where a virtual camera roams over and focuses on particular portions of the image, similar to how our own brain controls the focus of our attention [3]. In this case, the camera’s actions are guided by what the camera finds in the image itself: In essence, the camera searches the image to discover features that it recognizes, creating in the process a time series of experiences that guides further movements and eventually allows the camera to classify the image. We call this camera an “active categorical classifier,” or ACC for short.
Broadly speaking, the problem of classifying a spatial pattern is transformed into one of detecting differences between time series, namely the temporal sequence that the virtual camera generates in its sensors as it navigates the image. The method we propose here is inspired by models of visual attention [4], where attention to “salient” elements of an image or scene is guided by the image itself, such that only a small part of the incoming sensory information reaches short-term memory and visual awareness. Thus, focused attention overcomes the information-processing bottleneck imposed by massive sensory input (which can easily be 107 − 108 bits per second in parallel at the optic nerve [4]), and serializes this stream to achieve near-real-time processing with limited computational requirements.
In previous work, we have shown that it is possible to evolve robust controllers that navigate arbitrary mazes with near-perfect accuracy [5] and simulate realistic animal behavior [6]. Independently, we have shown that we can evolve simple spatial classifiers for hand-written numerals in the MNIST data set [7]. Here we use the same technology to evolve active categorical classifiers that “forage” on images and respond to queries about what they saw in the image without needing to examine the image again.
Typical simplified schematic of vasculature segmentation pipeline used to process two-photon microscope stacks is shown in figure 2.1. The image stacks suffer mainly from photon noise following a Poisson distribution [10] (i.e. the noise intensity depends on the underlying signal) with some Gaussian noise component added, which can be denoised directly with methods developed for Poisson noise (e.g. PURE-LET [128]). Alternatively the signal-dependency of the Poisson noise can be removed with a suitable transform such as Anscombe transform [135] that allows one to use denoising methods developed for Gaussian noise (e.g. BM3D/BM4D [132, 36]). Deconvolution is not done as commonly for multiphoton microscopy as compared to confocal microscopy [141], but if it is been done in can be done jointly with other image restoration operations [160] or as its independent step [92]. This part can be seen as the image restoration part with an attempt to recover the “original image” as well as possible corrupted by the imaging process.
In some cases the restored image is further simplified using some edge-aware smoothing operator such as anisotropic diffusion [139, 165], or as done by Persch et al. [160] who jointly apply the anisotropic diffusion inpainting (operation that attempts to replace lost or corrupted parts of the image data) with deconvolution and interpolation.
This step is followed by some “vesselness filter” or “vesselness enhancement” filter that is designed to enhance tubular structures such as vessels in the image. The best known filter of those is the Frangi’s filter [49] that has become outdated as it cannot properly handle crossings nor bifurcation methods, and several filters [109, 226, 198, 62, 144] have been proposed to correct the shortcomings of Frangi’s filter with none of them reaching a de facto standard status.
In our proposed deep learning-based network we are trying to replace the vessel enhancement and segmentation steps, and keep still using "traditional" filters with the im-
age restoration part (see discussion on how to get upgrade them as well in 5.1). There have been various "traditional" segmentation algorithms for vessel segmentations (for reviews see [97, 116]), and only the most relevant ones are analyzed here below.
In the schematic (figure 2.1) z-interpolation is placed after the segmentation, but it might have been placed as well before the segmentation algorithm [121, 228], or jointly with other image restoration operators [160]. The exact placing of the interpolation depends on the computation before and after it, but in our case we placed in the end to emphasize the gains of z-direction interpolation to mesh reconstruction as all our stacks used in this work are anisotropic (see table 1). Reconstructing meshes from noninterpolated anisotropic stacks with traditional Marching Cubes algorithm [117] typically leads to “staircasing effect” of the mesh while interpolation gives smoother reconstruction. Advanced mesh reconstruction methods are beyond the scope of this algorithm, but there have been efforts to improve biomedical mesh reconstruction [145, 177] mitigating the problems of triangulation based such as Marching Cubes. With the reconstructed vasculature mesh, it is then possible to for example do morphological analysis [140], calculate hemodynamic parameters [90], or analyze the functional diameter changes in response to external stimulus [121].
To the knowledge of the authors, deep learning frameworks including ConvNets have not yet been applied to segmentation of three-dimensional volumetric vasculature images. Despite the limited use of machine learning techniques in VESSEL12 challenge for lung vessels [175], there have been some work using machine learning techniques for vessel segmentation. Sironi et al. [196] for example used an unsupervised dictionary learning [103] approach that learned optimal separable convolutional filter banks for 2D vasculature segmentation (DRIVE dataset [203]), and for 3D olfactory projection fibers (DIADEM challenge [16]). The filter banks were then used with the popular Random Forests classifier [15] continuing the previous work from the same lab [58, 170]. The authors used their separable filter banks with ConvNets for image classification task but did not discuss about the possibility of using ConvNets with the image segmentation task. Very recently Maji et al. [134] applied ConvNets for the two-dimensional vasculature DRIVE database with promising performance.
Santamaria-Pang et al. [178] similarly used a dictionary learning approach to learn linear filters for detection of tubular-like structures from multiphoton microscopy stacks. The learned filters were fed to a Support Vector Machine (SVM, [210]) which was shown to provide a better segmentation accuracy compared to the vesselness filter introduced by Sato et al. [180]. Recently, Schneider et al. [185] used Random Forests for classification with multivariate Hough forests to infer probabilistic votes about the vessel center, jointly segmenting vasculature and extracting vessel centerline. The features were learned using steerable filter templates ([80]) at multiple scales instead of the dictionary learning approach. They showed that their learning-based approach outperformed both Oriented Optimal Flow (OOF, [109]) and Frangi’s filter [49] for vessel segmentation.
Sironi et al. [197] take a different approach in their paper
inspired by recent work on structured learning-based edge detectors ([39]). They combine structured learning with nearest neighbor-based output refinement step designed for situations where edges or thin objects are hard to detect explicitly by the neural network ([53]). They were able to reduce spatial discontinuities, isolated erroneous responses and topological errors of initial score maps from outputs of other algorithms, and when directly trained to segment two-dimensional blood vessels (DRIVE dataset [203]).
There is relatively more work devoted on natural image processing compared to biomedical image analysis. In natural image processing literature, the corresponding application to our biomedical image segmentation is semantic segmentation [125, 155, 24, 23], also referred as scene parsing [163] or scene labeling [45]. Semantic segmentation with natural images tries to answer to the question “What is where in your image?” for example segmenting the “driver view” in autonomous driving to road, lanes and other vehicles [89]. In typical semantic segmentation tasks there are a lot more possible labels than in our two-label segmentation of vessels and non-vessel voxels, further complicating the segmentation.
Most existing biomedical segmentation pipelines start with slice-by-slice two-dimensional processing for volumetric stacks, and only later transition to three-dimensional processing due to high computational cost of fully threedimensional pipelines [123, 214]. ConvNets with 3D filters had been used for example with block face EM images before [67], most of the 3D filter use being employed in video processing [83, 220, 243] where the 2D image with the time can be viewed as an anisotropic 3D image. Due to everincreasing computational performance in local GPU clusters, and cloud-based services such as Amazon AWS, IBM Softlayer, Microsoft Azure and Google Cloud Platform we expect to see more purely three-dimensional approaches such as the one proposed by Kamnitsas et al. [86] for brain lesion segmentation from MRI images.
Deep learning based approaches have been extensively used for volumetric electron microscopy (EM) segmentation [72, 133, 236, 114, 173]. Other biomedical image segmentation tasks with deep learning frameworks include for example brain segmentation [64, 129, 85, 205], prediction of Alzheimer’s disease from magnetic resonance imaging (MRI) scans [158], microscopic cell segmentation [102], glaucoma detection [26], computational mammography [40], pancreas segmentation [40], bi-ventrical volume estimation [251], and carotid artery bifurcation detection
[252] The use of deep learning neural networks is not limited to image analysis, and it can employed in various fields that can benefit from data-driven analysis in exploratory or predictive fashion. In neuroscience, in general the datasets are getting increasingly larger and more complex requiring more sophisticated data analysis tools [174]. There have been systems capable of constructing theories automatically in data-driven fashion [55]. Artificial neural networks lend themselves well for modeling complex brain function that emerge from activation of ensembles of neurons in which the studying of single neuron at a time is not sufficient [174].
For example, the circuit architecture of the mammalian hippocampus have been modeled to consist of series of sequential feedforward and recurrent neural networks [172]. Harvey et al. [63] used two-photon imaging to measure the calcium activity of mouse making behavioral choices in virtual maze. The temporal trajectory of neuron populations was shown to be predictive of the behavioral choice, thus being suitable for the use of recurrent neural networks to model the behavior. In addition to basic neuroscience, deep learning “expert systems” have been extended to clinical settings [232] for example for predicting clinical outcomes of radiation therapy [87], electroencephalographic (EEG) recording analysis [204], and future disease diagnosis and medicine prescription in routine clinical practice [29].
TFD results are shown in Table 1. For both TFDs1 and TFDs2, we analyzed both single and dual-sided transforming KNN. Results are compared against augmented KNN and regular KNN. Transforming KNN brings 6 – 10 % performance increase for TFDs1 and about 25 % performance increase for TFDs2. Augmented KNN also brings some amount of accuracy increase, but not as high as transforming KNN. This indicates that the augmented database does not cover as much as volume as the transforming distance, at least when the database is expanded tenfold.
The performance differences between TFDs1 and TFDs2 correspond to our analysis in Section 4.1. Accuracies in every column is higher in TFDs2 than in TFDs1. This is probably due to the fact that in TFDs2, every test image has at least two corresponding images in the KNN database, while in TFDs1, about half of the test images only have one. The relative performance increase is higher in TFDs2 than in TFDs1. This is probably because that in TFDs2, expression transformation was learned for every identity, while in TFDs1, this was done only on about half of the identities.
NORB results are given in Table 2. With a cross-validated K value, the SmallTrans KNN has a 4% performance increase over regular KNN. We are not surprised by the marginal improvement of the transforming KNN. This is because the NORB training set contains images of the same object taken from different azimuths (every 20◦), which already provides some degree of rotation invariance. Therefore SmallTrans and AnyTrans can only do slightly better, if they provide a finer rotation invariance, say rotations under 20◦ difference, for some cases. However, the rotation invariance afforded by the training set disappears when there are less examples in the database, which is further illustrated in Fig. 5 and discussed in Section 4.4.3.
Fig. 4 shows KNN accuracy with respect to different K values. For TFD, we can see that both regular KNN and transforming KNN degrade as K increases. This is mainly due to the lack of examples in its KNN database: if K is large, number of noise examples will be larger than true examples even the true examples might have higher similarities, which is also aggravated by the identity-imbalanced nature of TFD. In comparison, augmented KNN is the most robust to larger settings of K, due to the richness of same-identity examples in the KNN database.
For NORB, regular KNN accuracy decreases with increasing K. This is because without transforming distance, a test image should only be close to examples with the same class label and, with equal importance, the orientation in the KNN database. Therefore, test images cannot utilize many exam-
ples to provide effective distances. On the contrary, transforming KNN can utilize training examples regardless of their orientation, which results in an increasing robustness with increasing K.
Partial support for this research was received by the Spanish Government’s DGICYT research project: FFI2011-23238, “Innovation in scientific practice: cognitive approaches and their philosophical consequences” and by the Russian Ministry of education and science (agreement: 14.612.21.0001, ID: RFMEFI61214X0001).
One-Class Classification (OCC) [1] is a special case of general classification problem where the data of the positive (target) class is sufficiently available, whereas the negative class data is absent during the training time. This deficiency of data for the negative class may arise due several reasons, such as the difficulty in obtaining the data, higher cost in data collection or the rarity of their occurrence. Some examples for OCC are machine fault diagnosis, fraud detection, human fall detection, identifying rare disease. In most of these applications, it is easy to collect samples for the positive (target) class or the data describing the normal behaviour of the domain under consideration. However, the collection of negative samples may result in high cost in dollars, put health and safety of a person in danger. In other cases, the data in the negative class occurs rarely. Therefore, even if some samples are collected for the negative class, the training data will be severely skewed and it is difficult to build generalizable classifiers using the traditional supervised classification algorithms.
When the data for the negative class is too few or absent, a normal practice is to generate artificial data for the negative class [2] and convert the classification problem as a binary classification problem. Another possibility is to estimated the parameters of the distribution of negative class based on varying the parameters of the positive class [3]. However, these techniques heavily depend on the choice of parameters for the distribution of the positive class or the unseen negative class. The literature on OCC offers several alternatives to build a classification boundary based on the positive samples only [4]. In this paper, we choose a one-class nearest neighbour (OCNN) approach for detecting unseen samples of the negative class, when they were not present during the training phase. In principle, an OCNN method finds the high and low density regions based on the local neighbourhood of a test sample. Using a decision threshold, an OCNN accepts or rejects a test sample as a member of the target class. In its simplest form, an OCNN finds the 1st nearest neighbour of a test sample in the target class and then finds the 1st nearest neighbour of this neighbour in the target class. If the ratio of these distances is lower than a user-defined threshold, it is accepted as a member of the target class [5]. This method is simple and effective in finding instances of the unseen negative class; however, it is very sensitive to the
noise in the target class. This method may also be not effective when the dimension of the data is very high because of the problems associated with the euclidean distance metric. In an OCNN, the number of nearest neighbours and the value of decision threshold can be optimized but the sensitivity of the classifier w.r.t. noise in the positive class may change. There exists several variants of OCNN in the literature (see Section 2); however, these methods do not make relation between the other OCNN methods and do not clearly explain the reasons as to why a particular variant works better than the other.
A classifier ensembles is an approach to improve the performance of a classifier [6, 7] Classifier ensembles consist of many classifiers; the final result of a classifier ensemble depends on the combined results of individual classifiers of the ensemble. A classifier ensemble generally performs better than individual classifiers provided the member classifiers are accurate and diverse. As supervised K-Nearest Neighbour (KNN) classifiers are very robust to variations of the data set; therefore, ensemble techniques based on training data sampling (such as Bagging [8] and Boosting [9]) have not been successful to create their ensembles [10]. KNN classifiers are found sensitive to input features sampling; therefore, diverse NN classifiers can be created by using different feature space for different KNN classifiers to generate an accurate ensemble [10].
In this paper, we present a holistic view on the general problem of OCNN by discussing their different variants and creating accurate their ensembles. The main contributions of the paper are:
• We theoretically show the equivalence and relationship between various OCNN approaches and discuss the impact of choosing nearest neighbours on the decision threshold.
• We present two different types of ensemble methods – random subspace and random projection for the OCNN to study their performance when the feature space is changed. To the best of our knowledge, the suitability of random projection approach is investigated for the first time for OCNN.
• We present present a cross-validation method and a modified thresholding algorithm that utilize the outliers from the target class to optimize the parameters of the OCNN.
Our results on several benchmark and domain-specific real world data shows superior performance of the OCNN with random projection ensembles and give strong evidence that single OCNN may not be the right choice to detect unseen negative data during testing.
The rest of the paper is structured as follows. In Section 2, we present literature review on various variants of OCNN and their ensembles. Section 3 introduces two variants of OCNN that uses different numbers of nearest neighbours and decision threshold, followed by a theoretical analysis about their relation with other variants of OCNN and a brief discussion on various ensemble approaches that are used with the OCNN classifiers. In Section 4, we introduce a cross-validation method and a modified thresholding algorithm to optimize parameters for the OCNN classifiers using only the data from the target class. Experimental results are shown on various datasets in Section 5. Conclusions and future work are summarized in Section 6.
ar X
iv :1
60 2.
08 25
4v 1
[ cs
.D S]
2 6
Fe b
20 16
Theoretical Analysis of the k-Means Algorithm
– A Survey
Johannes Blömer∗ Christiane Lammersen† Melanie Schmidt‡
Christian Sohler§
February 29, 2016
The k-means algorithm is one of the most widely used clustering heuristics. Despite its simplicity, analyzing its running time and quality of approximation is surprisingly difficult and can lead to deep insights that can be used to improve the algorithm. In this paper we survey the recent results in this direction as well as several extension of the basic k-means method.
medical devices. In order to better facilitate the information offered by them, they need to automatically react to the intra-operative context. To this end, the progress of the surgical workflow must be detected and interpreted, so that the current status can be given in machine-readable form. In this work, Random Forests (RF) and Hidden Markov Models (HMM) are compared and combined to detect the surgical workflow phase of a laparoscopic cholecystectomy. Various combinations of data were tested, from using only raw sensor data to filtered and augmented datasets. Achieved accuracies ranged from 64% to 72% for the RF approach, and from 80% to 82% for the combination of RF and HMM.
Keywords: Surgical Data Science, Random Forest, Hidden Markov Model

In this section, we perform a more in-depth analysis using examples from both EN-RU and EN-DE pairs, in order to have a deeper understanding of the proposed algorithm and its remaining limitations. We only perform greedy decoding to simplify visualization.
EN→RU Results are shown in Fig 8. Since English and Russian are the both Subject-VerbObject (SVO) languages, the corresponding words may share the same order in both languages, which makes simultaneous translation easier. It is clear that the larger the target delay (AP or CW) is set, the more words are read before translating the corresponding words, which in turn results in better
The cos t of thecam pai
gn
isbas ica
lly bei ng pai d by mysal ary as a sen -- ato r . < e os >
< eos > .
deckt ge-ator
Sen-als alt
Geh-mein
durch genommen Grunde
im werden
Kampagne die für
Kosten Die READ
WRITE
The cos t of thecam pai
gn
isbas ica
lly cov ere d by my sal ary as a sen -- ato r . < e os >
< eos > .
deckt ge-ator Sen--
als alt
Geh-mein durch
genommen Grunde
im werden
Kampagne die für
Kosten Die READ
WRITE
translation quality. We also note that very early WRITE commonly causes bad translation. For example, for AP=0.3 & CW=2, both the models choose to WRITE in the very beginning the word “The”, which is unreasonable since Russian has no articles, and there is no word corresponding to it. One good feature of using NMT is that the more words the decoder READs, the longer history is saved, rendering simultaneous translation easier.
DE→EN As shown in Fig 1 and 7 (a), where we visualize the attention weights as soft alignment between the progressive input and output sentences, the highest weights are basically along the diagonal line. This indicates that our simul-
taneous translator works by waiting for enough source words with high alignment weights and then switching to write them.
DE-EN simultaneous translation is likely more difficult as German often uses Subject-ObjectVerb (SOV) constructions. As shown in Fig 1, when a sentence (or a clause) starts the agent has learned such policy to READ multiple steps to approach the verb (e.g. serviert and gestorben in Fig 1). Such policy is still limited when the verb is very far from the subject. For instance in Fig. 7, the simultaneous translator achieves almost the same translation with standard NMT except for the verb “gedeckt” which corresponds to “covered” in NMT output. Since there are too many words between the verb “gedeckt” and the subject “Kosten für die Kampagne werden”, the agent gives up reading (otherwise it will cause a large delay and a penalty) and WRITEs “being paid” based on the decoder’s hypothesis. This is one of the limitations of the proposed framework, as the NMT environment is trained on complete source sentence and it may be difficult to predict the verb that has not been seen in the source sentence. One possible way is to fine-tune the NMT model on incomplete sentences to boost its prediction ability. We will leave this as future work.
The visual analysis is based on the image number 16 of the DS2 test stack (see Section 2.2).
62
Explanation to the labels in Figure 7.2:
The visual results of Softmax training (a to c) have thicker membrane labels than on DS1. This is because the error masking was enabled here and, as a result, the network has seen the same amount of error pixels for both foreground and background.
(A) A bright spot, which is an error in the electron microscopy image. The membrane affected by it is misclassified by all networks and trainings. The local contrast enhancement with CLAHE did not help here.
(B) Mitochondrion with close proximity parallel to a cell membrane. The USK network removes the membrane with all trainings (c, f and i). U-Net (b, e and h) performs a little better, but still merges the two cells. Only the SK network does it correctly (a, d and g), but has some uncertainty on the mitochondrion instead (d).
(C) Diffuse section of a cell membrane. This is not an issue on most training/architecture combinations, except for U-Net with Softmax (b and h).
(D) Oriented structures, even when faint, are partially labeled as membrane when sharp enough and of similar thickness as the membrane. This is no issue when isolated within the cell and not cutting through a cell that should be connected. With convolutional networks, this is hard to impossible to label correctly. It would require more high level knowledge of the object, such as if the predicted membrane is enclosing a cell or not.
63
64
(E) Diffuse mitochondrion. The same situation as with B applies. Especially Uand USK-Net with Softmax training (b and c) get it wrong.
(F) Diffuse cell interior that is similar to the membrane texture. All networks see a membrane connection through this area. In the training data there are some examples of diffuse membranes, so the networks have slightly overfitted on the training data for this case.
65
Chapter 8
Conclusion
Watsonsim shows that it is feasible for a small team on a deadline to create a working question answering system based on existing search technologies, online sources, natural language processing tools, and readily available machine learning toolkits.
As in many projects of this type, a great portion of the time is spent on collecting and cleaning the target data. The easily parsed format of Wikipedia as well as the accessible APIs offered by Google and Microsoft are in part responsible for reducing the time to the first demonstration. As for developing the overall pipeline, extensive documentation of Watson from IBM has proven helpful and a large influence on the design of the project.
We start with the prediction regret bounds proved by Hutter (2001b) in case the learning distribution Q dominates the true distribution P . In the following, let cP denote the constant from Definition 3.3.
Theorem 3.43 (Hutter, 2007b, Eq. 5 & 8). For all P and Q,√ EPEQn − √ EPEPn ≤ √ 2KLn(P,Q).
The following bound on prediction regret then follows easily, but it is a factor of √ 2
worse than the bound stated by Hutter (2005, Thm. 3.36).
Corollary 3.44 (Expected Prediction Regret). For all P and Q,
0 ≤ EP [ EQn − EPn ] ≤ 2KLn(P,Q) + 2 √ 2KLn(P,Q)EPEPn .
Proof. From Theorem 3.43 we get
EP [ EQn − EPn ] = (√ EPEQn + √ EPEPn )(√ EPEQn − √ EPEPn ) ≤ (√ EPEQn + √ EPEPn )√ 2KLn(P,Q)
≤ (√ 2KLn(P,Q) + √ EPEPn + √ EPEPt )√ 2KLn(P,Q)
= 2KLn(P,Q) + 2 √ 2KLn(P,Q)EPEPn .
§3.5 Predicting 37
If Q dominates P , then we have KLn(P,Q) ≤ − ln cP :
KLn(P,Q) = ∑ x∈Xn P (x) log P (x) Q(x) ≤ ∑ x∈Xn P (x) log 1 cP = − log cP (3.5)
This invites the following corollary.
Corollary 3.45 (Prediction Regret for Dominance; Hutter, 2005, Cor. 3.49). If Q dominates P , then the following statements hold.
(a) EPEQ∞ is finite if and only if EPEP∞ is finite. (b) √ EPEQ∞ − √ EPEP∞ ∈ O(1)
(c) EPEQt /EPEPt → 1 for EPEPt →∞. (d) EP [ EQt − EPt ] ∈ O (√ EPEPt ) .
If the true distribution P is deterministic, we can improve on these bounds:
Example 3.46 (Predicting a Deterministic Measure). Suppose we are predicting a deterministic measure P that assigns probability 1 to the infinite string x1:∞. If P is dominated by Q, the total expected prediction regret EPEQ∞ is bounded by −2 ln cP by Corollary 3.44. This is easy to see: every time we predict a wrong symbol a 6= xt, then Q(a | x<t) ≥ Q(xt | x<t), so Q(xt | x<t) ≤ 1/2. Therefore Yt ≤ Yt−1/2 and by dominance Yt ≥ cP . Hence a prediction error can occur at most − log cP times. 3
Generally, the O(EPEPt ) bounds on expected prediction regret given in Corollary 3.45 are essentially unimprovable:
Example 3.47 (Lower Bounds on Prediction Regret). Set X := {0, 1} and consider the uniform measure λ from Example 2.15. For each time step t, we have λ(0 | x<t) = λ(1 | x<t) = 1/2, so the argmax in (3.4) ties and hence it does not matter whether we predict 0 or 1. We take two predictors P and Q, where P always predicts 0 and Q always predicts 1. Let Zt := E Q t − EPt . Since their predictions never match, Zt is an ordinary random walk with step size 1. We have (Weisstein, 2002)
lim sup t→∞ EP [EQt − EPt ]√ t
= √ 2/π
and for the law of the iterated logarithm (Durrett, 2010, Thm. 8.8.3)
lim sup t→∞ EQt − EPt√ 2t log log t = 1 P -almost surely.
Both bounds are known to be asymptotically tight. 3
While Example 3.47 shows that the bounds from Corollary 3.45 are asymptotically tight, they are misleading because in most cases, we can do much better. According

The Hamiltonian cycle problem (HCP) is an important, canonical combinatorial problem. Surprisingly, for the HCP in directed graphs, which we called directed HCP or DHCP, no effective exact algorithm has been developed. Our main result of this work is a novel and effective exact algorithm for the DHCP. Our algorithm utilizes an existing algorithm for the assignment problem and an existing method for Boolean satisfiability (SAT). Our work includes a new SAT formulation of the HCP and the AP, which can be potentially extended to other problems such as the TSP. Our experimental results on random and real problem instances showed that our new algorithm is superior to four known algorithms including one algorithm that takes advantage of the award-winning Concorde TSP algorithm. Furthermore, the first phase transition result on combinatorial problems was done on the HCP and later was extended to the DHCP. In this paper we experimentally verified the existence of a phase transition of the DHCP and refined the location where such a phase transition appears using our new exact DHCP algorithm.
A common issue when evaluating systems that deal with Natural Language is that results on different test collections are often contradictory. In the particular case of Text Clustering, a factor that contributes to this problem is that the average size of clusters can vary across different test beds, and this variability modifies the optimal balance between precision and recall. A system which tends to favor precision, creating small clusters, may have good results in a dataset with a small average cluster size and worse results in a test collection with a larger average cluster size.
Therefore, if we only apply F to combine single metrics, we can reach contradictory results over different test beds. As UIR does not depend on metric weighting criteria, our hypothesis is that a high UIR value ensures robustness of evaluation results across test beds.
5. See the work of Artiles et al. (2009) for an extended explanation.
In other words: given a particular test bed, a high UIR value should be a good predictor that an observed difference between two systems will still hold in other test beds.
The following experiment is designed to verify our hypothesis. We have implemented four different systems for the WePS problem, all based on an agglomerative clustering algorithm (HAC) which was used by the best systems in WePS-2. Each system employs a certain cluster linkage technique (complete link or single link) and a certain feature extraction criterion (word bigrams or unigrams). For each system we have experimented with 20 stopping criteria. Therefore, we have used 20x4 system variants overall. We have evaluated these systems over WePS-1a, WePS-1b and WePS-2 corpora6.
The first observation is that, given all system pairs, Fα=0.5 only gives consistent results for all three test beds in 18% of the cases. For all other system pairs, the best system is different depending of the test collection. A robust evaluation criterion should predict, given a single test collection, whether results will still hold in other collections.
We now consider two alternative ways of predicting that an observed difference (system A is better than system B) in one test-bed will still hold in all three test beds:
• The first is using F (A)− F (B): the larger this value is on the reference test bed, the more likely that F (A)− F (B) will still be positive in a different test collection.
6. WEPS-1a was originally used for training in the first WePS campaign, and WePS-1b was used for testing.
• The second is using UIR(A,B) instead of F: the larger UIR is, the more likely that F (A)− F (B) is also positive in a different test bed.
In summary, we want to compare F and UIR as predictors of how robust is a result to a change of test collection. This is how we tested it:
1. We select a reference corpus out of WePS-1a, WePS-1b and WePS-2 test beds.
Cref ∈ {WePS-1a,WePS-1b,WePS-2}
2. For each system pair in the reference corpus, we compute the improvement of one system with respect to the other according to F and UIR. We take those system pairs such that one improves the other over a certain threshold t. Being UIRC(s1, s2) the UIR results for systems s1 and s2 in the test-bed C, and being FC(s) the results of F for the system s in the test-bed C:
SUIR,t(C) = {(s1, s2)|UIRC(s1, s2) > t}
SF,t(C) = {s1, s2|(FC(s1)− FC(s2)) > t)}
For every threshold t, SUIR,t and SF,t represent the set of robust improvements as predicted by UIR and F, respectively.
3. Then, we consider the system pairs such that one improves the other according to F for all the three test collections simultaneously.
T = {s1, s2|FC(s1) > FC(s2)∀C}
T is the gold standard to be compared with predictions SUIR,t and SF,t.
4. For every threshold t, we can compute precision and recall of UIR and F predictions (SUIR,t(C) and SF,t(C)) versus the actual set of robust results across all collections (T ).
Precision(SUIR,t(C)) = |SUIR,t(C) ∩ T | |SUIR,t|
Recall(SUIR,t(C)) = |SUIR,t(C) ∩ T |
|T |
Precision(SF,t(C)) = |SF,t(C) ∩ T | |SF,t(C)|
Recall(SF,t(C)) = |SF,t(C) ∩ T |
|T |
We can now trace the precision/recall curve for each of the predictors F, UIR and compare their results. Figures 11, 12 and 13, show precision/recall values for F (triangles) and UIR (rhombi); each figure displays results for one of the reference test-beds: WEPS1a,WEPS-1b and WePS-27.
Altogether, the figures show how UIR is much more effective than F as a predictor. Note that F suffers a sudden drop in performance for low recall levels, which suggests that
7. The curve “parametric UIR” refers to an alternative definition of UIR which is explained in Section 8
big improvements in F tend to be due to the peculiarities of the test collection rather than to a real superiority of one system versus the other.
This is, in our opinion, a remarkable result: differences in UIR are better indicators of the reliability of a measured difference in F than the amount of the measured difference. Therefore, UIR is not only useful to know how stable are results to changes in α, but also to changes in the test collection, i.e., it is an indicator of how reliable a perceived difference is.
Note that we have not explicitly tested the dependency (and reliability) of UIR results with the number of test cases in the reference collection. However, as working with a collection of less than 30 test cases is unlikely, in practical terms the usability of UIR is granted for most test collections, at least with respect of the number of test cases.
6.1.1 Overview of the model The Affective Module is based on a set of rules that compute categories of emotions, moods and attitudes for the virtual recruiter, based on the contextual information given by the scenario and the detected affects (emotions, moods and attitudes) of the participant. The computation of the virtual agent’s emotions is based on the OCC model [28] and the computation of the agent’s moods is based on the ALMA model [15]. The details of the computation of emotions and
moods will not be presented in this paper; it can be found in [19].
The Affective Core receives a performance index as its input. The performance index, which fall in the range of [−1,+1], represents the overall performance of the youngster (its attitude, its affects, its vocal performance). The detected (d) performance is denoted as Pd. Similarly, a set of expected (e) performance index is received from the scenario module. This expected performance index is linked to the difficulty of the question. For example, if a question is easy, e.g. ”Did you find us easily?”, the performance index will be near of 1.
Formally,in our model, all affects of the recruiter correspond to a value in the interval of [0, 1] and we use A to denote the set of all affects. The different affects are categorised in terms of three subsets: E(t) (emotions), M(t) (moods) and A(t) (attitudes) are virtual recruiter’s simulated affects. These emotions are computed using expert rules based on the values of Pd(t) and Pe(t). All these rules are described in [19].
The list of virtual recruiter’s possible affects (emotions, moods and attitudes) that are represented in the model is given in Table 2. Note that this set is different from the affects that are actually detected and expected. It is based on the literature and on the mock interview corpus analysis (especially the knowledge elicitation phases mentioned in sections 4 and 5). The emotions are a simple subset of the OCC model [28] that was selected based on what practitioners, acting as recruiters, expressed during the mock interviews analysed. The moods originated from the ALMA model [15] are defined on 3 dimensions (Pleasure, Arousal and Dominance), but we limited them to the positive dominance zone (since recruiters do not show submissive moods in the context of job interviews). Moods of Table 2 are with positive or neutral dominance.
The computation of moods is based on emotions following ALMA [15]. In the context of our interview simulation, the period is determined by the number of cycle question/answer. Each answer leads to the computation of a new emotions set and these emotions influence the interviewer’s mood. The basis for the calibration is as follows: after five cycles of a specific emotion (anger for example), the virtual
recruiter will be in the corresponding mood (hostile). More details about the mood computation can be found in [19].
The way we compute attitudes follow this principle: an agent can adopt an attitude according to its personality [33] or according to its actual mood [39]. For example, an agent with a non-aggressive personality may still show an aggressive attitude if its mood becomes very hostile. The mood compensates the personality and vice versa. For this reason, we use a logical-OR as condition on these two dimensions. As a consequence, in our model, the attitude can be triggered by one of these two dimensions. Then, the maximum value (mood or personality) is kept to compute the corresponding attitude, as is classically done in Fuzzy logics.
We analyze in Figs. 14-16 Middleton’s contributions to Shakespeare’s plays, Macbeth, Measure for Measure, and Timon of Athens. The attribution of the full plays in Fig. 10 did not suggest that Middleton made any significant contribution to any of these plays. The intraplay analysis of Macbeth at the level of acts and scenes, shown in Fig. 14, supports this conclusion. A total of two scenes are assigned to Middleton over Shakespeare, namely Scenes 1.1 and 5.1. Scene 5.1 is attributed to Middleton by only a small margin of 1cn while Scene 1.1 is assigned by a more substantial margin of over 4cn. Scholars have often flagged Scenes 1.2, 3.5, and 4.1 as scenes revised or contributed by Middleton (Wells, 2009), although we do not find evidence of this in our analysis.
The case of Measure for Measure favors Shakespeare’s sole authorship even more; both the act and scene analysis displayed in Fig. 15 find Shakespeare to be the sole author of the play. If Middleton revised the original play as proposed by scholars (Wells, 2009; Taylor and Jowett, 1993), we do not find evidence that it comprised substantial fresh writing.
Of the three plays, we find that Middleton’s contribution was likely largest in Timon of Athens. While all five acts are attributed by our method to Shakespeare, in Act 3 it is by a margin of less than 1cn from Middleton; see Fig. 16. This is even more evident in the scene analysis. Middleton is a stronger candidate in Scenes 1.2, 3.2, and 3.4, with close ties in Scenes 3.1, 3.3, and 4.2. This assignment supports much of the claim of authorship provided in (Vickers, 2002; Wells, 2009), and is broadly consistent with the most thorough analysis that numbers only the scenes, 1 through 19 (Taylor and Lavagnino, 2007, pg. 467).
To overcome the power consumption issue of the GPU and Cloud based frameworks, many developers seek solutions at hardware level [16], [17], [18]. For the IC based accelerator,
Diannao [3] is one of the pioneers works solidifying the neural networks on the hardware circuits. Origami [19] present a tape-out accelerator with silicon measurements of power-, area- and I/O efficiency. Meanwhile, FPGA is more flexible due to the integration of the reconfigurable logic devices. Therefore, it can fit changing applications and parameters in neural networks [20], [21]. For example, Zhang et al. [2] explores the bandwidth for the parameters facing the limitation of an FPGA chip. Suda et al. [22] presents a design space exploration method OpenCL programming model approach, which can explore the trade-offs the parameters in the network topologies.
Besides the ASIC and FPGA-based accelerators, there have been numerous directions using emerging hardware technologies, such as Memristive Boltzmann Machine [23], and Processing-in-Memory techniques [24]. Energy efficient inference engine (EIE) uses compression by pruning the redundant connections and having multiple connections share the same weight [25].
Answer-set programming (ASP) is a popular framework to describe concisely search and combinatorial problems [14, 16]. It has been successfully applied in crypto-analysis, code optimization, the semantic web, and several other fields [18]. Problems are encoded by rules and constraints into disjunctive logic programs whose solutions are answer-sets (stable models). The complexity of finding an answer-set for a disjunctive logic program is ΣP2 -complete [4]. However this hardness result does not exclude quick solutions for large instances if we can exploit structural properties that might be present in real-world instances.
Recently, Fichte and Szeider [5] have established a new approach to ASP based on the idea of backdoors, a concept that originates from the area of satisfiability [20]. Backdoors exploit the structure of instances by identifying sets of atoms that are important for reasoning. A backdoor of a disjunctive logic program is a set of variables such that any instantiation of the variables yields a simplified logic program that lies in a class of programs where the decision problem we are interested in is tractable. By means of a backdoor of size k for a disjunctive logic program we can solve the program by solving all the
2k tractable programs that correspond to the truth assignments of the atoms in the backdoor. For each answer set of each of the 2k tractable programs we need to check whether it gives rise to an answer set of the given program. In order to do this efficiently we consider tractable programs that have a small number of answer sets (e.g., stratified programs [9]).
We consider target classes based on various notions of acyclicity on the directed/undirected dependency graph of the disjunctive logic program. A cycle is bad if it contains an edge that represents an atom from a negative body of a rule. Since larger target classes facilitate smaller backdoors, we are interested in large target classes that allow small backdoors and efficient algorithms for finding the backdoors.
Contribution
In this paper, we extend the backdoor approach of [5] using ideas from Zhao [23]. We enlarge the target classes by taking the parity of the number of negative edges or vertices on bad cycles into account and consider backdoors with respect to such classes. This allows us to consider larger classes that also contain nonstratified programs. Our main results are as follows:
1. For target classes based on directed bad even cycles, the detection of backdoors of bounded size is intractable (Theorem 1).
2. For target classes based on undirected bad even cycles, the detection of backdoors is polynomial-time tractable (Theorem 3).
The result (2) is a non-uniform polynomial-time result since the order of the polynomial depends on the backdoor size. An algorithm is uniform polynomialtime tractable if it runs in timeO(f(k)·nc) where f is an arbitrary function and c is a constant independent from k. Uniform polynomial-time tractable problems are also known as fixed-parameter tractable problems [3]. We provide strong theoretical evidence that result (2) cannot be extended to uniform polynomialtime tractability. Further, we establish that result (2) generalizes a result of Lin and Zhao [13].
Anomalies are usually considered as outliers, surprises, exceptions, noises, and novelties [2]. Anomaly detection can be achieved by many pattern recognition techniques. However, in many domains, e.g., medical and financial area, frauds or anomalies detection should be understandable and interpretable. A key reason being that the learned knowledge should be validated by humans before being used. We provide a brief literature review of anomaly detection using language models since they are one of the most understandable models. Lin and et al. represent time series using the Symbolic Aggregate approXimation (SAX) [3], and then use a contextfree grammar for rules’ generation [4]. Intuitively, anomalies tend to have low grammar density (novelty patterns are less compressed). Sekar and et al. learn program behaviors from system call sequences using a finite-state automaton for intrusion detection [5]. Timo and et al. propose a tailored behavior model (a probabilistic deterministic timed-transition automaton) to identify anomalies [6]. They first augment the learned automaton with timing information, after which traces ar X iv :1
70 5.
09 65
0v 1
[ cs
.L G
] 2
4 M
ay 2
01 7
of ATM observations are traversed by the model. Aggregated transition probabilities are then compared with a predetermined threshold for detecting anomalies.
In this paper, we study event sequences from a DVB system. These sequences contain the encryption information needed for viewing the stream content. We model these sequences using PDRTAs (Probabilistic Deterministic Real Timed Automata), which we learn unsupervised from a set of input sequences using the RTI+ algorithm (Real-Time Identification from Positive Data) [7]. first, we compute the time difference between two consecutive and distinct events to obtain timed strings. We then segment the time strings into frames. The sequences are fed into the RTI+ algorithm to learn a timed automaton. In the testing phrase, an anomaly of a sequence is identified if the sequence is not accepted by the learned model. This paper makes the following contributions: • To the best of our knowledge, this paper is the first one
using a timed automaton to detect anomalies in the DVB system. Two types of anomalies, i.e., data lost and timing error, are identified with low false positive rate. • The model provides highly interpretable insights for understanding the underlying process generating the data. Experts from the DVB area can easily monitor and validate the system under operation using such a model.
This paper is organized as follows. Section II introduces the data preprocessing. Section III discusses the learning algorithm and the experimental results. We make concluding remarks in Section IV.
treatment of other peers. As long as a peer declares itself as inconsistent, there is no much a neighbor can do. However, an inconsistent peer might decide to relax its own consistency requirements and send to other peers only “partially consistent” data, which would be transparent to those receiving peers.8
The peer consistent answers from a peer to a query are the semantically correct answers, which means that when answering the query, the peer consistently considers the data of its neighbors and the trust relationships with them.
Definition 3.3 Consider an instance D for the PDES schema P = 〈P,S,Σ, T rust〉, and a peer P ∈ P. Let Q(x̄ ) ∈ L(P) be a query, with x̄ a possibly empty list of free variables.
1. If Sol(P,D) 6= ∅: (a) If x̄ 6= ∅, a finite sequence c̄ of constants in U of the same length as x̄
is a peer consistent answer (PCA) to Q from P iff D |= Q[c̄] for every D ∈ Sol(P,D). (b) If Q is Boolean and D |= Q for every D ∈ Sol(P,D), then yes is the only PCA to Q. 2. If Sol(P,D) = ∅, then incP is the only PCA to Q(x̄ ). 2
For illustration, in Example 3.1, if P1 has to answer a query Q ∈ L(P1), it returns the answers that are simultaneously true in all its neighborhood solutions. For example, under this semantics, the answers to query Q(x ) : ∃yzR1(x , y , z ) posed to P1 will be 〈c〉, 〈f 〉, which are shared by all the six neighborhood solutions for P1 (or better, by their restrictions to P1’s schema). The answers to queries Q1 and Q2 posed by P1 to P2 are answered in the same way, taking into consideration the P2’s DECs.
We can see that the answers from a peer to a query are certain answers (Imielinski and Lipski 1984). This condition makes the data moved from one peer to a neighbor always certain. In particular, this allows us to treat external queries and inter-peer queries in a uniform manner. In particular, we will be in position to conceive and implement the passage of data from one peer to a neighbor as a query answering process.
Example 3.3 Consider the PDES schema P and instance D represented in graph G(P) in Figure 6. Here, Σ = {Σ(P1, P2),Σ(P2, P3),Σ(P4, P3)}, and:
- Σ(P1, P2)= {∀xyz (R2(x , y) ∧ S 2(y , z )→ R1(x , y , z )), ∀x (S 1(x )→ S 2(5, x ))}. - Σ(P2, P3)= {∀xy (S 2(x , y) → R3(x , y))}. - Σ(P4, P3)= {∀xyz (R3(x , y) → R4(x , y , 3))}.
In intuitive and procedural terms, if a query is posed to P1, it will send queries to P2, to check the satisfaction of the DECs in Σ(P1, P2). But, in order for P2 to answer those queries, it will send queries to peer P3 to check the DECs in Σ(P2,
8 An alternative to this design choice could be, in the case a peer P trusts an inconsistent peer Q more than itself, that P becomes or declares itself inconsistent as well. This alternative may be worth exploring, but we do not pursue it here any further.
Consistency and Trust in Peer Data Exchange Systems 1712 · L. Bertossi and L. Bravo R1
c 4 2 f 3 5
S1
3 7
P1 R2
c 4 d 5
S2
4 2 5 3
P2
<
R3
5 7 5 3
P3
=
R4
5 3 3
P4
=
Fig. 5. PDES for Example 3.8
turned by a peer who consistently considers the data of- and trust relationships with its neighbors.
Definition 3.7. Let Q(x̄) ∈ L(P) be a FO query. (a) When Sol(P) 6= ∅, a ground tuple t̄ is a peer consistent answer (PCA) to Q from P iff D |= Q(t̄) for every D ∈ Sol(P). If x̄ = ∅ and D |= Q for every D ∈ Sol(P), then yes is the PCA. (b) Otherwise, when Sol(P) = ∅, incP is the only PCA to Q(x̄). 2
Example 3.8. (extension of example 1.1) The DECs are Σ(P1, P2)= {∀xyz (R2(x, y) ∧ S2(y, z) → R1(x, y, z)), ∀x (S1(x) → S2(5, x))}, Σ(P2, P3) = {∀xy (S2(x, y) → R3(x, y))}, and Σ(P4, P3)= {∀xyz (R3(x, y)→ R4(x, y, 3))}. Here, N (P1) = {P1, P2}. The trust relationships are as shown in Figure 5. In intuitive and procedural terms, if a query is posed to P1, it will send queries to P2, to check the satisfaction of the DECs in Σ(P1, P2). But, in order for P2 to answer those queries, it will send queries to peer P3 to check the DECs in Σ(P2, P3). Since P3 is not connected to any other peer, it will answer P2’s queries using its material instance D(P3). Thus, the solutions for P1 and its peer consistent answers will be affected by the peers in AC(P1) = {P1, P2, P3}.
We can make this precise by applying the recursive definition of solution instance. Since P3 has no DECs with other peers, its only neighborhood solution is its instance D(P3). This is sent back to P2, who needs to repair the extended instance D = {R2(c, 4), R2(d, 5), S2(4, 2), S2(5, 3), R3(5, 7), R3(5, 3)} with respect to Σ(P2, P3). As P2 trusts P3 the same as itself, it can modify its own data or the data it got from P3. Assuming -for illustration purposes for the moment- that the distance between instances is given in terms of the symmetric set-difference, P2 has two neighborhood solutions: {R2(c, 4), R2(d, 5), S2(5, 3), R3(5, 7), R3(5, 3)} and {R2(c, 4), R2(d, 5), S2(4, 2), S2(5, 3), R3(5, 7), R3(5, 3), R3(4, 2)}. They minimally depart from D, and their restrictions to P2’s schema lead to two solutions for P2: Sol(P2) = {{R2(c, 4), R2(d, 5), S2(5, 3)}, {R2(c, 4), R2(d, 5), S2(4, 2), S2(5, 3)}}.
Peer P2 will send to P1 the intersection of its solutions, namely {R2(c, 4), R2(d, 5), S2(5, 3)}. Now, P1 has to repair {R1(c, 4, 2), R1(f, 3, 5), S1(3), S1(7), R2(c, 4), R2(d, 5), S2(5, 3)} with respect to Σ(P1, P2). Since P1 trusts P2 more, it will solve inconsistencies by modifying its own data, producing only one neighborhood solution: {R1(c, 4, 2), R1(f, 3, 5), R1(d, 5, 3), S1(3), R2(c, 4), R2(d, 5), S2(5, 3)}. Thus, Sol(P1) = {{ R1(c, 4, 2), R1(f, 3, 5), R1(d, 5, 3), S1(3)}}.
If P1 had received the query Q(x) : ∃yz(R1(x, y, z)∧S(y)), the only peer consistent answers would be 〈f〉, which is obtained from its only solution. 2 ACM Transactions on Computational Logic, Vol. V, No. N, Month 20YY.
Fig. 6. PDES for Example 3.3
P3). Since P3 is not connected to any other peer, it will answer P2’s queries using its initial material instance D(P3). Thus, the solutions for P1 and its peer consistent answers will be affected by the peers in AC(P1) = {P1, P2, P3}. More precisely, we can now apply the definitions of solution instance and peer consistent answer.
Since P3 has neither DECs with other peers nor local integrity constraints, its only neighborhood solution is its instance D(P3) ∈ D, which is sent back to P2. Now, P2 has to repair the extended instance D = {R2(c, 4), R2(d , 5), S 2(4, 2), S 2(5, 3), R3(5, 7), R3(5, 3)} with respect to Σ(P2, P3), which is not satisfied due to the presence of tuple S 2(4, 2). As P2 trusts P3 the same as itself, it can modify its own data or the data it got from P3.
Assuming -for illustration purposes for the moment- that the preorder relation on instances is given in terms of the symmetric set-difference, P2 has two neighborhood solutions: {R2(c, 4), R2(d , 5), S 2(5, 3), R3(5, 7), R3(5, 3)} and {R2(c, 4), R2(d , 5), S 2(4, 2), S 2(5, 3), R3(5, 7), R3(5, 3), R3(4, 2)}. They minimally depart from D , and their restrictions to P2’s schema lead to two solutions for P2: Sol(P2,D) = {{R2(c, 4),R2(d , 5),S 2(5, 3)}, {R2(c, 4), R2(d , 5), S 2(4, 2),S 2(5, 3)}}. Peer P2 will send to P1 the intersection of its solutions, namely Core(P2,D) =⋂ Sol(P2,D) = {R2(c, 4), R2(d , 5), S 2(5, 3)}. Now, P1 has to repair the extended instance {R1(c, 4, 2), R1(f , 3, 5), S 1(3), S 1(7), R2(c, 4), R2(d , 5), S 2(5, 3)} with respect to Σ(P1, P2), which is not satisfied.
Since P1 trusts P2 more, it will solve inconsistencies by modifying its own data, in this case inserting tuples into R1 and deleting tuples from S 1. This produces only one neighborhood solution: {R1(c, 4, 2), R1(f , 3, 5), R1(d , 5, 3), S 1(3), R2(c, 4), R2(d , 5), S 2(5, 3)}. Thus, Sol(P1,D) = {{R1(c, 4, 2), R1(f , 3, 5), R1(d , 5, 3), S 1(3)}}.
If P1 had received the query Q(x ) : ∃yz (R1(x , y , z ) ∧ S 1(y)), the only peer consistent answer woul have been 〈f 〉, obtained from its only solution. 2
Notice that when a peer Q passes its core, Core(Q, D̄), with D̄ a neighborhood instance, to a peer P, it is delivering the peers consistent answers to the atomic queries from P to Q of the form QR(x̄ ) : R(x̄ ), where R ∈ S(Q). However, when P computes its local PCAs to a query, it does not use its own core, ⋂ Sol(P,D), but the collection Sol(P,D) as a whole. The reason is that this core is unnecessarily restrictive for peer consistent query answering. For example, if the query is Q : ∃xP(x ), and P’s solutions are {P(a)} and {P(b)}, the PCA to this Boolean query would be no if evaluated on the empty core, but yes according to our definition.
Index Terms—simple recurrent networks, gradient vanishing, regularization
I. INTRODUCTION
Recurrent Neural Networks (RNNs) are known as universal approximators of dynamic systems [1], [2]. Since RNNs are able to simulate any open dynamical system, they have a broad spectrum of applications such as time series forecasting [3], control of plants [4], language modeling [5], speech recognition [6], neural machine translation [7] and other domains. The easiest way to create an RNN is adding the feedback connections to the hidden layer of multilayer perceptron. This architecture is known as Simple Recurrent Network (SRN). Despite of the simplicity, it has rich dynamical approximation capabilities mentioned above. However, in practice training of SRNs using first-order optimization methods is difficult [8]. The main problem is well-known vanishing/exploding gradients effect that prevents capturing of long-term dependencies in data. Vanishing gradients effect is a common problem for recurrent neural networks with sigmoid-like activation functions which uses a backpropagation method for calculation of derivatives. Deep feedforward neural networks with many hidden layers also feels this effect. Hochreiter and Schmidhuber designed a set of special synthetic benchmarks for testing RNNs on ability to capture long-term dependencies [9]. They
showed that ordinary SRNs are very ineffective to learn correlations in sequential data if distance between the target events is more than 10 time steps. The solution could be using more advanced second-order optimization algorithms such as Extended Kalman Filter [10], LBFGS, Hessian-Free optimization [11], but they require much more memory and computational resources for state-of-the-art networks. Another and the most popular solution is designing a neural networks architecture with more suitable dynamics. Echo State Networks (ESNs) proposed by Jaeger [12] may be considered as big reservoirs of sparsely connected neurons and randomly initialized weights which produces chaotic dynamics. For ESNs the gradients are computed for the last non-recurrent weights layer only. Experiments show that this may be enough for capturing longterm dynamics [13]. At the same time, ESNs often seems to have abundant number of free parameters. We also mention such an alternative to temporal neural networks as hierarchical sequence processing with auto-associative memories [14] that use distributed coding.
Another approach that was specially designed for catching the long-term dependencies is Long-Short Term Memory (LSTM) [9]. These neural networks are designed to adaptively reset or update their memory. They have specially designed complex structure that includes input and forgetting gates and they have constant error flow carousel. Currently it is probably the most popular family of RNNs models that shows stateof-the-art performance in several domains including speech recognition [6], image captioning [15] and neural machine translation. An idea of using input/forgetting gates inspired a lot of followers, Gated Recurrent Units (GRU) networks is probably one of the most successful of them [7]. Finally, the united team from Google and Facebook performed a grand experiment on finding the best architecture for RNNs [16]. They numerically evaluated 10,000 random architectures with 230,000 hyperparameter configurations in total and obtained a couple of new advanced models and some recommendations for easy improving of standard LSTMs. At the same time, a question how to train SRNs for catching the longterm dependencies is still a topic of current interest. It is highly desirable at least for better understanding of underlying processes of the training inside the recurrent and deep neural
IEEE 1 | P a g e
ar X
iv :1
60 6.
07 76
7v 1
[ cs
.N E
] 2
4 Ju
n 20
networks. Also, SRNs are more compact and fast working models of RNNs in comparison with ESNs and LSTMs that is very important for implementation to mobile and embedded devices. Recent research shows the ability to train SRNs for long term dependencies up to 100 time steps and more using several new techniques [17], [18]–[20]. It includes very accurate initialization of start weights, scaling down the big gradients, using advanced Nesterov momentum. One of the most common methods for preventing the vanishing gradients effect from this pool is known as ”‘gradient regularization”’ [20], [21]; also it was independently proposed in [22] as ”‘method of pseudoregularization”’. The idea of gradient regularization is adding to the target cost function an additional term that controls the norm of the backpropagated gradients. In this way, neural network learns not only to fit the train data but also to keep the flow of gradients into the certain range. In this paper we propose a new method to perform the gradient regularization by selection of proper samples in dataset using the simple criteria.
This paper presents an unsupervised dictionarybased method qwn-ppv to automatically generate polarity lexicons. Although simpler than similar automatic approaches, it still obtains better results on the four extrinsic evaluations presented. Because it only depends on the availability of a LKB, we believe that this method can be valuable to generate on-demand polarity lexicons for a given language when not sufficient annotated data is available. We demonstrate the adaptability of our approach by producing good performance polarity lexicons for different evaluation scenarios and for more than one language.
Further work includes investigating different graph projections of WordNet relations to do the propagation as well as exploiting synset weights. We also plan to investigate the use of annotated corpora to generate lexicons at word level to try and close the gap with those that have been (at least partially) manually annotated.
The qwn-ppv lexicons and graphs used in this paper are publicly available (under CC-BY license): http://www.rodrigoagerri.net/resources/#qwnppv. The qwn-ppv tool to automatically generate polarity lexicons given a WordNet in any language will soon be available in the aforementioned URL.
In this section we describe an approach to dimensionality reduction that is easy to implement. Note that there are at least two possible approaches to take advantage of reduced dimensionality. First, it is possible to use the dimensionality information to limit the algorithm to work only in the significant dimensions of Y . Second, it is possible to modify the bilinear program to have a small dimensionality. While changing the algorithm may be more straightforward, it limits the use of the advanced pivot point selection methods described in Section 3.3. Here, we show how to implement the second option in a straightforward way using singular value decomposition.
The dimensionality reduction is applied to the following bilinear program:
maximize w,x,y,z rT1 x+ s T 1w + x TCy + rT2 y + s T 2 z subject to A1x+B1w = b1 A2y +B2z = b2 w, x, y, z ≥ 0
(23)
Let C = SV TT be a singular value decomposition. Let T = [T1, T2], such that the singular value of vectors ti in T2 is less than the required . Then, a bilinear program with reduced dimensionality may be defined as follows:
maximize w,x,ȳ,y,z rT1 x+ s T 1w + x TSV T1ȳ + rT2 y + s T 2 z subject to T1ȳ = y
A1x+B1w = b1 A2y +B2z = b2 w, x, y, z ≥ 0
(24)
Note that ȳ is not constrained to be non-negative. One problematic aspect of reducing the dimensionality is how to define the initial polyhedron that needs to encompass all feasible solutions. One option is to make it large enough to contain the set {y ‖y‖2 = 1}, but this may be too large. Often in practice, it may be more efficient to first triangulate a rough approximation of the feasible region, and then execute the algorithm on this triangulation.
We investigated the usefulness of Stanford adposition attachment style as an alternative to the Prague style, using a large set of 30 treebanks for evaluation. We especially focused on multi-source cross-lingual delexicalized parser transfer, as one of the targets behind the design of Universal Stanford Dependencies is to be more cross-lingually consistent than other annotation styles.
We managed to confirm that for supervised parsing, Prague annotation style is favourable over Stanford style, as has been already stated in literature. However, in the parser transfer setting, Stanford style adposition attachment proved to generally perform better than the Prague style, thanks to its abstraction from the high interlingual variance in adposition usage. Moreover, even better results are achieved by at once combining outputs of parsers trained on treebanks of both Prague and Stanford adposition attachment style, eventually reaching an improvement of +0.39% UAS absolute over the Prague style baseline. Our results are further confirmed by experiments using smaller and more diverse subsets of training treebanks, where the advantage of Stanford style often becomes even more pronounced, reaching an improvement of up to +2.24% over the Prague style baseline.
In future, we intend to evaluate the effect of other annotation style differences, such as the coordination structures. We also plan to try to incorporate more fine-grained morphological information than the UPOS tags, probably by only including a given feature if it seems to be shared between the src and tgt language, as always including all of them performed very poorly in preliminary experiments.
• an, as, in, of, or
• Freq., Loc., [Vedic
• –divasa, adya, ajjatagge, ajjato, and, base, being, day, demonstr., dyaus, from, morning, not, old, phrase, present, pron., the, this, with
• &
• +
• Mhvs
• s., v.
•“on, ‹-›, Ajja，&,（=,（Page,（adv.）,（read,（see
• –kālaṁ, 10）, 15，64., 32，23., Ajjā, D.I，85；, DA.I，235., Dh.326；, III，425, J.I，279；, J.VI，180；, Kern，Toev., Pv.I，117, PvA.6，23；, Sn.75，153，158， 970，998；, Vin.I，18；, a3）, agga3）, ajja-tagge，see, ajjā；, bahutaṁ, day” ], diva），thus, dyā, idāni, onward，henceforth, to-day，now
• PvA.59）；, adyā，a, agge（?）, dyā，a°,“food”）；
108
Data: Pali: gūhanā
In [1], a visual tracking method, termed Struck, was introduced based on SSVM. The core idea is to train a tracker by optimizing the Pascal image overlap score using SSVM. Here we follow the same general setting of this structured tracking method, but with our StructBoost, instead of SSVM. We use decision stumps as the weak learner. More details are described in Section 3.5.
We use an on-line tracking setting for StructBoost tracker in our experiment. We only use the first 3 labeled frames for initialization and training our StructBoost tracker. We then update our tracker by re-training the model with sequent frames during the course of tracking. In the i-th frame (represented by xi), we first perform a prediction step to output the detection box, then collect training data for tracker update. In the prediction step, we solve the inference in (2) to output the prediction box (represented by yi) of current frame. For solving the inference in (2), we simply sample about 2000 bounding boxes around the prediction bounding box of last frame (represented by yi−1), one sampled bounding box is denoted by y, and search the most confident bounding box over all sampled boxes y as the prediction yi. In the first 3 labelled frames for initialization, we use the labelled bounding box as yi.
11
12
13
After the prediction, we collect training data by sampling about 200 bounding boxes around the current prediction yi. We use the training data in recent 60 frames to re-train the tracker for every 2 frames. We search over those sampled bounding boxes for finding the most violated constraint of each frame in the training process, which analogue to the prediction inference.
For StructBoost, the maximum number of weak learners is set to 300. The regularization parameter is selected from 100.5 to 102. We use the down-scaled gray-scale raw pixels and HOG as image features. For HOG feature, we use the code in [32]. For comparison, we also run the AdaBoost trackers using the same setting as our StructBoost tracker. For AdaBoost training, the maximum number of weak learners is set to 500. The AdaBoost tracker is a simple binary model. When updating (or initializing) AdaBoost tracker, we collect positive training boxes that significantly overlap with the predicted bounding box of the current frame (overlap above 0.8), and negative training boxes with small overlap (overlap lower or equal to 0.3).
We also compare our trackers with a few state-of-the-art tracking methods, including Struck [1] (with a buffer size
of 50), multi-instance tracking (MIL) [33], fragment tracking (Frag) [34], online AdaBoost tracking (OAB) [35], and visual tracking decomposition (VTD) [36]. OAB has two versions with two different settings (r = 1 means only one positive example per frame and r = 5 means five positive examples per frame for training. They are referred to as OAB1 and OAB5 here. See [33].). The test video sequences “coke, tiger1, tiger2, david, girl and sylv” were used in [1]. The sequences “shaking, singer” are obtained from [36], and the rest sequences are from [37].
Table 4 reports the Pascal overlap score of various tracking methods on testing video sequences. Our StructBoost tracker performs best on most test sequences. Compared with the binary AdaBoost tracker, StructBoost has a significantly higher score. Note that here Struck uses Haar features. When Struck uses a Gaussian kernel defined on raw pixels, the performance is slightly different [1], and ours still outperforms Struck in most cases. This might be due to the fact that our StructBoost selects relevant features (300 features selected here), and the SSVM of [1] uses all the image patch information which may contain noises.
Figure 3 plots the Pascal overlap scores frame by frame on several video sequences. It clearly shows that StructBoost outperform other methods in most cases. Compared to AdaBoost, StructBoost performs better at almost all
14
frames. The main reason is that StructBoost directly maximizes the overlap, while AdaBoost is trained by optimizing the classification error, which is not directly related to the Pascal overlap score.
The central location errors of compared methods are shown in Table 5. Our method also achieve the best results in most cases, which reveals that optimizing the overlap score also helps minimize the central location errors. We also plot the central location errors of different methods frame by frame on several sequences in Figure 4. These results prove the superior performance of StructBoost for tracking.
Some tracking examples are shown in Figure 5. In our experiments, the output space of StructBoost is the bounding box’s coordinates and the scale is fixed. However, it is easy to incorporate scale changes, rotation and transforms into the output space due to the flexibility of StructBoost.
In this report, we aim at proposing a comparative
investigation on SVMs and ELMs for classification based on deep convolutional features. Therefore, we adopt the deep convolutional activated features (DeCAF) from [17] for experiments. The structures of CNN for training on the ImageNet with 1000 categories are the same as the proposed CNN in [10]. The basic structure of the adopted is illustrated in Fig.1, which includes 5 convolutional layers and 3 fully-connected layers. Further details of the CNN training architecture and features can be referred to [10, 17].
We now discuss the actual analysis outcomes, on a per-domain basis. We first consider only TorchLight, then give some details on the comparison of analysis (III) success rates to those obtained by search probing. Before we begin, a few words are in order regarding the comparison between SP and SP1s. With R = 1, 10, 100, 1000, the success rates are identical in 99.83%, 99.14%, 97.5%, 94.66% of our 1160 benchmark instances respectively; in 99.83%, 99.14%, 99.31%, 98.97% of the instances, the success rates differ by at most 5%. Thus, a small runtime cut-off does not adversely affect the success rates of search probing (because long searches are rare). This being so, we henceforth do not discuss the data for SP vs. SP1s separately. We compare TorchLight’s analysis (III) success rates to those of SP only.
The guarantees of Proposition 1 are confirmed, i.e., guaranteed global analysis (I) succeeds as described in Logistics, Miconic-STRIPS, Movie, and Simple-TSP. It never succeeds in any other domain, though. In some domains, fractions of the gDGs are successful. Precisely, the maximum fraction of successful gDGs is 97% in Satellite, 50% in Ferry, 33.33% in TPP, 22.22% in Driverlog, 20% in Depots, 13.33% in Tyreworld, and 12.5% in BlocksworldArm. However, if the fraction is below 100% then nothing is proved, so this data may at best be used to give an indication of which aspects of the domain are “good-natured”. Guaranteed local analysis (II) generally is not much more applicable than global analysis. Thus we now concentrate on approximate local analysis (III) exclusively.
Proposition 2 is backed up impressively. Even with R = 1000, analysis (III) succeeds in every single sample state of Ferry, Gripper, Elevators, and Transport.15 This indicates strongly that the potentially sub-optimal relaxed plans do not result in a loss of information here. Indeed, the analysis yields high success rates in almost all domains where local minima are non-present or limited. This is not the case for the other domains, and thus TorchLight can distinguish domains with “easy” h+ topology from the “hard” ones. Consider Figure 3, showing mean analysis (III) success rates per-domain with R = 1. (The picture is similar for R = 10, 100, 1000; cf. Table 3 below.)
The domains whose h+ topology is not known are shown separately on the right hand side in Figure 3. For the other domains, we see quite nicely that “harder” domains tend to have lower success rates. In particular, the easiest domains in the bottom class all have 100% success rates (95% in the case of Zenotravel), whereas the hardest domains in the top right corner only have around 50% or less. In the latter domains, to some extent the
15. Historically, this observation preceded Proposition 2, as well as the h+ topology categorization of Elevators and Transport as per Figure 1. That is, these hand-made analyses were motivated by observing TorchLight’s analysis outcome.
low success rates result from the recognition of dead ends by FF’s heuristic function. For example, if during random sampling we make random vehicle moves consuming fuel, like in Mystery and Mprime, then of course chances are we will end up in a state where fuel is so scarce that even a relaxed plan does not exist anymore. This is most pronounced in Airport, where all sample states here have infinite heuristic values. However, the capabilities of the analysis go far beyond counting states on recognized dead ends. In Blocksworld-Arm, for example, there are no dead ends at all and still the success rate is only 30%, clearly indicating this as a domain with a difficult topology.
To some extent, based on the success rates we can even distinguish Pipesworld-Tankage from Pipesworld-NoTankage, and Mprime from Mystery (in Mprime, fuel can be transferred between locations). The relatively high success rate in Depots probably relates to its transportation aspects. In Grid, in 20% of cases our analysis is not strong enough to recognize the reasons behind non-existence of local minima; these reasons can be quite complicated (Hoffmann, 2003). Dining-Philosophers does not really have a favorable h+ topology. Its rather excessive bound 31 is due to the very particular domain structure where philosophers behave in strictly symmetrical ways (Hoffmann, 2005). Apart from this, the only strong outliers are Driverlog, Rovers, Hanoi, and Blocksworld-NoArm. All of these are more problems of the hand-made analysis than of TorchLight’s. In Driverlog and Rovers, deep local minima do exist, but only in awkward situations that don’t tend to arise in the IPC instances. Thus the hand-made analysis, which is of a worst-case nature, is too pessimistic here. The opposite happens in Hanoi and Blocksworld-NoArm, where the absence of local minima is due to rather idiosyncratic reasons. For example, in Hanoi the reason is that h+ is always equal to the number of discs not yet in goal position – in the relaxation, one can always accomplish the remaining goals one-by-one, regardless of the constraints entailed by their positioning. Hanoi and Blocksworld-NoArm are not actually “easy to solve” for
FF, and in that sense, from a practical perspective, the low success rates of TorchLight’s analysis (III) provide the more accurate picture.
Table 3 gives a complete account of per-domain averaged success rates data, including all domains, all values of R, the rates obtained on initial states, and using SP instead of TorchLight. This serves to answer three questions:
(1) Is it important to sample random states, rather than only analyzing the initial state?
(2) Is it important to sample many random states?
(3) How competitive is analysis (III) with respect to a search-based analysis?
The answer to question (1) is a clear “yes”. Most importantly, this pertains to domains with dead ends, cf. our brief discussion above. It is clear from Table 3 that, in such domains, analyzing sI results in a tendency to be too optimistic. To see this, just consider the entries for Airport, Dining-Philosophers, Freecell, Mystery, Openstacks, Parc-Printer, Pathways, TPP, Trucks, and Woodworking. All these domains have dead ends, for a variety of reasons. The dead ends do not occur frequently at initial state level, but do occur frequently during random walks – cf. column “DE” in Table 3. (Interestingly, in a few domains – most notably the two Pipesworlds – the opposite happens, i.e., success rates are lower for sI than for the sample states. It is not clear to us what causes this phenomenon.)
If we simply compare the sI column with the R = 1000 column for analysis (III), then we find that the result is “a lot” different – more than 10% – in 22 of the 37 domains. To some extent, this difference between initial states and sample states may be just due to the way these benchmarks are designed. Often, the initial states of every instance are similar in certain ways (no package loaded yet, etc). On the other hand, it seems quite natural, at least for offline problems, that the initial state is different from states deeper down in the state space (consider transportation problems or card games, for example).
The answer to question (2) is a clear “no”. For example, compare the R = 1 and R = 1000 columns for analysis (III). The difference is greater than 10% in only 6 of the 37 domains. The peak difference is in Openstacks, with 16.6% for R = 1000 vs. 0% for R = 1. The average difference over all domains is 4.17%. Similarly, comparing the R = 1 and R = 1000 columns for SP results in only 5 of 37 domains where the difference is greater than 10%, the peak being again in Openstacks, 20.8% for R = 1000 vs. 4.4% for R = 1. The average difference over all domains is 3.7%.
The answer to question (3) is a bit more complicated. Look at the columns for analysis (III) respectively SP with R = 1000. The number of domains where the difference is larger than 10% is now 11 out of 37, with a peak of 64.6% difference in Scanalyzer. On the one hand, this still means that in 26 out of 37 domains the analysis result we get is very close to that of search (average difference 2.18%), without actually running any search! On the other hand, what happens in the other 11 domains? In all of these, the success rate of SP is higher than that of TorchLight. This is not surprising – it basically means that TorchLight’s analysis is not strong enough here to recognize all states that are not on local minima.
Interestingly, this weakness can turn into an unexpected advantage. Of the 11 domains in question, 8 domains – Blocksworld-Arm, Depots, Mprime, Pipesworld-Tankage, PipesworldNoTankage, PSR, Scanalyzer, and Sokoban – do contain deep local minima.16 Thus, in these 8 domains, we would wish our analysis to return small success rates. TorchLight grants this wish much more than SP does. Consider what happens when using SP instead of analysis (III) in Figure 3. For Mystery, PSR, and Sokoban, the change is not dramatic. However, Blocksworld-Arm is marked with average success rate 93 instead of 30, putting it almost on par with the very-simple-topology domains in the bottom class. Similarly, PipesworldTankage, Pipesworld-NoTankage, and Scanalyzer are put almost on par with these. Depots
16. Sokoban has unrecognized dead-ends (in the relaxation, blocks can be pushed across each other) and therefore local minima. In Scanalyzer, analyzing plants misplaces them as a side effect, and bringing them back to their start position, across a large circle of conveyor belts, may take arbitrarily many steps. See Figure 3 for the other 6 domains.
actually receives a 100, putting it exactly on par with them. Thus the SP analysis outcome actually looks quite a bit worse, in 5 of the domains.
What causes these undesirably high success rates for SP? The author’s best guess is that, in many domains, the chance of randomly finding a state on a local minimum is low. In large-scale experiments measuring statistics on the search space surface under FF’s heuristic function (Hoffmann, 2003), it was observed that many sampled states were not local minima themselves, but where contained in “valleys”. Within a valley, there is no monotonically decreasing path to a goal state. Such a state may not be a local minimum because, and only because, one can descend deeper into the valley. It seems that SP correctly identifies most valley states to not be local minima, thus counting as “good” many states that actually are located in difficult regions of the search space. This is a weakness not of SP, but of success rate as a search space feature.17 Why does this weakness not manifest itself as much in analysis (III)? Because that analysis is “more picky” – it takes as “good” only states that qualify for particular special cases. These tend to not occur as often in the difficult domains.
Of course, it is easy to construct examples turning the discussed “strength” into a real weakness of TorchLight’s analysis quality. This just does not seem to happen a lot in the present benchmarks. Now, having said that, the present benchmarks aren’t well suited to bring out the theoretical advantage of analysis (III) either. The analysis offers unlimited lookahead depth at low-order polynomial cost. However, even with R = 1000, in 23 of the 37 domains the highest exit distance bound returned is 0, i.e., every exit path identified consists of a single operator. These cases could be handled with a much simpler variant of analysis (III), looking only at operators o0 that are directly applicable in s, and thus removing the entire machinery pertaining to SG predecessors of x0. Still, that machinery does matter in cases that are quite natural. The highest exit distance bound returned is 10 in Grid and 7 in Transport. More generally, in any transportation domain with a non-trivial road-map, it is easy to construct relevant situations. For example, say the road map in Transport forms N “cities”, each with diameter D and at least one vehicle, distances between cities being large relative to D. Then, in a typical state, around N vehicle moves will be considered helpful by FF: at least 1 per city since local vehicles will be preferred by the relaxed plan. All successor states will have identical h+ until a package can be loaded/unloaded. The typical number of steps required to do so will grow with D. If, for example, the vehicle is in the “outskirts” and the packages are in the “city center”, then around D/2 steps are required, and finding an exit takes runtime around ND/2. Then small values of N and D already render search probing either devoid of information (if the runtime cut-off is too small), or computationally infeasible (recall that the probing should be a “quick” pre-process to the actual planning). By contrast, analysis (III) easily delivers the correct success rate 100%.
Recent proliferation of the World Wide Web, and common availability of inexpensive storage media to accumulate over time enormous amounts of digital data, have contributed to the importance of intelligent access to this data. It is the sheer amount of data available that emphasizes the intelligent aspect of access—no one is willing to or capable of browsing through but a very small subset of the data collection, carefully selected to satisfy one’s precise information need.
Research in artificial intelligence has long aimed at endowing machines with the ability to understand natural language. One of the core issues of this challenge is how to represent language semantics in a way that can be manipulated by computers. Prior work on semantics representation was based on purely statistical techniques, lexicographic knowledge, or elaborate endeavors to manually encode large amounts of knowledge. The simplest approach to represent the text semantics is to treat the text as an unordered bag of words, where the words themselves (possibly stemmed) become features of the textual object. The sheer ease of this approach makes it a reasonable candidate for many information retrieval tasks such as search and text categorization (Baeza-Yates & Ribeiro-Neto, 1999; Sebastiani, 2002). However, this simple model can only be reasonably used when texts are fairly long, and performs sub-optimally on short texts. Furthermore, it does little to address the two main problems of natural language processing (NLP), polysemy and synonymy.
c©2009 AI Access Foundation. All rights reserved.
Latent Semantic Analysis (LSA) (Deerwester, Dumais, Furnas, Landauer, & Harshman, 1990) is another purely statistical technique, which leverages word co-occurrence information from a large unlabeled corpus of text. LSA does not use any explicit human-organized knowledge; rather, it “learns” its representation by applying Singular Value Decomposition (SVD) to the words-by-documents co-occurrence matrix. LSA is essentially a dimensionality reduction technique that identifies a number of most prominent dimensions in the data, which are assumed to correspond to “latent concepts”. Meanings of words and documents are then represented in the space defined by these concepts.
Lexical databases such as WordNet (Fellbaum, 1998) or Roget’s Thesaurus (Roget, 1852) encode important relations between words such as synonymy, hypernymy, and meronymy. Approaches based on such resources (Budanitsky & Hirst, 2006; Jarmasz, 2003) map text words into word senses, and use the latter as concepts. However, lexical resources offer little information about the different word senses, thus making word sense disambiguation nearly impossible to achieve. Another drawback of such approaches is that creation of lexical resources requires lexicographic expertise as well as a lot of time and effort, and consequently such resources cover only a small fragment of the language lexicon. Specifically, such resources contain few proper names, neologisms, slang, and domain-specific technical terms. Furthermore, these resources have strong lexical orientation in that they predominantly contain information about individual words, but little world knowledge in general. Being inherently limited to individual words, these approaches require an extra level of sophistication to handle longer texts (Mihalcea, Corley, & Strapparava, 2006); for example, computing the similarity of a pair of texts amounts to comparing each word of one text to each word of the other text.
Studies in artificial intelligence have long recognized the importance of knowledge for problem solving in general, and for natural language processing in particular. Back in the early years of AI research, Buchanan and Feigenbaum (1982) formulated the knowledge as power hypothesis, which postulated that “The power of an intelligent program to perform its task well depends primarily on the quantity and quality of knowledge it has about that task.”
When computer programs face tasks that require human-level intelligence, such as natural language processing, it is only natural to use an encyclopedia to endow the machine with the breadth of knowledge available to humans. There are, however, several obstacles on the way to using encyclopedic knowledge. First, such knowledge is available in textual form, and using it may require natural language understanding, a major problem in its own right. Furthermore, even language understanding may not be enough, as texts written for humans normally assume the reader possesses a large amount of common-sense knowledge, which is omitted even from most detailed encyclopedia articles (Lenat, 1997). Thus, there is a circular dependency—understanding encyclopedia articles requires natural language understanding capabilities, while the latter in turn require encyclopedic knowledge. To address this situation, Lenat and his colleagues launched the CYC project, which aims to explicitly catalog the common sense knowledge of the humankind.
We developed a new methodology that makes it possible to use an encyclopedia directly, without the need for manually encoded common-sense knowledge. Observe that an encyclopedia consists of a large collection of articles, each of which provides a comprehensive exposition focused on a single topic. Thus, we view an encyclopedia as a collection of con-
cepts (corresponding to articles), each accompanied with a large body of text (the article contents). We propose to use the high-dimensional space defined by these concepts in order to represent the meaning of natural language texts. Compared to the bag of words and LSA approaches, using these concepts allows the computer to benefit from huge amounts of world knowledge, which is normally accessible to humans. Compared to electronic dictionaries and thesauri, our method uses knowledge resources that are over an order of magnitude larger, and also uniformly treats texts that are arbitrarily longer than a single word. Even more importantly, our method uses the body of text that accompanies the concepts in order to perform word sense disambiguation. As we show later, using the knowledge-rich concepts addresses both polysemy and synonymy, as we no longer manipulate mere words. We call our method Explicit Semantic Analysis (ESA), as it uses knowledge concepts explicitly defined and manipulated by humans.
Our approach is applicable to many NLP tasks whose input is a document (or shorter natural language utterance), and the output is a decision based on the document contents. Examples of such tasks are information retrieval (whether the document is relevant), text categorization (whether the document belongs to a certain category), or comparing pairs of documents to assess their similarity.1
Observe that documents manipulated in these tasks are given in the same form as the encyclopedic knowledge we intend to use—plain text. It is this key observation that allows us to circumvent the obstacles we enumerated above, and use the encyclopedia directly, without the need for deep language understanding or pre-cataloged common-sense knowledge. We quantify the degree of relevance of each Wikipedia concept to the input text by comparing this text to the article associated with the concept.
Let us illustrate the importance of external knowledge with a couple of examples. Without using external knowledge (specifically, knowledge about financial markets), one can infer little information from a very brief news title “Bernanke takes charge”. However, using the algorithm we developed for consulting Wikipedia, we find that the following concepts are highly relevant to the input: Ben Bernanke, Federal Reserve, Chairman of the Federal Reserve, Alan Greenspan (Bernanke’s predecessor), Monetarism (an economic theory of money supply and central banking), inflation and deflation. As another example, consider the title “Apple patents a Tablet Mac”. Without deep knowledge of hitech industry and gadgets, one finds it hard to predict the contents of the news item. Using Wikipedia, we identify the following related concepts: Apple Computer2, Mac OS (the Macintosh operating system) Laptop (the general name for portable computers, of which Tablet Mac is a specific example), Aqua (the GUI of Mac OS X), iPod (another prominent product by Apple), and Apple Newton (the name of Apple’s early personal digital assistant).
For ease of presentation, in the above examples we only showed a few concepts identified by ESA as the most relevant for the input. However, the essence of our method is representing the meaning of text as a weighted combination of all Wikipedia concepts. Then,
1. Thus, we do not consider tasks such as machine translation or natural language generation, whose output includes a new piece of text based on the input. 2. Note that we correctly identify the concept representing the computer company (Apple Computer) rather than the fruit (Apple).
depending on the nature of the task at hand we either use these entire vectors of concepts, or use a few most relevant concepts to enrich the bag of words representation.
The contributions of this paper are twofold. First, we propose a new methodology to use Wikipedia for enriching representation of natural language texts. Our approach, named Explicit Semantic Analysis, effectively capitalizes on human knowledge encoded in Wikipedia, leveraging information that cannot be deduced solely from the input texts being processed. Second, we evaluate ESA on two commonly occurring NLP tasks, namely, text categorization and computing semantic relatedness of texts. In both tasks, using ESA resulted in significant improvements over the existing state of the art performance.
Recently, ESA was used by other researchers in a variety of tasks, and consistently proved to be superior to approaches that do not explicitly used large-scale repositories of human knowledge. Gurevych, Mueller, and Zesch (2007) re-implemented our ESA approach for the German-language Wikipedia, and found it to be superior for judging semantic relatedness of words compared to a system based on the German version of WordNet (GermaNet). Chang, Ratinov, Roth, and Srikumar (2008) used ESA for a text classification task without explicit training set, learning only from the knowledge encoded in Wikipedia. Milne and Witten (2008) found ESA to compare favorably to approaches that are solely based on hyperlinks, thus confirming that the wealth of textual descriptions in Wikipedia is exlicitly superior to using structural information alone.
We investigate the performance of the regression learning method on two tasks: verb disambiguation, and transitive sentence similarity. In each case the system must compose SVO triples and compare the resulting semantic representations.
For the verb disambiguation task we use the GS2011 dataset [7]. This dataset consists of pairs of SVO triples in which the subject and object are held constant, and the verb is manipulated to highlight different word senses. For example, the verb draw has senses that correspond to attract and depict. The SVO triple report draw attention has high similarity to report attract attention, but low similarity to report depict attention. Conversely, child draw picture has high similarity to child depict picture, but low similarity to child attract picture. The gold standard consists of human judgements of the similarity between pairs, and we measure the correlation of the system’s similarity scores to the gold standard judgements.
For the transitive sentence similarity task we use the KS2013 dataset [9]. This dataset also consists of pairs of SVO triples, but the verb as well as the subject and object vary. For example, author write book and delegate buy land are judged by most annotators to be very dissimilar, while programme offer support and service provide help are considered highly similar. Again, we measure the correlation between the system’s similarity scores and the gold standard judgements.
The results of the patient satisfaction questionnaire are shown in Table 4. Interestingly, both control and learned policy group participants reported increasing their physical activity. The learned policy population reported that the SMS messages helped them increase and maintain the level of their activity, significantly more than did the control population (p < 10−3). None of the participants in the control constant weekly reminder group felt that the SMS messages were helpful. Both groups said they received enough messages, though we interpret this result for the control population as unanimous lack of satisfaction with the unchanging wording of the feedback they received.
We note that there is an interplay between Horner schemes and CSEE: a certain “optimal” Horner scheme may reduce the number of multiplications the most, but may expose less common subexpressions than a “mediocre” Horner scheme. Thus, we need to find a way to obtain a Horner scheme that reduces the number of operations the most after both Horner and CSEE have been applied.
Finding appropriate Horner schemes is not a trivial task, for at least four reasons. First, there are no known local heuristics. For the Traveling Salesman Problem (TSP), the distance between two cities can be used as a heuristic [11], and more specialized heuristics are able to solve symmetric TSP instances with thousands of cities (a historic example is a TSP with 7397 cities [12], [13]). Second, the Horner scheme is applied to an expression. This means that the scheme has a particular context: the nth entry applies to the subexpressions that are created after the first n − 1 entries in the Horner scheme have been applied to the expression. Third, Horner schemes are asymmetric: a scheme has a well-defined beginning and end. This is, e.g., in contrast with TSPs with closed paths (the most common subclass), since they have circle symmetry (translation and mirror symmetry). Fourth, the evaluation of a Horner scheme and CSEE is slow: for some benchmark expressions the evaluation took multiple seconds on a 2.4 GHz computer (see table 1). Our attempted parallelization of the evaluation function was unsuccessful, since the Horner scheme evaluation function is too fine-grained.
Keywords: irrelevance, independence, requisite in formation, belief networks, influence diagrams, d separation.
1 Introduction
One of the benefits of belief networks and influence diagrams is that so much knowledge is captured in the graphical structure. Without the need to compute with or even assess possible states and probabilities, many important properties of a model can be recog nized (Pearl 1988; Shachter 1988). To fully specify a network, the possible states and probability distribu tions must be obtained for all variables. For a partic ular inference query or decision problem, only some of that information is requisite.
Because this information is stored in the network structure, it lends itself to efficient analysis . In partic ular, statements of conditional irrelevance (or indepen dence) in belief networks can be verified in time linear in the size of the graph (Geiger et al 1989). Identifica-
tion of requisite information can also be determined in time linear in the size of the graph. These algorithms have been generalized to deal with deterministic nodes in belief networks and influence diagrams (Geiger et al 1990; Shachter 1990).
This paper introduces the Bayes-Ball algorithm, a sim pler and more efficient algorithm to identify condi tional irrelevance and requisite information. For belief networks, Bayes-Ball runs in time linear in the size of the active part of the graph, so it is considerably faster when most of a graphical knowledge base is irrelevant. It also corrects an error in the requisite information al gorithm given in Geiger (1990). More significantly, for decision problems it runs in time linear in the size of the graph; up until now the fastest algorithm (Shachter 1990) has been O((number of decisions)(graph size)). Finally, the decision algorithm has been extended to allow multiple separable value nodes.
The identification of irrelevant nodes and requisite in formation is a fundamental operation in any belief net work or influence diagram processor. It allows quick pruning of graphical knowledge bases, provides simple explanations for fundamental relationships (Poh and Horvitz 1996), and even suggests new algorithms (Lin and Drudzel 1997). Although it has not been pub lished before now, the Bayes-ball algorithm for belief networks has been freely shared in the community and is implemented in systems such as Knowledge Indus tries' DXPress™ and Microsoft's MSBN™.
Section 2 contains the notation and framework for the paper and the previous results from the literature. Sec tion 3 develops and proves the Bayes-ball algorithm for belief networks, and Section 4 extends and refines it for influence diagrams.
2 Notation and Framework
2.1 Irrelevance
A structured belief network B = (N, A, F) consists of nodes N and directed arcs A which together form a directed acyclic graph G(N,A), and a subset F of the nodes that are deterministically (functionally) re-
a)
Figure 1: Different assessment orders reveal different irrelevances
lated to their parents. Corresponding to each node j is an uncertain variable Xj. Consider the belief net work shown in Figure 1a. This represents a contest in which the prize depends on the state of two flipped coins. Coin 1 and Coin 2 are oval probabilistic nodes and Win Prize is a double oval deterministic node.
In referring to the graph G(N, A), for any node i EN, Pa( i) denotes the parents of i and De( i) denotes the descendants of i.
A joint probability distribution over XN is said to ad mit a directed factorization with respect to a struc tured belief network B = (N, A, F) if Xj is a deter ministic function of XPa(j) for all j E F and
Pr{XN} = IT Pr{XjiXPa(j)}· jEN
Given a joint probability distribution for XN, Pr{XN }, X1 is probabilistically irrelevant (usually called "independent") to XL given XK for some J, K, L � N if
Pr{X1IXK,XL} = Pr{XJIXK}·
Given a structured belief network B = (N, A, F) and sets J, K, L � N, XJ is said to be irrelevant to XL given XK in B, denoted XJ ..is XLIXK, if XJ is prob abilistically irrelevant to XL given XK for any joint probability distribution Pr{XN} that admits a di rected factorization with respect to B.
Less formally, XJ is irrelevant to XL given XK in B if, having observed XK, one can learn nothing more about XJ by also observing X£. The following proposition is well known (Pearl1988).
Proposition 1 (Irrelevance Decomposition) Given any structured belief network B = (N, A, F) and any subsets J, K, L � N, XJ ..is XLIXK if and only if
Xj ..is XtiXK for all j E J and l E L.
It is important to understand that probabilistic irrel evance is not the same as the irrelevance represented by a belief network. For example, Proposition 1 does not hold for particular probability distributions un less their independence is completely representable in a structured belief network. Consider the coin flip ping contest shown in Figure 1a. The decision-maker believes that the two flips are irrelevant but that the prize is determined by them. If he believes that the coins are both fair and the prize will be awarded if the two coins match, then the network in Figure 1b is
Bayes-Ball: The Rational Pastime 481
also valid, since he believes that the state of one coin tells him nothing about whether he will win. (That would not be true if the coins were not fair or the contest were won with two heads.) A misapplication of Proposition 1 would suggest that seeing both coins would tell him nothing about whether he will win!
2.2 Deterministic Irrelevance
Given any structured belief network B = (N, A, F) and a deterministic node j E F, the variable Xj is certain or effectively observed if the parents of node j, Pa(j), are observed. Similarly, if each parent of j E F is either observed or effectively observed, then the variable Xj is also effectively observed. More formally, if the variables XK are observed for some K � N, a node j is said to be functionally determined by K if either j E K or xj is a deterministic function of XK. The set of nodes functionally determined by K, FK, can be described by the recursive equation,
FK +---K U {i E F: Pa(i) � FK },
corresponding to nested instances of effective observa tion. For example, in Figure la, if the two coins are observed then it is functionally determined whether the prize will be awarded.
Proposition 2 (Deterministic Irrelevance) Given a structured belief network B = ( N, A, F) and sets J, K � N, XJ is functionally determined by XK in belief network B if and only if XJ ..is XNIXK.
As a special case of Proposition 2,
XpK ..is XNIXK for all K � N.
2.3 Target Sets
Efficient computation of the following sets is one ob jective of this paper. The other objective is efficient computation of the related requisite sets for sequential decision problems represented by influence diagrams.
The irrelevant nodes for XJ given XK, denoted XN,(JIK), are those nodes which are conditionally ir relevant to XJ given XK,
The requisite or required information needed to com pute Pr{XJ IXK} depends on the probabilistic irrele vance, rather than the conditional irrelevance revealed in the belief network graph. Thus, from the graph we can only recognize the information which "might" be needed for any probability distribution.
The requisite probability nodes for J given K, denoted N1,(JIK), are those nodes for which conditional prob ability distributions (and possible states) might be needed to compute Pr{XJIXK}· The conditional probability distribution for a deterministic node is usu ally specified as a deterministic function.
482 Shachter
The requisite observations for J given K, Ne(J\K) � K, are those observed nodes for which observations (and hence the possible states which might be ob served) might be needed to compute Pr{XJ\XK }.
In previous papers (Shachter 1988; Shachter 1990), these sets were referred to as the "minimal sets of rel evant nodes," N1T(J\K) = Nv(J\K) and Nn(J\K) = Nv(J\K) U Ne(J\K).
2.4 D-separation
The Bayes-ball algorithm is strongly based on the con cept of d-separation (Geiger et al 1989; Pearl 1988; Pearl et al 1990) and its deterministic generalization, D-separation (Geiger et al 1990).
An active path from J to L given K is a simple trail (or undirected path) between i E L and j E J, such that every node with two incoming arcs on the trail is or has a descendant in K; and every other node on the trail is not functionally determined by K. As a special case, active paths can be just a single node, that is, i = j.
Given sets of nodes, J, K, and L from belief network B, K is said to D-separate J from L in B if there is no active path from J to L given K. This condition determines all irrelevancies represented in the belief network (1990).
Theorem 1 (D-Separation) Given a structured be lief network B = (N, A, F) and J, K, L � N, XJ .ls XL\XK if and only if K D-separates J from Lin B.
Geiger (1989; 1990) devised linear-time reachability al gorithms for finding active paths from J given K that inspired the Bayes-ball algorithm. Unfortunately, ac tive paths are not adequate for gathering requisite in formation since they cannot enter clusters of function ally determined nodes. The active path algorithms also must search the entire graphical model to find the ancestors of K and the nodes functionally determined byK.
An alternative characterization of the irrelevance rep resented by a belief network involves a conversion to related undirected graphical structures (Lauritzen et al 1990). Those results could be generalized for deter minacy following the appmach in (Shachter 1991) but, due to the nature of the conversion, the undirected ap proaches cannot find all nodes irrelevant to J given K in linear time.
3 The Bayes-Ball Algorithm
This section presents two versions of the Bayes-ball algorithm, a simpler, preliminary version and a more refined, final version. The final version determines the irrelevant and requisite sets for a given inference query in time linear in the size of the belief network.
3.1 Simpler Bayes-Ball
The simpler version of the Bayes-ball algorithm for J given K sends a bouncing ball to visit nodes in the belief network, starting from nodes J. Depending on the type of node and the direction from which the ball came to visit (from parents; from children), the ball can pass through the node (from any parent to all chil dren; from any child to all parents), bounce back (from any parent to all parents; from any child to all chil dren), or be blocked. This is summarized in Figure 2 in which:
• an unobserved probabilistic node passes balls through but also bounces back balls from children;
• an observed node bounces back balls from parents but blocks balls from children; and
• a deterministic unobserved node always passes balls through.
Algorithm 1 (Simpler Bayes-Ball Algorithm) The algorithm explores a structured belief network B = (N, A, F) with respect to the expression Pr{XJ\XK}·
1. Visit each node in J (as if from one of its chil dren).
2. When visiting a node j:
(a) If the visit to j is from a child: i. If j E K then do nothing;
ii. otherwise, if j E F then visit each of j 's parents;
iii. otherwise, visit each of j 's parents and each of j 's children.
{b) If the visit to j is from a parent: i. If j E K then visit each of j 's parents;
ii. otherwise, visit each of j 's children.
As an example of the simpler Bayes-ball algorithm, consider the belief network shown in Figure 3a, in which J = { 6} (denoted by the parents of the "ghost" rounded rectangle) and K = { 2, 5} (denoted by shad ing). The simpler Bayes-ball algorithm is illustrated in
Figure 3: The simpler and refined versions of the Bayes-ball algorithm applied to a small belief network.
Figure 3b. Starting from the rounded rectangle, visit its parent, node 6. From node 6 pass the ball through to nodes 3 and 5 (and to the children of node 6 if it had any). Node 5 does not send the ball anywhere, but node 3 bounces it to its children, nodes 2 and 6, (and would pass it through to its parents if it had any). Node 6 has no children to pass to, but node 2 bounces the ball back to its parents, nodes 1 and 3. In turn, nodes 1 and node 3 both bounce it back to node 2. Although the ball will keep bouncing forever, at this point it has visited all of the nodes it will ever visit from all of the directions it will ever visit them.
3.2 The Final Bayes-Ball Algorithm
An obvious improvement to this algorithm is to main tain a list of nodes to be visited from parents and from children. Another, more critical improvement is to only visit the same arc in the same direction once. (This is not only more efficient, but necessary to termi nate the infinite loops.) It is accomplished by marking the top of a node when the ball is sent to the node's parents, marking the bottom of the node when the ball is sent to the node's children, and checking an observed node when it is visited. These marks not only allow the algorithm to terminate, but they also record significant results. Returning to the belief net work shown in Figure 3a, apply these modifications to obtain the network shown in Figure 3c. At the start, node 6 is visited (as if from its child) and it sends the ball to its parents and children, marking its top and bottom. Node 5 does not send the ball anywhere, so it is checked but not marked. Now node 3 is visited from its child node 6, so it sends the ball to its parents and children and marks its top and bottom. Node 6 receives the ball from parent node 3 and it would send it to its children if its bottom were not marked already. Node 2 receives the ball from parent node 3, sends it to its parents, and marks its top. Node 1 receives the ball from child node 2, sends it to its parents and chil dren, and marks its top and bottom. Finally node 2 and node 3 receive the ball and recognize that there is nothing new to do with it.
The Bayes-ball algorithm was run on the belief net work queries shown in parts a, c, e, and g of Figure 4
Bayes-Ball: The Rational Pastime 483
Figure 4: The Bayes-ball algorithm applied to some small belief networks
to obtain the networks shown in parts b, d, f, and h. In Figure 4b, nodes 1 and 2 are not visited. In Figure 4d, nodes 4, 5, and 6 are not visited because observed node 2 will not let the ball pass through, although node 2 does bounce it back to its parents. In Figure 4f, the ball is passed through nodes 5 and 4, but it is not passed through node 3, so node 1 is not visited. Fi nally, in Figure 4h, the ball is passed through nodes 6, 5, 4, and 2, but because there is no bounce back from node 1, the deterministic node 2 never passes the ball through to node 3.
These examples help to illustrate some of the proper ties of the Bayes-ball algorithm:
• the node j is visited by the algorithm if and only if observing Xj might change one's belief about X1 given XK\{i};
• we need no information about any node which has not been visited;
• we might need to know what state was observed for any observed node which is visited;
• the ball bounces back to a parent from a child only if that child is observed, such as node 2 in Figure 4d;
• the ball never bounces back to a child from a par ent functionally determined by K, such as nodes 1 and 2 in Figure 4h;
• the ball is passed to parents to find requisite infor mation and passed to children looking for relevant observations;
• any node not marked on its bottom is irrelevant to J given K;
484 ShacMer
• we might need a conditional probability distri bution (or deterministic function) for any node marked on its top.
Algorithm 2 (Bayes-Ball) The algorithm explores a structured belief network B = (N, A, F) with respect to the expression Pr{X1IXK} and constructs the sets of relevant and requisite nodes.
1. Initialize all nodes as neither visited, nor marked on the top, nor marked on the bottom.
2. Create a schedule of nodes to be visited, initialized with each node in J to be visited as if from one of its children.
3. While there are still nodes scheduled to be visited:
(a) Pick any node j scheduled to be visited and remove it from the schedule. Either j was scheduled for a visit from a parent, a visit from a child, or both.
{b) Mark j as visited. (c) If j � K and the visit to j is from a child:
i. if the top of j is not marked, then mark its top and schedule each of its parents to be visited;
ii. if j � F and the bottom of j is not marked, then mark its bottom and sched ule each of its children to be visited.
(d) If the visit to j is from a parent: i. If j E K and the top of j is not marked,
then mark its top and schedule each of its parents to be visited;
ii. if j � K and the bottom of j is not marked, then mark its bottom and sched ule each of its children to be visited.
4. The irrelevant nodes, Ni(JIK), are those nodes not marked on the bottom.
5. The requisite probability nodes, N11(JIK), are those nodes marked on top.
6. The requisite observation nodes, Ne(JIK), are those nodes in K marked as visited.
3.3 Bayes-Ball Proofs
The section proves that Bayes-ball properly computes the irrelevant and requisite sets and does it in sub linear time.
Theorem 2 (Bayes-Ball Irrelevance) Given a structured belief network B = (N, A, F) and J, K, L � N, X1 _l_B XLIXK if and only if L � Ni(JIK), as de termined by the Bayes-Ball algorithm.
Proof: Based on Theorem 1 it is sufficient to show that a node l will be marked on the bottom if and only if there is an active path from J to l given K.
First, any node i is or has a descendant inK if and only if it would always bounce the ball back to its parents after one of them sent the ball to it, since the bounce back can only come from an observed descendant.
Second, i is functionally determined by K if and only if it would never bounce the ball back to its children after one of them sent the ball to it, since any unobserved probabilistic ancestor would bounce it back.
Therefore, given any active path from J to l, Bayes ball can travel the path (perhaps with the detours just described) and it will be visiting the children of node l, either by passing through or bouncing back. Similarly, if the Bayes-ball is sent from l to its children then there must be an active path from J to l . D
Theorem 3 (Bayes-Ball Requisite Sets) Given a structured belief network B = (N, A, F), for any J, K � N, the Bayes-ball algorithm determines the requisite probability nodes, N11( JIK), and the requisite observation nodes, Ne(JIK).
Proof: Whether a node j is visited does not depend at all on whether it is observed or deterministic, but only on the properties of other nodes leading to it. If the algorithm visits an unobserved probabilistic node j, then it will mark j on the bottom and, by the preced ing theorem, j appears relevant to J given K. Thus, the algorithm will visit node j if and only if, keeping all other nodes unchanged but changing j to be unob served and probabilistic, Xi might be relevant to X1 given XK.
As a result, the state of an observed node k provides no information about J given K unless it is visited. Thus, Ne(JIK) comprises those nodes inK which are visited during the algorithm.
To determine N11(JIK), add a new observed parent (a probability distribution/deterministic function) to every node. These new nodes would not have req uisite observations unless they were visited, and this would occur only when their corresponding children have been marked on top. D
The last part of the preceding proof relates to the flaw in the requisite information algorithm in Geiger (1990). In that paper, relevance is used to recognize requisite distributions, but that can introduce uncer tainty and eliminate functional determinacy. On the other hand, in this work, requisite information is based on the need for observations and thus no spurious un certainty is introduced. Bayes-ball is able to gather both irrelevance and requisite information by recog nizing their dist;...•ction-visiting parents for requisite information and visiting children for relevance.
Theorem 4 (Bayes-ball Complexity) Given a structured belief network B = (N, A, F), for any J, K � N, the Bayes-ball algorithm runs in O(I NI + I Avi ), where A,, are the arcs incident to the nodes marked during the algorithm. In the worst case,
it is linear time in the size of the graph.
Proof: Each node has to be visited to initialize flags and to harvest results and then at most once for each "active" incident arc. Only the active arcs will be vis ited, because the algorithm searches locally to deter mine whether a node has an observed descendant or is functionally determined. D
4 Bayes-Ball for Decisions
The results in Section 3 determining requisite sets for probabilistic inference can be applied to problems of decision making under uncertainty (Shachter 1 986; Shachter 1 988) . In this section, the Bayes-ball algo rithm is adapted to determine the requisite informa tion for decision problems represented by influence di agrams, yielding a new, significantly more efficient and powerful algorithm.
The influence diagram is a belief network augmented with rectangular decision nodes, representing vari ables under the control of the decision maker, and rounded rectangular value nodes, representing the cri terion whose expectation the decision maker maxi mizes to choose among decision alternatives. The deci sions, D, are assumed to be ordered in time, d1, ... , dm, and the uncertain variables are partitioned into sets E, W1, ... , Wm+1 such that the uncertain variables Wi are assumed to be observed before decision � but af ter decision ll,;_1. Some of the uncertainties, Wm+l, will not be observed before any of the decisions, while others, E, have already been observed. We assume that if there are multiple value nodes in V that they are related by a separable function, that is, either a sum or a product (of nonnegative factors). This struc ture is illustrated in Figure 5. Implicit in the figure is the no forgetting assumption that at the time of any decision, the decision maker will remember all pre vious decisions and the uncertain variables known at the time they were made. These are captured in the information sets, I(di), for each decision d;. It follows that I(dl) = W1 U E and I(di) = Wi U �-1 U I(di-1) for i = 2, . . . , m . These information sets are indicated in the influence diagram by information arcs into the decision nodes.
Consider the influence diagram shown in Figure 6a modeling the design of an experiment. The decision maker seeks to maximize expected Benefit minus Cost.
Benefit depends on the uncertain variable State and the decision Act. He has already observed History which depends on State, and before making the Act decision, he will observe Experiment, which depends on both the Design decision and the State. There is a Cost to the Design decision. This problem is solved recursively through dynamic programming to deter mine an optimal policy, d:r,(xr(dm)) for the latest de cision, dm as a function of the information available at the time of the decision. Only the value nodes which are descendants of dm are affected by this pol icy, Vm = VnDe(dm) (Heckerman and Shachter 1 995; Shachter and Peot 1992) . This policy must satisfy
E{Vmld;',(XJ(d,.)), XJ(d,.)} = maxdE{Vmld, XJ(d,.)}·
This suggests the use of the Bayes-ball algorithm to determine the requisite sets. The decision dm can be replaced by the optimal_ policy to obtain the influence diagram shown in Figure 6c, in which Act is now a de terministic function of the requisite observations, De sign, Experiment, and History. At this point, the value nodes Vm are irrelevant of any earlier decisions given the requisite observations at the time of decision drn. So the value for drn-l can be characterized in terms of Vm-l and the requisite observations from the first step. Continuing with the example, Cost, Design, Ex periment, and History are the goals for Design, and the policy is a function of History as shown in Figure 6e.
Suppose instead that the decision-maker were able to observe the State before the Act decision, as shown in Figure 6g. Now, State is the only requisite observed node before Act, yielding the diagram shown in Fig-
486 Shachter
ure 6i. As a result, Cost and State are the goals for Design as shown in Figure 6k.
The requisite probability nodes for decision i, denoted N�, are those nodes for which conditional probability distributions (and possible states) might be needed to evaluate the decision problem starting with decision i. N� is the set of requisite probability nodes for the entire decision problem.
The requisite observations for decision i , denoted N�, are those nodes which can be observed before decision i that might be worth observing (and hence for which the possible states might be needed) in order to evalu ate the decision problem starting with decision i. N� is the set of requisite observation nodes before the en tire decision problem, assuming that we have observed nodes E now; if E = 0 then N� = 0.
Algorithm 3 (Decision Bayes-Ball) This algorithm constructs the sets of requisite nodes for an influence diagram with separable value nodes V , evidence nodes E, and time-ordered decision nodes D = {d1, . . . ,dm}·
1. start with the last decision, dm:
(a) Determine the relevant value nodes, Vm = V n De(dm). {If there is only one value node for the problem, it should be in Vm. )
{b) Run the Bayes-ball algorithm on Vml{dm} U I( dm), ignoring any informational arcs.
(c) If drn is not marked as visited then deci sion dm is irrelevant-it has no effect on the decision-maker's value.
{d) The requisite observation nodes for decision dm are the nodes in I(dm) marked as visited,
(e) The requisite probability nodes starting with decision dm are the nodes marked on top,
2. iterate backwards for each earlier decision di, i = m - 1, . . . ,1:
{a)
{b)
(c)
{d)
(e) Determine the relevant value nodes, Vi = V n (De(di) \ De(d;+l)), ignoring any infor mational arcs in determining descendants. Resume the Bayes-ball algorithm on Vi U N�+ll { di} U I ( di), ignoring any informational arcs. If di is not marked as visited then decision d; is irrelevant. The requisite observation nodes for decision di , N�, are those nodes in I(di) marked as visited. The requisite probability nodes starting with decision d;, N�, are all of the nodes marked on top.
3. compute the requisite information at the start of the decision problem {now) given current observa tions E:
(a)
{b)
(c)
Resume the Bayes-ball algorithm on N! JE, ignoring any informational arcs.
The requisite observations needed now, N�, are those nodes in E marked as visited. (If E is empty, then no observations are needed now. )
The requisite probability nodes starting now, N�, are all of the nodes marked on top.
Consider the influence diagram examples shown in Fig ure 6. As explained earlier, the left column represents the influence diagram at each step as decisions are re placed by policies for the situations without and with observation of State before the Act decision. The right column shows how the Bayes-ball algorithm for de cisions is applied sequentially to the original problem structure. Bayes-ball works effiCiently on influence dia grams by resuming rather than restarting at each step. The sets computed by the algorithm are shown in the following table:
I Figure 6 I i I N' e N' E ajb 2 Design, History, Benefit, History,
Experiment Experiment, State c/d 1 History above+ Cost e/f 0 History above i/h 2 State Benefit i/j 1 History Benefit, Cost, History, State k/l 0 History above
Theorem 5 (Decision Bayes-Ball) Given an in fluence diagram with separable value nodes and evi dence now, Algorithm 3 will determine the requisite node sets in O(JNI + JAI), that is, linear time in the number of nodes and arcs.
Proof: The correctness of the requisite sets follows from the Bayes-ball algorithm and the decision theory summarized in this section, so the big question here is the computational complexity.
There are two issues to be resolved there. First, the sets of value nodes can be determined in linear time in the size of the graph. Second, the Bayes ball algorithm for each decision can be resumed dur ing the algorithm, keeping the previous results. This works because the information sets are monotone, E � {d1} U I(d1) � ... � {dm} U I(dm)· Once the algorithm has been run on Vi U N�+1J{d;} UI(d;), it is easy to run it on lti-1 U N�J{di-1} U I(di-1)· The nodes ({di} U I(di) \ {di-1} U I(d;-1 )) = {d;} U Wi are no longer observed, but the previously requisite observations N�+l \ N� are now scheduled to receive messages from their children, so they will now pass the Bayes-ball through to their parents automatically! The Bayes-ball sets of nodes visited, marked on the
top, and marked on the bottom, can grow monotoni cally as the decisions are visited in reverse order. The computation of N� is still correct, since only the cur rently observed visited nodes are included and the number of node visits is O( INI + IAI). D
5 Conclusions
This paper has introduced a simple and efficient algo rithm to compute irrelevant and requisite sets for infer ence and decision problems represented as structured belief networks and influence diagrams. Because these procedures can be used before states and distributions are assessed they are quite useful. Although the ef ficiency gain over earlier belief network algorithms is modest (sub linear vs. linear time), the computation of requisite information is performed at the same time as irrelevance is determined.
The significant improvement presented here is in determining requisite information for decision prob lems. The new algorithm is linear time instead of O((number of decisions)(graph size)), and can exploit separable values.
These algorithms recognize the special properties of deterministic relationships. Such models are becom ing increasingly useful as new developments arise in causal models (Heckerman and Shachter 1995). An interesting extension of the deterministic model would be to represent determinism in more than one assess ment order, such as representing when deterministic relationships are invertible.
Another extension is to apply Bayes-ball to cyclical networks (Pearl and Dechter 1996). Bayes-ball can be applied to such networks without any modifications.
Although the algorithm recognizes the full informa tion requirements to value decision problems, it can be modified to recognize some additional efficiencies in the dimensions of policy. For example, in Figure 6k, the choice of Design would not depend on History if the total value were the sum of Cost and Benefit, although the value derived would indeed depend on History. As a result we wouldn't need a probability distribution for History and State to determine the optimal policy, but we would need them to value that policy.
Acknowledgements
This paper has benefited from the comments, sugges tions, and ideas of many friends and students, most no tably Danny Geiger, Derek Ayers, Mark Peot, David Heckerman, Marcia Tsugawa, Robert Korsan, Richard Barlow, and anonymous referees.
References
Geiger, D., T. Verma, and J. Pearl. "d-separation: from theorems to algorithms." In Fifth Workshop
Bayes-Ball: The Rational Pastime 487
on Uncertainty in Artificial Intelligence in U ni versity of W indsor, Ontario, 118-125, 1989.
Geiger, D., T. Verma, and J. Pearl. "Identifying in dependence in Bayesian networks." Networks 20 (1990): 507-534.
Heckerman, D. and R. Shachter. "Decision-Theoretic Foundations for Causal Reasoning." Journal of Ar tificial Intelligence Research 3 (1995): 405-430.
Lauritzen, S. L., A. P. Dawid, B. N. Larsen, and H.-G. Leimer. "Independence properties of directed Markov fields." Networks 20 (1990): 491-505.
Lin, Y. and M. J. Drudzel. "Computational Advan tages of Relevance Reasoning in Bayesian Belief Net works." In Uncertainty in Artificial Intelligence: Proceedings of the Thirteenth Conference, 342- 350. San Mateo, CA: Morgan Kaufmann, 1997.
Pearl, J. Probabilistic Reasoning in Intelligent Systems. San Mateo, CA: Morgan Kaufmann, 1988.
Pearl, J. and R. Dechter. "Identifying Independen cies in Causal Graphs with Feedback." In Uncer tainty in Artificial Intelligence: Proceedings of the Twelfth Conference, 420-426. San Mateo, CA: Morgan Kaufmann, 1996.
Pearl, J., D. Geiger, and T. Verma. "The Logic of In fluence Diagrams." In Influence Diagrams, Belief Nets, and Decision Analysis, eds. R. M. Oliver and J. Q. Smith. 67-87. Chichester: Wiley, 1990.
Poh, K. L. and E. Horvitz. "A Graph-Theoretic Anal ysis of Information Value." In Uncertainty in Ar tificial Intelligence: Proceedings of the Twelfth Conference, 427-435. San Mateo, CA: Morgan Kauf mann, 1996.
Shachter, R. "A Graph-Based Inference Method for Conditional Independence." In Uncertainty in Ar tificial Intelligence: Proceedings of the Seventh Conference, eds. B D'Ambrosio, P Smets, and P Bonissone. 353-360. San Mateo, CA: Morgan Kauf mann, 1991.
Shachter, R. D. "Evaluating Influence Diagrams." Ops. Rsrch. 34 (November-December 1986): 871- 882.
Shachter, R. D. "Probabilistic Inference and Influence Diagrams." Ops. Rsrch. 36 (July-August 1988): 589-605.
Shachter, R. D. "An Ordered Examination oflnfluence Diagrams." Networks 20 (1990): 535-563.
Shachter, R. D. and M. A. Peot. "Decision Making Using Probabilistic Inference Methods." In Uncer tainty in Artificial Intelligence: Proceedings of the Eighth Conference, 276-283. San Mateo, CA: Morgan Kaufmann, 1992.
For GNU Prolog, we designed a specific language to define FD constraints which is both flexible and powerful. The basic X in r is sufficient to define simple arithmetic constraints but too restrictive to handle constraints like min(X ,Y ) = Z or reified constraints, both of which need some form of delay mechanism. Another limitation is that it is not possible to explicitly indicate the triggers for a particular propagator: these are deduced from the indexical used in the X in r primitives. The GNU Prolog constraint definition language, FD, has then been designed to allow the user to define complex constraints and proposes various constructs to overcome these limitations. FD programs are compiled into C by the fd2c translator. The resulting C program is then compiled and the object fits into the compilation scheme shown in Figure 1. We present the main features of the constraint definition language by means of a few examples.
Ω L(Γ).
8If no axiomatisation of L is given, we can proceed more brute force and set R = {A1, . . . , An → A | {A1, . . . , An} ⊢L A}.
Proof. We first show that ∆¬ = AbΩ \ φ¬ is conflict-free. Assume for a contradiction that it is not and hence that there is a B ∈ Ω for which Γ∪∆¬ ⊢ B,¬B. Hence, by the compactness of L and Fact 3, Γ ⊢ ∨
Θ for some finite Θ ⊆ ∆. Let Θ be ⊂-minimal with this property. Hence, Θ ∈ ΣL(Γ). However, then φ ∩Θ 6= ∅, a contradiction.
We now show that ∆¬ is stable. For this, let ¬B ∈ AbΩ \ ∆¬. Hence, B ∈ φ. With Fact 1.2, there is a Θ ∈ ΣL(Γ) such that {B} = φ ∩ Θ. Since Γ ⊢ ∨
Θ, by Fact 3 also Γ∪ (Θ¬ \ {¬B}) ⊢ B. By the monotonicity of L, Γ∪∆¬ ⊢ B which means that ∆ attacks B.
Since∆¬ is conflict-free and attacks everyA ∈ AbΩ\∆¬, it is easy to see that ∆¬ is closed and stable.
Example 5 (Ex. 1 contd). Take AbΩ = {¬(A ∧ ∼A) | A ∈ LCLuN} and R an adequate rule system for CLuN. Where Γ = {∼p,∼q, p ∨ q, p ∨ r, q ∨ s}. There are two stable extensions: AbΩ \{¬(p∧∼p)} and AbΩ \{¬(q∧∼q))}. To see this observe that e.g. Γ ∪ {¬(q ∧ ∼q)} ⊢CLuN p ∧ ∼p.
Lemma 2. If ∆¬ ⊆ AbΩ is conflict-free in ABF Ω L (Γ) then there is a φ ∈ ΦL(Γ) for which ∆ ⊆ Ω \ φ. Proof. Suppose ∆ 6⊆ Ω \ φ for all φ ∈ ΦL(Γ) and ∆ ⊆ Ω. By Fact 1, Ω \ ∆ is not a choice set of ΣL(Γ). Thus, there is a Θ ∈ ΣL(Γ) for which Θ ⊆ ∆. Since Γ ⊢ ∨
Θ, also Γ ∪ (Θ \ {A}) ⊢ ¬A for any A ∈ Θ. Thus, Γ ∪ ∆ is not L-consistent since Γ∪∆ ⊢ A,¬A by monotonicity. By Fact 2, Γ ∪ ∆ ⊢R(L) A,¬A and thus, ∆ is not conflict-free in
We conducted experiments in python using the Theano and Keras libraries (The Theano Development Team, 2016), (Chollet, 2015). All of our code ran on a single Nvidia Titan X GPU, while using the CnMEM and cuDNN (5.103) extensions, and we visualized our results using matplotlib (Hunter, 2007). We used the same seed in all our calculations to insure the starting weights were equivalent in every set of experiments. The only source of randomness stemmed from the non-deterministic behavior of the cuDNN libraries.
We seek to study the attributes which might disclose the holistic picture of the overall engagement rate of AI-related tweets. This rate provides us with an information on patterns of public interests and perceptions in AI. We measure the engagement by considering the ‘favorites’, ‘likes’, ‘replies’ and ‘mentions’ of a Twitter post. We first compute Twitter engagement statistics that are shown in Table 5.
Tweets made by AIT are more likely to be retweeted than favorited by the users on this platform. 63.3% of their tweets are retweeted atleast once. This is significantly higher than the general dataset which is 11.99% as shown by the existing literature (Suh et al. 2010). Where as, the tweets posted by EAIT received more number of favorites than retweets as only 35% of the tweets are retweeted atleast once.
Tweets from both the categories of users have higher probabilities of containing atleast one user handle. This may suggest that users are more likely to interact or engage in discussions with each other about AI on Twitter. On the other hand, 68.5% of the tweets posted by AIT in our dataset have atleast one url shared as part of the tweet where as 48% of the tweets made by EAIT have urls. Sharing large percentage of urls by AIT compared to EAIT could be one of the reasons for receiving more retweets than favorites. Literature (Java et al. 2007; Kwak et al. 2010; Yang and Counts 2010) considers retweeting as one of the features to measure information diffusion. Based on these results, tweets posted by AIT diffuse faster (higher retweet rate) than the tweets posted by EAIT.
Since one of the main features of the proposed architecture is an ability to learn indicators of compromise IOCs, below it is shown to what types of traffic neurons in the layer just before the first pooling are sensitive. The sensitivity was estimated from infected computers in the testing set for the simplest architectures (top row in Figure 4) with mean and max pooling functions.
We have not observed much difference between IOCs learned by network with mean and max pooling functions. Learned IOCs included:
• tunneling through url (example shown in appendix due to its length);
• sinkholed domains such as hxxp://malware.vastglow s.com, hxxp://malware.9f6qmf0hs.ru/a.htm?u=3969 23, hxxp://malware.ywaauuackqmskc.org/.
• domains with repetitive characters such as hxxp://ww wwwwwwwwwwvwwwwwwwwwwwwwwwwwwvwwwwwwwwwwwwwwww
wwwwwwwwwwwwvww.com/favicon.ico or hxxp://ibuyi tttttttttttttttttttttttttttttttttttibuyit.com/ xxx.zip;
• https traffic to raw domains such as hxxps://209.12 6.109.113/;
• subdomain generated by an algorithm on a hosting domain, for example d2ebu295n9axq5.webhst.com, d2e2 4t2jgcnor2.webhostoid.com, or dvywjyamdd5wo.we bhosteo.com;
• Download of infected seven-zip: d.7-zip.org/a/7z93 8.exe3.
The SDH problem is defined as the following minimization problem:
min B,W,P
‖Y −W⊤B‖2 + λ‖W‖2 + ν‖B−P⊤X‖2, (4)
where ‖ · ‖ is the Frobenius norm, and λ ≥ 0 and ν ≥ 0 are balance parameters. The first term includes the classification model explained in Sec. 2.3. The second term is a regularizer for W to avoid overfitting. The third term indicates the fitting errors due to binary coding.
In this optimization, it is sufficient to compute P, i.e., if P is obtained, B can be obtained by (1), and W can be obtained from the following simple least squares equation:
W = ( BB⊤ + λI )−1 BY⊤. (5)
However, due to the difficulty of optimization, the optimization problem of (4) is usually divided into three subproblems of the optimization of B,W, and P. Thus, the following alternating optimization is performed:
(i) Initialization: B is initialized, usually randomly.
(ii) F-Step: P is computed by the following simple least squares method:
P = ( XX⊤ )−1 XB⊤. (6)
(iii) W-Step: W is computed by (5).
(iv) B-Step: After fixing P and W, equation (4) becomes:
min B
‖Y‖2 − 2Tr ( YW⊤B ) +Tr ( B⊤WW⊤B )
+ ν ( ‖B‖2 − 2Tr ( P⊤XB ) + ‖P⊤X‖2 )
⇒ min B
Tr ( B⊤QB+ F⊤B ) ,
(7)
where
Q=WW⊤∈RL×L, F=−2 ( WY + νP⊤X ) ∈RL×N .
(8) Note that Tr ( B⊤B ) = LN . The trace can be rewritten as
min {bi}
N∑
i=1
b⊤i Qbi + f ⊤ i bi, (9)
where fi ∈ RL is the i-th column vector of F. {bi} are actually independent of one another. Therefore, it reduces to the following 0-1 integer quadratic programming problem for each i-th sample:
∀i min bi∈{−1,1}L b⊤i Qbi + f ⊤ i bi. (10)
(v) Iterate steps (ii)∼(iv) until convergence.
ar X
iv :1
70 2.
08 49
5v 2
[ cs
.A I]
4 M
ar 2
01 7
The appetite of the public and prominent intellectuals for the study of the ethical implications of artificial intelligence has increased in recent years. One captivating possibility is that artificial intelligence research might result in a ‘superintelligence’ that puts humanity at risk. (Russell 2014) has called for AI researchers to consider this possibility seriously because, however unlikely, its mere possibility is grave. (Bostrom 2014) argues for the importance of considering the risks of artificial intelligence as a research agenda. For Bostrom, the potential risks of artificial intelligence are not just at the scale of industrial mishaps or weapons of mass destruction. Rather, Bostrom argues that artificial intelligence has the potential to threaten humanity as a whole and determine the fate of the universe. We approach this grand thesis with a measure of skepticism. Nevertheless, we hope that by elucidating the argument and considering potential objections in good faith, we can get a better grip on the realistic ethical implications of artificial intelligence. This paper is in that spirit. We consider the argument for this AI doomsday scenario proposed by Bostrom (Bostrom 2014). Section 1 summarizes Bostrom’s argument and motivates the work of the rest of the paper. In focuses on the conditions of an “intelligence explosion” that would lead to a dominant machine intelligence averse to humanity. Section 2 argues that rather than speculating broadly about general artificial intelligence, we can predict outcomes of
Copyright c© 2015, Sebastian Benthall.
artificial intelligence by considering more narrowly a few tasks that are essential to instrumental reasoning. Section 3 considers recalcitrance, the resistance of a system to improvements to its own intelligence, and the ways it can limit intelligence explosion. Section 4 contains an analysis of the recalcitrance of prediction, using a Bayesian model of a predictive agent. We conclude that prediction is not something an agent can easily improve upon autonomously. Section 5 discusses the implication of these findings for further investigation into AI risk.
We consider the worst case wherein the space of partial feasible compositions explored by Algorithm 4 is a tree rooted at ⊥; let b be its maximum branching factor (corresponding to the maximum number of extensions produced by the functional composition algorithm), and h its height (corresponding to the maximum number of components used in a composition that satisfies ϕ). In the worst case, in each iteration of Algorithm 4, every element of L, the list of current partial feasible compositions, ends up in the non-dominated set θ.
Each level in the tree corresponds to one iteration of Algorithm 4, and at the lth iteration, in the worst case there are bl nodes in L. Hence, the complexity of the lth iteration is O ( (bl)2d + blfe )
, where the first term corresponds to computing the non-dominated set among the current set of partial feasible compositions, and the second term corresponds to computing the feasible extensions of each partial feasible composition. Hence, the overall complexity of Algorithm 4 is O ( ∑h
l=0 (b 2ld+ blfe)
)
≈ O(b2hd+ bhfe).
We further conducted experiments on the algorithms using simulated problem instances to study how the algorithms perform in practice, which we describe next.
As the metric we proposed here benchmarks the unnormalized level models, in this section we propose a simple sentence level language model that we can use to show the efficacy of our metric. This new model is simply an unfolded Recurrent Neural Network Language Model [10] build at sentence level and trained to maximize the margin between a valid sentence and its distorted version.
The Recurrent Neural Network based Language model can be defined recursively using the following equations
x(t) = [ w(t− 1)T s(t− 1)T ]T , (9)
s(t) = f(Ux(t)), and (10) y(t) = g(Ws(t)). (11)
Equations (9) and (10) can be seen as building latent space representations of phrases using words and history and (11) can be seen as predicting the probability of this word given the context. This phrasal representation built in (9) and (10) then would be treated as the history for the next step. A standard sigmoidal
nonlinearity is used for f and the probability distribution function g is a standard softmax.
If we limit the context to sentence levels and move the probability space to the sequence of the words or n-grams, equation (9) and (10) can be seen as composition function building phrase x(t), of the length n, from sub-phrase x(t − 1), of the length n− 1, and the nth word w(t). Equation (11) can be seen as building the unnormalized probability p̃ over the phrase x(t). We can rephrase the equations (9), (10) and (11) as
x(t) = f ( U [ x(t− 1) w(t) ]) , and (12)
y(t) = g(Wx(t)). (13)
Here we use the standard sigmoidal non linearity for the function f and the identity function for g.
We now define the score of a length N sentence W as
S(W ) = N∑ t=1 y(t). (14)
The probability of the sentence can now be modeled as an exponential distribution
p(W ) = 1 Z e−S(W ). (15)
where Z is the partition function and the contrastive entropy from (7) can be calculated as
Hc(T ) = 1/N ∑ W∈T ( S(Ŵd)− S(W ) ) . (16)
where Ŵd is the distorted version ofW with distortion percentage d.
Training is done using a contrastive criterion where we try to maximize the distance between the in-domain sentence and its distorted version. This formulation is similar to one followed by Collobert et al. [8] and Okanohara et al. [7] for language modeling and by Smith and Eisner [4] for POS tagging. Mathematically, we can define this pseudo discriminative training objective as
θ? = argmin θ ∑ d∈D max { 0, 1− S(Wd) + S(Ŵd) } . (17)
where Ŵd is the distorted version of sentence Wd and θ = (U,X,W ) is the parameter of the model.
This simplistic sentence level recurrent neural network model is implemented in python using Theano [11] and is available at https://github.com/kushalarora/sentenceRNN.
5. Experiments We use the Pen Treebank dataset with the following splits and preprocessing: Sections 0-20 were used as training data, sections 21-22 for validation and 23-24 for testing. The training, validation and testing token sizes are 930k, 74k and 82k respectively. The vocabulary is limited to 10k words with all words outside this set mapped to a special token < unk >.
We start by examining the distortion generation mechanism. As the evaluation includes the word level models, we need to preserve the word count. To do this, we restrict distortions to only two types: substitution and transpositions. For
substitutions, we randomly swap the current word in the sentence with a random word from the vocabulary. For transposition, we randomly select a word from the same sentence and swap it with the current one. For each word in a sentence, there are three possible outcomes: no distortion with probability xN , substitution with probability xS and transposition with probability xT with xN + xS + xT = 1.
Now, let’s start by considering the sentence level RNN model proposed in section 4. For contrastive entropy to be a
good measure for sentence level models, the following assertions should be true: i) contrastive entropy should monotonically increase with distortions, ii) contrastive entropy of training set should go down with each epoch, and iii) contrastive entropy should increase with increase in training distortion margin. Figures 1, 2 and 3 show that the assertions made above empirically hold. We see a monotonic increase in contrastive entropy with distortion and training distortion margin in Figures 1 and 3 respectively. Figure 2 shows the contrastive entropy increase for training data with epochs. All sentence level RNN model referred above and elsewhere in this paper were trained using gradient descent with learning rate of 0.1 and `2 regularization coefficient of 10−3.
Finally, we would like to compare the standard word level baseline models and our sentence level language model on this new metric. The objective here is to verify the hypothesis that between two language models, the superior one should be able to better distinguish the test sentence from their distorted versions. This is akin to saying that a better language model should have higher contrastive entropy value with similar or higher cross entropy ratio. Tables 2 and 3 shows the results for our experiments. The results were generated using the open source language modeling SRILM toolkit [12] for n-gram models and the RNNLM toolkit [13] for the RNN language model. The RNN model used had 200 hidden layers, with class size of 50. The sRNN-75(10) row in Tables 2 and 3 indicates that the sentence level RNN model was trained with latent space size of 75 and with training distortion level of 10%. All the results here were averaged over 10 runs.
As hypothesized, the contrastive entropy rises in Table 2’s columns 2 to 4 and correlates negatively with perplexity for word level models—i.e. the models expected to do better on perplexity do better on Contrastive entropy as well. Rows 4 to 6 compare sentence level RNN models. Here too, as expected, sRNN trained with distortion level of 10% outperforms sRNN trained with distortion margin of 50%. Now, let’s compare word level models to our sentence level model. We can see that sRNN-75(50) performs worse compared to RNN for all levels and worse than 3-gram and 5-gram models for 10% and 30%. This can be attributed to the training distortion margin of 50% which encourages the sRNN to see anything with less than 50% distortion as in-domain sentences. On the other hand sRNN trained with distortion level of 10% performs the best as compared to all other models as it has been tuned to label slightly un-grammatical sentences or ones that have slightly un-natural structure as out of domain.
Table 3 shows that scaling is not an issue for word level models as ratios are more or less the same. Sentence level models at 10% distortion do better than all the word-level models on both metrics which demonstrates their superior performance.
sRNN-75(50) is an interesting case. At test distortion level of 30% it is clearly inferior to all word level models as it was trained on a distortion margin of 50%. With 50% test distortion the result is unclear as it does worse on contrastive entropy but better on contrastive ratio.
The objective function defined in Equation 5 contains a loss function, which up until now referred to the empirical loss of the ensemble, or the zero-one loss. However, the zeroone loss contains a strong discontinuity and can result in optimization procedures failing due to the majority voting combination. For instance, if all classifiers of the ensemble
are wrong on some instances, replacing one of those poor classifiers with a better one will not make the ensemble correctly classify those instances, resulting in the same performance with regards to the objective function, even though this classifier would be a good choice for the ensemble.
The performance of ensembles will be considered with regards to their classification margin, which will let us derive a more suitable loss function (Schapire and Freund, 2012). Given an ensemble of classifiers E outputting label predictions on a binary problem Y ∈ {−1, 1}, the normalized margin for a sample {x, y} is defined as follows:
M(E, x, y) = 1 |E| ∑ h∈E yh(x). (6)
The normalized margin M ∈ [−1, 1] takes the value 1 when all classifiers of the ensemble correctly classify the sample x, −1 when all the classifiers are wrong, and somewhere in between otherwise. In the case of multi-class problems, predictions of classifiers can be brought back to a binary domain by attributing 1 for a correct classification and −1 for a misclassification. The margin becomes:
Mmc(E, x, y) = 1 |E| ∑ h∈E [1− 2l0−1 (h(x), y)]. (7)
We will now derive some loss functions from the margin. The margin itself could be the objective, since it is desirable that the margin of the ensemble be high. It must be rescaled to really become a loss function, giving a margin-based loss function:
lM (E, x, y) = 1−M(E, x, y)
2 . (8)
This loss function should not be used to optimize an ensemble because it is directly maximized only by the accuracy of individual classifiers. In other words, given a validation dataset XV and a set of classifiers H to evaluate, the classifier minimizing Equation 8 is always the classifier with the lowest empirical error on its own, without regards to the ensemble performance. Therefore, the loss function must not give the same weight to all classifiers without regards to ensemble performance, while also being smooth. A loss function which achieves this is the margin-based loss function taken to the power of two:
lM2(E, x, y) = (1−M(E, x, y))2
4 . (9)
This places a higher emphasis on samples misclassified by the ensemble, and decreases the importance of samples as the margin grows closer to 1. Since it meets the required properties, the squared margin loss function will be used as the ensemble loss function in this work.
Figure 4 shows an example of the two loss functions discussed in this section, the zero-one loss and the squared margin loss, applied on the same ensemble. Both losses have different scales, with the empirical error scale on the left and the squared margin scale on the right of the figure. We can see that these two loss functions lead to different optimal hyperparameters given the same ensemble. In other words, the hyperparameters minimizing the objective function according to the models of f(γ|E) are different with the two loss functions.
Considering that a loss function other than the empirical error will be used during the optimization, it will probably be beneficial to rebuild an ensemble from scratch using the final pool of classifiers trained once the optimization is over. Hence, a post-hoc ensemble generation will be performed after the hyperparameter optimization.
We proposed a method of binary embedding for high-dimensional data. Central to our framework is to use a type of highly structured matrix, the circulant matrix, to perform the linear projection. The proposed method has time complexity O(d log d) and space complexity O(d), while showing no performance degradation on real-world data compared with more expensive approaches (O(d2) or O(d1.5)). The parameters of the method can be randomly generated, where interesting theoretical analysis was carried out to show that the angle preserving quality can be as good as LSH. The parameters can also be learned based on training data with an efficient optimization algorithm.
Feature representation is one of the most critical part of the project. The features should be of low enough level and be general enough that most of the chess knowledge is still discovered through learning. On the other hand, a well designed board representation can significantly improve the performance of the system, by making useful features easier to learn. The goal of the representation is not to be as succinct as possible, but to be relatively untangled, and smooth in feature space. For example, while in principle, neural nets can learn the rules of chess and figure out that the influence of sliding pieces ends at the first piece along each direction, it is much more efficient to give it this information as part of the board representation.
For neural networks to work well, the feature representation needs to be relatively smooth in how the input space is mapped to the output space. Positions that are close together in the feature space should, as much as possible, have similar evaluations.
This means many intuitive and naive representations are not likely to work well. For example, many previous attempts at using neural networks for chess represent positions as bitmaps, where each of the 64 squares is represented using 12 binary features indicating whether a piece of each of the 12 piece types exist on the square or not. This is unlikely to work well, because in this 768-dimensional space, positions close together do not necessarily have similar evaluation. As an illustration, see Figure 1 below. In this case, the position on the left should have similar evaluation to the position in the middle, because the bishops are in the same region of the board. The position on the right should have a very different evaluation, because the bishop is somewhere else entirely.
In the bitmap feature representation, the 3 positions all have the same distance to each other in the feature space. That means the network will have a hard time generalizing the advantages of having pieces on specific locations to general regions on the board.
A much better representation is to encode the position as a list of pieces and their coordinates. This way, positions that are close together in the feature space would usually have similar evaluation. Since the feature vector needs to be the same length regardless of how many pieces are on the board, we use a slot system. The first two slots are reserved for the kings, followed by two slots for the queens, then four slots for the knights, four slots for the bishops, four slots for the rooks, and finally sixteen slots for the pawns. In addition to coordinates, each slot also encodes presence or absence of the piece, as well as some additional information about each piece such as whether its defended or not, and how far can it move along each direction (for sliding pieces). In case of promotions it is possible to have more than the starting number of some of the pieces for some piece types. In those very rare cases, the extra pieces are not encoded in the slots system, though piece counts are encoded separately, so they will be scored for their existence (but not position).
We evaluated board representations by training neural networks to predict the output of Stockfish’s evaluation function in a supervised fashion, given 5 million positions as input, in the board representation under evaluation. The positions are uniformly randomly sampled from databases of high quality games, either between human grandmasters or between computers. They include games in all phases and situations, and they are all labeled by Stockfish.
It is hoped that if we can find a board representation that allows a network to efficiently learn Stockfish’s evaluation function, it will be flexible enough to model at least the kind of chess knowledge we have right now, and hopefully unknown knowledge in similar format in the future. The challenge is to achieve that while keeping the board representation as general as possible, to not limit its ability to learn new things.
In the end, we arrived at a feature representation consisting of the following parts -
• Side to Move - White or black
• Castling Rights - Presence or absence of each of the 4 castling rights (white long castle, white short castle, black long castle, black short castle)
• Material Configuration - Number of each type of pieces
• Piece Lists - Existence and coordinates of each (potential) piece. There is 1 slot per side for kings, 1 slot for queens, 2 slots for rooks/bishops/knights, and 8 slots for pawns. Each slot has normalized x coordinate, normalized y coordinate, whether piece is present or absent, and the lowest valued attacker and defender of the piece. Pieces are assigned slots based on their
position when possible. For example, if only a, b, g, h pawns are present, they will occupy the 1st, 2nd, 7th, and 8th slots respectively.
• Sliding Pieces Mobility - For each sliding piece slot, we encode how far they can go in each direction (before being stopped by either the board’s edge, or a piece). We also tried the same for non-sliding pieces (kings, knights, pawns), and saw no improvement, showing that the neural network is powerful enough to learn their movement patterns without much trouble.
• Attack and Defend Maps - The lowest-valued attacker and defender of each square.
There are 363 features in total.
The piece lists make up the bulk of the feature representation. They allow the network to efficiently learn things like centralisation of knights and bishops, rooks on opponent’s pawn rank, centralisation of the king in end games, and advancement of pawns in general. They also allow the network to learn the importance of mobility for different pieces in different phases of the game, as well as some concepts of king safety based on distances between different types of attacking pieces and the opponent’s king. As it turned out, these features are by far the most important features, because many important concepts can be represented as linear combinations of pieces’ coordinates, or linear combinations of pieces’ coordinates with thresholds. In fact, the system performs at a reasonable level with just these features.
The side to move flags is added so the network can learn the concept of tempo in chess - the fact that all else being equal, it’s better to be the moving side, because in almost all situations, there is an available move that is better than passing (making no move). This is not always true. In particular, there is a class of positions known as Zugzwang, where all legal moves worsen the situation for the moving side, but the moving side still has to make a move because passing is illegal in chess. The value of a tempo also depends on game phase, material configurations, as well as pawn structure. These are all left in the hands of the neural network to discover.
The material configuration section is almost entirely redundant, since the same information can be derived from the existence flags in the piece slots. However, they are still useful in cases of promotions, where we may run out of piece slots. With piece count features, at least they can be given a bonus for their existence. Also, having these numbers allow the network to more easily discover concepts like piece synergies, and how material combinations can affect the importance of other features - for example, moving the king towards the centre is often good in the end game, but very dangerous in the opening and middle game, where the opponent still has many pieces on the board.
The attack and defend maps allow the network to learn concepts related to board control. This is difficult for the network to derive from the piece lists, because piece lists are piece-centric and not square-centric. That is why these maps are provided even though the information is theoretically computable from other features.
For example, the position in Figure 2 above would be represented as follows (mobility, attack and defend maps omitted for brevity) in Table 1:
Additionally, we assessed the current understanding of the state of privacy in search. More specifically, we asked, whether the participants believe that most current search engines store the following attributes: DGDR, DAGE, DOCC, DZIP, ASEA, DCRY, TNWS, DEML. We found that most participants (96%) assume that search engines know the country (DCRY) a searcher is from. Also, most participants (86%) assume that all searches are stored by the search engines. Fewer participants believe that demographic information such as occupation (16%), age (28%) or gender (34%)
are known to the search engine. This result is an important baseline in order to understand the sensitivity classification of the individual attributes.
We now propose a new framework for bounding the pseudo-dimension of many well-structured classes of real-valued functions. Suppose F is some set of real-valued functions whose pseudo-dimension we wish to bound. Suppose that, for each f ∈ F , f can be “factored” into a pair of functions (f1, f2) such that f2(f1(x), x) = f(x) for any x. There are always “trivial” factorings, where the function f2 = f or f1(x) = x, but the interesting case arises when both f1(x) and f2 (fixing f1(x)) depend in a very limited way upon x. In particular, if the set of functions {f1} are very structured, and fixing f1(x) the set of functions {f2} only depend upon x in some very mild way, this will imply that F itself has small pseudo-dimension. Intuitively, this will allow us to “bucket” functions by their values according to f1 on some sample, and bound the pseudo-dimension of each of those buckets separately.
Our particular technique for showing such a property is first to show that the set of functions {f1} are linearly separable in a dimensions, then to fix some sample S of size m and some f1, and to upper-bound by b the pseudodimension of the set of functions f2 whose associated f ′1 agrees with the labeling of f1 on S. The following definition captures precisely what we mean when we say that the function class F factors into these two other classes of functions. If f1(x) reveals too much about x, it will be difficult to prove linear separability; similarly, if f2 depends too heavily on x, it will be difficult to prove a bucket has small pseudo-dimension.
Definition 3.1 ((a, b)-factorable class) Consider some F = {f : X → R}. Suppose, for each f ∈ F , there exists (f1, f2), f1 : X → Y, f2 : Y × X → R such that f2(f1(x), x) = f(x) for every x ∈ X . Let
F1 = {f1 : (f1, f2) is a decomposition of some f ∈ F}
and F2 = {f2 : (f1, f2) is a decomposition of some f ∈ F}.
The set F (a, b)-factors over Q if:
(1) F1 is a-dimensionally linearly separable over Q ⊆ Y .
(2) For every f1 ∈ F1 and sample S ⊂ X of size m, the set
F2|f1(S) = {f ′ 2 : X → R, f ′ 2(x) = f2(f ′ 1(x), x)|f1(S) = f ′ 1(S) and (f ′ 1, f2) is a decomposition of some f ∈ F}
has pseudo-dimension at most b.
We now give an example of a simple class which satisfies this definition. One could easily bound the pseudodimension of this example class using a direct shattering argument, but it will be instructive to work through our definition of (a, b)-separability.
Example 3.2 Fix some set G = {g1, . . . , gk} ⊂ Rk. Suppose F = {f : f(x) = maxg∈Gf⊆G g · x} is the set of all functions which take the maximum of at most k common linear functions in a fixed set G. We will show that F (kd, Õ(kd))-factors over [k], where each j ∈ [k] will represent which of the k linear functions is maximizing for a particular input. That is, for some f,Gf ⊆ G, let f1(x) = argmaxt:gt∈Gf gt · x and f2(t, x) = gt · x. Thus, we have a valid factoring:
f2(f1(x), x) = f2(argmaxt:gt∈Gf gt · x, x) = gargmaxt:gt∈Gf gt·x · x = maxgt∈Gf gt · x = f(x).
It remains to show that F1 is d-dimensionally linearly separable and to bound the pseudo-dimension of F2|f1 . We start with the former. Let Ψ(x, t)t′j = I[t′ = t] · xj for t′ ∈ [k], j ∈ [d]. Then, let w f tj = I[gt ∈ Gf ] · gtj . The dot product will then be Ψ(x, t) · wf = ∑
t′
I[t′ = t] · I[gt′ ∈ Gf ]gt′ · x
which will be maximized when t = argmaxt′:gt′∈Gf gt′ · x, or when t = f1(x). So, F1 is linearly separable in kd dimensions over [k].
Now, fix f1 ∈ F1; we will show the pseudo-dimension of F2|f1 is at most Õ(kd). For any fixed sample S = (x1, . . . , xm), f1(xt) is fixed for all t ∈ [m], implying that the input to all f2 ∈ F2|f1 , (f1(x
t), xt), is fixed. Finally, by definition of f ′2,
f ′2(x t) = f2(f1(x t), xt) = gf1(xt) · x t.
Thus, for each j ∈ [k], the subset Sj ⊆ S for which f1(xt) = j for all xt ∈ Sj , f ′2 is just a linear function in d dimensions of xt with coefficients gj , Thus, since linear functions in d dimensions have pseudo-dimension at most d+ 1, there are at most md+1 labelings which can be induced on Sj , and at most mk(d+1) labelings of all of S. This implies PD(F2|f1) is at most Õ(kd).
We now present the main theorem about the pseudo-dimension of classes which are (a, b)-factorable. The proof of this theorem first exploits the fact that linearly separable classes have a “small” number of possible outputs for a sample of size m. Then, fixing the output of the linearly separable function, the second set of functions’ pseudo-dimension is small. The proof of the theorem is relegated to the appendix due to space considerations.
Theorem 3.3 Suppose F is (a, b)-factorable over Q. Then,
PD(F) = O (max ((a+ b) ln(a+ b), a ln |Q|)) .
Intuitively, when F1 is linearly separable in a dimensions, it can induce at most ma|Q|a many labelings of m samples, and fixing such a sample and its labeling, because F2 has pseudo-dimension at most b, it can induce at most mb labelings of m samples with respect to their thresholds.
While the range of F1 might be all of Q, it will regularly be helpful to only need to prove linear separability of F1 only over “realizable” labels for particular inputs. If F1 has the property that for every input x, every f1 ∈ F1 labels x with one of a smaller set of labels Qx ( Q, then it suffices to prove linear separability for x over Qx. The following remark makes this claim formal; its proof can be found in Section E. So, we will be able to focus on proving linear separability of F1 over a label space which depends upon the inputs x. This will be particularly useful when describing auctions in the next section, whose allocations are in certain cases highly restricted by their inputs.
Remark 3.4 Suppose for each x ∈ X , there exists some Qx ⊆ Q such that f1(x) ∈ Qx ⊆ Q for all f1 ∈ F1, and that for for each x, F1 is linearly separable in a dimensions for that x over Qx. Assume there is a subset of dimension T+ ⊆ [a] for which wf
t∈T+ ≥ 0 and ∑ t∈T+ w f t > 0 for all f . Suppose that for all x ∈ X, f ∈ F1,
maxy∈Qx Ψ(x, y) · w f ≥ 0. Then, F1 is linearly separable over Q in a dimensions as well.
four state-of-the-art constraint solvers; Choco, ECLiPSe, Gecode, and Minion. To assess the impact of design decisions, instances of the five problem classes n-Queens, Golomb Ruler, Magic Square, Social Golfers, and Balanced Incomplete Block Design are modelled and solved with each solver. The results of the experiments are not meant to give an indication of the performance of a solver, but rather investigate what influence the choice of algorithms and data structures has.
The analysis of the impact of the design decisions focuses on the different ways of memory management, behaviour with increasing problem size, and specialised algorithms for specific types of variables. It also briefly considers other, less significant decisions.
Podemos acrescentar a regra Cut ai↑ ao sistema BV sem aumentar o conjunto de fórmulas prová-
vies em MLL + mix+ seq. Ou seja, a regra Cut é admissível em BV.
Em outras palavras, o sistema BV é completo para MLL + mix + seq sem ter a regra Cut,
o que torna sua implementação muito mais prática.
3.1.5.1 Teorema de splitting
A idéia de splitting pode ser entendida considerando um sistema de seqüentes em que não há regras de weakening e contraction. Considere o exemplo da lógica linear multiplicativa. Se temos uma demonstração do seqüente:
⊢ F{A⊗B}
onde F{A⊗ B} é uma fórmula que contém a subfórmula A⊗ B, sabemos com certeza que em algum lugar na demonstração existe uma, e apenas uma, instância da regra⊗ que separa A e B juntamente com seu contexto. Temos então a seguinte situação:
− Π1
‖ ‖
⊢ A,Φ
− Π2
‖ ‖
⊢ B,Ψ ⊗
⊢ A⊗B,Φ,Ψ
∆ ‖ ‖
⊢ F{A⊗B} equivale a
− Π2
‖ ‖
[B,Ψ] Π1 ‖ ‖
([A,Φ], [B,Ψ]) s
[([A,Φ], B),Ψ] s
[(A,B),Φ,Ψ]
∆ ‖ ‖
[F (A,B),Γ]
Podemos considerar, como mostrado à esquerda, uma demonstração para o seqüente dado como composta de três partes, ∆, Π1 e Π2. No cálculo das estruturas existem muitas demonstraçãos diferentes correspondentes à mesma demonstração em cálculo de seqüentes: elas diferem apenas na possível ordenação das regras. Regras no cálculo das estruturas têm menor granularidade emaior aplicabilidade. Mas, entre todas as demonstraçãos, deve haver uma que se encaixe no esquema da demonstração à direita da figura acima. Este exemplo ilustra precisamente a idéia por trás da técnica de splitting.
A derivação ∆ acima implementa uma instância de context reduction 6 e um splitting pró-
prio. De forma geral, podemos enunciar estes princípios da seguinte forma:
1. Context reduction: Se S{R} é provável, então S{ } pode ser reduzido, de baixo para
cima em uma demonstração, à estrutura [{ }, U ], tal que [R,U ] é provável. No exemplo acima, [F{ },Γ] é reduzido a [{ },Γ′ ] para algum Γ′.
6Como veremos adiante, context reduction é um lema necessário para o teorema de splitting.

Both high accuracy and interpretability are desirable for classifiers. Previous classifiers such as NNDTW can be accurate, but provide limited insights into the temporal characteristics. Interval features can be used to capture temporal characteristics, however, the huge feature space can result in many splits having the same entropy gain. Furthermore, the computational complexity becomes a concern when the feature space becomes large.
Time series forest (TSF) proposed here addresses the challenges by using the following two strategies. Firstly, TSF uses a new splitting criterion named Entrance gain that combines the entropy gain and a distance measure to
identify high-quality splits. Experimental studies on 45 benchmark data sets show that the Entrance gain improves the accuracy of TSF. Secondly, TSF randomly samples O(M) features from O(M2) features, and thus makes the computational complexity linear in the time series length. In addition, each tree in TSF is grown independently, and, therefore, modern parallel computing techniques can be leveraged to speed up TSF.
TSF is an ensemble of trees and is not easy to understand. However, we propose the temporal importance curve, calculated from TSF, to capture the informative interval features. The temporal importance curve enables one to identify the important temporal characteristics.
TSF uses simple summary statistical features, but outperforms widely used alternatives. More complex features, such as wavelets, can be also used in the framework of TSF, which potentially can further improve the accuracy performance, but at the cost of interpretability.
In summary, TSF is an accurate, efficient time series classifier, and is able to provide insights on the temporal characteristics useful for distinguishing time series from different classes. We also note that TSF assumes that the time series are of the same length. Given a set of time series with different lengths, techniques such as dynamic time warping can be used to align the time series into the same length. Still, directly handling time series with varying lengths would make TSF more convenient to use, and future work
includes such an extension.
We first describe two kinds of similarity measures for relevance ranking in the context of our opinion QA problem as follows.
1) Okapi BM25: One of the standard relevance ranking measures for information retrieval, Okapi BM25 is a bag-ofwords ‘tf-idf’-based ranking function that has been successfully applied in a number of problems including QA tasks [5], [6]. Particularly, for a given question q and a review r, the standard BM25 measure is defined as
bm25 (q, r) = n∑ i=1 idf (qi)× f(qi, r)× (k1 + 1) f(qi, r) + k1 × (1− b+ b× |r|avgrl ) , (1)
where qi, i = 1, . . . , n are keywords in q, f(qi, r) denotes the frequency of qi in r, |r| is the length of review r and avgrl is the average review length among all reviews.3 Here idf (qi), the inverse document frequency of qi, is defined as
idf(qi) = log N − n(qi) + 0.5 n(qi) + 0.5 , (2)
where N = |R| is the total number of reviews and n(qi) is the number of reviews which contain qi.
2) Rouge-L: Next we consider another similarity measure, Rouge-L [7], which is a Longest Common Subsequence (LCS) based statistic. For a question q and a review r, if the length of their longest common subsequence is denoted as LCS (q, r), then we have RLCS = LCS (q, r)/|q| and PLCS = LCS (q, r)/|r|. Now Rouge-L is defined as
FLCS = (1 + β2)RLCSPLCS RLCS + β2PLCS , (3)
where β = PLCS/RLCS .
3In practice we set k1 = 1.5 and b = 0.75.
In this example, we train embeddings on the English Wikipedia snapshot in April 2015. The training procedure goes as follows:
1. Use extractwiki.py to cleanse aWikipedia snapshot, and generate cleanwiki.txt, which is a stream of 2.1 billion words; 2. Use gramcount.plwith cleanwiki.txt as input, to generate top1grams-wiki.txt; 3. Use gramcount.pl with top1grams-wiki.txt and cleanwiki.txt as input, to generate top2grams-wiki.txt; 4. Use factorize.py with top2grams-wiki.txt as input, to obtain 25000 core embeddings, saved into 25000-500-EM.vec; 5. Use factorize.py with top2grams-wiki.txt and 25000-500-EM.vec as input, and Tikhonov regularization coefficient set to 2, to obtain 55000 noncore embeddings. The word vectors of totally 80000 words is saved into 25000-80000-500-BLKEM.vec; 6. Repeat Step 5 twice with Tikhonov regularization coefficient set to 4 and 8, respectively, to obtain extra 50000 × 2 noncore embeddings. The word vectors are saved into 25000-130000-500-BLKEM.vec and 25000-180000-500-BLKEM.vec, respectively; 7. Use evaluate.py to test 25000-180000-500-BLKEM.vec.
7According to the expression cd2/µn2, the speedup rate on Wikipedia should be 1/60 of that on RCV1. But some common overhead of Numpy matrix operations is more prominent on the smaller matrices when d is small, which reduces the speedup rate on smaller d. Hence the ratio of the two speedup rates is 1/12 in practice.
This paper presented an experiment in machine-learningREG that takes speaker-dependent information into account, and which makes use of a simple training method based on speaker profiles to circumnavigate the issue of data sparsity. By grouping speakers according to their overspecification preferences, we were able to sketch a speakerdependent REG model that was shown to outperform the standard use of individual speaker’s information proposed in previous work.
Despite the overall positive results of this initial experiment, we may of course ask which alternative training methods may be considered for the task. More specifically, since using more training data - as we did by considering groups of similar speakers - has improved results, it may be the case that by simply training our REG models on the data provided by all speakers, we could improve results even further. Although we presently do not seek to validate this claim (which in any case would defeat the purpose of using speaker-dependent information in REG), there is plenty of evidence to suggest that this would not be the case. Studies such as in [3,16], for instance, have consistently shown that using individual training datasets for each speaker outperforms speaker-independentREG and, in particular, the work in [4] has shown that SVM-based REG models generally produce best results when trained on personalised datasets.
Finally, we notice that the present experiment has focused on a single aspect of referential behaviour, namely, on the issue of overspecification preferences across speakers. As future work, we would like not only to refine the current method (e.g., by distinguishing between target and landmark overspecification preferences, amongmany other options.) but also to consider the issue of attribute choice (e.g., by grouping speakers according to their preferred referential attributes.)
A user interface is the view of a database interface that is seen by the user.
 Form-based Interfaces
 Text-based Interfaces
 GIS Interface
It is well-known that neural network based-language models can learn semantically meaningful embeddings to continuous space (Bengio et al., 2003; Mikolov et al., 2013; Cho et al., 2014). We expected to see a similar characteristic with the proposed system-call language model. The 2D projection of the calls using the embedding matrix W learned from the system-call language model was done by t-SNE (Van der Maaten & Hinton, 2008) and shown in Figure 5. Just as the natural language model, we can expect that calls having similar co-occurrence patterns are positioned in similar locations in the embedded space after training the system call language model. We can clearly see that calls having alike functionality are clustered with each other.
The first obvious cluster would be the read-write call pair and the open-close pair. The calls of each pair were located in close proximity in the space, meaning that our model learned to associate them together. At the same time, the difference between the calls of each pair appears to be almost the same in the space, which in turn would mean our model learned that the relationship of each pair somewhat resembles.
Another notable cluster would be the group of select, pselect6, ppoll, epoll wait and nanosleep. The calls select, pselect6 and ppoll all have nearly identical functions in that they wait for some file descriptors to become ready for some class of I/O operation or for signals. The other two calls also have similar characteristics in that they wait for a certain event or signal as well. This could be interpreted as our model learning that these ‘waiting’ calls share similar characteristics.
Other interesting groups would be: readlink and lstat64 which are calls related to symbolic links; fstatat64 and fstat64 which are calls related to stat calls using file descriptors; pipe and pipe2 which are nearly identical and appear almost as one on the embedding layer. These cases show that our model is capable of learning similar characteristics among the great many system calls.
Similarly to the call representations, we expected that attack sequences with the same type would cluster to each other, and we tried to visualize them. However, for various reasons including the
lack of data, we were not able to observe this phenomenon. Taking the fact that detecting abnormal patterns from normal patterns well would be sufficiently hard into consideration, learning representation to separate different abnormal patterns with only seen normal patterns would also be an extremely difficult task.
We use the hand-written digit training and testing cases from the on-line database available in [22]. It has a training set of 60,000 examples, and a test set of 10,000 examples. And all of them are labeled.
The AWS cloud-computing platform is built up by multiple EC2 instances. Up to 32 EC2 nodes have been used in these experiments for comparing the performance of experiments in different running instances. Massive input data and intermediate results are stored in the distributed cache offered by the platform. The raw input data is in the size of 300 megabytes and stored distributed across the platform. Each EC2 instances has Intel 64 bit CPU, 16 GB memories and high network performance. Moreover each node can be boosted up to 4 virtual CPU with the performance triple increased. Open source framework Hadoop is adopted by AWS for the distributed architecture of those nodes.
Amazon EC2 instances provide a number of additional features to deploy, manage, and scale our applications. Multiple storage options based on our requirements can be choose. Details about how EC2 instances work can be found in figure 5.

Revisiting Self Organizing Maps (SOM): Intrinsic dynamics of SOM are inspired from developed animal brain where each part is known to be receptive to different sensory inputs and which has a topographically organized structure[7]. This phenomena, which is called as ”receptive field” in visual neural systems [6], is simulated with SOM, where neurons are represented by weights that are calibrated to make neurons sensitive to different type of inputs. Elicitation of this structure is furnished by competitive learning approach.
input with M instances X = {x1, x2..., xM}. Let N = {n1, n2, ..., nK} be the locations of neuron units on the SOM map andW = {w1, w2, ..., wK} be the associated weights. The neuron whose weight vector is most similar to the input instance xi is called as the winner and denoted by v̂. The weights of the winner and units in the neighbourhood are adjusted towards the input vector at each iteration t with delta learning rule (Eq.1).
wtj = w t−1 j + h(ni, nv̂ : t, σt)[xi − wt−1j ] (1)
Update step is scaled by the window function h(ni, nv̂ : t, σt) for each SOM unit, inversely proportional to the distance to the winner (Eq.2). Learning rate is a gradually decreasing value, resulting in larger updates at the beginning and finer updates as the algorithm evolves. σt defines the neighbouring effect so with the decreasing σ, neighbour update steps are getting smaller in each epoch. Note that, there are different alternatives for update and windows functions in SOM literature.
h(ni, nv̂ : t, σt) = t exp
−||nj − nv̂||2
2σt2 (2)
Clustering and outlier detection with RSOM: We introduce excitation scores E = {e1, e2, . . . , eK} where ej , the score for neuron unit j, is updated as in Eq.3.
etj = e t−1 j + ρ t(βj + zj) (3)
As in the original SOM, window function is getting smaller with each iteration.
Here, zj is the activation or win count for the unit j, for one epoch. ρ is learning solidity scalar that represents the decisiveness of learning with dynamically increasing value, assuming that later stages of the algorithm has more impact on the definition of salient SOM units. ρ is equal to the inverse of the learning rate . βj is the total measure of the activation of jth unit in an epoch, caused by all the winners of the epoch but the neuron itself (Eq.4).
βj = u∑ v̂=1 h(nj , nv̂)zv̂ (4)
At the end of the iterations, normalized ej is a quality value of a unit j. Higher value of ej indicates that total amount of excitation of the unit j in whole learning period is high thus it is responsive to the given class of instances and it captures notable amount of data. Low excitation values indicate the contrary. RSOM is capable of detecting outlier units via a threshold θ in the range [0, 1].
Let C = {c1, c2, . . . , cK} be the cluster centres corresponding to each unit. cj is considered to be a salient cluster if ej ≥ θ, and an outlier cluster otherwise.
The excitation scores E are the measure for saliency of neuron units in RSOM. Given the data belonging to a category, we expect that data is composed of sub-categories that share common properties. For instance red images might include darker or lighter tones to be captured by clusters but they are supposed to share a common characteristics of being red. In that sense, for the calculation of the excitation scores we use individual activations of the units as well as the activations as being in a neighbourhood of another unit. Individual activations measure the saliency of being a salient cluster corresponding to a particular subcategory, such as lighter red. Neighbourhood activations count the saliency in terms of the shared regularity between sub-categories. If we don’t count the neighbourhood effect, some unrelated clusters would be called salient since large number of outlier instances could be grouped in a unit, e.g. noisy white background patches in red images.
Outlier instances of salient clusters, namely the outlier elements should also be detected. After the detection of outlier neurons, statistics of the distances between neuron weight wi and its corresponding instance vectors (assuming weights prototyping instances grouped by the neuron) is used as a measure of instance divergence. If the distance between the instance vector xj and its winner’s weight ŵi is more than the distances of other instances having the same winner, xj is raised as an outlier element. We exploit box plot statistics, similar to [15]. If the distance of the instance to its cluster’s weight is more than the upper-quartile value, then it is detected as an outlier. The portion of the data, covered by the upper whisker is decided by τ .
RSOM provides good basis of cleansing of poor instances whereas computing cost is relatively smaller since RSOM is capable of discarding items with one shot of learning phase. Therefore, an additional data cleansing iteration after clustering phase is not required. All the necessary information (excitation scores, box plot statistics) for outliers is calculated at runtime of learning. Hence, RSOM is suitable for large scale problems.
RSOM is also able to estimate number of intrinsic clusters of the data. We use PCA for that purpose, with defined variation value ν to be captured by the principle components. Given data and ν, principle components are found and number of principle components describing the data with variation ν is used as the number of clusters for the further processing of RSOM. If we increase ν, RSOM latches more clusters therefore ν should be carefully chosen.
Discussion of other methods on outlier detection with SOM: [13, 14] utilise the habitation of the instances. Frequently observed similar instances excites the network to learn some regularities and divergent instances are observed as outliers. [5] benefits from weights prototyping the instances in a cluster. Thresholded distance of instances to the weight vectors are considered as indicator of being outlier. In [23], aim is to have different mapping of activated neuron for the outlier instances. The algorithm learns the formation of activated neurons on the network for outlier and inlier items with no threshold. It suffers from the generality, with its basic assumption of learning from network mapping. LTD-KN [20] performs Kohonen learning rule inversely. An instance activates only the winning neuron as in the usual SOM, but LTD-KN updates winning neuron and its learning windows decreasingly.
These algorithms only eliminate outlier instances ignoring outlier clusters. RSOM finds outlier clusters as well as the outlier instances in the salient clusters. Another difference of RSOM is the computation cost. Most of outlier detection algorithms model the data and iterate over the data again to label outliers. It is not suitable for large scale data. RSOM has the ability to detect outlier clusters and the items all in the learning phase. Thus, there is no need for learning a model of the data first, then detecting outliers, it is all done in a single pass in our method. With the support of GPGPU programming RSOM scales to large amount of data.
F
, P
F
, M
DB
( ),
DB
, P
DB
and others used below, see Section 3.
70. By consistent, we mean probabilities assigned to logical formulas respect the laws of probability such as
0 P (A) 1, P (:A) = 1 P (A) and P (A _B) = P (A) + P (B) P (A ^B).
(Proof) Recall that a closed formula has an equivalent prenex disjunctive normal form that belongs to . We prove the proposition for formulas in by using induction on the multi-set ordering over fr( ) j 2 g. If r( ) = ;, has no quanti cation. So the proposition is correct by Lemma A.1. Suppose otherwise. Write = G[Q
1
Q
2
Q
n
F ]
where Q
1
Q
2
Q
n
F indicates a single occurrence of a factor in G.
71
We assume Q
1
= 9x
(Q
1
= 8x is similarly treated). We also assume that bound variables are renamed to avoid
name clash. Then G[9xQ
2
Q
n
F ] is equivalent to 9xG[Q
2
Q
n
F ] in light of the validity
of (9xA) ^B = 9x(A ^B) and (9xA) _B = 9x(A _ B) when B contains no free x.
P
DB
( ) = P
DB
(G[Q
1
Q
2
Q
n
F ])
= P
DB
(9xG[Q
2
Q
n
F [x] ])
= lim
k!1
P
DB
(G[Q
2
Q
n
F [t
1
]] _ _ G[Q
2
Q
n
F [t
k
] ])
= lim
k!1
P
DB
(G[Q
2
Q
n
F [t
1
] _ _ Q
2
Q
n
F [t
k
] ])
= lim
k!1
P
F
(f 2
F
jM
DB
( ) j= G[Q
2
Q
n
F [t
1
] _ _Q
2
Q
n
F [t
k
] ]g)
(by induction hypothesis)
= P
F
(f 2
F
jM
DB
( ) j= 9xG[Q
2
Q
n
F [x]]g)
= P
F
(f 2
F
jM
DB
( ) j= g) Q.E.D.
We next prove a theorem on the i de nition introduced in Section 4. Distribution semantics considers the program DB = F [ R as a set of in nitely many ground de nite clauses such that F is a set of facts (with a probability measure P
F
) and R a set of rules,
and no clause head in R appears in F . Put
head(R)
def = fB j B appears in R as a clause headg:
For B 2 head(R), let B W
i
(i = 1; 2; : : :) be an enumeration of clauses about B in R.
De ne i (B), the i (if-and-only-if) form of rules about B in DB
72
by
i (B)
def = B $W
1
_W
2
_
Since M
DB
( ) is a least Herbrand model, the following is obvious.
Lemma A.2 For B in head(R) and 2
F
, M
DB
( ) j= i (B).
Theorem A.1 below is about i (B). It states that at general level, both sides of the i
de nition p(x) $ 9y
1
(x = t
1
^W
1
) _ _ 9y
n
(x = t
n
^W
n
) of p( ) coincide as random
variables whenever x is instantiated to a ground term.
Theorem A.1 Let i (B) = B $W
1
_W
2
_ be the i form of rules about B 2 head(R).
P
DB
(i (B)) = 1 and P
DB
(B) = P
DB
(W
1
_W
2
_ ).
71. For an expression E, E[ ] means that may occur in the speci ed positions of E. If
1
_
2
in E[
1
_
2
]
indicates a single occurrence of
1
_
2
in a positive boolean formula E, E[
1
_
2
] = E[
1
]_E[
2
] holds.
72. This de nition is di erent from the usual one (Lloyd, 1984; Doets, 1994) as we are here talking at ground
level. W
1
_W
2
_ is true if and only if one of the disjuncts is true.
(Proof)
P
DB
(i (B)) = P
DB
(f! 2
DB
j ! j= B ^ (W
1
_W
2
_ )g)
+P
DB
(f! 2
DB
j ! j= :B ^ :(W
1
_W
2
_ )g)
= lim
k!1
P
DB
(f! 2
DB
j ! j= B ^
k
_
i=1
W
i
g)
+ lim
k!1
P
DB
(f! 2
DB
j ! j= :B ^ :
k
_
i=1
W
i
g)
= lim
k!1
P
F
(f 2
F
jM
DB
( ) j= B ^
k
_
i=1
W
i
g)
+ lim
k!1
P
F
(f 2
F
jM
DB
( ) j= :B ^ :
k
_
i=1
W
i
g)
(Lemma A.1)
= P
F
(f 2
F
jM
DB
( ) j= i (B)g)
= P
F
(
F
) (Lemma A.2)
= 1
It follows from P
DB
(i (B)) = 1 that
P
DB
(B) = P
DB
(B ^ i (B)) = P
DB
(W
1
_W
2
_ ): Q.E.D.
We then prove a proposition useful in probability computation. Let
DB
(B) be the
support set for an atom B introduced in Section 4 (it is the set of all explanations for B). In the sequel, B is a ground atom. Write
DB
(B) = fS
1
; S
2
; : : :g and
W
DB
(B) = S
1
_S
2
_
73
De ne a set
B
by
B
def = f! 2
DB
j ! j= B $
_
DB
(B)g:
Proposition A.2 For every B 2 head(R), P
DB
(
B
) = 1 and P
DB
(B) = P
DB
(
W
DB
(B)).
(Proof) We rst prove P
DB
(
B
) = 1 but the proof exactly parallels that of Theorem A.1
except that W
1
_W
2
_ is replaced by S
1
_S
2
_ using the fact that B $ S
1
_S
2
_
is true in every least Herbrand model of the form M
DB
( ). Then from P
DB
(
B
) = 1, we
have
P
DB
(B) = P
DB
(B ^ (B $
_
DB
(B)))
= P
DB
(
_
DB
(B)): Q.E.D.
Finally, we show that distribution semantics is a probabilistic extension of the traditional least Herbrand model semantics in logic programming by proving Theorem A.2. It says that the probability mass is distributed exclusively over possible least Herbrand models.
De ne as the set of least Herbrand models generated by xing R and varying a subset
of F in the program DB = F [R. In symbols,
73. For a set K = fE
1
; E
2
; : : :g of formulas,
W
K denotes a (-n in nite) disjunction E
1
_ E
2
_
def = f! 2
DB
j ! =M
DB
( ) for some 2
F
g:
Note that as is merely a subset of
DB
, we cannot conclude P
DB
( ) = 1 a priori, but the
next theorem, Theorem A.2, states P
DB
( ) = 1, i.e. distribution semantics distributes the
probability mass exclusively over , i.e. possible least Herbrand models.
To prove the theorem, we need some preparations. Recalling that atoms outside head(R)[
F have no chance of being proved from DB, we introduce
0
def = f! 2
DB
j ! j= :D for every ground atom D 62 head(R) [ Fg:
For a Herbrand interpretation ! 2
DB
, !j
F
(2
F
) is the restriction of ! to those atoms
in F .
Lemma A.3 Let ! 2
DB
be a Herbrand interpretation.
! =M
DB
( ) for some 2
F
i ! 2
0
and ! j= B $
W
DB
(B) for every B 2 head(R).
(Proof) Only-if part is immediate from the property of the least Herbrand model. For if-part, suppose ! satis es the right hand side. We show that ! = M
DB
(!j
F
). As ! and
M
DB
(!j
F
) coincide w.r.t. atoms not in head(R), it is enough to prove that they also give
the same truth values to atoms in head(R). Take B 2 head(R) and write
W
DB
(B) =
S
1
_ S
2
_ Suppose ! j= B $ S
1
_ S
2
_ Then if ! j= B, we have ! j= S
j
for some j,
thereby !j
F
j= S
j
, and hence M
DB
(!j
F
) j= S
j
, which implies M
DB
(!j
F
) j= B. Otherwise
! j= :B. So ! j= :S
j
for every j. It follows that M
DB
(!j
F
) j= :B. Since B is arbitrary,
we conclude that ! and M
DB
(!j
F
) agree on the truth values assigned to atoms in head(R)
as well. Q.E.D.
Theorem A.2 P
DB
( ) = 1.
(Proof) From Lemma A.3, we have
= f! 2
DB
j ! =M
DB
( ) for some 2
F
g
=
0
\
\
B2head(R)
B
:
P
DB
(
B
) = 1 by Proposition A.2. To prove P
DB
(
0
) = 1, let D
1
; D
2
; : : : be an enumeration
of atoms not belonging to head(R) [ F . They are not provable from DB = F [ R, and hence false in every least Herbrand model M
DB
( ) ( 2
F
). So
P
DB
(
0
) = lim
m!1
P
DB
(f! 2
DB
j ! j= :D
1
^ ^ :D
m
g)
= lim
m!1
P
F
(f 2
F
jM
DB
( ) j= :D
1
^ ^ :D
m
g)
= P
F
(
F
) = 1:
Since a countable conjunction of measurable sets of probability measure one has also
probability measure one, it follows from P
DB
(
B
) = 1 for everyB 2 head(R) and P
DB
(
0
) =
1 that P
DB
( ) = 1. Q.E.D.
This project addresses the problem of sentiment analysis in twitter; that is classifying tweets according to the sentiment expressed in them: positive, negative or neutral. Twitter is an online micro-blogging and social-networking platform which allows users to write short status updates of maximum length 140 characters. It is a rapidly expanding service with over 200 million registered users [24] - out of which 100 million are active users and half of them log on twitter on a daily basis - generating nearly 250 million tweets per day [20]. Due to this large amount of usage we hope to achieve a reflection of public sentiment by analysing the sentiments expressed in the tweets. Analysing the public sentiment is important for many applications such as firms trying to find out the response of their products in the market, predicting political elections and predicting socioeconomic phenomena like stock exchange. The aim of this project is to develop a functional classifier for accurate and automatic sentiment classification of an unknown tweet stream.
Chapter 1
INTRODUCTION
For video features, we mainly focus on the face part. As the face shape provides import clues for facial expression, we use the landmarks’ location of the face as face shape feature. After feature normalization for each clip, these features can also reflect the head movement and head pose. The 49 landmarks’ locations are then PCA whitened [Bengio, 2012], with the final 20 dimensions are kept.
ar X
iv :1
50 6.
04 42
2v 1
[ cs
.L G
] 1
4 Ju
This work builds upon previous efforts in online incremental learning, namely the Incremental Gaussian Mixture Network (IGMN). The IGMN is capable of learning from data streams in a single-pass by improving its model after analyzing each data point and discarding it thereafter. Nevertheless, it suffers from the scalability point-of-view, due to its asymptotic time complexity of O ( NKD3 )
for N data points, K Gaussian components and D dimensions, rendering it inadequate for high-dimensional data. In this paper, we manage to reduce this complexity to O ( NKD2 )
by deriving formulas for working directly with precision matrices instead of covariance matrices. The final result is a much faster and scalable algorithm which can be applied to high dimensional tasks. This is confirmed by applying the modified algorithm to high-dimensional classification datasets.
Candidate pattern sentences are Arabic sentences which are retrieved from the web when searching with instances. The proposed system searches for Arabic texts on the web with seed instances and using Google search engine. For each input instance (e1, e2); it retrieves the first 20 top summaries results that contain the two terms e1 and e2. It was found that results above the 20 summaries contain unrelated or repeated results. Therefore, increasing the number collected above 20 summaries, does not affect the number of extracted patterns. So, choosing 20 top summaries for each pair is suitable. Then all these sentences are downloaded into a text file.
This work was supported by US Public Health Service National Institutes of Health (NIH) grants K22LM011576-03 and U24CA194362-01.

Keywords: Deep learning, stacked autoencoder, RBM, backpropagation, contrastive divergence, MNIST
A propositional formula in Conjunctive Normal Form (CNF), using n Boolean variables x1, x2, . . . , xn, is defined as a conjunction of clauses, where a clause is a disjunction of literals. A literal is either a variable xi or its complement x̄i. The Propositional Satisfiability (SAT) problem consists of deciding whether there exists a truth assignment to the variables such that the formula is satisfied.
The Maximum Satisfiability (MaxSAT) can be seen as an optimization version of the SAT problem. In MaxSAT, the objective is to find an assignment to the variables of a CNF formula that minimizes the number of unsatisfied clauses. Notice that minimizing the number of unsatisfied clauses is equivalent to maximizing the number of satisfied clauses.
In a partial MaxSAT formula ϕ = ϕh ∪ ϕs, some clauses are considered as hard (ϕh), while others are declared as soft (ϕs). The goal in partial MaxSAT is to find an assignment to the formula variables such that all hard clauses in ϕh are satisfied, while minimizing the number of unsatisfied soft clauses in ϕs. There are also weighted variants of MaxSAT where soft clauses are associated with weights greater than or equal to 1. In this case, the objective is to satisfy all hard clauses and minimize the total weight of unsatisfied soft clauses. In this paper, we focus solely on partial MaxSAT, but the proposed approach can be generalized to its weighted variants. Furthermore, in all algorithms we assume that the set of hard clauses ϕh is satisfiable. Otherwise, the MaxSAT formula does not have a solution. This can easily be checked through a SAT call on ϕh.
Algorithm 1: Linear Search Unsat-Sat Algorithm Input: ϕ = ϕh ∪ ϕs Output: satisfying assignment to ϕ
1 (ϕW , VR, λ)← (ϕh, ∅, 0) 2 foreach ωi ∈ ϕs do 3 VR ← VR ∪ {ri} // ri is a new relaxation variable 4 ωR ← ωi ∪ {ri} 5 ϕW ← ϕW ∪ {ωR} 6 while true do 7 (st, ν, ϕC)← SAT(ϕW ∪ {CNF( ∑ ri∈VR
ri ≤ λ)}) 8 if st = SAT then 9 return ν // satisfying assignment to ϕ
10 λ← λ+ 1
The most recent state of the art MaxSAT solvers are based on iterative calls to a SAT solver. One of the most classic approaches is the linear Sat-Unsat algorithm that performs a linear search on the number of unsatisfied clauses. In this case, a new relaxation variable is initially added to each soft clause and the resulting formula is given to a SAT solver. Whenever a solution is found, a new cardinality constraint on the number of relaxation variables is added, such that solutions where a higher or equal number of relaxation variables assigned the value 1 are excluded. The cardinality constraint is encoded into a set of propositional clauses, which are added to the working formula [3,12,16]. The algorithm stops when the SAT call is unsatisfiable. As a result, the last solution found is an optimal solution of the MaxSAT formula.
A converse approach is the linear search Unsat-Sat presented in Algorithm 1. Here, a lower bound λ is maintained between iterations of the algorithm. Initially, λ is assigned value 0. In each iteration, while the working formula given to the SAT solver (line 7) is unsatisfiable, λ is incremented (line 10). Otherwise, an optimal solution to the MaxSAT formula has been found (line 9).
Observe that a SAT solver call on a CNF formula ϕW returns a triple (st, ν, ϕC), where st denotes the status of the solver: satisfiable (SAT) or unsatisfiable (UNSAT). If ϕW is satisfiable, then ν stores the model found for ϕW . Otherwise, ϕC contains an unsatisfiable subformula that explains a reason for the unsatisfiability of ϕW .
Several of the most effective algorithms for MaxSAT take advantage of the current SAT solvers being able to produce certificates of unsatisfiability. Since the SAT solver is able to identify unsatisfiable subformulas, several MaxSAT algorithms use it to delay the relaxation of soft clauses. An example is the MSU3 algorithm [15] presented in Algorithm 2. Observe that this algorithm also performs an Unsat-Sat linear search, but soft clauses are only relaxed when they appear in an unsatisfiable subformula.
Although more sophisticated MaxSAT algorithms exist [19], an implementation of MSU3 algorithm on the Open-WBO framework was the best performing
Algorithm 2: MSU3 Algorithm Input: ϕ = ϕh ∪ ϕs Output: satisfying assignment to ϕ
1 (ϕW , VR, λ)← (ϕ, ∅, 0) 2 while true do 3 (st, ν, ϕC)← SAT(ϕW ∪ {CNF( ∑ ri∈VR
ri ≤ λ)}) 4 if st = SAT then 5 return ν // satisfying assignment to ϕ 6 foreach ωi ∈ (ϕC ∩ ϕs) do 7 VR ← VR ∪ {ri} // ri is a new variable 8 ωR ← ωi ∪ {ri} // ωi was not previously relaxed 9 ϕW ← (ϕW \ {ωi}) ∪ {ωR}
10 λ← λ+ 1
non-portfolio algorithm at the MaxSAT Solver Evaluation in 20143. One of the crucial features for its success relies on the fact that only one SAT solver instance needs to be created [16]. Therefore, a proper implementation of MSU3 should take advantage of incrementality in SAT solver technology. In this paper, the MSU3 algorithm is further improved with structural information of the problem instance to solve.
We model our objective function following Weston et al. (2011), by using a weighted approximate-rank pairwise loss, learned with stochastic gradient descent. The mapping from g(x) to the low dimensional space Rm is a linear transformation, so the model parameters to be learnt are the matrix M ∈ Rkn×m as well as the embedding of each possible frame label, represented as another matrix Y ∈ RF×m when there are F frames in total. The training objective function minimizes∑
x ∑ ȳ L ( ranky(x) ) [γ + s(x, y)− s(x, ȳ)]+
where x, y are the training inputs and their corresponding correct frames, and ȳ are negative frames (that do not correspond with x), γ is the margin and s(x, y) is the score between an
31
input and a frame. Further, [x]+ = max(0, x) denotes the standard hinge loss and ranky(x) is the rank of the positive frame y relative to all the negative frames:
ranky(x) = ∑ ȳ I(s(x, y) ≤ γ + s(x, ȳ)),
and L(η) converts the rank to a weight, e.g. L(η) = ∑η
i=1 1/i.
The purpose of L is to convert the rank to a weighting of the given pairwise constraint comparing d and d̄. Choosing L(η) = Cη for any positive constant C optimizes the mean
rank, whereas a weighting such as L(η) = ∑η
i=1 1/i (adopted here) optimizes the top of
the ranked list, as described in Usunier et al. (2009). To train with such an objective, we can employ stochastic gradient descent.
For speed the computation of ranky(x) is then replaced with a sampled approximation: sample N items ȳ until a violation is found, i.e. max(0, γ + s(x, ȳ) − s(x, y)) > 0 and then approximate the rank with (F − 1)/N ; see Weston et al. (2011) for more details on this procedure. For the choices of the stochastic gradient learning rate, margin (γ) and dimensionality (m), please refer to §3.5.4-§3.5.5. Note that an alternative approach could learn only the matrix M , and then use a knearest neighbour classifier in Rm, as in Weinberger and Saul (2009). The advantage of learning an embedding for the frame labels is that at inference time we need to consider only the set of labels for classification rather than all training examples. Additionally, since we use a frame lexicon that gives us the possible frames for a given predicate, we usually only consider a handful of candidate labels. If we used all training examples for a given predicate for finding a nearest-neighbour match at inference time, we would have to consider many more candidates, making the process very slow.
With respect to the number of mappings AMLR and LogMap2 produce close results, with 2901 and 2902 mappings, respectively. ALCOMO removes 80 mappings more. However, with respect to incoherency, ALCOMO produces a repair with only 10 incoherent classes, the same number as AMLR. LogMap and LogMap2 produce alignments with a high number of incoherent classes. Thus, AMLR produces the best results with respect to number of mappings removed and the coherence degree.
To show that the repaired alignment provided by the other systems could be improved by AMLR, we also repaired their respective alignments. The results show that AMLR considerably improves the incoherence degree of LogMap and LogMap2 by reducing it to 10 incoherent classes, as AMLR and ALCOMO. Moreover, AMLR produce optimal and near-optimal repairs for LogMap and LogMap2 repaired alignments, respectively. This was possible by applying the cluster strategy described in Section 4.3.
With respect to ALCOMO, AMLR did not remove any mappings, which was expected since ALCOMO already had the same number of incoherent classes as AMLR.
Moreover, we were able to produce an optimal repair for LogMap case, and, at least, near-optimal minimal repairs for the remaining alignments produced by LogMap, LogMap2 and ALCOMO.
There are several recent works that attempt to match profiles across different Internet services. Some of these works utilize private user data, while some, like ours, use publicly available data. An example of a work that uses private data is Balduzzi et al. [2]. They use data from the Friend Finder system (which includes some private data) provided by various social networks to link users across services. Though one can achieve a relatively high level of success by using private data to link user accounts, we are interested in using only publicly available data for this task. In fact, as mentioned earlier, we do not even consider publicly available information that could explicitly identify a user, such as names, birthdays and locations.
3 Several methods have been proposed for matching user profiles using public data [11,26,22,16,14,27,9,7]. These works differ from ours in two main aspects. First, in some of these works, the ground truth data is collected by assuming that all profiles that have the same screen name are from the same users [9,7]. This is not a valid assumption. In fact, it has been suggested that close to 20% of accounts with the same screen name in Twitter and Facebook are not matching [6]. Second, almost all of these works use features extracted from the user profiles [11,26,22,16,14,27,9]. Our work, on the other hand, is blind to the profile information and only utilizes users’ activity patterns (linguistic and temporal) to match their accounts across different social networks. Using profile information to match accounts is contrary to the best practices of stylometry since it assumes and relies on the honesty, consistency and willingness of the users to explicitly share identifiable information about themselves (such as location).
Related work in ontology recommendation highlights the importance of addressing two different input types: text corpora and lists of keywords [27]. The original Ontology Recommender, while offering users the possibility of selecting among these two recommendation scenarios, would treat the input data in the same manner. To satisfy users’ expectations, the system should process these two input types differently, to better reflect the information coded in the input about multi-word boundaries.
BioFrameNet28 (Dolbey, Ellsworth y Scheffczyk 2006; Dolbey 2009) es una extensión de FrameNet para el dominio de la biología molecular centrado particularmente en los procesos de transporte intracelular de proteínas. Se trata de un proyecto actualmente discontinuado que fue el objeto de la tesis doctoral de su creador (Dolbey 2009).
Dado que se trata de una extensión, la estructura de BioFrameNet y su metodología son similares a la de FrameNet. BioFrameNet está formado solamente por dos marcos: Protein_transport y Cause_protein_transport. El marco Protein_transport, que está enlazado con FrameNet mediante la relación Inheritance al marco Motion (Dolbey 2009: 75), cuenta con 4 elementos del marco y 32 unidades léxicas. Por su parte, Cause_protein_transport es el causativo de Protein_transport y, por tanto, les une la relación entre marcos: Is Causative of. Cause_protein_transport tiene 18 unidades léxicas y tiene un EM más que Protein_transport por ser su causativo.
El corpus utilizado para llevar a cabo el proyecto es una colección de textos anotados por el Laboratorio de Bioinformática de Lawrence Hunter de la Universidad de Colorado en Denver (Estados Unidos). Los textos anotados son descripciones de productos génicos obtenidos de la base de datos Gene29, del Centro Nacional de Información Biotecnológica de Estados Unidos (NCBI). Dicha anotación estaba basada en una jerarquía de clases creada exclusivamente para clasificar fenómenos biológicos con el objetivo de introducir la información en una base de conocimiento. Esto es, no se tomaron en consideración cuestiones lingüísticas durante la anotación, a diferencia de BioFrameNet (Dolbey 2009: 8). El posterior uso de ese corpus anotado llevó al autor a destacar que dicha clasificación basada en conocimientos biológicos no tenía una correspondencia perfecta en la estructura representada en la lengua:
28 BioFrameNet no está disponible en línea. 29 Disponible en: <http://www.ncbi.nlm.nih.gov/gene/>.
La representación de la variación contextual mediante definiciones terminológicas fléxibles
128
[T]he differences in biology which motivated division of the knowledge base in to the classes listed above are not directly reflected in the grammar and frame semantics of the language used to characterize these differences (Dolbey 2009: 9).
Así pues, mientras que el Laboratorio de Bioinformática de Lawrence Hunter había definido cinco tipos de mecanismos de transporte de proteínas de acuerdo con criterios biológicos (protein transport, gated nuclear transport, transmembrane transport, vesicular transport y endocytosis), en BioFrameNet, tras el análisis lingüístico y siguiendo las pautas para la creación de marcos en FrameNet, se decidió crear solamente dos marcos distintos: Protein_transport y Cause_protein_transport (Dolbey 2009: 72), siendo simplemente el segundo la variante causativa del primero.
Como queda patente en el proyecto BioFrameNet, la organización de marcos desde un punto de vista conceptual, como es el caso de la TBM, dará como resultados una división del conocimiento en marcos distintos de los que daría una configuración en marcos basada en un análisis sintáctico-semántico como el caso de FrameNet y sus derivados.
EvoGrader is a free, online, on-demand assessment service. It automatically grades written (typed) evolutionary explanations produced in response to ACORNS assessment items (Assessment of COntextual Reasoning about Natural Selection, Nehm et al. 2012a) and “ACORNS-like” assessment items (e.g. questions found in Bishop and Anderson 1990; and many other evolution education studies). All of these instruments contain similar item formats: openended questions that ask students to explain how patterns of evolutionary change occurred (from the standpoint of a biologist). Such assessment tasks have been shown to be very useful for understanding student thinking processes (Opfer et al. 2012) and measuring evolutionary understanding (Nehm and Schonfeld 2008, 2010). ACORNS (and ACORNS-like) items are also useful for high school and undergraduate educators because they allow students to practice building scientific explanations, communicating their ideas through writing, and documenting their understanding of the core idea of natural selection (Demastes et al. 1995). Despite their usefulness, these types of assessment tasks take large amounts of time to score. EvoGrader was developed to solve this problem. EvoGrader is a web portal (see www.evograder.org) that contains an on-demand query box requesting the upload of students’ written evolutionary explanations (in .csv format). After upload and “one-click” analysis, EvoGrader generates reports illustrating the presence or absence of accurate scientific ideas (key concepts), non-normative concepts (naïve ideas), and holistic reasoning models (pure scientific, “mixed”, and pure non-normative) in each student’s
response and in the sample of responses as a whole. The system rapidly performs the difficult work of grading written items that for the past 30 years have been used in science education research and practice (for a review, see Nehm and Ha 2011). Responses to 86 different items can be scored using the EvoGrader web portal, which relies on machine-learning methods.
Supervised machine learning: the core of EvoGrader EvoGrader uses machine learning methods to extract key concept scores, naïve idea scores, and holistic reasoning model scores from text responses. An integral part of supervised machine learning in this case is a large corpus of explanations previously scored by domain experts. This corpus (i.e. training set) helps the software “learn” what to look for in the written explanations, and lies at the heart of EvoGrader portal. In order to understand how EvoGrader works, it is necessary to understand some basics about machine learning. The analysis of restaurant ratings will be used as an example to illustrate how machine learning can be applied in order to classify text Perhaps a restaurant owner is interested in customers’ opinions about the meals that are served, and collects a large number of written reviews online. In order to determine how many customers liked and did not like the meals, the restaurant owner must categorize the reviews into groups: e.g., positive reviews and negative reviews. In order to score the reviews a rubric with classification rules would need to be developed. For example, if a review included positive terms such as ‘good’, ‘nice’, or ‘delicious’, then it could be classified into the ‘positive review’ category. On the other hand, if the review included negative terms such as ‘bad’, ‘unpalatable’, or ‘disgusting’, the review could be classified into the ‘negative review’ category. Combinations of terms–such as ‘not’ + key words (e.g., not good, not bad)–could also classified into categories (e.g., ‘not good’ classified as a “negative review”). With these simple classification rules, the restaurant owner categorizes 300 of the 600 reviews. Given the amount of time it took to score these 300 reviews, the owner decides to hire a new employee to classify the remaining reviews. In order for the new employee to score the additional reviews, the owner needs to teach the employee how to classify them. There are two options for doing so. First, the owner could show the classification rules (e.g., rubrics) and the lists of terms to the new employee so that he could understand and apply the rules. The problem with this method is that the large number of classification rules makes it difficult to efficiently process the scores. An alternative approach would be to give the new employee all of the previously classified reviews and ask the new employee to infer what classification rules produced the scores (i.e., negatives and positives). Using the
second method, the owner only has to provide the scored reviews, and not specify all of the rules and terms used to characterize each review. Now let us assume that the new employee is replaced by a computer (i.e., machine) that is capable of either using the predefined classification rules, or coming up with new rules based on the pre-classified data. This frames the restaurant review scoring as a machine-based text classification problem. The first method outlined above (i.e., specifying classification rules and terms characteristic of positive and negative restaurant reviews) is known as rule based text analysis and has been utilized in prior studies in science education (see [Nehm and Haertig 2012], and [Haudek et al. 2012]). The second method outlined above (i.e., using existing sets of classified data to infer the scoring rules) is how EvoGrader works, and is known as supervised machine learning (see [Nehm et al. 2012b], [Ha et al. 2011]). The primary goal of supervised machine learning is to discover classification rules given a corpus of classified data and then to apply those rules to score new unlabeled data. Machine learning methods are often used to minimize human effort and interactions while enhancing automation. Returning to the restaurant example, if the owner decides to take the second approach (i.e., having the new employee use the classified data to infer classification rules and diagnostic text terms) the new employee will need to identify ‘positive reviews’ and ‘negative reviews’ and discover key terms diagnostic of each category. In supervised machine learning, the identification of these diagnostic classification terms is known as feature extraction. Extracting discriminating and independent features is key to any machine learning process. Next, the employee has to come up with a procedure for building the classification rules based upon the key terms in each review and with respect to each review’s category. This procedure is known as model construction. The new employee could score a subset of the owner’s reviews and compare his classification with the owner’s classification to estimate how well his rules worked. In machine learning, this step is known as model validation. Once the new employee’s classification rules produce identical review ratings as the owner’s ratings, he is qualified to classify new sets of reviews. Classification of new reviews is known as prediction. In our restaurant example, we can consider the owner to be the human rater for the 300 reviews (i.e., the training dataset) and the employee as the “machine” that performs feature extraction, model construction, model validation, and classification of new reviews. Similar to the employee’s job of inferring the classification rules that the restaurant owner used to score the reviews as “positive” or “negative,” EvoGrader attempts to develop classification rules that predict the presence or absence of particular evolutionary ideas or concepts in a given written explanation. Unlike the restaurant review, where only one classification target is present (positive or negative review), many different classification targets (ideas or concepts) could be present in students’ responses to questions about natural selection. Consequently, different scoring models (i.e., classifiers) are needed for each target. Thus, for each concept in students’ responses, feature extraction, model construction, and model validation must be performed. EvoGrader architecture: back-end training and front-end scoring Because EvoGrader utilizes supervised machine learning, which in turn relies on discovering classification rules in a corpus of human-classified data, very large pre-scored data sets (text corpora) are required for both system training (i.e., learning from existing data) and system testing (i.e., examining the strength of classification algorithms). EvoGrader is divided into two major components: (1) a back-end training component, and (2) a front-end scoring component (Figure 1). We discuss details of the back-end training and front-end scoring components below. Component 1: Backend training. The construction of scoring models, which takes place in the back-end training part of EvoGrader, is an offline, one-time process. The purpose of the back-end training component is to generate scoring models for each concept that will be fed into the front-end component of EvoGrader. Constructing scoring models is a very resource-intensive process; a large corpus is needed for system training, which itself requires large amounts of time, processing power, and memory consumption. Once the scoring models are generated, however, they can be stored permanently and retrieved on-demand by the front-end component. The backend training component includes three parts: (a) the corpus of human-scored responses; (b) the LightSIDE training box; and (c) the automatic computer scoring models (Figure 1). A very large and diverse human-scored corpus is needed to build classifiers that are capable of accurately predicting the presence or absence of concepts in new samples of written responses. The current version of EvoGrader makes use of 10,270 scored evolutionary explanations written by 2,978 participants (e.g., non-majors, first-year biology majors, senior biology majors, anthropology majors, and experts in evolutionary science [PhD students, postdoctoral fellows, and full-time faculty]). For each of the six key concepts and three naive ideas that have been defined in the evolution education literature (for details, see Nehm et al. 2010), a response was scored by human raters as present (1) or absent (0). Table 1 provides examples of student explanations from the corpus along with humangenerated scoring patterns. Two raters (a PhD student in biology education and an evolution expert) scored these explanations and demonstrated acceptable inter-rater human agreement (>0.81 kappa) for all normative scientific ideas and non-normative naive ideas. On average, it took four minutes to score the presence or absence of nine ideas
in one explanation. In total, it took approximately 685 hours for the two raters to generate the corpus that EvoGrader utilizes (note that these scored explanations were a byproduct of other research studies, and were not only generated for machine-learning purposes). The final scored corpus used for system training was based on consensus scores between the two human raters. The back-end of EvoGrader relies on the supervised machine learning tools of LightSIDE Labs’ open-source program known as LightSIDE (Mayfield et al. 2013). LightSIDE performs three important functions in EvoGrader: feature extraction, model construction, and model validation. Using the human-scored corpus as a training set, algorithms are sought that yield nine unique scoring models (corresponding to the nine concepts). Offline, using the human-scored corpus as a training set, LightSIDE develops scoring models for each of the nine concepts (see Table 1). The goal of this process is to find classifiers that fit the training dataset maximally, and subsequently predict scores on new datasets with high accuracy. Two approaches are used to determine whether these goals are met: cross validation and human expert review. To build the nine scoring models offline, the humanscored corpus is imported into the LightSIDE application. Feature extraction in LightSIDE begins by turning each response in the corpus into a “bag of words” (Harris 1954). This approach simplifies text representation and reduces the dimensionality of the feature space. An inherent assumption in the bag of words representation is that words are considered independent of their place in the text. All of the words that have appeared at least once in the corpus are entered into a ‘corpus dictionary.’ Highly frequent neutral words (e.g., “the, of, to”), punctuations (e.g., “,”), and very infrequent words (e.g., “shall”) are removed from the corpus in order to minimize noise. The remaining words are significant and as long as they appear at a frequency above a cutoff value they will be included in the dictionary. Stemming may also be used, which treats words with the same stem (e.g., the stem “adapt” is common to “adapt”, “adapting”, and “adaptation”) as a single word. Bigram modeling (Cavnar and Trenkle 1994) may also be used and allows for the creation of double-word terms (e.g., “passing on”, “differential survival”) in the dictionary. It is important to emphasize that different concepts typically require different feature extraction settings (e.g., unigrams vs. bigrams, stemming vs. non-stemming, low vs. high cutoff values) (see Ha et al. 2013). For example, two bigram features – ‘had_to’, ‘so_that’— are comprised of highly frequent neutral words (e.g., had, to, so, that); nevertheless, for some concepts–such as the non-normative concept of evolutionary needs/goals–they are crucial for building effective scoring models. The statistically optimized conditions for feature extraction for all nine scoring algorithms in EvoGrader are shown in Table 2. After building the corpus dictionary, each written explanation is converted into a vector of zero/nonzero values. These vectors are called feature vectors (Asadi and Lin). Each of the student responses in the corpus is modeled as a sparse vector. The non-zero values for each vector are the frequencies of the words present in the corresponding response, and the zero values represent all of the other words in the corpus dictionary that did not appear in that
M oharreriet http://w
particular response. The outputs of the feature extraction stage are these feature vectors. Once feature extraction has been completed, model building and validation begin. In order to build scoring models, decision functions must be generated that are based on running the learning algorithm on the feature vectors. The objective of this process is to build binary classifiers for each concept that are capable of accurately predicting the presence or absence (1/0) of a concept in a given explanation. Unpublished research (Nehm, unpublished data) suggests that Sequential Minimal Optimization (SMO) (Platt 1999) is the most effective algorithm for the corpus used in EvoGrader. Each binary classifier represents a decision function for one of the concepts. Therefore, SMO training is performed nine times (once for each concept). For each concept, inputs to the SMO training algorithm include feature vectors and corresponding human scores for that concept. SMO training is an iterative process that repeatedly chooses weight factors for the dictionary words based on the feature vectors and human scores for that concept. These iterations (again, for each individual concept) will continue until the binary classifier that is generated at the end of the iteration is able to classify all of the responses correctly with certain error threshold. Training the classifiers is a one-time process. To validate each of these nine classifiers (for each of the nine concepts), ten-fold cross-validation is performed on each concept model. The cross-validation process involves segregating the corpus into k subsets (e.g., ten-fold refers to 10 subsets) and performing model construction with k – 1 subsets (e.g., nine subsets if ten-fold cross-validation is used); model validation is performed with the last subset. The ten-fold cross-validation approach used in EvoGrader repeats this process 10 times and averages the results so that each subset can be validated once. The training corpus may contain the data misclassified by the cross-validation process (Muhlenbach et al. 2004). Although the reasons for the misclassified data are usually the mislabeling of the data or spelling errors, the reasons are not apparent in many cases because of the complexity of the machine learning algorithm. In such cases, we can remove the misclassified data from the original training corpus and re-train the scoring algorithm using the new corpus (Muhlenbach et al. 2004). After feature extraction and model construction are completed for the nine binary classifiers, their performance is compared to performance thresholds (90% accuracy, and 0.8 kappa coefficients). If they fail, they are subjected to feature refinement. For example, using bigrams and unigrams might produce better performance than using unigrams alone (see Ha et al. 2013, for details on this process). Once the scoring models meet these benchmarks, they are saved and used by the front-end scoring component of EvoGrader. Component 2: Front-end scoring. The scoring models generated in the back-end component of EvoGrader are used in the front-end component to classify each new response as “present” or “absent” for each of the nine concepts. To perform prediction, each new response is converted into a vector representation (as discussed above). Using the weights generated previously in the SMO training step, the system calculates whether the vector belongs to the “absent” class or to the “present” class.
The scoring component of EvoGrader is supported by Amazon’s Elastic Cloud service (EC2). EC2 provides resizeable, on-demand computation capacity in the ‘cloud’. In terms of scalability of the hosting service, EC2 is an appropriate fit for EvoGrader because ‘bursty’ http traffic and sudden processing loads for scoring large datasets is anticipated to be common; it is difficult to predict when users will upload data files for analysis. Hosting EvoGrader on a virtual private cloud provides a flexible web service environment that is capable of rescaling processing and memory power to maintain a reasonable response time for all of the users irrespective of the number of concurrent users. The front-end scoring component of EvoGrader includes: (a) system-calls for prediction and generation of the comprehensive result file, and (b) interpretation and visualization. We discuss each of these components in turn. System-calls for prediction and generation of the comprehensive result file. EvoGrader requires users to format student response data prior to upload into the web portal. Step-by-step video instructions are provided in the web portal (and written instructions are included in the Additional file 1). In brief, student identifiers and typed explanations are pasted into separate cells in a spreadsheet and the file is then saved in .csv (comma separated values) format. Response files can contain an unlimited number of typed explanations to up to eight different ACORNS items in one run (unlimited numbers of runs can be performed). File setup usually takes less than five minutes. After upload to the EvoGrader portal, the system performs a round of preprocessing and verifies that the response file has been formatted properly. Then, for each item, scoring model files from the back-end training component are used to execute online scoring using LightSIDE. As noted above, LightSIDE is a standalone application, and therefore its prediction engine cannot service clients directly over the web. EvoGrader’s front-end performs this function. The application server generates a system call to invoke the local LightSIDE predictor. This system call will send an interrupt message to the operating system on the application server, and will initiate the scoring process using the parsed response file and the pre-constructed scoring models. In files that include responses to multiple items, EvoGrader generates temporary files for each item and passes them sequentially to the LightSIDE prediction engine. The automated scoring process usually takes less than an hour and the results are stored on the application server. As soon as the final temporary file corresponding to the last item is scored, the application server merges all of the results to produce a unified comprehensive result file for all of the items included in that run. At this stage, another system call from LightSIDE invokes loading of a java servlet that provides a downloadable result file. Interpretation and visualization. The downloadable result file contains raw scoring results for all of the concepts and reasoning models. However, instructors do not need to download any files to discover actionable insights about their student response file. EvoGrader automatically generates charts and tables to help instructors interpret and visualize response patterns (Figure 2). Using Java servlet technology on the application server, as soon as the result file is generated, in the background a parallel thread starts executing a general analysis of the results to generate required data for tables and charts. This parallel processing enhances resource and time utilization. Bar charts, pie charts, and bubble charts, visually represent types of reasoning models and concept use patterns in the sample (See Figure 2).
For a game with N Experts, the best bound, uniform in T , is given by [3, Theorem 2.3]:
LT ≤ LnT + √ 2T lnN + √ lnN
8 . (19)
The bounds (14) and (17) with ǫ = 1/N are always worse than (19). In the bound (17) the leading coefficient at √ T lnN is √ 2 times as much. In the
bound (14) the coefficient at √ T lnN is the same, but the other terms are larger, and even the asymptotics is worse when N is fixed and T → ∞. However, it appears that the bound (19) cannot be transferred to ǫ-quantile regret RǫT = LT −LǫT . The proof of Theorem 2.3 in [3] heavily relies on tracking the loss of only one best Expert, and it is unclear whether the existence of several good (or identical) Experts can be exploited in this proof. The experiments
reported in [4] show that algorithms with good best Expert bounds may have rather bad performance when the nominal number of Experts is much greater than the effective number of Experts.
The first (and the only, as far as we know) bound specifically formulated for ǫ-quantile regret is proven for the NormalHedge algorithm in [4, Theorem 1]:
LT ≤ LǫT + √ ( 1 + ln 1
ǫ
)( 3(1 + 50δ)T + 16 ln2 N
δ
(
10.2
δ2 + lnN
))
, (20)
which holds uniformly for all δ ∈ (0, 1/2]. Note that this bound depends on the effective number of actions 1/ǫ and at the same time on the nominal number of actions N . The latter dependence is weak, but probably prevents the use of NormalHedge with infinitely many Experts.
The main advantage of our bounds in Theorems 8 and 9 is that they are perfectly in terms of the effective number of Experts. In a sense, the DFA does not need to know in advance the number of Experts.
Remark 4. To obtain a precise statement about the unknown number of Expert, one can consider the setting where Experts may come at some later steps; the regret to a late Expert is accumulated over the steps after his coming — it is a simple time selection function (see Subsection 4.3), which switches from 0 to 1 only once. Our algorithms and bounds can be easily adapted for this setting: we must consider infinitely many Experts almost all of which are inactive; and then proceed similarly to Theorem 11.
Both our bounds are worse than (20) asymptotically when ǫ and N are fixed and T → ∞. In this case, the regret term in (20) grows as √ 3T ln(1/ǫ) + 3T , whereas in (17) it grows as √ 4T ln(1/ǫ) + 7 √ T and in (14), the worst bound, it grows as √
5T ln lnT + 2T ln(1/ǫ). On the other hand, our bounds are better when T is relatively small. The term ln lnT is small for any reasonable practical application (e. g., ln lnT < 4 if T is the age of the universe expressed in microseconds), and then the main term in (14) is √ 2T ln(1/ǫ), which even fits the optimal bound (19). Bound (17) improves over (20) for T ≤ 106 ln4 N . Now let us say a few words about known algorithms for which an ǫ-quantile regret bounds were not formulated explicitly, but can easily be obtained. The Weighted Average Algorithm, which is used to obtain bound (19), can be analysed in a manner different from [3, Theorem 2.3], see [11]. Then one can obtain the following bound for ǫ-quantile regret:
LT ≤ LǫT + 1
c
√ T ln 1
ǫ + c
√ T ,
where the constant c > 0 is arbitrary but must be fixed in advance. If ǫ is not known and hence c cannot be adapted to ǫ, the leading term is O( √ T ln 1ǫ ), which is worse than (17) for small ǫ (that is, if we consider a large effective number of actions).
For the Aggregating Algorithm [13] (which can be considered as a special case of the DFA for a certain supermartingale, as shown in [5]), the bound can be trivially adapted to ǫ-quantile regret:
LT ≤ cLǫT + a ,
where the possible constants c ≥ 1 and a depend on the loss function. However, in the case of DTOL or arbitrary convex games, the constant c is strictly greater that 1 and the bound may be much worse than (14) and (17) (when LǫT grows
significantly faster than √ T ). At the same time, this bound is much better when LǫT ≈ 0 (there is at least ǫ fraction of “perfect” Experts ). For the standard setting with the known number of Experts, other “small loss” bounds, of the form LT ≤ LnT + O( √
LnT ), were obtained. The authors of [4] posed an open question whether similar bounds can be obtained if the (effective) number of actions is not known. We left the question open.
In order to explore the quality of our CEP module, we ran a test comprising of 20 simulations generated by the traffic micro-simualtor of GTL along with annotations of congestions. The annotations of congestions include the location and the time the congestion is detected. First, we evaluated the quality of our Congestion pattern against the annotated data. We checked the proportion of detections by our EPA that were annotated in the data as congestions (precision) and second, the proportion of congestions we were able to detect out of all the annotated congestions (recall). In all our simulations our precision was 100%, while the average recall over all the simulations was 72%. This can be easily explained: the rule implemented has been given to us by the domain expert, who is the one to identify the congestions in the simulations, thus giving a perfect precision. However, when implementing the pattern we applied a “stricter” criterion for the rule than the one in the simulator: we took into account not just the average speed critical thresholds, but also density thresholds, therefore we have a less success rate in the recall of the results, i.e., there were annotations of congestion in the data that we “missed”.
As a second step, we aimed at checking a more interesting question, that is, whether the inclusion of uncertainty aspects enables us to predict a congestion in the highway before it reaches critical thresholds, as opposed to detecting it once it happens. We addressed this question by having two EPNs, once including uncertainty aspects and the other one without uncertainty, i.e. deterministic; and running the tests twice, one time for each EPN (with and
without uncertainty). This is a common approach in CEP engines dealing with uncertainty (see for example in [7]). The deterministic case served as baseline, as we knew at this stage that all our congestions have been detected correctly. The precision of our results indicates the proportion of congestions we were able to predict (in other words, PredictedCongestion pointed out correctly to a congestion), whereas the recall indicates the proportion of congestions we were able to detect out of all the annotated congestions (in other words, PredictedCongestion pointed out correctly out of all congestions). We used a threshold of 0.6 in the certainty attribute to determine whether to consider PredictedCongestion as a congestion. In other words, only PredictedCongestion alerts with a certainty value larger than 0.6 were considered in our calculations of precision and recall. In these tests, the average precision was 91% and the average recall was 75%. Furthermore, PredictedCongestion event is emitted 3 to 4 minutes before a Congestion is detected, thus enabling the system to take proactive actions in order to alleviate these congestions. The recall average indicates that there are other situations that cause congestions which are not detected by our pattern. Further analysis shows that these situations are characterized by “jumping data”, meaning, the values of speed and density tend to jump thus not satisfying the increasing build-up which is required in our pattern. We are currently investigating these “jumping” cases to see if we can identify some common behavior/pattern.
The proposed top-down algorithms are incremental in nature whereas the existing bottomup approach is not incremental. After generating a specified number of ordered solutions, our methods can generate the next solution incrementally without needing to restart itself, whereas the existing approach needs to be restarted. For example, after generating the first 10 ordered solutions, ASG and LASG generate the 11th solution directly from the data structures maintained so far by these algorithms and perform necessary updates to these data structures. Whereas, BU needs to be restarted with input parameter 11 for generating the 11th solution. In Table 9 we compare the time needed to generate the subsequent 11th solution and 12th solution incrementally after generating first 10 solutions. In order to have more clarity in the comparison among the running times of the respective algorithms, we have used higher precision (upto the 6th decimal place) while reporting the running time in Table 9. Clearly, both ASG and LASG outperform BU for generating the 11th and 12th solution in terms of the time requirement.
Input ← Query S, Tree T Output ← Document Set D Bl ← ComputeBound(Tree T.left, Query q) # using eqn 2 Br ← ComputeBound(Tree T.right, Query q) #using eqn 2
#getLast: Returns the element with least similarity with query if (Bl ≥ getLast(queue)) searchL=True if (Br ≥ getLast(queue)) searchR=True
if(searchL and searchR)
if (Bl > Br) queue ← SearchTree(Query S, Tree T.left) else
queue ← SearchTree(Query S, Tree T.right) else if (searchL and !searchR)
queue ← SearchTree(Query S, Tree T.left) else if (!searchL and searchR)
queue ← SearchTree(Query S, Tree T.right) else
return (queue)
The ’extra data’ we used for training of the deep network is composed of two large static image datasets of facial expressions for the seven emotion classes.
The first and larger one is the Google dataset [5] consisting of 35,887 images with the seven facial expression classes: angry, disgust, fear, happy, sad, surprise and neutral. The dataset was built by harvesting images returned from Google’s image search using keywords related to expressions, then cleaned and labeled by hand. We use the grayscale 48× 48 pixel versions of these images. The second one is the Toronto Face Dataset (TFD) [35] containing 4,178 images labeled with basic emotions, essentially with only fully frontal facing poses.
To make the datasets compatible (there are big differences, for instance variation among subjects, lighting and poses), we applied the following registration and illumination normalization strategies:
Registration To build a common dataset, TFD images and frames from the competition dataset had to be integrated with the Google dataset, for which we used the following procedure: For image registration we used 51 of the 68 facial keypoints extracted by the mixture of trees method from [40]. The face contour keypoints returned by this model were ignored in the registration process. Images from the Google dataset and the AFEW datasets have different poses, but most faces are frontal views.
To reduce noise, the mean shape of frontal pose faces for each dataset was used to compute the transformation between the two shapes. For the transformation the Google data was considered as base shape and the similarity transformation was used to define the mapping. After inferring this mapping, all data was mapped to the Google data. TFD images have a tighter fit around faces, while Google data includes a small border around the faces. To make the two datasets compatible, we added a small noisy border to all images of TFD.
Illumination normalization using isotropic smoothing To compensate for varying illumination in the merged
dataset, we used the diffusion-based approach introduced in [17]. We used the isotropic smoothing (IS) function from the INface toolbox [33,38] with the default smoothness parameter and without normalization as post-processing. A comparison of original and IS-preprocessed face images is shown in figure 2.
We used several deep learning architectures for building detectors for communication systems. Different architectures were considered for symbol-by-symbol detection as well as sequence detection. These algorithms have many potentials in systems where the underlying physical models of the channel are unknown or inaccurate. For example, molecular communication, which has many potential applications in medicine is very difficult to model using tractable analytical models. We use an experimental platform that simulates in-vessel chemical communication to collect experimental data for training and testing deep learning algorithms. We show that deep learning sequence detectors can improve the detection performance significantly compared to a baseline approach that also does not rely on channel models. This demonstrates the promising performance deep learning detection algorithms could have in designing future communication systems. Some interesting open problems that we would like to explore in the future are as follows. First, we would like to perform more parameter tuning to find better neural networks for detection. Another important problem we plan to explore is how resilient deep learning detectors are to changing channel conditions, for example, as the concentration of acid and base is changed, and what would be good protocols for quickly retraining the network when a change in channel conditions is detected. Finally, we plan to collect more data on our platform, and make our dataset publicly available to other researchers.
In the first experiment we checked to what extent the overhead of the shattering approach can be minimized by using intensional representations and immutable objects that are shared whenever possible. We ran tests on the following set of parfactors:
Φ = {〈 /0,{gQ(),g1(a)},F0〉, [0] 〈{X 6= a},{gQ(),g1(X)},F1〉, [1] 〈 /0,{g1(X),g2(X)},F2〉, [2] 〈 /0,{g2(X),g3(X)},F3〉, [3] . . . ,
〈 /0,{gk−1(X),gk(X)},Fk〉, [k] 〈 /0,{gk(X)},Fk+1〉} [k+1] .
All functors had the range size 10 and we set Q to the instance of gQ(). We computed the marginal JQ(Φ). Lifted inference with shattering first performed total of k splits, then proceeded with 2k +1 multiplications and 2k summations regardless of the elimination ordering. Lifted inference with splitting as needed performed 1 split, k +2 multiplications and k + 1 summations (for the experiment we used the best elimination ordering, that is gk,gk−1, . . . ,g1).
Figure 5 shows the results of the experiment where we varied k from 1 to 100. Even though lifted inference with shattering used virtually the same amount of memory as lifted inference with splitting, it was slower because it performed more arithmetic operations.
In general, tensor decomposition is an optimization problem, i.e., minimizing the difference between the decomposed tensor and the target tensor. We employ Tensor Power Method (TPM) [1]. TPM is known to explain the same variance with less rank compared to ALS [1] because the rank-1 tensors found in the early steps of the process explains most of the variances in the target tensor.
TPM approximates a target tensorW by adding rank-1 tensors iteratively. First, TPM finds a rank-1 tensor,Wdecomposed, to approximate W by minimizing ||W − Wdecomposed||2 in a coordinate descent manner. The main idea in the decomposition is that it utilizes the residual Wresidual = W − Wdecomposed, so that the next iteration approximates the residual tensor Wresidual by minimizing ||Wresidual − Wdecomposed||2. This continues until the number of rank-1 tensors found is equal to R. More details can be found in [1].
As a standard reinforcement learning structure, we set the environment mentioned in Section III-A as e. At each discrete time step, the agent selects an action at from the defined action set. In this paper, the action set consists of five moving commands, namely left, half-left, straight, halfright and right. Detailed assignments of speeds related to the moving commands are introduced in Section IV. The only perception by the robot is the depth image xt taken from the kinect camera after every execution of the action. Unlike that the reward rt in Atari games is the change of the game’s score, the only feedback used as the reward is a binary state, indicating whether the collision occurs or not. It is decided by checking the minimum distance lt through the depth image taken by the kinect camera. Once the collision occurs, we set a negative reward tter to represent the termination. Conversely, we grant a positive reward tmove to encourage the collision-free movement.
The exploration sequences st in the simulated environment is regarded as a Markov Decision Process (MDP). It is an alternate combination of moving commands and depthimage states where st = {x1, a1, x2, a2, . . . , at−1, xt}. The sequence terminates once the collision happens. As the assumption of MDP, xt+1 is completely decided by (xt, at) without any references with the former states or actions in st. The sum of the future rewards until the termination is Rt. With a discounted factor γ for future rewards, the sum of future estimated rewards is Rt = ∑T t′=t γ
t′−trt′ , where T means the termination time-step. The target of reinforcement learning is to find the optimal strategy π for the action decision through maximizing the action-value function Q∗(x, a) = maxπE[Rt|xt = x, at = a, π]. The essential assumption in DQN [2] is the Bellman equation, which
2http://www.ros.org
transfers the target to maximize the value of r+γQ∗(x′, a′) as
Q∗(x, a) = Ex′∼e[r + γmax a′ Q∗(x′, a′)|x, a]
Here x′ is the state after acting action a in state x. DQN estimated the action-value equation by convolutional neural networks with weights θ, so that Q(s, a, θ) ≈ Q∗(s, a).
Algorithm 1 Deep reinforcement learning algorithm 1: Initialize the weights of evaluation networks as θ−
Initialize the memory D to store experience replay Set the collision distance threshold ls
2: for episode = 1,M do 3: Randomly set the turtlebot to a start position Get the minimum intensity of depth image as lt 4: while lt > ls do 5: Capture the depth image xt 6: With probability ε select a random action at
Otherwise select at = argmaxaQ(xt, a; θ −)
7: Move with the selected moving command at Update lt with new depth information 8: if lt < ls then 9: rt = rter
xt+1 = Null 10: else 11: rt = rmove Capture the new depth image xt+1 12: end if 13: Store the transition (xt, at, rt, xt+1) in D
Select a batch of transitions (xk, ak, rk, xk+1) randomly from D
14: if rk = rter then 15: yk = rk 16: else 17: yk = rk + γmaxa′ Q(xk+1, a
′; θ−) 18: end if
Update θ through a gradient descent procedure on the batch of (yk −Q(φk, ak; θ−))2
19: end while 20: end for
In this paper, we use three convolutional layers for feature extractions of the depth image and use additional three fully-connected layers for exploration policy learning. The structure is depicted as red and green cubes shown in Fig. 2. To increase the non-linearity for better data fitting, each Conv or Fully-connected layer is followed by a Rectified Linear Unit (ReLU) activation function layer. The number under each Conv+ReLU or FullyConnected+ReLU cube is the number of channels of the output data related to this cube. The network takes a single depth raw image as the input. The five channels of the final fully-connected layer fc3 are the values of the five moving commands. Besides, to avoid the overfitting in the training procedure, both of the first two fully-connected layers fc1 and fc2 are followed with a dropout layer. Note that dropout layers are eliminated in test procedure [24].
Algorithm 1 shows the workflow of our revised deep reinforcement learning process. Similar as [2], we use the memory replay method and the ε-greedy training strategy to control the dynamic distribution of training samples. After the initialization of the weights for convolutional networks shown in Fig. 2, set a distance threshold ls to check if the turtlebot collides with any obstacles. At the beginning of every repeated exploration loop, the turtlebot is randomly set to a start point among the 12 pre-defined start points shown in Fig. 1. That extends the randomization of the turtlebot locations from the whole simulation world and keeps the diversity of the data distribution saved in memory replay for training.
For the update of weights θ, yk is the target for the evaluation network to output. It is calculated by summing the instant reward and the future expectation estimated by the networks with the former weights as mentioned before in the Bellman equation. If the sampled transition is a collision sample, the evaluation for this (xk, ak) pair is directly set as the termination reward rter. Setting the training batch size to be n, the loss function is
L(θi) = 1
n n∑ k [(yk −Q(xk, ak; θi))2]
After the estimation of Q(xk, ak) and maxa′ Q(xk+1, a′) with the former θ−, the weights θ of the network will be updated through back-propagation and stochastic gradient descent.
The presented paper deals with the problem of semantic clustering of search engine results page (SERP). The problem arises from the obvious fact that many user queries are ambiguous in some way. Thus, search engines strive to diversify their results and to present such results that are related to as many query interpretations as possible. For example, Google search for the Russian word ‘максим’ returns:
1. five results related to a popular singer, 2. two results for a magazine, 3. one result for http://lib.ru, Maxim Moshkow’s electronic library, 4. one result for a proper name.
However these results are not sorted by their meaning and are returned simply according to their relevance ranking, which for many of them seems to be almost equal. The obvious way to cluster the results is by the words their snippets share. Unfortunately, often snippets for results belonging to one query sense do not have a single content word in common (except for the query itself, which is useless). Cf. two snippets for the first query meaning from the example above:
1. ‘МакSим начинает самостоятельно заниматься своей карьерой, пишет новые песни. В этот период певица выступает как малобюджетный проект, ...’ 2. ‘МакSим презентовала видеоклип «Я буду жить», получивший широкую огласку еще до момента появления видео в сети.’
They do not have a single common word, but still belong to one meaning (popular singer).
Moreover, snippets for different query senses can share some words. Cf. two snippets from the same search engine results page. They share the word ‘автор’ (‘author’), however the first snippet relates to the first meaning, while the second snippet shows the third one:
ar X
iv :1
40 9.
16 12
v2 [
cs .C
L ]
2 6
O ct
2 01
4
1. ‘МакSим (Марина Абросимова) – одна из самых популярных и коммерчески успешных певиц в России, являющаяся автором и исполнителем...’ 2. ‘Работает с 1994 года. Книги и тексты, разбитые по жанрам и авторам.’
That means that there is a need for more sophisticated way to cluster search results. We should somehow learn which senses the query has and with which words these meanings are (probabilistically) associated. One of the possible ways to solve this problem is by extracting co-occurrence statistics from large corpora. The idea behind this is that word meaning is in fact the sum (or the average) of its uses. So, meaning is a function of distribution (cf. [1]). Thus, if we know with which words the query typically co-occurs and how these neighbors are related to each other, then we know the ‘sense set’ of the query. After that we can somehow measure semantic similarity of each search snippet on the SERP with each of the senses and map them to each other. This information can then be used to either rank the results, or mark them with appropriate labels.
The structure of the paper is as follows. In Section 2 we briefly overview work previously done on the subject. Section 3 describes the process of building co-occurrence graphs from large Russian corpora. In Sections 4 and 5 we conduct an experiment on clustering SERPs with ambiguous queries from Mail.ru search engine with the help of the methods described before. The results are evaluated in Section 6. Section 7 draws conclusions concludes and provides suggestions for further research.
A naive implementation would introduce unnecessary overhead to the overall computation since it comes with the overhead of reasoning over and storage of overlapping sets of knowledge. A more refined version of both WFS fixpoint and least fixpoint of TP,J(I) is defined in Algorithm 1 and Algorithm 2 respectively.
Algorithm 1 Optimized WFS fixpoint opt WFS fixpoint(P): . input: program P
1: K0 = opt lfp(P+, /0, /0); . output: set of literals Ki−1, Ui−1 2: i = 0; 3: repeat 4: Ui = Ki ∪ opt lfp(P, Ki, Ki); 5: i++; . next “inference step” 6: Ki = Ki−1 ∪ opt lfp(P, Ki−1, Ui−1); 7: until Ki−1.size() == Ki.size() 8: return Ki−1, Ui−1;
Algorithm 2 Optimized least fixpoint of TP,J(I) opt lfp(P, I, J): . precondition: I ⊆ lfp(TP,J( /0))
1: S = /0; . input: program P, set of literals I and J 2: new = /0; . output: set of literals S (lfp(TP,J(I) - I) 3: repeat 4: S = S ∪ new; 5: new = T(P, (I ∪ S), J); 6: new = new - (I ∪ S); 7: until new == /0 8: return S;
Our first optimization is the changed calculation of the least fixpoint of TP,J(I) (opt lfp), which is depicted in Algorithm 2. Instead of calculating the least fixpoint starting from I = /0, for a given program P and a set of literals J, we allow the calculation to start from a given I, provided that I ⊆ lfp(TP,J( /0)), and return only the newly inferred literals (S) that led us to the least fixpoint. Thus, the actual set of literals that the least fixpoint of TP,J(I) consists of is I ∪ S. In order to reassure correctness we need to take into consideration both I and S while calculating the least fixpoint, namely new literals are inferred by calculating TP,J(I ∪ S). However, we use a temporary set of inferred literals (new) in order to eliminate duplicates (new = new − (I ∪ S)) prior to adding newly inferred literals to the set S (S = S ∪ new). Note that the set of literals I remains unchanged when the optimized least fixpoint is calculated.
The optimized version of the least fixpoint is used, in Algorithm 1, for the computation of each set of literals K and U. K0 is a special case where we start from I = /0 and J = /0, and thus, unable to fully utilize the advantages of the optimized least fixpoint.
The proposed optimizations are mainly based on the monotonicity of the well-founded semantics as given in Lemma 2.1. Note that in this section, the indices of the sets K and U found in Lemma 2.1 are adjusted to the indices used in Algorithm 1 in order to facilitate our discussion.
Since Ki ⊆ Ui, for i≥ 0 (see Lemma 2.1), the computation of Ui can start from Ki, namely I = Ki. Thus, instead of recomputing all literals of Ki while calculating Ui, we can use them to speed up the process. Note that the actual least fixpoint of Ui is the union of sets Ki and opt lfp(P, Ki, Ki), as the optimized least fixpoint computes only new literals (which are not included in given I).
Since Ki−1 ⊆ Ki, for i ≥ 1 (see Lemma 2.1), the computation of Ki can start from Ki−1, namely I = Ki−1. Once opt lfp(P, Ki−1, Ui−1) is computed, we append it to our previously stored knowledge Ki−1, resulting in Ki. In addition, a WFS fixpoint is reached when Ki−1 = Ki, namely when Ki−1 and Ki have the same number of literals.
Proof If Ki−1 = Ki, for i≥ 1, then Ui−1 = Ki−1 ∪ opt lfp(P, Ki−1, Ki−1) = Ki ∪ opt lfp(P, Ki, Ki) = Ui Thus, fixpoint is reached as (Ki−1,Ui−1) =(Ki,Ui).
According to Theorem 2.2, having reached WFS fixpoint at step i, we can determine which literals are true, undefined and false as follows: (a) true literals, denoted by Ki, (b) undefined literals, denoted by Ui − Ki and (c) false literals, denoted by BASE(P) − Ui.
Although for Ki calculation only new literals are inferred during each “inference step”, for Ui we have to recalculate a subset of literals that can be found in Ui−1, as literals in Ui−1 − Ki−1 are discarded prior to the computation of Ui. However, the computational overhead coming from the calculation of opt lfp(P, Ki, Ki) reduces over time since the set of literals in Ui − Ki becomes smaller after each “inference step” due to Ki−1 ⊆ Ki and Ui−1 ⊇ Ui, for i≥ 1, (see Lemma 2.1).
We may further optimize our approach by minimizing the amount of stored literals. A naive implementation would require the storage of up to four overlapping sets of literals (Ki−1, Ui−1, Ki, Ui). However, as Ki ⊆ Ui, while calculating Ui, we need to store in our knowledge base only the sets Ki and opt lfp(P, Ki, Ki), since Ui = Ki ∪ opt lfp(P, Ki, Ki).
As Ki−1 ⊆ Ki, for the calculation of Ki, we need to store in our knowledge base only three sets of literals, namely: (a) Ki−1, (b) Ui−1 − Ki−1 = opt lfp(P, Ki−1, Ki−1) and (c) currently calculating least fixpoint opt lfp(P, Ki−1, Ui−1). All newly inferred literals in opt lfp(P, Ki−1, Ui−1), are added to Ki (replacing our prior knowledge about Ki−1), while literals in Ui−1 - Ki−1 = opt lfp(P, Ki−1, Ki−1) are deleted, if fixpoint is not reached, as they cannot be used for the computation of Ui.
A WFS fixpoint is reached when Ki−1 = Ki, namely when no new literals are derived during the calculation of Ki, which practically is the calculation of opt lfp(P, Ki−1, Ui−1). Since (Ki−1,Ui−1) = (Ki,Ui), we return the sets of literals Ki−1 and Ui−1, representing our fixpoint knowledge base.
In practice, the maximum amount of stored data occurs while calculating Ki, for i≥ 1, where we need to store three sets of literals, namely: (a) Ki−1, (b) Ui−1 − Ki−1 and (c) opt lfp(P, Ki−1, Ui−1), requiring significantly less storage space compared to the naive implementation.
By the end of a progression step, some of the remaining active units (if any are left) have their advancing condition broken. Recall that this happens for a unit u when either pos(u) /∈ π(u) or u is placed on its precomputed path but the next location on the path is not blank. A repositioning step ensures that a well positioned state is reached (i.e., all active units have the advancing condition satisfied) before starting the next progression step.
A simple and computationally efficient method to perform repositioning is to undo a block of the most recent moves performed in the preceding progression step. Undoing a move means carrying out the reverse move. Solved units are not affected. For those remaining active units, we undo their moves, in reverse global order, until a well positioned state is encountered. We call this strategy reverse repositioning. An example is provided in Section 5.1.
Proposition 9. If the reverse repositioning strategy is used at line 7 of Algorithm 1 (when needed), then all progression steps start from a well positioned state.
Proof. This lemma can be proven by induction on the iteration number j in Algorithm 1. Since the initial state is well positioned (this follows easily from Definitions 1 and 3), the proof for j = 1 is trivial. Assume that a repositioning step is performed before starting the iteration j + 1. In the worst case, reverse repositioning undoes all the moves of the remaining active units (but not the moves of the units that have become solved), back to their original positions at the beginning of j-th progression step. In other words, we reach a state s that is similar to the state s′ at the beginning of the previous progression step, except that more units are on their targets in s. Since s′ is well positioned (according to the induction step), it follows easily that s is well positioned too.
This work was supported by DARPA under agreement numbers HR0011-15-2-0025 and FA8750-13-2-0008. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of any of the organizations that supported the work.
In this paper, several component modules and a system organization for offline handwritten Bengali script segmentation process is described. The experiment has demonstrated the system’s ability to identify majority of
the segmentation points of the handwritten words. Future research works may concentrate on the analysis of recognition based segmentation approach. The experiment excluded the compound characters available in Bengali script. Extensive analysis on these compound characters may lead to an overall improvement in the generality of the process. Future work may also include identification of overlapping pen-strokes with an improvement of all the existing modules.
An array of microphones provides multiple monaural recordings, which contain information indicative of the spatial origin of a sound source. When sound sources are spatially separated, with sensor array inputs one may localize sound sources and then extract the source from the target location or direction. Traditional approaches to source separation based on spatial information include beamforming, as mentioned in
Sect. I, and independent component analysis [6] [72]. Sound localization and location-based grouping are among the classic topics in auditory perception and CASA [9] [12] [140].

The final task we consider is the following: if we are given the words and the topic of the current sentence, can we predict the topic of the next sentence? This is an interesting problem for dialog systems, where we ask the question: given the utterance of a speaker, can we predict the topic of their next utterance? This can be used in various applications in dialog systems, e.g., intent modeling.
The sentence topic prediction problem can be formulated as follows: given a model with parameters Θ, words in the sentence si and corresponding topic Ti, find the next sentence topic Ti+1 that maximizes the following probability – P (Ti+1|si, Ti,Θ). Note that in this case we train a model to predict the topic target instead of the joint word/topic target, since we empirically determined that training a model with a joint target gave lower accuracy in predicting the topic compared to a model that only tries to predict the topic as a target.
abilistic reasoning. Counting the number of satisfying assignments of a propositional formula (#SAT) is a closely related problem of fundamental theoretical importance. Both these problems, and others, are members of the class of sum-of-products (SUMPROD) problems. In this paper we show that standard backtracking search when augmented with a simple memoization scheme (caching) can solve any sum-of-products problem with time complexity that is at least as good any other state-of-the-art exact algorithm, and that it can also achieve the best known time-space tradeoff. Furthermore, backtracking’s ability to utilize more flexible variable orderings allows us to prove that it can achieve an exponential speedup over other standard algorithms for SUMPROD on some instances.
The ideas presented here have been utilized in a number of solvers that have been applied to various types of sum-of-product problems. These system’s have exploited the fact that backtracking can naturally exploit more of the problem’s structure to achieve improved performance on a range of problem instances. Empirical evidence of this performance gain has appeared in published works describing these solvers, and we provide references to these works.
We thank the anonymous reviewers and Kareem Kouddous for their feedback. Bowman acknowledges support from a Google Faculty Research Award and gifts from Tencent Holdings and NVIDIA Corporation. We thank Koko for contributing a unique dataset for this research.
Taxonomy induction is a well-studied task, and multiple different lines of work have been proposed in the prior literature. Early work on taxonomy induction aims to extend the existing partial taxonomies (e.g., WordNet) by inserting missing terms at appropriate positions. Widdows [39] places the missing terms in regions with most semantically-similar neighbors. Snow et al. [34] use a probabilistic model to attach novel terms in an incremental greedy fashion, such that the conditional probability of a set of relational evidence given a taxonomy is maximized. Yang and Callan [40] cluster terms incrementally using an ontology metric learnt from a set of heterogeneous features such as co-occurrence, context, and lexico-syntactic patterns.
A different line of work aims to exploit collaboratively-built semi-structured content such as Wikipedia for inducing large-scale taxonomies. Wikipedia links millions of entities (e.g., Johnny Depp) to a network of inter-connected categories of different granularity (e.g. Hollywood Actors, Celebrities). WikiTaxonomy [29, 30] labels these links as hypernymy or non-hypernymy, using a cascade of heuristics based on the syntactic structure of Wikipedia category labels, the topology of the network and lexico-syntactic patterns for detecting subsumption and meronymy, similar to Hearst patterns [13]. WikiNet [24] extends WikiTaxonomy by expanding nonhypernymy relations into fine-grained relations such as part-of, located-in, etc. YAGO induces a taxonomy by employing heuristics linking Wikipedia categories to corresponding synsets in WordNet
[14]. More recently, Flati et al. [7] and Gupta et al. [9] propose approaches towardsmultilingual taxonomy induction fromWikipedia, resulting in taxonomies for over 270 languages. However, as pointed out by Hovy et al. [16], these taxonomy induction approaches are non-transferable, i.e., they only work for Wikipedia, because they employ lightweight heuristics that exploit the semi-structured nature of Wikipedia content.
Although taxonomy induction approaches based on external lexical resources achieve high precision, they usually suffer from incomplete coverage over specific domains. To address this issue, another line of work focuses on building lexical taxonomies automatically from a domain-specific corpus or Web. Kozareva and Hovy [19] start from an initial set of root terms and basic level terms and use hearst-like lexico-syntactic patterns recursively to harvest new terms from the Web. Hypernymy relations between terms are induced by searching theWeb again with surface patterns. The graph of extracted hypernyms is subsequently pruned using heuristics based on the out-degree of nodes and the path lengths between terms. Velardi et al. [38] extract hypernymy relations from textual definitions discovered on the Web, and further employ an optimal branching algorithm to induce a taxonomy.
More recently, Bordea et al. [4, 5] introduced the first shared tasks on open-domain Taxonomy Extraction, thus providing a common ground for evaluation. INRIASAC, the top system in 2015 task, uses features based on substrings and co-occurrence statistics [8] whereas TAXI, the top system in 2016 task, uses lexico-syntactic patterns, substrings and focused crawling [28].
In contrast to taxonomy induction approaches which use external resources, taxonomy induction approaches from a domain corpus or Web typically face two main obstacles. First, they assume the availability of a clean input vocabulary of seed terms. This requirement is not satisfied for most domains, thus requiring a time-consuming manual cleaning of noisy input vocabularies. Second, they ignore the relationship between terms and senses. For example, taxonomies induced from WordNet or Wikipedia produce different hypernyms for each sense of the term apple (e.g., apple is a fruit or a company). To tackle the second obstacle, taxonomy induction approaches from a domain corpus employ domain filtering to perform implicit sense disambiguation. This is done by removing hypernyms corresponding to domain-irrelevant senses of the terms [38]. Although taxonomies should ideally contain senses rather than terms, term taxonomies have shown significant efficacy in a variety of NLP tasks [2, 3, 38].
To put it in context, our approach is similar to the previous attempts at inducing taxonomies without using external resources such as WordNet or Wikipedia. One key differentiator, however, is that it is robust to the presence of significant noise in the input vocabulary, thus dealing with the first obstacle above. To deal with the second obstacle, our approach performs implicit sense disambiguation via domain filtering at two different steps: (i) domain filtering of subsequences (cf. Section 2.2); (ii) assigning lower cost for likely in-domain edges when applying the minimum-cost flow optimization (cf. Section 2.2 & 2.3).
Our mobile phones produce metadata every time we send or received a text or a phone call. These metadata – recording who calls or texts who, for how long, and from where – provide a detailed view of human behavior including mobility at large-scale. This data has great potential for good but often lacks basic demographic information, which is why there has recently been a growing interest in predicting demographic information, such as age and gender, from mobile phone metadata. Previous approaches relied on standard machine learning algorithms and hand-engineered features (Sarraute et al., 2014; Frias-Martinez et al., 2010).
Convolutional networks (ConvNets) have recently systematically outperformed existing approaches in analyses of large-scale image datasets (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014). We show in this work how a ConvNet can be used to predict demographic information such as age and gender from standard mobile phone metadata.
An overview of the workflow of our system is shown in Figure 2. For a given programming problem, we first use the set of all syntactically correct student submissions to train a neural network in the training phase for learning a token sequence model for all valid token sequences that is specific to the problem. We then use the SYNFIX algorithm to find small corrections to a student submission with syntax errors using the token sequences predicted from the learnt model. These corrections are then used for providing feedback in terms of potential fixes to the syntax errors. We now describe the two key phases in our workflow: i) the training phase, and ii) the SYNFIX algorithm.
Each reading (frame) in a synchronized raw input stream of accelerometer and gyroscope data has the form {ax, ay, az, ωx, ωy, ωz} ∈ R6, where a represents linear acceleration, ω angular velocity and x, y, z denote projections on corresponding axes, aligned with the phone. There are two important steps we take prior to feature extraction. Obfuscation-based regularization — it is important to differentiate between the notion of “device” and “user”. In the dataset we collected (Section VI), each device is assigned to a single user, thus all data is considered to be authentic. However, in real-world scenarios such as theft, authentic and imposter data may originate from the same device.
In a recent study [8], it was shown that under lab conditions a particular device could be identified by a response of its motion sensors to a given signal. This happens due to imperfection in calibration of a sensor resulting in constant offsets and scaling coefficients (gains) of the output, that can be estimated by calculating integral statistics from the data. Formally, the measured output of both the accelerometer and gyroscope can be expressed as follows [8]:
a = ba + diag(γa)ã, ω = bω + diag(γω)ω̃, (1)
where ã and ω̃ are real acceleration and angular velocity vectors, ba and bω are offset vectors and γa and γω represent gain errors along each coordinate axes.
To partially obfuscate the inter-device variations and ensure decorrelation of user identity from device signature in the learned data representation, we introduce low-level additive (offset) and multiplicative (gain) noise per training example. Following [8], the noise vector is obtained by drawing a 12- dimensional (3 offset and 3 gain coefficients per sensor) obfuscation vector from a uniform distribution µ ∼ U12[0.98, 1.02].
Data preprocessing — In addition, we extract a set of angles α{x,y,z} and ϕ{x,y,z} describing the orientation of vectors a and ω in the phone’s coordinate system (see Fig. 1), compute their magnitudes |a| and |ω| and normalize each of the x, y, z components. Finally, the normalized coordinates, angles and magnitudes are combined in a 14-dimensional vector x(t) with t indexing the frames.
Classical Chinese Poetry
†Chao-Lin Liu Fairbank Center for Chinese Studies, Harvard University, USA
Department of Computer Science, National Chengchi University, Taiwan chaolin@nccu.edu.tw
We are motivated by the recent development of bridges between ITP systems and their libraries such as Mizar/MML [3], Isabelle/HOL [28], and HOL Light [5]/Flyspeck [4] on one side, and ATP/SMT systems such as E [21], Vampire [13] and Z3 [18] on the other side. The work on such systems in the last decade has shown that many top-level lemmas in the ITP libraries can be re-proved by ATPs after a suitable translation to common ATP formats like TPTP, when given the right previous lemmas as axioms [23, 16, 2, 10]. Automated selection of the right previous lemmas from such large libraries, and also automated construction and selection of the right theorem-proving strategies for proving new conjectures over such large libraries is an interesting AI problem, where complementary AI methods such as machine learning from previous proofs and other heuristic selection methods have turned out to be relatively successful [9, 6, 15, 1, 17, 27, 26, 11]. Since 2008 the CASC LTB (Large-Theory Batch) competition division [22] has been measuring the performance of various ATP/AI methods on batches of problems that usually refer to a large common set of axioms. These problems have been generated from the large libraries of Mizar, HOL Light and Isabelle, and also from the libraries of common-sense reasoning systems such as SUMO [19] and Cyc [20]. This has in the long run motivated the ATP/AI developers to invent and improve methods and strategies that are useful on such classes of problems. Such methods then get integrated into strong advising services (often “cloud-based”) for the ITP users, such as HOL(y)Hammer [7], Sledgehammer [14], and MizAR [25, 8].
The plan of work that we intend to follow with ACL2 is analogous to the procedure that was relatively successful with Mizar, Isabelle and HOL Light: (i) define a translation of the ACL2 formulas to the common TPTP formats (initially FOF), (ii) export all ACL2 theorems and definitions into the TPTP format, (iii) find and export the ACL2-proof dependencies between the ACL2 theorems, (iv) apply machine learning to such proof dependencies to obtain strong recommendations (premise-selection) systems that suggest the lemmas useful for proving a new conjecture, and (v) test the performance of the ATP systems and their strategies on the problems translated from ACL2, either by re-proving the theorems from their exact ACL2 proof dependencies, or by using premise selection to recommend the right lemmas. We present the initial work in these directions and the initial evaluation here.
∗Supported by NWO project Effective Layered Verification of Networks on Chips (ELVeN) under grant no. 612.001.108 †Supported by NWO grant Knowledge-based Automated Reasoning
Stacked Auto-Encoder (SAE) is a kind of deep learning algorithm for unsupervised learning. Which has mult i layers that project the vector representation of input data into a lower vector space. These projection vectors are
dense representations of the input data. As a result, SAE can be used for image compres sion. Using chaotic logistic map, the compression ones can further be encrypted. In this study, an application of image compression and encryption is suggested using SAE and chaotic logistic map. Experiments show that this application is feasible
and effective. It can be used for image transmission and image protection on internet simultaneously.
Keywords: Stacked Auto-Encode, deep learning, image protection, image feature, image compression, image encryption.
In order to give a formulation of the principles stated above, we will suppose from now on that there are at least two propositional variables, i.e. the set P of atomic formulas has cardinality greater or equal to two. In particular, there will be at least four interpretations inWP.
The first property we present will be called Standard Domain. It states some “richness” in the set of results of the fusion process. This is an approximation of the first principle which says that any result is possible in the framework of the restrictions of the system.
(ESF-SD) For any i in S, for any triple of interpretations w, w′ and w′′ in WP, and any couple of epistemic states Ew,w′ and Ew′,w′′ , such that [[B(Ew,w′ )]] = {w,w′} and [[B(Ew′,w′′ )]] = {w′,w′′}, the following conditions hold: (i) There exists an i-profile Ei such that B
(∇(Ei, Ew,w′ )) ≡ ϕw,w′ and B(∇(Ei, Ew′,w′′ )) ≡ ϕw′,w′′ (ii) There exists an i-profile Ei such that B
(∇(Ei, Ew,w′ )) ≡ ϕw,w′ and B(∇(Ei, Ew′,w′′ )) ≡ ϕw′ (iii) There exists an i-profile Ei such that B
(∇(Ei, Ew,w′ )) ≡ ϕw and B(∇(Ei, Ew′,w′′ )) ≡ ϕw′,w′′ (iv) There exists an i-profile Ei such that B
(∇(Ei, Ew,w′ )) ≡ ϕw and B(∇(Ei, Ew′,w′′ )) ≡ ϕw′ (ESF-SD) establishes that any result is possible when the constraints have beliefs with at most two
models. Actually, this postulate is more related to the non-imposition postulate in Social Choice Theory.
Observation 1 It is worth noting that for basic fusion operators (those satisfying Theorem 1), the satisfaction of this postulate is equivalent to the following fact: for any agent i in S, any triple of interpretations w, w′ and w′′ inWP, and any total preorder between these interpretations, there is an i-profile Ei such that = Ei {w,w′,w′′}.
The previous Observation and some natural combinatorial arguments tell us that if the epistemic space is reduced to the consistent formulas modulo logical equivalence and the function B is the identity then the basic fusion operators satisfying the Maximality Condition can not satisfy (ESF-SD). That is precisely the next result.
Theorem 4. Consider the epistemic space (E, B,LP) where E = LP∗/ ≡, with LP a set of formulas built over two propositional variables and B(ϕ) = ϕ for every element6 ϕ in E. Let ∇ : P(S,E) × E −→ E be an ES basic fusion operator satisfying the Maximality Condition. Then ∇ does not satisfy (ESF-SD).
The next result shows the richness and versatility of the fusion operators which satisfy this property:
Proposition 3. Let (E, B,LP) be an epistemic space, S be a set of agents and ∇ an ES basic fusion operator. If ∇ satisfies (ESF-SD), then for any agent i in S, any i-profile Ei, any different triple of interpretations w, w′, w′′ inWP and any epistemic states Ew,w′ , Ew,w′′ , Ew′,w′′ in E, such that [[B(Ew,w′ )]] = {w,w′}, [[B(Ew,w′′ )]] = {w,w′′} and [[B(Ew′,w′′ )]] = {w′,w′′}, the following conditions hold:
6 By abuse we write ϕ for the equivalence class of the propositional formula ϕ.
(i) For all j in S, there exists a j-profile E j such that – B (∇(E j, Ew,w′ )) ≡ B(∇(E j, Ew,w′′ )) ≡ ϕw, and – B
(∇(E j, Ew′,w′′ )) ≡ B(∇(Ei, Ew′,w′′ )). (ii) For all k in S, there exists a k-profile Ek such that
– B (∇(Ek, Ew,w′ )) ≡ ϕw,
– B (∇(Ek, Ew′,w′′ )) ≡ ϕw′′ , and
– B (∇(Ek, Ew,w′′ )) ≡ B(∇(Ei, Ew,w′′ )).
The following two properties try to capture the meaning of the second of the principles above mentioned. The first of them is given in terms of similarity between the epistemic states of the agents. Actually, it is natural to think that if the agents totally coincide i.e. all have the same epistemic state, then the result of the group can be determined for any individual agent. More precisely, we have the following postulate called Unanimity condition:
(ESF-U) For all N in F ∗(S), for each N-profile Φ and for every epistemic state E in E, if Ei = E j, for any pair i, j in N, then, for all i in N, B (∇(Φ, E)) ≡ B(∇(Ei, E)). (ESF-U) establishes that if all the agents in the society have the same epistemic state, then the result of the fusion is exactly the fusion (or the revision7) of the society conformed by any of the agents given the integrity constraints.
The following result is important. It establishes that the ES fusion operators satisfy the Unanimity condition.
Proposition 4. Let ∇ be an ES combination operator. If ∇ satisfies (ESF2), (ESF7) and (ESF8), it satisfies (ESF-U).
The converse of this result does not hold, as can be seen in Section 5. In the following result we give a semantic characterization of the Unanimity condition.
Proposition 5. Let ∇ be an ES basic fusion operator. Then the following statements are equivalent: (i) ∇ satisfies (ESF-U).
(ii) For all N in F ∗(S), for each N-profile Φ, for every epistemic state E in E such that ∣∣∣∣[[B(E)]]∣∣∣∣ ≤ 2, if
Ei = E j for each pair i, j in N, then B (∇(Φ, E)) ≡ B(∇(Ei, E)), for all i in N.
(iii) The assignment Φ 7→ Φ, representing ∇, satisfies the following property: (u) For each society N, for each N-profile Φ, if Ei = E j for each couple i, j in N, then Φ= Ei for all i
in N.
Note that this last condition is closer to the formulations of Unanimity in Social Choice Theory. Now we give a second syntactical formulation of the second principle: if all the agents reject a given alternative, this alternative will be rejected in the result of fusion. This form of excluding unanimity is known in Social Choice Theory like Pareto condition. We keep this name for the following postulate: (ESF-P) For all N inF ∗(S), for each N-profileΦ, for all epistemic states E and E′ in E, if ∧∧i∈N B(∇(Ei, E)) 0 ⊥ and B(∇(Ei, E)) ∧ B(E′) ` ⊥ for all i in N, then B(∇(Φ, E)) ∧ B(E′) ` ⊥.
7 When the society is formed by one unique agent, it is well known that the process of fusion corresponds to a revision of the epistemic state of the agent by the integrity constraints (see for instant [15]).
The following result entails that ES fusion operators satisfy the Pareto condition:
Proposition 6. If ∇ is an ES combination operator that satisfies (ESF7) and (ESF8W), then ∇ also satisfies (ESF-P).
The converse is not true as will be seen in Section 5. The Pareto condition has also a semantical characterization that we give in the next result:
Proposition 7. Let ∇ be an ES basic fusion operator. Then the following statements are equivalent: (i) ∇ satisfies (ESF-P)
(ii) For all N in F ∗(S), for every N-profile Φ, for all epistemic states E and E′ in E such that ∣∣∣∣[[B(E)]]∣∣∣∣ ≤ 2,
if B (∇(Ei, E)) ∧ B(E′) ` ⊥ for all i in N, then B(∇(Φ, E)) ∧ B(E′) ` ⊥
(iii) The assignment Φ 7→ Φ, representing ∇, satisfies the following property: (p) For all N in F ∗(S), for each N-profile Φ, for all interpretations w y w′, if w Ei w′ for all i in N,
then w Φ w′
We continue stating the syntactical postulate aiming to catch the third principle: the fusion process depends only on how the restrictions in the individual epistemic states are related.
In Social Choice Theory this idea is captured by the postulate known as Independence of Irrelevant Alternatives. In our framework this postulate will be stated in the following manner:
(ESF-I) For all N in F ∗(S), for all N-profiles Φ and Φ′, for every epistemic state E we have B(∇(Φ, E)) ≡ B (∇(Φ′, E)), whenever for each epistemic state E′ such that B(E′) ` B(E), we have B(∇(E j, E′)) ≡
B (∇(E′j, E′)) for all j ∈ N.
This property, called Independence condition, essentially says the following: Given an integrity constraint, if each agent in the fusion process has two possible choices of epistemic states, and if revising these epistemic states by integrity constraints having beliefs stronger than the given integrity constraint, the beliefs of the epistemic states resulting coincide, then the result of the fusion of the society of agents under the given integrity constraint is the same, at the level of beliefs, for any choice of the epistemic state made by each agent.
The next result gives a simplification of the Postulate (ESF-I) for the ES basic fusion operators and also a semantic characterization.
Proposition 8. Let ∇ be an ES basic fusion operator. Then the following statements are equivalent: (i) ∇ satisfies (ESF-I)
(ii) For all N in F ∗(S), for all N-profiles Φ and Φ′, for all E in E such that ∣∣∣∣[[B(E)]]∣∣∣∣ ≤ 2, if
B (∇(E j, E)) ≡ B(∇(E′j, E)), for all j in N, then B(∇(Φ, E)) ≡ B(∇(Φ′, E))
(iii) The assignment Φ 7→ Φ, representing ∇, satisfies the following property: (ind) For all N inF ∗(S), for all N-profilesΦ andΦ′, for all interpretations w and w′, if Ei {w,w′}= E′i {w,w′},
for all i in N, then Φ {w,w′}= Φ′ {w,w′} It is worth noting that in presence of (ESF-I) and the basic postulates, Postulate (ESF-U) entails (ESF-
P), i.e. unanimity is stronger than Pareto condition. That is the following proposition:
Proposition 9. If an ES basic fusion operator ∇ satisfies (ESF-U) and (ESF-I), then it also satisfies (ESFP).
The converse is not true, that is, even in presence of (ESF-I), Postulate (ESF-P) does not entail (ESF-U). Actually, it is not difficult to build operators satisfying (ESF-P), (ESF-I) but for which (ESF-U) does not hold (see Section 5). Moreover, as we can see in Section 5, (ESF-P) does not entail (ESF-I), even if (ESF-U) holds.
Next we state the postulate related with the fourth principle: the group belief base, obtained as the result of the fusion process, does not depend on one unique agent. Actually, we establish the negative form which says that there is an agent that imposes his will, a dictatorial agent. This is the postulate that the good operators should avoid.
(ESF-D) For all N in F ∗(S) there exists an agent dN in N such that, for all N-profile Φ and for all epistemic state E in E, B(∇(Φ, E)) ` B(∇(EdN , E)). The operators satisfying the previous postulate are called Dictatorial operators. Suppose that the operator ∇ is dictatorial, then given N in F ∗(S), an agent dN satisfying the property in (ESF-D), is called a dictador in N or simply N-dictator, with respect to ∇.
The following result is a semantic characterization of Dictatorial operators:
Proposition 10. Let ∇ be an ES basic fusion operator. Then the following statements are equivalent: (i) ∇ is dictatorial. (ii) For all N in F ∗(S), there exists dN in N such that, for every N-profile Φ and for all epistemic state E in E, if
∣∣∣∣[[B(E)]]∣∣∣∣ ≤ 2 then B(∇(Φ, E)) ` B(∇(EdN , E)). (iii) The assignment Φ 7→ Φ, representing ∇, satisfies the following property:
(d) For all N in F ∗(S), there exists dN in N such that for all interpretations w and w′, if w EdN w′, then w Φ w′.
The next result establishes that Dictatorial operators satisfy the Pareto Condition.
Proposition 11. Let ∇ be an ES combination operator. If ∇ satisfies (ESF-D), then ∇ satisfies (ESF-P). However, there are dictatorial operators for which (ESF-U) does not hold (see Section 5).
DOI : 10.5121/ijnlc.2014.3404 53
Hybrid approaches for automatic vowelization of Arabic texts are presented in this article. The process is made up of two modules. In the first one, a morphological analysis of the text words is performed using the open source morphological Analyzer AlKhalil Morpho Sys. Outputs for each word analyzed out of context, are its different possible vowelizations. The integration of this Analyzer in our vowelization system required the addition of a lexical database containing the most frequent words in Arabic language. Using a statistical approach based on two hidden Markov models (HMM), the second module aims to eliminate the ambiguities. Indeed, for the first HMM, the unvowelized Arabic words are the observed states and the vowelized words are the hidden states. The observed states of the second HMM are identical to those of the first, but the hidden states are the lists of possible diacritics of the word without its Arabic letters. Our system uses Viterbi algorithm to select the optimal path among the solutions proposed by Al Khalil Morpho Sys. Our approach opens an important way to improve the performance of automatic vowelization of Arabic texts for other uses in automatic natural language processing.
KEYWORDS
Arabic language, Automatic vowelization, morphological analysis, hidden Markov model, corpus
Named Entity Evolution Recognition on the Web is more complicated compared to NEER on traditional newspapers by the amount of noise, mainly caused by the dynamic language that is used on Blogs, for example. Our goal was to adapt the BaseNEER approach to be more robust to this noise. The intention of BlogNEER is to filter out noisy terms and thus, achieve results in recall and precision that are comparable to BaseNEER. Our evaluation shows that the proposed filtering mechanisms achieve encouraging results. The apriori filters reduce noise prior to the co-reference detection while a-posteriori filters, including semantic filtering, increase the accuracy by filtering the results afterwards. Our evaluation of BaseNEER on Web data showed extremely low precision and proved the need for advanced and semantic filtering for this specific domain. With BlogNEER we achieved a significantly higher precision on Blogs and even obtained results which are comparable to BaseNEER applied to a newspaper dataset. The recall of BlogNEER compared to BaseNEER on the New York Times dataset is similar on Technorati, but lower on Blogs08.
In this section we discuss how each extension on BaseNEER affected the results. We review the utilized methods and give ideas for future work on BlogNEER.
6.1 Pre-processing and A-priori Filtering
The low recall achieved on Blogs08 is caused by the frequent noisy terms in the dataset, which we consider distinctive for the Web. The noise does not affect the recall directly, however, it leads to larger contexts. As these contexts contain many frequent terms that may be related to the query term (complementary terms) but are not co-references, they weaken the relative frequency of the actual co-references. By filtering out these frequent noisy terms using frequency filtering techniques, we filter out correct co-references of the query term as well. Thus, the a-priori frequency filter lowers the recall even before the terms reach the coreference detection step. This leads to a lower recall of 64% for BlogNEER, even before the a-posteriori filtering, compared to 90% of BaseNEER. On the more qualitative Technorati dataset, which is not representative for the Web in general as it consists only of top blogs that are typically professional, yet still in “Web language”, BlogNEER achieves a recall of 87% before the a-posteriori filtering. This is close to the result of BaseNEER applied to the high quality New York Times dataset.
The dataset reduction step (s. Sect. 4.3) in the BlogNEER process helps to focus on the documents that are relevant for a query. With this step relevant terms are emphasized and become more frequent in relation to terms that are not related to the query. Consider the query President Obama with the presidential election as its change period and imagine, for some reason, at the same time sport blogs extensively report about the president of some sports club. As the query is performed for the query sub-terms separately (i.e., President and Obama) the articles about the sports president would be considered as well as the articles about the presidential election. Thus, the name of the sport club and its president are most likely among the most frequent terms. By filtering out the documents from the sport blogs in the dataset reduction step, the frequency gap between the intended President Obama and the sports president can be increased.
Our results indicate that pre-processing and a-priori filtering are a crucial parts in the NEER process. To overcome the challenges discussed above, further investigation is required to obtain a higher initial precision and recall. The recall of the context graph limits the recall of the entire NEER process and is therefore an important step. Clustering techniques could support differentiating between e.g., different domains and only retrieving documents from the domain of the query term.
6.2 A-posteriori Filtering
In contrast to the a-priori filtering we could evaluate each a-posteriori filtering step using precision and recall (s. Sect. 5). The evaluation showed that both the a-posteriori frequency as well as the semantic filters are very effective in increasing precision. As is mostly the case, high precision comes at the expense of recall (after frequency filtering we have a decrease in recall by 21% on Blogs08 and 10% on Technorati, after semantic filtering by 28% on Blogs08 and 20% on Technorati). Even though the semantic filter sometimes filters out correct co-references erroneously, it is more effective in filtering false positives. Hence, for queries for which BlogNEER does not detect any real co-references, the semantic filter can still filter out most false positive candidates. This indicates, BlogNEER with semantic filtering can, in addition to finding co-references with a high precision, able to filter out false positive co-references for terms that have not changed.
The semantic filtering proposed with BlogNEER is the first approach to involve external resources in the BaseNEER method. Because of the wide-spread use of the Web and the search mechanism for finding information on the Web, coupled with the increasing time spans of the documents, there is a need for reliable NEER detection in this domain. Therefore, the next logical step for NEER is to make use of Web data. In this paper, we have taken a first step in this direction. As our evaluation results show, with semantic filtering we achieved a precision gain of 19% while losing only 7% recall on Blogs08 compared to the result af-
ter a-posteriori frequency filtering. On Technorati the precision gain with the semantic filtering is 9% with a recall loose of 10%. However, the semantic filter currently has one decisive drawback; it can only be applied to terms that are known by a knowledge base. However, for these terms, the co-references are typically known as well. This issue is discussed in the following subsection.
In several data mining applications, ranging from identifying distinct control regimes in complex plants to characterizing different types of stocks in terms of price and volume movements, one builds an initial classification model that needs to be applied to unlabeled data acquired subsequently. Since the statistics of the underlying phenomena being modeled often changes with time, these classifiers may also need to be occasionally rebuilt if performance degrades beyond an acceptable level. In such sit-
This work has been supported by NSF Grants (IIS-0713142 and IIS-1016614) and by the Brazilian Research Agencies FAPESP and CNPq. Author’s addresses: A. Acharya, Department of Electrical and Computer Engineering, University of Texas at Austin; Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org. c© 2012 ACM 1556-4681/2012/04-ART01 $10.00 DOI 10.1145/0000000.0000000 http://doi.acm.org/10.1145/0000000.0000000
ACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.
uations, it is desirable that the classifier functions well with as little labeling of new data as possible, since labeling can be expensive in terms of time and money, and it is a potentially error-prone process. Moreover, the classifier should be able to adapt to changing statistics to some extent, given the afore-mentioned constraints. This paper addresses the problem of combining multiple classifiers and clusterers in a fairly general setting, that includes the scenario sketched above. An ensemble of classifiers is first learnt on an initial labeled training dataset which can conveniently be denoted by “source” dataset. At this point, the training data can be discarded. Subsequently, when new, unlabeled target data is encountered, a cluster ensemble is applied to it to yield a similarity matrix. In addition, the previously learnt classifier(s) can be used to obtain an estimate of the class probability distributions for this data. The heart of our technique is an optimization framework that combines both sources of information to yield a consensus labeling of the target data. General properties of a large class of loss functions described by Bregman divergences are exploited in this framework in conjunction with Legendre duality and a notion of variable splitting that is also used in alternating direction method of multipliers [Boyd et al. 2011]) to yield a principled and scalable solution. Note that the setting described above is different from transductive learning setups where both labeled and unlabeled data are available at the same time for model building [Silver and Bennett 2008], as well as online methods where decisions are made on one new example at a time, and after each such decision, the true label of the example is obtained and used to update the model parameters [Blum 1998]. Additional differences from existing approaches are described in the section on related works. For the moment we note that the underlying assumption is that similar new instances in the target set are more likely to share the same class label. Thus, the supplementary constraints provided by the cluster ensemble can be useful for improving the generalization capability of the resulting classifier system, specially when labeled data for training the base classifiers is scarce. Also, these supplementary constraints provided by unsupervised models can be useful for designing learning methods that help determine differences between training and target distributions, making the overall system more robust against concept drift. To highlight these additional capabilities that are useful for transfer learning, we provide a separate set of empirical studies where the target data is related to but significantly different from the initial training data. The remainder of this paper is organized as follows. After addressing related work in Section 2, the proposed optimization framework and its associated algorithm — namedOAC3, fromOptimization Algorithm for Combining Classifiers and Clusterers — are described in Section 3. This particular algorithm has been briefly introduced in [Acharya et al. 2011]. A convergence analysis of OAC3 is reported in Section 4, while Section 5 analyses its convergence rate. An experimental study illustrating the potential of the proposed framework for a variety of applications is reported in Section 6. Finally, Section 7 concludes the paper. Notation. Vectors and matrices are denoted by bold faced lowercase and capital letters, respectively. Scalar variables are written in italic font. A set is denoted by a calligraphic uppercase letter. The effective domain of a function f(y), i.e., the set of all y such that f(y) < +∞ is denoted by dom(f), while the interior and the relative interior of a set Y are denoted by int(Y) and ri(Y), respectively. For yi,yj ∈ Rk, 〈yi,yj〉 denotes their inner product. A function f ∈ Ck ′
if all of its first k′ derivatives exist and are continuous.
ACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.
In this Section, the attention is drawn to the possible applications of having a reliable short text language detection algorithm. Even though language detection by itself is an interesting and challenging task, a lot more conclusions and appealing statistics can be drawn after having it already applied on real world data. Therefore, the connections of the predicted languages with the UI language and the location are investigated.
Fig. 5 shows the relationship of the predicted language vs. the UI language. It is not so surprising to notice that users belonging to many different nationalities (assuming that the nationalities usually correspond the users’ UI languages) tweet in English or that many users have English as their UI language, independent of which language they tweet in. On the other hand, it is interesting to see that users which tweet often in Spanish have, apart from English, Spanish and Portuguese set up as their UI languages. This illustrates the geographic and lexical similarity of those languages. It is also interesting to notice that e.g. users that tweet in Dutch have mostly English as their UI language (78.04%), followed by Dutch (21.96%), compared to the users who tweeted in French which mostly have French as their UI language (93.06%), followed by English (5.56%). This might be a good indication of how widespread the usage of English language in a particular country is.
In Fig. 6, the predicted languages are compared with regard to the UTC time when the tweets in those languages are posted. The time is presented as hours in 0-23 range, where e.g. the number 5 represents all tweets posted between 05:00h and 05:59h. It can be seen that some languages (e.g. English, Portuguese, Spanish) have rather even occurrence distribution throughout the whole day, since those languages are very widespread throughout different continents and therefore different timezones as well. On the contrary, there are no Chinese tweets after 4 pm UTC time, since then the local time in China is 12 am. As expected, with most of the languages prevalent mostly in one timezone (e.g. French, Dutch), peaks in the number of tweets can be observed in the late afternoon and evening.
In this paper, we proposed a restart schedule for SA using the modified Lam annealing schedule. Our restart schedule eliminates the need to know the annealing length a priori. Relying on the often demonstrated property of SA that single long runs typically outperform multiple short runs, our restart schedule increases the annealing length at an exponential rate. The shorter runs at the beginning enable quickly finding “good” solutions, while the increasing an-
nealing lengths of the restarts enable approximating the end of run performance of a single long run of SA.
Our restart schedule supports parallel implementation, using parallel independent SA instances that vary in initial annealing length, and with exponentially increasing restart lengths. The initial annealing lengths are staggered to ensure that longer runs are already in progress as shorter runs complete. Our aim is to balance the risk associated with errors in determining the time available for problem solving.
The end-of-run behavior observed in experiments, both sequential as well as in parallel, with a sequence-dependent scheduling problem confirm the commonly found property that a longer SA run outperforms restarts of a shorter run. However, performance during the run is often overlooked. For example, although FAL-1 performs best at the end of run, FAL-1/2 achieved better results at the mid-way point. Our annealing length schedule VAL, and in parallel P-VAL, exhibited stronger anytime behavior throughout the run.
Among the several bloggers that propose their golden rules to identify suspicious Twitter accounts, we consider the “7 signals to look out for recognizing Twitter bots”, according to the founder of the social media website stateofsearch.com [11]. The “7 signals to look out for” to recognize Twitter bots are listed in Table 3.
The rule 3 has been implemented considering the tweet as a single unit. We consider the last 20 tweets of each timeline. For the rule 4, we consider the existence of a duplicate profile picture when at least 3 accounts within the dataset have the same profile picture. For the rule 5, we consider as tweets posted from API all those tweets not being posted from the website twitter.com. For rules 6 and 7, when looking for an account’s friends or followers list, Twitter only gives information about the current list, with no details about past friends or followers. Moreover, Twitter does not disclose any temporal data related to the moment a user stared following, or got followed by, another user. This means that the only way to check a user’s follow/unfollow behavior (rule 7) is to continuously monitor full friends and followers complete lists. The same applies with respect to the measurement of the delay experienced when a user follows (and replies to) other users (rule 6). As further detailed in Section 6, the Twitter rate limits in the use of the APIs makes it practically infeasible to monitor friends and followers lists of even a small group of users. Therefore, we did not apply rules 6 and 7 to our datasets, since that would require to continually monitor those accounts. This also means that those rules cannot be used to support an automatic detection process, since they require an interactive process to be evaluated.
From science fiction films to novels, humans have always fantasized – or needed – to have an emotional relationship with intelligent machines.
Many people in the society seem to think that the objective of creating intelligent machines is to “imitate humans” or create a new species of “humans”. This misunderstanding has led to the irrational fear of “machines taking over humans” by some people. Their reasoning is obvious – if intelligent machines are supposed to imitate humans then as they become more and more human-like they are bound to have humanly desire for power and dominance. It is obvious if one believes in the premises that we are creating machines to “imitate humans”. However, this is far from the reality of artificial intelligence research.
Rather than trying to build some Frankenstein surrogate of the human race, the objective of intelligent machine research and development has always been to help humans. As such, even when we build robot “companions” we are working to create health benefits for the elderly or educational benefits for the young.
In the past couple of decades, interactive dialog systems have been designed as software programs either for the desktop, embedded in an enterprise solution, as cloud services, or as mobile applications. They would have a synthesized voice. Since the 1990s, voice interactive designers have tried to make the dialog prompts more natural, and speech synthesis has made great progress to enable computer voice to sound human like. However, such systems remain invisible and virtual. Even after giving these applications names like Siri or Cortana, users remain emotionally indifferent to such systems as if they are merely using an ATM machine for transactions.
One reason behind this might be something that has been studied by human-robot interaction researchers [39]. It is known that physical embodiment of an intelligent system, whether in virtual simulation or in a robotic form, is important for users to feel related and empathize with the system [34].
More importantly though, physical robots, even extremely humanlike androids, seem cold and distant to humans because while they can sometimes be built to look and even sound emotional, they do not recognize or respond to human emotions and intent. Roboticists make great efforts to build robots in anthropomorphic form so that humans can empathize with them [22], and to have embodied cognition [10]. only to find human users disappointed by the lack of reciprocal empathy from these robots.
It follows that we shall embody interactive dialog systems in simulated or robotic forms. It is also important that we give such systems the ability to both recognize human emotions and intent, as well as expressing its own. Before we share our lives with robots, they need to be able to recognize human emotion and intent, through natural language communications, through facial expression and gesture communications.
In this paper, we describe a proposed framework for building a robotic interactive system with an “empathy module”. In Section 2, we describe the design of a prototype empathetic virtual robot system. In Section 3 we describe the personality analysis of our system and in Section 4 the need for the system to handle user challenges to our empathetic interactive virtual robot. To enable different features described in Sections 2 to 4, we need speech recognition, emotion and sentiment recognition from audio and text. We describe our current approaches in these areas in Sections 5 to 6. In Section 5 we first present a brief over view of different deep learning architectures. Section 6 describes our current approach of hybrid HMM-DNN speech recognition system for interactive systems. Section 7 describes our approach of emotion recognition from audio with and without feature engineering. We then discuss sentiment recognition from speech and text in Section 8 with the special case of humor recognition in conversations in Section 9. We summarize and discuss future work in Section 10.
A word may have several instances in a document, each of which is characterized by its POS. The objective of this step is to identify the POS of each instance in a given material. For this aim, we simply rely on utilizing the Stanford POS Tagger. As the synsets associated with a word’s instance are grouped in WordNet according to their part of speech, this step aims to limit the synsets to be examined in the next disambiguation steps to those having the same POS as the target instance.
An important question in the field of theoretical machine learning is that of learnability and learning rates. We have explored this question for various learning problems in both statistical and online learning frameworks. In the statistical learning framework we provide the first general characterization of learnability in the general setting using the notion of stability of learning algorithms. We also provided a generic algorithm for learning in the statistical learning framework. As for the problem of learnability in the online framework while we don’t yet have a complete picture we introduced various complexity measures analogous to the
188
ones in statistical learning framework. We also provide characterization of online learnability for real valued supervised learning problem.
An integral part of machine learning is optimization. While the question of learnability and learning rates are central to machine learning theory, from a practical point of view one would like to consider problems that are efficiently learnable. To address this issue in a general way, we considered convex learning and optimization problems in both statistical and online learning framework. We used the notion of oracle complexity to address issue of efficiency. For the online learning problems, we showed mirror descent is universal and near optimal. That is whenever a convex problem is online learnable, it is learnable with near optimal rates using mirror descent. Since mirror descent is a first order method (sub-gradient based) we could infer that for online learning scenario mirror descent is near optimal in terms of both rates and oracle complexity. We also explored connections between learning in the various frameworks and oracle based optimization. For the statistical convex learning problem, unlike online setting, in general it is not true that mirror descent is universal. However we saw that for problems we would encounter in practical applications though, this was in fact the case. Mirror descent would indeed be near optimal. We also saw that for certain offline optimization problems in high enough dimensions, mirror descent can again shown to be near optimal.
We expect the work to provide a better understanding of learning algorithms especially from the perspective of optimization. While it is common that for machine learning practitioners optimization is often an after thought and is in a sense mainly a computational issue, through this work we would like to stress that learning can be seen as optimization and should in fact be seen as so. On the other hand, we also show some strong connections between optimization and showed how tools from learning theory can be used to prove results on optimization. Hence we would also like to stress overall the strong and inevitable connections between the two.
In this work we also used several concepts from the theory of Banach space geometry. It would certainly be interesting to see if more connections can be made and techniques from Banach space geometry be used to prove more results about learning and optimization.
189

formulations—BNP-DFS and CG-TSP—tend to scale poorly compared to our newer formulations. Interestingly, BNP-PICEF tends to perform worse than the base PICEF and HPIEF; we hypothesize that this is because branch-and-price-based methods are necessarily more “heavyweight” than standard IP techniques, and the small size of presently-fielded kidney exchange pools may not yet warrant this more advanced technique. Perhaps most critically, both PICEF and HPIEF clear real match runs in both exchanges within seconds.
In the NLDKSS results, the wide fluctuation in mean run time as the chain cap is varied can be explained by the small sample size of available NLDKSS instances, and the fact that the algorithms other than HPIEF and PICEF occasionally timed out at one hour. By contrast, each of the HPIEF and PICEF runs on NLDKSS instances took less than five seconds to complete. We also note that the LP relaxation of PICEF and HPIEF are very tight in practice; the LPR bound equaled the IP optimum for 614 of the 663 runs carried out on NLDKSS data.
We remark that the BNP-DCD model due to Klimentova et al. [2014] was run on all NLDKSS instances where the chain cap L was equal to 0. Larger values of L could not be tested since the current implementation of the model in our possession does not accept NDDs in the input. However for the case that L = 0 the BNP-DCD model was the fastest for all NLDKSS instances.
Finally we note that the solver of Glorie et al. [2014] was executed on the NLDKSS instances with a chain cap of L, for 0 ≤ L ≤ 4. It was found that on average the execution time was 8.9 times slower than the fastest solver from among all the others executed on these instances as detailed at the beginning of Section 5. PICEF was the fastest solver on 40% of occasions.
Different authors have proposed to provide networks with the ability to learn to select the computations that will be applied. This is the case for example for classification in [2, 24] based on Reinforcement learning techniques, in [19] based on gating mechanisms, in [17] based on evolutionary algorithms or even in [4] based on both RL and evolutionary techniques. One strong difference with our approach is that these models are only guided by the final predictive performance of the network. For example in [24], a controller neural net can propose a “child” model architecture, which can then be trained and evaluated before trying a new architecture. The process is repeated iteratively until a good architecture is obtained. Moreover, in this approach each generated architecture is learned to convergence on the training set, resulting in a very low training speed while our model learns the parameters of the modules and the architecture simultaneously.
When the objective is to learn time or memory efficient models, different methods have been proposed. The most common approach is to simplify the learned network typically by removing some connections between neurons. The oldest approach is certainly the Optimal Brain Surgeon [7] which removes weights in a classical neural network architecture. The problem of network compression can also be seen as a way to speedup a particular architecture, for example by using quantization of the weights of the network [22]. Pruning and quantization techniques can also be combined [6] to obtain impressive performances.
Other algorithms include the use of hardware efficient operations that allow a high speedup. For example [3] combines algorithmic optimizations and architecture improvements to speed up the response time of a network while keeping high accuracy. This technique is effective but requires a strong expertise to find the best optimization and is also highly specific to each architecture and/or
hardware. Note that the computation speed can be reduced by using parallelized inference approaches which is not the topic of this paper.
In some other works, authors learn time-efficient models in an end-to-end manner. For example, the model proposed in [1] can be seen as an extension of dropout where the probability of sampling neurons is conditioned on currently processed input (conditional computation). In the same vein, the model proposed in [15] aims at learning routes in Super networks. It is certainly the most similar approach to ours, while keeping several important differences. First, the model is based on an explicit computation cost formulation while our approach can be used for any type of cost. Moreover, while our model converges to a static architecture, allowing to keep only the blocks used by the trained model, their model learns dynamic routes, causing an overhead in the computation cost and forcing to keep the whole network to do further inferences. At last, the number of possible architectures explored is smaller in this approach than in ours.
We report in this section on a tool that we have been developing, called SamIam, for finetuning network parameters (Laskey, 1995; Castillo, Gutiérrez, & Hadi, 1997; Jensen, 1999; Kjærulff & van der Gaag, 2000; Darwiche, 2000). Given a belief network, some evidence e, which is an instantiation of variables E in the belief network, and two events y and z of variables Y and Z respectively, where Y, Z 6∈ E, our tool can efficiently identify parameter changes needed to enforce the following types of constraints:
Difference: Pr(y | e) − Pr(z | e) ≥ ǫ;
Ratio: Pr(y | e)/Pr(z | e) ≥ ǫ.
These two constraints often arise when we debug belief networks. For example, we can make event y more likely than event z, given evidence e, by specifying the constraint, Pr(y | e) − Pr(z | e) ≥ 0, or we can make event y at least twice as likely as event z, given evidence e, by specifying the constraint, Pr(y | e)/Pr(z | e) ≥ 2. We will discuss next how one would enforce the two constraints, but we need to settle some notational conventions and technical preliminaries first.
Variables are denoted by upper-case letters (A) and their values by lower-case letters (a). Sets of variables are denoted by bold-face upper-case letters (A) and their instantiations are denoted by bold-face lower-case letters (a). For a variable A with values true and false, we use a to denote A = true and a to denote A = false. The CPT for variable X with parents U defines a set of conditional probabilities of the form Pr(x | u), where x is a value of variable X, u is an instantiation of parents U, and Pr(x | u) is a probability known as a network parameter and denoted by θx|u. We finally recall a basic fact about belief networks. The probability of some instantiation x of all network variables X equals the product of all network parameters that are consistent with that instantiation. For example, the probability of instantiation fire, tampering , smoke, alarm, leaving , report in Figure 1 equals .01× .98× .9× .99× .12× .01, which is the product of network parameters (from Figure 2) that are consistent with this instantiation.
The first step in an annotation project is, naturally, defining the scheme, but many other tasks must be executed to go from an annotation scheme to an actual set of cleanly annotated files useful for other tasks.
FastBDT (development version 24.04.2016)3 was compared against other SGBDT implementations used usually in high energy physics:
• TMVA (Hoecker et al., 2007) (ROOT version 6.06/00) – The multivariate analysis package of the ROOT (Brun and Rademakers, 1997) data analysis library developed by CERN (GPLv2 license);
• scikit-learn (Pedregosa et al., 2011) (version 0.17.1) – A machine learning library written in python (BSD license);
2. E.g. by running the loop over the data-points in parallel using OpenMPI (OpenMP Architecture Review Board, 2015) #pragma omp parallel for like it is done in XGBoost after calling its predict function. 3. git hash ce39fa9ac8cd0e94a5b7d5cdef34300e5d372a63
• xgboost (Chen and Guestrin, 2016) (development version 22.04.2016)4 – A modern implementation of BDTs which performed well in the Higgs Boson challenge (Chen and He, 2015) (Apache license).
The used dataset contains 1 million data-points, 35 features and a binary target. It was split into equal parts, used during the fitting and application phase, respectively. The dataset was produced using Monte Carlo simulation of the decay of a D0 meson into one charged pion, one neutral pion and one kaon. A common classification problem in high energy physics. However, the nature of the data has no influence on the execution time of the SGBDT algorithm in the considered implementations, since no optimizations which prune features using their estimated separation power, as described in Appel et al. (2013), are employed.
In a first preprocessing step the dataset is converted into the preferred data-format of each implementation:
• ROOT TTree for TMVA, • numpy ndarray for scikit-learn, • DMatrix for XGBoost • and FastBDT::EventSample for FastBDT.
Afterwards, the fitting and application steps are performed for each implementation. Each step is measured individually using std::chrono::high_resolution_clock. The preprocessing time is small5 with respect to the fitting time for most (> 95%) of the investigated hyperparameter configurations. All results stated below are valid as well if one considers the preprocessing as part of the fitting-phase.
The execution time of each implementation is measured five times for each considered set of hyper-parameters. If not stated otherwise the following hyper-parameters are chosen:
• depth of the trees = 3
4. git hash b3c9e6a0db0a7eb755949ac6b26e3ef805738350 5. less than < 20% of the training time
• number of trees = 100 • number of features = 35 • number of training data-points = 500000 • sampling-rate = 0.5 • shrinkage = 0.1
All implementations have additional hyper-parameters which are not shared by the other implementations. The respective default values are used in these cases.
Two versions of XGBoost are considered: The single-core (named just XGBoost in the following) and multi-core version (named XGBoost-i7 in the following). In addition a simple multi-core version of FastBDT is considered in the application-phase measurements (named FastBDT-i7 in the following).
All measurements are performed on an Intel(R) Core(TM) i7-4770 CPU (@ 3.40GHz) with a main memory of 32 GigaByte. The code used to perform the measurements can be found in the FastBDT repository.
Automatic text summarization is the holy grail for people battling information overload, which becomes more and more acute over time. Hence it has attracted many researchers from diverse fields since the 1950s. However, it has remained a serious challenge, especially in the case of single news articles. The single document summarization competition at Document Understanding Conferences (DUC) was abandoned after only two years, 2001-2002, since many automatic summarizers could not outperform a baseline summary consisting of the first 100 words of a news article. Those that did outperform the baseline could not do so in a statistically significant way [27]. Summarization can be extractive or abstractive [21]: in extractive summarization sentences are chosen from the article(s) given as input, whereas in abstractive summarization sentences may be generated or a new representation of the article(s) may be output.
Extractive summarization is popular, so we explore whether there are inherent limits on the performance of such systems.1 We then generalize existing ? Research supported in part by NSF grants DUE 1241772, CNS 1319212 and DGE 1433817 1 Surprisingly, despite all the attention extractive summarization has received, to our knowledge,
no one has explored this question before.
ar X
iv :1
70 4.
05 55
0v 1
[ cs
.C L
] 1
8 A
models for summarization and define compressibility of a document. We explore this concept for documents from three genres and then unify new and existing heuristics for summarization in a single framework. Our contributions:
1. We show the limitations of single and multi-document extractive summarization when the comparison is with respect to gold-standard human-constructed abstractive summaries on DUC data (Section 3). (a) Specifically, we show that when the documents themselves from the
DUC 2001-2002 datasets are compared using ROUGE [19] to abstractive summaries, the average Rouge-1 (unigram) recall is around 90%. On ROUGE evaluations, no extractive summarizer can do better than just returning the document itself (in practice it will do much worse because of the size constraint on summaries). (b) For multi-document summarization, we show limits in two ways: (i) we concatenate the documents in each set and examine how this “superdocument” performs as a summary with respect to the manual abstractive summaries, and (ii) we study how each document measures up against the manual summaries and then average the performance of all the documents in each set. 2. Inspired by this view of documents as summaries, we introduce and explore a generalized model of summarization (Section 4) that unifies the three different dimensions: abstractive versus extractive, single versus multidocument and syntactic versus semantic. (a) We prove (in Appendix) that constructing certain extractive summaries
is isomorphic to the min cover problem for sets, which shows that not only is the optimal summary problem NP-complete but it has a greedy heuristic that gives a multiplicative logarithmic approximation. (b) Based on our model, we can define the compressibility of a document. We study this notion for different genres of articles including: news articles, scientific articles and short stories. (c) We present new and existing heuristics for single-document summarization, which represent different time and compressibility trade-offs. We compare them against existing summarizers proven on DUC datasets.
Although many metrics have been proposed (more in Section 2), we use ROUGE because of its popularity, ease of use and correlation with human evaluations.
The Actias system information modelling methodology has the goal of defining a business sketch model based of the small part of the world associated with the business Information System. This small part of the world corresponds to the business universe of discourse (UoD). The models of the data specification must be seen as possible states of the UoD, and are the information storable on its structures or equivalently stored in the business IS. This process is incremental following the Business Knowledge needs and evolution. Therefore the problem of knowing whether the specification and its changes are compatible with the existent IS information structure become visible. This requires the development of a strategy to validate the model against the IS structure. For this we were inspirited by the G. Karcs and P. van Bammel approach, where the IS structure is first mapped to an intermediate specification. Based on the CWM (Common Warehouse Meta-data) description a business IS warehouse, we produce the IS representation using a relational schema, see Figure 2, which can be seen as a sketch [8], a first approximation to the business conceptual model, helping the user on the specification work. This relational schema will be used to verify if the update of business model is able to store all the information present on the IS. After performing the information compatibility test, some
times is convenient, for operational reasons, to generate or change the internal Actias system Data Warehouse, improving its structure based on the new knowledge. Helping on the definition of sketch views and data streams for the extraction process specification methodology.
The sketch structure definition or enrichment is done using a CASE tool, implementing a refinement hierarchic methodology allowing the gradual introduction of grater levels of detail through the sketch diagram structure. Having as main goal the minimization of potential specification complexity based on a Modelling Knowledge library, containing helpful structures. Producing as result a process to generate or change the warehouse, reflecting directly the structures specified by the enriched sketch.
The information sketch modelling methodology can be decomposed, see Figure 3, into five generic phases: Base relational model acquisition, sketch model enrichment (using the CASE tool), implementation sketch generation, sketch model validation and warehouse structural revision using the implementation sketch. The produced warehouse adaptation tries to make easier its use on the extraction processes. Since, it requires the definition of a data stream, which will be seen as a query result, defined on the user sketch model view selecting in it a part of the general sketch having associated a data dictionary [11].
Consider a Simple Recurrent Neural Network (Fig. 1) that at each time step k receives an external input u(k), previous internal state z(k − 1) and produces output y(k + 1):
a(k) = u(k)win + z(k − 1)wrec + b, z(k) = f(a(k)), y(k + 1) = g(z(k)wout),
(1)
here win is a matrix of input weights, wrec is matrix of recurrent weights, wout is matrix of output weights, a(k) is known as “presynaptic activations”, z(k) is a network’s state, f(·) and g(·) are nonlinear activation functions for hidden and output layer respectively. In this work we always use tanh function for hidden layer and optionally softmax or linear function depending on the target problem (classification or regression). Also in our equations data vectors u(k) are vector rows (not vector columns) because it is often implemented in many software tools like MATLAB and therefore is more useful for practice.
After processing an input sequence u(1),u(1) . . . ,u(k) and producing the output y(k + 1), target error E(k + 1) is calculated. There are different options for target error function, the most popular choices are mean squared error for regression
and cross-entropy error for classification. To train neural network using gradient-based optimization algorithm we have to calculate derivatives of error function subs to network’s weights ∂E∂w . To perform this, a well-known Backpropagation Through Time (BPTT) method for calculation of dynamic derivatives is used. The idea of BPTT is unrolling the recurrent neural network back in time. In this way, RNN is presented as a deep feedforward neural network with shared weights where each layer is referring to one of retrospective time steps.
Then a standard backpropagation is applied to this deep network, immediate derivatives ∂E∂w(n) are calculated. In derivations below we use a framework very similar to [20] but based on studying evolution of local gradients δ(k) for the backpropagation procedure.
The dynamic derivative is a sum of immediate derivatives: ∂E ∂w = ∑h n=1 ∂E ∂w(k−n) , where n = 1, ..., h, where h is BPTT’s truncation depth. An intermediate variable δ ≡ ∂E∂a called a “local gradients” or simply “deltas” is usually introduced for convenience. If deltas are known for specific layer n then corresponding immediate derivatives can be obtained easily:
∂E ∂win(n) = u(n)T δ(n) and ∂E∂wrec(n) = z(n− 1) T δ(n).
For the last layer δ is an error residual, for the intermediate layers deltas are incrementally calculated according to very famous backpropagation formula:
δj(n− 1) = f ′(aj(n− 1)) ∑ i wijrecδi(n). (2)
Let’s write this equation in a matrix form:
δ(n− 1) = δ(n)wTrecdiag(f ′(a(n− 1))). (3)
where diag converts a vector into diagonal matrix. Equation (3) may be rewritten using is Jacobian matrix J(n) = ∂z(n)∂z(n−1) :
δ(n− 1) = δ(n)J(n), (4)
where
J(n) = wTrecdiag(f ′(a(n− 1))). (5)
Now we can use an intuitive understanding of exploding/vanishing gradients problem that was deeply investigated in classic [8], [23] and modern papers [18], [20]. As it can be seen from (5), norm of the backpropagated deltas is strongly dependent on norm of the Jacobians. Moreover, they actually are product of Jacobians: δ(n−h) = δ(n)J(n)J(n−1)...J(n− h+1). The “older” deltas are, the more Jacobian matrices were multiplied. If norm of Jacobians are more than 1, the gradients will grow exponentially in most cases. It refers to the RNN’s behavior where long-term components are more important than short-term ones. Vice versa, if norm of Jacobians are less than 1, this leads to vanishing gradients and “forgetting” the longterm events.
At the same time, even if both factors in (5) have norm 1, it still not guarantee the same norm of J(n). In [20] the power iteration method was used to formally analyze product of Jacobian matrices and obtain tight conditions for when the
IEEE 2 | P a g e
gradients explode or vanish. It was shown that it is sufficient for the largest eigenvalue of the recurrent weight matrix wTrec to be smaller than 1 for long term components to vanish and necessary for it to be larger than 1 for gradients to explode.
The first step in the design of a Genetic Programming is the design of the problem representation. In this section we will present the representation design of the system.
The evaluation suite here includes standard baselines for comparison, automated metrics for learning, human judgement for evaluation and detailed analysis for diagnostics. While each are individually useful, their combination gives a comprehensive analysis of a complex problem space.
Using R, we train two models using random forests. Random forests were chosen for several reasons. They are well-suited for large numbers of categorical variables. Additionally, they allow non-linearity in combining features, without an
6 Brown corpus: http://www.hit.uib.no/icame/brown/bcm.html 7 CMUDict v0.07, Carnegie Mellon University.
explosion in the required size of the training set, in order to avoid overfitting. Non-linearity makes random forests more powerful than linear models such as logistic regression. We randomly split the data using stratified sampling into a training set (75% of the data) with the rest for testing.
The accuracy of the rhythm model on test data was 86.79%. Figure 2 shows the confusion matrix. The row labels correspond to the duration found in the test sets, and column labels are the predicted labels. The number after the underscore corresponds to the number of dots in the durations. So, for example, a dotted sixteenth note is 16th 1.
As you can see in the confusion matrix, the model tends to perform better on rhythms which occur more frequently in the data set. A larger data set will enable us to test whether the trend will continue.
The accuracy of the melody model was 72.28%. Figure 2 shows the confusion matrix. The numbers correspond to the scale degrees, and the label after the underscore represents the accidentals. So, for example, 1 none represents the first scale degree without accidentals. As with the rhythm model, we see that accuracy is higher when there are more examples.
Lastly, our machine learning model allows us to evaluate the importance of each feature. Figure 3 depicts the mean decrease Gini (which measures node impurity in tree-based classification) of each feature in the rhythm model (left) and the melody model (right). In both models, all 5 previous notes’ scale degrees are some of the most important features. This suggests that including features for additional previous notes may be helpful. This also explains why Markov chains seem to do well for this task. However, we see that other features are also prominent.
For example, the rhythm model is most heavily influenced by beat strength, revealing the potential of using a data-driven approach over Markov chains,
which do not give access to such features. Finally, it is important to note that lyrical features such as word rarity and frequency also play an important role.
The above theorems imply small discrepancy for a single smoothed halfspace h ∈ Hw, but this does not yet imply small discrepancy discχ(P,Hw), for all choices of smoothed halfspaces simultaneously. And in a smoothed range space, the family Hw is not finite, since even if the same set of points have vh(p) = 1, vh(p) = 0, or are in the slab Sw, infinitesimal changes of h will change SDEP (h). So in order to bound discχ(P,Hw), we will show that there are polynomial in n number of smoothed halfspaces that need to be considered, and then apply a union bound across this set.
Theorem 6.1. For P ⊂ R2 of size n, for Hw, and value Ψ(n, δ) = O (√ ` w log n δ ) for δ > 0, we can
choose a coloring χ such that Pr[discχ(P,Hw) > Ψ(n, δ)] ≤ δ.
Proof. We define a net of smoothed halfspaces Sα ⊂ Hw where any smoothed halfspace h ∈ Hw assigns a value vh(p) to a point p ∈ P , then there always exists a smoothed halfspace s ∈ Sα such that ∀p∈P |vh(p)− vs(p)| ≤ ας . Since there are only |P | = n points, the difference ∑ p∈P |vh(p) − vs(p)| is no more than nας . By setting α = 1/nς we can ensure that discχ(P, h) < discχ(P, s) + 1. Thus if all s ∈ Sα have small discrepancy, then all smoothed halfspaces in Hw have small discrepancy.
We now describe a construction of Sα (illustrated in Figure 3) of size at most O(n4) and then apply the union bound in Lemma 6.1 to only increase the discrepancy in that bound by a √ log n factor. First consider the halfspace with boundary passing through each pair of points p, p′ ∈ P . For each such halfspace, and for each point (p or p′) it passes through, consider 4w/α rotations around that point (wlog p). Make the increment of the rotation such that the closest point p′F on the rotated boundary F increases a distance ‖p′ − p′F ‖ of α/2 in each next rotation. That is, the projection distance ‖p′ − p′F ‖ on each rotation around p is a distance of α/2, α, 3α/2, ..., 2w; this is repeated in each direction. Now, for each rotated halfspace, consider 4w/α translations in the direction normal to the halfspace. There are 2w/α translations in the normal direction, and its opposite, at increments of α/2 (e.g., α/2, α, 3α/2, ... 2w).
Since α = 1/nς and w = O(1/ς), then 4w/α = O(n). Thus the size of Sα is O(n4): for each of O(n2) pairs, there are O(n) rotations and for each rotations there are O(n) translations.
We now show for any h ∈ Hw how to map to the smoothed halfspace in s ∈ Sα such that for all p ∈ P that |vh(p)− vs(p)| ≤ ας . First consider all points P ∩ Sw, where Sw is the slab defined by h. If the slab is empty then the closest two points p, p′ ∈ P would generate one translation and rotation s ∈ Sα that moved both of them out of the slab, causing all of the same values vh(pi) = vs(pi) ∈ {0, 1}. Otherwise, for any point p in the slab, there exists some rotation moving pF by at most α/2 and another rotation moving pF by at most α/2 resulting in |vh(p)−vs(p)| ≤ ‖p−pF ‖·ς ≤ (α/2+α/2) ·ς = ας . However, we need to ensure this holds for all points simultaneously. The translations affect ‖p − pF ‖ for all points the same (at most α/2), but the rotations can affect further away points by more. Thus, we choose the two points p, p′ ∈ Sα that maximize ‖pF − p′F ‖, and consider the closest rotation of h to one of the smoothed halfspaces s ∈ Sα that they generate. The rotation will affect all other points less than it will those two, and thus at most α/2, as desired.
Finally we set the probability of failure in Lemma 6.1 as δ′ = Ω(δ/|Sα|) for each smoothed halfspace. This implies that for Ψ(n, δ) = C √
` w log(2/δ ′) = O (√ ` w log n δ ) , the Pr[discχ(P,Hw) > Ψ(n, δ)] ≤
δ.
We experiment with the proposed model on public LCSTS corpus. The baseline is the best result reported in [Hu et al., 2015]4. Our modified uni-GRU achieves a slight improvement over the reported results. The Bi-GRU attention-based model achieves a better performance, confirming the usefulness of bi-directional models for summarization as well as that our implementation is the state-of-the-art and serves as a very strong baseline in the CNN dataset discussed above. Note that since the input text length of LCSTS is far shorter than the CNN documents, each containing about 100 words and roughly 6-8 sentences, we show that distraction does not improve the performance, but in contrast, when documents are longer, its benefits are significant, achieving the biggest improvement as discussed earlier. This suggests the effectiveness of distraction modeling in helping summarize the more
4We thank the authors of [Hu et al., 2015] for generously sharing us the latest output of their models, which achieves a better performance than the results reported in [Hu et al., 2015]. We reported here the updated scores higher performance as our baseline.
challenging longer documents, where summarization is often more necessary than for short texts.
We also compare our models with the simple baseline that selects the first N numbers of word tokens from the input documents, which reaches its maximal Rouge scores when the first 30 tokens were taken, and achieves Rouge-1, Rouge2, and Rouge-L at 25.5, 14.1 and 21.4. And our models are significantly better than that. For the CNN data set, choosing the first three sentences achieves the best results, which reach Rouge-1, Rouge-2, and Rouge-L at 26.1, 9.6 and 17.8, respectively. Since the CNN data is news data, the baseline of selecting first several sentences has known to be a very strong baseline. Again, the models we explore here are towards performing abstractive summarization.
Generalized imaging can be simulated as an EDI operator, and where LI is always retentive, GI is only retentive under a reasonable condition. Let δGI be defined as follows.
δGI (α, w, w ′) :=  1 if w = w ′ and w 6 α 1/|Min(α, w ′,d)| if w ∈ Min(α, w ′,d) 0 otherwise
Proposition 4.9. bGIα= bEDIGI α. That is, GI is EDI-compatible. Proof. Let w 6 α. Then, independent of the definition of δGI , bGIα(w) = bEDIGI α(w) = 0.
Now let w α. Then,
bGIα(w) = ∑ w ′∈W
w∈Min(α,w ′,d)
b(w ′)/|Min(α, w ′,d)|
= ∑ w ′∈W b(w ′)δGI−(w, w ′),
where
δGI−(w, w ′) := {
1/|Min(α, w ′,d)| if w ∈ Min(α, w ′,d) 0 otherwise
Noting that the first line of the definition of δGI is applicable only if w 6 α, then the result follows when combining the cases when w 6 α and w α.
For example, the reader can confirm that b0.3/0.7GI¬q = b0.3/0.7EDIGI ¬q and b1.0GI¬q = b1.0EDIGI ¬q . Whereas Proposition 4.9 says that there exists a weight function for simulating generalized imaging with EDI,
Proposition 4.10 says that the function cannot be inverse-distance.
Proposition 4.10. There exists no inverse-distance weight function δ∗ for which bGIα= bEDI∗α. Proof. Lewis imaging is a specialization of generalized imaging. The proposition is thus entailed by Proposition 4.7.
Lemma 4.1. Let w α. Then, ∀w ∈W,Min(w,α,d) = {w} iff d is faithful.
Proof. Recall that Min(α, w,d) = {w ′ α | ∀w ′′ α,d(w ′, w) ≤ d(w ′′, w)}. Assume d is faithful. Then for all w, w ′ ∈W , if w 6= w ′, then d(w, w ′) > 0. By identity of d , d(w, w) = 0. Hence, Min(w,α,d) = {w}. Assume d is not faithful. Then there exists a pair of worlds w, w ′ ∈ W , such that if w 6= w ′, then d(w, w ′) = 0. Hence, {w, w ′} ⊆ Min(w,α,d). Therefore, it is not the case that Min(w,α,d) = {w}. Proposition 4.11. δGId is retentive iff d is faithful.
Proof. Assume d is faithful. Assume w, w ′ α and w 6= w ′. Only the second and third lines of the definition of δGI are potentially applicable. By Lemma 4.1, we know that whenever w α, then Min(w ′,α,d) = {w} such that w = w ′. Hence, due to our assumption that w 6= w ′, the second line cannot be applicable. Therefore, δGI (α, w, w ′) = 0.
Assume d is not faithful. Assume w, w ′ α and w 6= w ′. Only the second and third lines of the definition of δGI are potentially applicable. By Lemma 4.1, we know that whenever w α, then {w, w ′} ⊆ Min(w ′,α,d) such that w 6= w ′. Hence, due to our assumption that w 6= w ′, the second line might be applicable. Therefore, it could be that δGI (α, w, w ′) = 1/|Min(α, w ′,d) > 0.
Faithfulness is a reasonable property to expect of a distance function. δGId is thus retentive under reasonable conditions.
Result 10 Band power topographies show event-related “desynchronization/synchronization” typical for motor tasks
Before moving to ConvNet visualization, we examined the spectral amplitude changes associated with the different movement classes in the alpha, beta and gamma frequency bands, finding the expected overall scalp topographies (see Figure 11). For example, for the alpha (7–13 Hz) frequency band, there was a class-related power decrease (anti-correlation in the class-envelope correlations) in the left and right pericentral regions with respect to the hand classes, stronger contralaterally to the side of the hand movement , i.e., the regions with pronounced power decreases lie around
25
Tonio Ball 3 RESULTS
Envelope class correlation maps
the primary sensorimotor hand representation areas. For the feet class, there was a power decrease located around the vertex, i.e., approx. above the primary motor foot area. As expected, opposite changes (power increases) with a similar topography were visible for the gamma band (71–91 Hz).
Result 11 Input-feature unit-output correlation maps show learning progression through the ConvNet layers
We used our input-feature unit-output correlation mapping technique to examine the question how correlations between EEG power and the behavioral classes are learnt by the network. Figure 12 shows the input-feature unit-output correlation maps for all four conv-pooling-blocks of the deep ConvNet, for the group of subjects of the High-Gamma Dataset. As a comparison, the Figure also contains the correlation between the power and the classes themselves as described in Section A.5.1. The differences of the absolute correlations show which regions were more correlated with the unit outputs of the trained ConvNet than with the unit outputs of the untrained ConvNet; these correlations are naturally undirected. Overall, the input-feature unit-output correlation maps became more similar to the power-class correlation maps with increasing layer depth. This gradual progression was also reflected in an increasing correlation of the unit outputs with the class labels
26
Tonio Ball 3 RESULTS
with increasing depth of the layer (see Figure 13).
Result 12 Input-perturbation network-prediction correlation maps show causal effect of spatially localized band power features on ConvNet predictions
We show three visualizations extracted from input-perturbation network-prediction correlations, the first two to show the frequency profile of the causal effects, the third to show their topography.
Thus, first, we computed the mean across electrodes for each class separately to show correlations between classes and frequency bands. We see plausible results, for example, for the rest class, positive correlations in the alpha and beta bands and negative correlations in the gamma band (see Figure 14).
Then, second, by taking the mean of the absolute values both over all classes and electrodes, we computed a general frequency profile. This showed clear peaks in the alpha, beta and gamma bands (see Figure 15). Similar peaks were seen in the means of the CSP binary decoding accuracies for the same frequency range.
Thirdly, scalp maps of the input-perturbation effects on network predictions for the different frequency bands, as shown in Figure 16, show spatial distributions expected for motor tasks in the alpha, beta and — for the first time for such a non-invasive EEG decoding visualization — for the high gamma band. These scalp maps directly reflect the behavior of the ConvNets and one needs to
27
Tonio Ball 3 RESULTS
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120
Frequency [Hz]
Hand (L) Hand (R)
Feet Rest 0.0004
0.0000 0.0004
C or
re la
tio n
Figure 14: Input-perturbation network-prediction correlations for all frequencies for the deep ConvNet, per class. Plausible correlations, e.g., rest positively, other classes negatively correlated with the amplitude changes in frequency range from 20 Hz to 30 Hz.
be careful when making inferences about the data from them. For example, the positive correlation on the right side of the scalp for the Hand (R) class in the alpha band only means the ConvNet increased its prediction when the amplitude at these electrodes was increased independently of other frequency bands and electrodes. It does not imply that there was an increase of amplitude for the right hand class in the data. Rather, this correlation could be explained by the ConvNet reducing common noise between both locations, for more explanations of these effects in case of linear models see Haufe et al. (2014). Nevertheless, for the first time in non-invasive EEG, these maps clearly revealed the global somatotopic organization of causal contributions of motor cortical gamma band activity to decoding right and left hand as well as foot movements.
In summary, our visualization methods proved useful to map the spatial distribution of the features learned by the ConvNets to perform single-trial decoding of the different movement classes and in different physiologically important frequency bands.
28
Tonio Ball 3 RESULTS
29
Tonio Ball 4 DISCUSSION
There are two important directives in the C/C++ programming language, which can be used for modeling and controlling a verification process: ASSUME and ASSERT. The ASSUME directive can define constraints over (non-deterministic) variables, and the ASSERT directive is used to check system’s correctness w.r.t. a given property. Using these two statements, any off-the-shelf C/C++ model checker (e.g., CBMC [52], CPAChecker [57], and ESBMC [53]) can be applied to check specific constraints in optimization problems, as described by Eq. (1).
Here, the verification process is iteratively repeated to solve an optimization problem using intrinsic functions available in ESBMC (e.g., __ESBMC_assume and __ESBMC_assert). We apply incremental BMC to efficiently prune the state-space search based on counterexamples produced by an SMT solver. Note that completeness is not an issue here (cf. Definitions 1 and 2) since our optimization problems are represented by loop-free programs [58].
I. INTRODUCTION
Deep neural networks have been extremely successful over the past few years, achieving state of the art performance on a large number of tasks such as image classification [2], face recognition [3], sentiment analysis [4], speech recognition [5], etc. One can spot a general trend in these papers: results tend to get better as the amount of training data increases, along with an increase in the complexity of the deep network architecture. However, increasingly complex deep networks can take weeks or months to train, even with high-performance hardware. Thus, there is a need for more efficient methods for training deep networks.
Deep neural networks learn high-level features by performing a sequence of non-linear transformations. Let our training data set A be composed of n data points a1, a2, . . . , an ∈ Rm and corresponding labels B = {bi}ni=1. Let us consider a 3-layer network with activation function f . Let X1 and X2 denote the weights on each layer that we are trying to learn, i.e., X1 denotes the weights between nodes of the first layer and the second layer, and X2 denotes the weights between nodes of the second layer and the third layer. The learning problem for this specific example can be formulated as the following optimization problem:
minimize X1,X2
∥∥f(f(A ·X1) ·X2)−B∥∥22 (1)
The activation function f can be any non-linear mapping, and is traditionally a sigmoid or tanh function. Recently, rectified linear (ReLu) units (f(z) = max{0, z}) have become popular because they tend to be easy to train and yield superior results for some problems [6].
The non-convex objective (1) is usually minimized using iterative methods (such as back-propagation) with the hope of converging to a good local minima. Most iterative schemes generate additive updates to a set of parameters x (in our case, the weight matrices) of the form
x(k+1) = x(k) + ∆x(k) (2)
where ∆x(k) is some appropriately chosen update. Notice we use slightly different notation here from standard optimization literature in that we incorporate the step size or learning rate t(k) within ∆x(k). This is done to help us describe other optimization algorithms easily in the following sections. Thus, ∆x(k) denotes the update in the parameters, and comprises of a search direction and a step size or learning rate t(k), which controls how large of a step to take in that direction.
Most common update rules are variants of gradient descent, where the search direction is given by the negative gradient g(k):
∆x(k) = −t(k)g(k) = −t(k)∇f(x(k)) (3)
Since the size of the training data for these deep networks is usually of the order of millions or billions of data points, exact computation of the gradient is not feasible. Rather, the gradient is often estimated using a single data point or a small batch of data points. This is the basis for stochastic gradient descent (SGD) [7], which is the most widely used method for training deep nets. SGD requires manually selecting an initial learning rate, and then designing an update rule for the learning rate which decreases it over time (for example, exponential decay with time). The performance of SGD, however, is very sensitive to this choice of update, leading to adaptive methods that automatically adjust the learning rate as the system learns [8], [9].
When these descent methods are used to train deep networks, additional problems are introduced. As the number of layers in a network increases, the gradients that are propagated back to the initial layers get very small. This dramatically slows down the rate of learning in the initial layers, and slows down convergence of the whole network [10].
ar X
iv :1
51 0.
04 60
9v 1
[ cs
.C V
] 1
5 O
ct 2
01 5
Recently, it has also been shown that for high-dimensional non-convex problems, such as deep networks, the existence of local minima which have high error relative to the global minima is exponentially small in the number of dimensions. Instead, in these problems, there is an exponentially large number of high error saddle points with low curvature [1], [11], [12]. Gradient descent methods, in general, move away from saddle points by following the directions of negative curvature. However, due to the low curvature of small negative eigenvalues, the steps taken become very small, thus slowing down learning considerably.
In this paper, we propose a method that alleviates the problems mentioned above. The main contribution of our method is summarized below:
• The learning rates are specific to each layer in the network. This allows larger learning rates to compensate for the small size of gradients in shallow layers.
• The learning rates for each layer tend to increase at low curvature points. This enables the method to quickly escape from high-error, low-curvature saddle points, which occur in abundance in deep network.
• It is applicable to most existing stochastic gradient optimization methods which use a global learning rate.
• It requires very little extra computation over standard stochastic gradient methods, and requires no extra storage of previous gradients required as in AdaGrad [9].
In Section II, we review some popular gradient methods that have been successful for deep networks. In Section III, we describe our optimization algorithm. Finally, in Section IV we compare our method to standard optimization algorithms on datasets like MNIST, CIFAR10 and ImageNet.

The language model is a key component in a translation system which is responsible for the fluency of the output translations. It drives a good part of the end to end translation quality and considerable improvements can often be achieved by using more monolingual data to
15
train the language model. In this section, we explain how language models work, while Section 2.5 shows how translation models and language models are combined together in a unified scoring model.
Language models are statistical models used to score how likely a sequence of words is to occur in a certain language by means of a probability distribution. Let w = wM1 be a sentence in the target language and P (w) the probability distribution defined by the model. According to the chain rule of probability, P (w) can be decomposed as the product of the probabilities of each target word given its preceding context:
P (w) = M∏ i=1 P (wi|wi−11 ). (2.2)
To prevent the model from relying on distributions computed from very sparse statistics, a n-1th order Markov assumption is typically incorporated in the model:
P (w) = M∏ i=1 P (wi|wi−1i−n+1). (2.3)
Back-off n-gram models are the default language modeling implementation used in
machine translation. These models estimate the conditional probability as:
P (wi|wi−1i−n+1) = c(wii−n+1)
c(wi−1i−n+1) , (2.4)
where c(wii−n+1) and c(w i−1 i−n+1) represent the number of times w i i−n+1 and w i−1 i−n+1 are observed in the monolingual corpus. In their raw form, n-gram language models do not accurately estimate rare n-grams. Over the years, a number of smoothing techniques have been proposed in order to address this problem (Jelinek and Mercer, 1980; Katz, 1987; Kneser and Ney, 1995; Chen and Goodman, 1999).
Machine translation toolkits like Moses (Koehn et al., 2007) and cdec (Dyer et al., 2010) have plugins for several open source implementations of back-off n-gram models: SRILM (Stolcke, 2002), IRSTLM (Federico et al., 2008) and KenLM (Heafield, 2011). Heafield (2011) shows his implementation is superior to SRILM and IRSTLM both in terms of speed and memory usage. In fact, two separate implementations are provided as part of KenLM. One is optimized for speed and uses a hash table with linear probing to look up n-gram weights. The other, optimized for memory, but still faster than the alternatives, relies on a trie and makes use of floating point quantization.
16
Back-off n-gram models are used as the default language modeling choice in machine translation because they produce very good results if enough monolingual data is available. They are also fast to train and query and their definition is very intuitive. On the other hand, a n-gram language model stores a numerical value for every n-gram in the training corpus. As a result, even the most compact implementations (e.g. the KenLM trie implementation) require tens of gigabytes of memory for a decently sized monolingual corpus (see Section 4.7). In conclusion, back-off n-gram models are not suitable for memory constrained environments. Chapter 4 investigates neural language models as a space-efficient alternative to n-gram language models.
We consider the number of messages that one node needs to send when the algorithm is running. Again, let the maximum degree of each node be ∆. At the very step, the node needs to send its observation xn to its neighbors, leading to communication cost of O(M∆). After that, this node only needs to update its neighbors about its estimate v(k)n . The number of messages sent for each power iteration is O(∆S). Overall, the communication complexity of the distributed power iteration method running for p steps is O(M∆ + p∆S). The overall communication complexity of GETESD-D using O(k) power iterations is O(kM∆ + kp∆S).
As regards the context, two main parameters are involved: (1) The context window size to consider, i.e.
the number of context words c to count for a given word w. We can either count only context words that occurs after w (asymmetric context window), or we can count words surrounding w (symmetric context window). (2) The type of context to use, i.e. which words are to be chosen for defining the context dictionary D. Do we need all the words, the most frequent ones or, on the contrary, the rare ones? Figure 1 presents the performance obtained on the benchmark datasets for all the five scenarios described in Section 3.1 with different sizes of context. No dimensionality reduction has been applied in this analysis. Similarities between words are calculated with the Hellinger distance between the word probability distributions. For the word analogy task, we used the objective function 3CosMul defined by Levy and Goldberg (2014), as we are dealing with explicit word representations in this case.
Rémi Lebret1,2, Ronan Collobert1
WINDOW SIZE
1 10
baikal (no37415)
mälaren lake titicaca siberia balaton amur
ladoga basin
ilmen volga
special-need (no165996)
at-risk preschool
school-age kindergarten low-income teachers
hearing-impaired schools
grade-school vocational
Being a recent field, the ETS domain is providing room for new ICT applications. In [Chappin, Dijkema, 2009], an agent-based simulation model is presented to analyze the impact of ETS on the CO2 emissions by power generation companies. The model is implemented through a layered software system where agents specifications are ontology-based, coded in Java and with Repast [Collier, 2001] as run-time platform. However, in this work, formal knowledge representation of the ETS domain is not addressed. Other works dealing with some of our issues as must be searched within the wider domain of sustainability science. This discipline, introduced at the beginning of this century, requires contributions of scientists from fields of the natural and socio-economic sciences. In this setting, a structured knowledge base encompassing the relevant concepts, problems and findings may represent a common grounding for the collaboration work of the scientists in this multi-disciplinary setting and a valuable source of information for decision makers. Currently, there are only few efforts devoted to providing a formal specification of knowledge in the scope of sustainability science. In this line, the work by [Kumazawa et al., 2009], who has pioneered this discipline, proposes a reference model for a knowledge-structuring tool, using ontology engineering methods and technology. Guidelines for developing a sustainability science ontology are given, starting from a basic structure of 5 general concepts such as goal, problem, countermeasure, evaluation, and domain concept, and about 562 specific concepts concerning the sustainability science. The work [Kraines, Guo, 2011] proposes a knowledge sharing system for sustainability science. These authors have developed a web-based platform, called EKOSS, enabling semantic annotation of scientific documents and data by the domain experts and incorporating a reasoning engine to automatically identify semantic links among the knowledge resources, in order to create a knowledge network. This system is based on an ontology called SCINTENG starting from a set of 4 core concepts, such as activity, event, physical object, and substance or material. From these concepts, they have detailed the concepts belonging to the sustainability science domain and the analysis of low carbon society scenarios (corresponding to the contextual and scenario views of our reference model). With these works we share the motivation for the knowledge base as a means for collaborative work of the various domain experts and the idea of incremental development of the ontology, starting from a small set of core concepts. In particular, we have realized a domain-specific knowledge base and developed semantics-based services to manage and analyse LCS data. We deem that semantic techniques may add value as the increase of available open governmental data ([Reichman et al., 2011], [Gang-Hoon et al., 2014]) requires coping with the variety and heterogeneity characterizing different sectors by means of integrated approaches [Kelly et al., 2013]. Here, such techniques are applied to the EU ETSrelated knowledge, a specific subfield of the LCS. Differently from the works cited above, our solution aims at enriching existing relational databases with semantics, by linking the database schemas to an ontology and using semantic rules for data validation and query. Indeed, this solution increases systems flexibility when varying the rules to be applied and, by working at logical level, eases migration from a technical solution to another one. Our aim is to demonstrate the efficacy of our solution by exploring the Italian context. A step forward will be integration with scenarios data to set up the basis for a decision support system for the Italian Ministry of Economic Development.
Performance of applications relying on natural language processing may suffer from the fact that the processed documents might contain lexically different, yet semantically related, text segments. The task of recognizing synonym text segments, which is better known as paraphrase recognition, or detection, is challenging and difficult to solve, as shown in the work of Pasca (2005). The task itself is important for many text related applications, like summarization (Hirao, Fukusima, Oku-
mura, Nobata, & Nanba, 2005), information extraction (Shinyama & Sekine, 2003) and question answering (Pasca, 2003). We experimentally evaluate the application of Omiotis in the paraphrasing detection task (Section 4.2), using the Microsoft Research Paraphrase Corpus (Dolan et al., 2004). The application of Omiotis in paraphrase detection is straightforward: given a pair of text segments, we compute the Omiotis score between them, using Equation 10 and Algorithm 2. Higher values of Omiotis for a given pair denote stronger semantic relation between the examined text segments. The task is now reduced to define a threshold, above which an Omiotis value can be considered as a determining sign of a paraphrasing pair. In the experimental evaluation of Omiotis, we explain in detail how we have selected this threshold for the paraphrase recognition task.
In a similar manner, by using Equation 10 and Algorithm 2, the semantic relatedness scores for pairs of sentences can be computed. For this task, we are using the data set of Li et al. (2006) to evaluate Omiotis, comprising 30 sentence pairs, for which human scores are provided. In Section 4 we describe in detail the experimental set up.
When training a mapping function using a set of images, we prefer not to make use of all the pixels as such a dense sampling would result in unbalanced training data. For example, we could have too many pixels from large “sky” regions while relatively few from smaller “person” regions, which could eventually result in a serious bias in the trained mapping function. In addition, an overly dense sampling unnecessarily increases the training cost, as we need to handle millions of pixel samples. Therefore, we apply a superpixel based method to collect training samples. For each training image I , we first apply the graph-based segmentation [Felzenszwalb and Huttenlocher 2004] to divide the image into small homogeneous yet irregularly shaped patches, each of which is called a superpixel. Note that a superpixel in a smooth region may be larger than one in a region with more high-frequency details. We require that the color transform returned by our mapping function at the centroid of a superpixel be used for predicting with sufficient accuracy the adjusted color of all pixels within the same superpixel. To avoid bias, we randomly sample a fixed number of pixels from every superpixel. Let ν be any superpixel from the original images (before adjustment) in Λ, and Sν be the set of pixels sampled from ν. We revise the cost function in (1) as follows to reflect our superpixel-based sampling and local smoothness requirement.∑
ν ∑ j∈Sν ‖ Φ(Θ, xν)V (cj)− yj ‖2, (3)
where Θ represents the set of trained weights in the neural network, xν is the feature vector constructed at the pixel closest to the centroid of ν, V (cj) denotes the color basis vector of a sample pixel within ν, and yj denotes the adjusted color of the same sample within ν.
So far, we argued that centrality measures based on Semivalues in general, and on the Shapley value in particular, can help analyse networks in situations where simultaneous incidents may occur. Furthermore, we presented polynomial time algorithms to compute both the Shapley value-based and Semivalue-based betweenness centralities, for weighted graphs as well as unweighted graphs. This section provides an empirical evaluation of the above contributions. In particular, Section 5.5.1 compares our centrality measures against standard betweenness centrality, in a scenario where simultaneous node failures are simulated. In particular, we compare the effectiveness of both measures when tackling Problem 3. Section 5.5.2 evaluates the running time of our algorithms, and compares them against the Monte Carlo method—the only available alternative in the literature.
We carry out our experiments on weighted and unweighted random scale-free graphs, which are created using the preferential attachment mechanism introduced by Barabási and Albert [1999]. We also carry out experiments on two real-life network: (1) the email communication networks introduced by Guimerà et al. [2003], which contains 1133 nodes and 5451 edges, and (2) the neural system of C. elegans studied by Watts and Strogatz [1998], which contains 297 nodes and 2359 edges.
Keywords: finite automata, shortest synchronizing sequence, ASP
Decentralized Q-learning can be used to improve spatial reuse in dense wireless networks, enhancing performance as a result of exploiting the most rewarding actions. We have shown in this article, by means of a toy scenario, that Stateless Qlearning in particular allows finding good-performing configurations that achieve close-to-optimal (in terms of throughput maximization and proportional fairness) solutions.
However, the competitiveness of the presented fullydecentralized environment involves the non-existence of a Nash Equilibrium. Thus, we have also identified high variability in the experienced individual throughput due to the constant changes of the played actions, motivated by the fact that the reward generated by each action changes according to the opponents’ ones. We have evaluated the impact of the parameters intrinsic to the learning algorithm on this variability showing that it can be reduced by decreasing the exploration degree and learning rate. The individual reduction on the throughput variability occurs at the expense of losing aggregate performance.
This variability can potentially result in negative effects on the overall WN’s performance. The effects of such a fluctuation in higher layers of the protocol stack can have severe consequences depending on the time scale at which they occur. For example, noticing high throughput fluctuations may trigger congestion recovery procedures in TCP (Transmission Control Protocol), which would harm the experienced performance.
We left for future work to further extend the decentralized approach in order to find collaborative algorithms that allow the neighbouring WNs to reach an equilibrium that grants acceptable individual performance. Acquiring any kind of knowledge about the neighbouring WNs is assumed to solve the variability issues arisen from decentralization. This information may be directly exchanged or inferred from observations. Furthermore, other learning approaches are intended
to be analysed in the future for performance comparison in the resource allocation problem.
ar X
iv :1
51 1.
09 26
3v 4
[ cs
.L G
] 2
8 Ju
l 2 01
6
A
Scalable and Accurate Online Feature Selection for Big Data
KUI YU, Simon Fraser University XINDONG WU, Hefei University of Technology and University of Vermont WEI DING, University of Massachusetts Boston JIAN PEI, Simon Fraser University
Feature selection is important in many big data applications. Two critical challenges closely associate with big data. Firstly, in many big data applications, the dimensionality is extremely high, in millions, and keeps growing. Secondly, big data applications call for highly scalable feature selection algorithms in an online manner such that each feature can be processed in a sequential scan. We present SAOLA, a Scalable and Accurate OnLine Approach for feature selection in this paper. With a theoretical analysis on bounds of the pairwise correlations between features, SAOLA employs novel pairwise comparison techniques and maintain a parsimonious model over time in an onlinemanner. Furthermore, to deal with upcoming features that arrive by groups, we extend the SAOLA algorithm, and then propose a new group-SAOLA algorithm for online group feature selection. The group-SAOLA algorithm can online maintain a set of feature groups that is sparse at the levels of both groups and individual features simultaneously. An empirical study using a series of benchmark real data sets shows that our two algorithms, SAOLA and group-SAOLA, are scalable on data sets of extremely high dimensionality, and have superior performance over the state-of-the-art feature selection methods.
Additional Key Words and Phrases: Online feature selection, Extremely high dimensionality, Group features, Big data

Concept2 categorization, also known as concept learning or noun categorization, is a process of assigning a concept to one candidate category, given a set of concepts and candidate categories. Traditionally, concept categorization is achieved by concept clustering due to the lack of category representations. Since our model can generate representations of categories, we propose a new method of using nearest neighbor (NN) classification to directly categorize each concept to a certain category.
We explore the use of deep learning hierarchical models for problems in financial prediction and classification. Financial prediction problems – such as those presented in designing and pricing securities, constructing portfolios, and risk management – often involve large data sets with complex data interactions that currently are difficult or impossible to specify in a full economic model. Applying deep learning methods to these problems can produce more useful results than standard methods in finance. In particular, deep learning can detect and exploit interactions in the data that are, at least currently, invisible to any existing financial economic theory.
Key Words: Deep Learning, Machine Learning, Big Data, Artificial Intelligence, LSTM Models, Finance, Asset Pricing, Volatility
∗Bartlit Beck Herman Palenchar & Scott LLP, and GreyMaths Inc. †Booth School of Business, University of Chicago, and GreyMaths Inc. ‡Mathematical Institute, University of Oxford, and GreyMaths Inc.
ar X
iv :1
60 2.
06 56
1v 1
[ cs
.L G
] 2
1 Fe
b 20
16
We carried out three sets of experiments on an apparatus consisting of two Ritar 6V batteries connected to the circuit shown in Figures 16 and 17. In our simulation tests we demonstrated the performance of our approach on suites of 8 batteries, but performing the same experiments on the physical apparatus would have been too time-consuming. Each of our 2-battery experiments took over 11 hours to drain the batteries and, if anything went wrong during an experiment, such as loss of communications with the PC, the experiment had to be restarted resulting in the loss of a day or more.
When performing the experiments we noticed that the Arduino distorts all measured values: time and voltages, and therefore amps and internal resistance. Its distortions appear consistent across all experiments, resulting in systematic error. In particular, all of the times we measured suggest that the Arduino measures 1 hour every 1.4 hours of real time, so a 7 or 8 hour lifetime measured by the Arduino is actually approximately 10 to 11 hours of real time. We report all data values directly from the Arduino measurements, unadjusted for the systematic errors, so it can be borne in mind that our lifetime values are considerably longer when measured in “real” time. For consistency, all other times are reported in the same relative measures (in practice, timing of load control and discharge curves and other values were all performed using the Arduino clock, so the measurements are entirely consistent with one another).
We randomly generated 10 different load profiles, drawn from the same distribution as we used to train our policy, each alternating between 0.2 and 0.3 Amps and having intervals of constant load of durations that are distributed around 30 minutes with a distribution as shown in Figure 22.
2. The experiments we report for load profiles 1–6 were run with batteries having been cycled up to 15 times. For later profiles we did observe that some of the batteries showed behaviour that suggested a slight deterioration in performance and it is possible that lifetimes are lower for these experiments than would be the case for new batteries.
For each load profile we ran best-of-two and the plan-based policy so that we could perform a direct comparison of lifetime achieved and number of switches performed. This resulted in 16 load-execution experiments. For the first two load profiles we restricted the best-of-two policy to switch at most every 5 minutes, so that the best-of-two policy and the plan-based policy switched a similar number of times in an entire run. Our simulation results suggest that the plan-based policy should switch no more than about 20 times, but our experiments reveal that the noise in the sensor data leads to errors in the estimation of the state of charge which cause the policy to switch more frequently than we would anticipate. Frequent switching indicates that the policy is responding to spurious artifacts in the sensed data and to the variability in the real behaviour of the batteries. We discuss this further in Section 8.
The plan-based policy was applied every 36 seconds (0.01 hours), reflecting the granularity of the plans and learned policy. We also ran an experiment in which the best-of-two policy was allowed to switch every 36 seconds, to ensure that the results we obtained were not biased by offering the plan-based policy a faster reaction time, to changes in the battery state of charge, than best-of-two.
We wanted to establish whether the plan-based policy can achieve similar lifetimes to the bestof-two policy with a lower numbers of switches. We also wished to confirm that it is better than the naive but simple policy of sequencing, in which the first battery is used until it is dead, and then the second battery is used. This should be obvious (the sequencing policy is much worse in simulation), but the observed behaviour of the plan-based policy is superficially similar to sequencing, since it favours mostly using one battery until it is heavily discharged before switching to the second battery for significant intervals, so we thought it useful to perform a physical comparison. In the case of a 2-battery setup sequencing involves only 1 switch (the minimum number of switches possible in the two battery case).
We ran 21 complete experiments in total. In all of the plots showing battery voltages during these experiments, the last lowest point on the battery voltage curves (the red and green curves) are the points at which the corresponding battery died.
Figure 23 shows the best-of-two policy running on the second load profile. The curves show the characteristic discharge/recovery pattern, separated by a step separation caused by the internal resistance of the battery (when the battery is recovering its voltage is open circuit, when it is loaded it is then reduced by the internal resistance).
The load and voltage curves for the red curve (battery B1) are fuzzy because there is more noise in the readings from these sensors than for the other battery. This phenomenon is consistently a problem for B1 and is not dependent on the battery, but appears to be a feature of the circuit itself.
The strange striations for the green (B2) curve at the start of the graph are due to a failure of the Arduino to correctly capture the battery voltage over this period, but it does not affect the performance of the policy (we have simple fail safes to ensure that spurious data of this sort do not affect our performance).
Figure 24 shows the behaviour of the plan-based policy running on the second load profile. The top two curves represent the usage of the two batteries, B1 and B2. Battery B1 (the red curve) is used for the first 10,000 half-seconds, then B2 is briefly used before the policy switches back to B1 until about half way through the run. In the second half of the graph, the two batteries are interleaved, and the rising curves of B1 correspond to the periods in which B2 is in use and B1 is resting.
The alternating load is represented by the bottom two curves. It can be seen that when the load changes, the measured voltage changes (the top curve registers a slight blip). This is because of the internal resistance which means that there is a lower voltage loss in the battery when the current changes. We would expect this to be about 34mV (if the internal resistance is 0.34Ω) because the difference in current is 0.1A. It is actually higher than that, but this appears to be because there is a slight over-reaction to changes in the load, causing the battery voltage to drop sharply when the battery is first loaded, and then pull back, while the battery tends to recover sharply, and then fall back in line, when its load is reduced.
Figure 25 shows the best-of-two policy and the plan-based policy both being run on the second load profile side-by-side. The red plots are B1 and green are B2. The blue and purple points shows
where B1/B2 serviced the load (and the value of the load) for best-of-two, while the black points, slightly displaced above these, show where B2 serviced the load under the plan-based policy (B1 serviced the load the rest of the time). The voltage curves for the plan-based policy have been offset from curves for best-of-two so that they can be displayed on the same plot. The labelling on the y-axis has been removed to avoid confusion. We can see three interesting features:
1. The plan-based policy tends to use B1 first and B2 second, although not sequentially.
2. The plan-based policy runs for longer, demonstrating that increased lifetime is achieved.
3. Best-of-two essentially alternates between the batteries (minor variations are due to slight discrepancies in the batteries and other factors).
Figure 26 shows a comparison of the plan-based policy working on the first and second load profiles. The performance of the policy on the first load profile is shown in the upper voltage curves and the upper load curves, while the curves for the second load profile have been displaced to differentiate them. The plot highlights the similarity in the way the policy manages the batteries in each case: the general strategy is to runB1 until it is at the knee, resting it only briefly in this period, then oscillate between B1 and B2 at low frequency for a while, before entering a period in which B1 is rapidly switched with B2 as B1 converges on empty. The policy then finishes off with B2.
An interesting difference is a consequence of the (random) loads: B1 is faced with heavier loads during the first part of the second profile, so it dies faster than in the first profile. However, B2 faces a slightly less arduous time during the second half of the second profile and manages to last considerably longer. In particular, the load in the interval 30,000–33,000 was a high load serviced by B2 in the first profile, while the same period happens to be a lower load in the second profile. This is a key reason why B2 dies faster in the first profile: its available charge is depleted in that period and there is no real opportunity to rest it after that point. The final period of load in the first profile is a high load and that kills B2 quickly, while the final period of load in the second profile is a lower one. This allows B2 to recover some of its bound charge over that period, depleting its available charge more slowly and sustaining it a little longer in that critical period.
In Figure 26 the upper policy execution switches frequently in the window between 41,000 and 43,000 half seconds, just before B1 dies. This is because the plan-based policy includes a default action to switch to the other battery to avoid the currently loaded battery dying prematurely. The reason for this is to protect the batteries and the policy from the effects of errors in the sensor data that propagate into the state of charge model. The effect of the default action in this case is to cause the policy to switch to B2 when B1 is almost out of charge, but back to B1 as soon as it has recovered enough to be able to be loaded once again (according to the state of charge model).
Figure 27 shows the policy for the first load profile again, this time plotted with the estimated available charge (based on the voltage readings and the voltage model). The graph shows several important features. The black crosshairs mark the estimated available charge (measured in 0.1mAh units) for B1 and the grey crosshairs show it for B2. The discontinuities are due to the changing
load values. There should be no discontinuity, because the model adjusts for the load (using our estimated internal resistance), but it is clear that there is an additional effect here that we cannot capture this way. As we have already mentioned, it is also the case that the discrepancy between battery terminal voltage readings for the different loads should be 0.1A × 0.34Ω = 34mV , where 0.1A is the difference in load and 0.34Ω is the internal resistance, but the graph shows differences that are much greater. This effect appears to worsen as the battery discharges (see the widening gaps between the loaded and unloaded voltages recorded for the batteries in the red/green curves — particularly for the red curve). However, interestingly, the voltage-capacity model seems to be marginally less unstable for lower states of charge (the steps get slightly smaller in these cases for the black curve).
As can also be seen, the available charge model breaks down in some situations (when the observations cannot be fitted consistently to the initial state we assumed for the battery). This leads to some of the available charge values being negative (particularly in the 42000–45000 period). This causes the policy to revert to the default action, but the somewhat simplistic implementation of the default leads to the oscillation between batteries during this period.
Figure 28 shows the results obtained by draining the batteries in sequence, using the second load profile. This performance is optimal in terms of switching, but the lifetime achieved is much shorter than that achieved by the plan-based policy and similar to the lifetime of the best-of-two for this case. The fact that best-of-two does worse than sequential scheduling for this profile is probably due to variation in the battery behaviour: it seems likely that best-of-two should perform more similarly to the results in the other load profiles.
It can be clearly seen that the plan-based policy achieves a consistently longer lifetime than the best-of-two policy, with significantly reduced switching. The results are summarised in Table 5.
For A “ M ´ N P Rnˆn and provided that A and M are nonsingular, Varga [1962] showed that the iterates xk`1 “ M ´1Nxk ` M ´1b converge to the unique solution of the linear system of equation Ax “ b. In the policy evaluation problem, we are working with the linear system of equations pI ´ γPσqv “ rσ where σ is the target policy to evaluate. We now show that the reward and transition models (4) precisely corresponds to the notion of a matrix splitting for the matrix I ´ γPσ .
Theorem 1 (Matrix Splitting in the Gating Model). Let A 9“I ´ γPσ, M 9“I ´ γP7 and N 9“γPK, then A “ M ´N is a regular splitting. Proof: M is an “M-matrix” (see chapter 6 of Berman and Plemmons [1979]). M-matrices have the property of being inverse-positive, that is M´1 exists with M ě 0, fulfilling the definition of a regular splitting (see def. 3.5 of Varga [1962])
Corollary 1. For the regular splitting of theorem 1,
1. ρpγpI ´ γP7q ´1PKq ă 1
2. The successive approximation method based on the generalized Bellman operator (5) converges for any initial vector v0.
Proof: This follows directly from the fact that options induce a regular splitting through the operator L in the gating model. See [Varga, 1962, Theorem 3.13] for a general proof.
Since a set of options and the policy over them induce a matrix splitting, a choice of options is in fact a choice of algorithm for solving MDPs. An important property of an iterative solver, besides its computational efficiency, is that it should converge to a solution of the original problem. We should therefore ask ourselves whether the iterates corresponding to (5) converge to the true value function underlying a given target policy. Theorem 2 shows that the successive approximation method induced by a set of options and policy over them is consistent [Young and Rheinboldt, 1971] given that the marginal action probabilities is equal to the target policy.
Theorem 2 (Consistency of Policy Evaluation in the Gating Model). The iterative method associated with the splitting (5)
vk`1 “ pI ´ γP7q ´1rσ ` γpI ´ γP7q ´1PKvk, k ě 0
is a consistent policy evaluation method in the gating model if the set of options and policy over them is such that σ pa | sq “ ř w µ pw | sqπw pa | sq @a P A, s P S where σ is the target policy to be evaluated. Proof: Let vW,µ be the unique solution to the generalized Bellman equations (5), we have:
vW,µ “ ` I ´ γpI ´ γP7q ´1PK ˘´1 pI ´ γP7q ´1rσ
“ ` pI ´ γP7q ´1 ppI ´ γP7q ´ γPKq ˘´1 pI ´ γP7q ´1rσ
“ pI ´ γPσq ´1 rσ “ vσ
Therefore vW,µ is also the solution to the policy evaluation problem for the policy σ.
While many set of options can satisfy the marginal condition in the gating model, not all of them would converge equally fast. Using comparison theorems for regular matrix splittings [Varga, 1962] we can better understand the effect of modelling the world at different timescales on the asymptotic performance of the induced algorithms.
Theorem 3 (Predict further, plan faster). In the gating model, if a set of options ĂW has the same intra-option policies and policy over options with some other setW but whose termination functions are such that β rwpsq ď βwpsq @w P W , s P S, then 0 ď ρpĂM´1 rNq ď ρpM´1Nq ă 1. Proof: By theorem 1, the two sets of options induce a corresponding regular splitting. The claim then follows from [Varga, 1962, Theorem 3.32] since rN ď N (componentwise): rNps, s1q ď γ ÿ
w
µ pw | sq ÿ
a
πw pa | sq ÿ
s1
P ` s1 ˇ̌ s, a ˘ βwps 1q “ N .
Theorem 3 consolidates the idea that modelling the world over longer time horizons increases the asymptotic rate of convergence. This also becomes apparent when writing (5) in the following form:
pI ´ γP7qv “ pI ´ γP7qv ` prσ ´ pI ´ γPσqvq
v “ v ` pI ´ γP7q ´1prσ ´ pI ´ γPσqvq (6)
Therefore, options enter the linear system of equations pI´γPσqv “ rσ through the preconditioning matrixM [Saad, 2003] and yield the following transformed linear system of equations:
pI ´ γP7q ´1pI ´ γPσqv “ pI ´ γP7q ´1rσ (7)
As the options timescales increase and βwpsq “ 0 @w P W , s P S, then P7 “ Pσ and the solution is obtained directly on the right hand side of (7). The corresponding generalized Bellman operator also becomes Lp8qv 9“pI ´ γPσq ´1rσ and solves the original system in one iteration. On the other hand, if the termination functions are such that the options terminate after only one step, we get Lp0qv 9“rσ ` γPσv, the usual one-step Bellman operator T of the value iteration algorithm. Since the spectral radius associated with matrix splitting methods is given by ρpM´1Nq, we also have the following: ρpM´1Nq “ ρpM´1pM ´Aqq “ ρ ` I ´ pI ´ γP7q ´1pI ´ γPσq ˘ ď }I ´ pI ´ γP7q ´1pI ´ γPσq} .
This suggests that in terms of asymptotic performances, a good set of options should be such that it induces a preconditioning matrixM that is close to I ´ γPσ in some sense but whose inverseM ´1 is easier to compute.
Our manually labelled election dataset is sampled from tweets collected about the 2015 Venezuela parliamentary election using the well-known pooling method [16]. It covers the period of one month before and after the election date (06/12/2015) in Venezuela. We use the Terrier information retrieval (IR) platform [13] and the DFReeKLIM [2] weighting model designed for microblog search to retrieve tweets related to 21 query terms (e.g. “violencia”, “eleccion” and “votar”). Only the top 7 retrieved tweets are selected per query term per day, making the size of the collection realistic for human assessors to examine and label the tweets. Sampled tweets are merged into one pool and judged by 5 experts who label a tweet as: “Election-related” or “Not Election-related”. To determine the judging reliability, an agreement study was conducted using 482 random tweets that were judged by all 5 assessors. Using Cohen’s kappa, we found a moderate agreement of 52% between all assessors. For tweets without a majority agreement, an additional expert of Venezuela politics was used to further clarify their categories. In total, our election dataset consists of 5,747 Spanish tweets, which contains 9,904 unique words after preprocessing (stop-word removal & Spanish Snowball stemmer). Overall, our labelled election dataset covers significant events (e.g. Killing of opposition politician Luis Diaz [1]) in the 2015 Venezuela parliamentary election. From the general statistics shown in Table 1, we observe that the dataset is unbalanced; the majority class (Non-Election) has 1,000 more tweets than the minority class (Election).
– sorts for geometric objects types, e.g., point, segment, circle, triangle; – parametric functions describing objects parameters e.g., x(point), r(circle); – qualitative relations, e.g., rccEC(circle, circle), coincident(point, circle). . Example 1: combining topology and size Consider a program describing three circles a, b, c such that a is discrete from b, b is discrete from c, and a is a proper part of c, declared as follows: :- sorts circle. :- objects a, b, c :: circle. :- constants . :- variables C, C1, C2 :: circle. {x(C)=X}. {y(C)=X}. {r(C)=X}. rccDR(a,b)=true. rccDR(b,c)=true. rccPP(a,c)=true. ASPMT(QS) checks if the spatial relations are satisfiable. In the case of a positive answer, a parametric model and computation time are presented. The output of the above mentioned program is: r(a) = 0.5 r(b) = 1.0 r(c) = 0.25 x(a) = 1.0 x(b) = 1.0 x(c) = 1.0 y(a) = 3.0 y(b) = 1.0 y(c) = 3.0 This example demonstrates that ASPMT(QS) is capable of computing composition tables, in this case the RCC–5 table for circles [25]. Now, consider the addition of a further constraint to the program stating that circles a, b, c have the same radius: <- r(a)=R1 & r(b)=R2 & r(c)=R3 & (R1!=R2 | R2!=R3 | R1!=R3).
This new program is an example of combining different types of qualitative information, namely topology and size, which is a non-trivial research topic within the relation algebraic spatial reasoning community; relation algebraic-based solvers such as GQR [17,29] will not correctly determine inconsistencies in general for arbitrary combinations of different types of relations (orientation, shape, distance, etc.). In this case, ASPMT(QS) correctly determines that the spatial constraints are inconsistent:
UNSATISFIABLE; Z3 time in milliseconds: 10; Total time in milliseconds: 946
. Example 2: combining topology and relative orientation Given three circles a, b, c let a be proper part of b, b discrete from c, and a in contact with c, declared as follows: :- sorts
circle. :- objects
a, b, c :: circle. :- constants
. :- variables
C, C1, C2 :: circle.
{x(C)=X}. {y(C)=X}. {r(C)=X}. rccPP(a,b)=true. rccDR(b,c)=true. rccC(a,c)=true.
Given this basic qualitative information, ASPMT(QS) is able to refine the topological relations to infer that (Figure 1a): i) a must be a tangential proper part of b ii) both a and b must be externally connected to c.
ASPMT(QS): Non-Monotonic Spatial Reasoning 9
ab c a b c
a b c a b c
(a)
ab c a b c
a b c a b c
(b)
Fig. 1: Reasoning about consistent and refinement by combining topology and relative orientation.
i t i i lit ti i f r ti , ( ) i l t r t t l i l r l ti t i f r t t ( i r ): i) t t ti l t f ii) t
t t ll t t . r(a) = 1.0 r(b) = 2.0 r(c) = 1.0 x(a) = 1.0 x(b) = 0.0 x(c) = 3.0 y(a) = 0.0 y(b) = 0.0 y(c) = 0.0 rccTPP(a,b) = true rccEC(a,c) = true rccEC(b,c) = true
We then add an additional constraint that the centre of a is left of the segment between the centres b to c. ... left_of(center(a),center(b),center(c)). ASPMT(QS) determines that this is inconsistent, i.e., the centres must be collinear (Figure 1b). UNSATISFIABLE;
5 Empirical Evaluation and Examples
In this section we present an empirical evaluation of ASPMT(QS) in comparison with other existing spatial reasoning systems. The range of problems demonstrate the unique, non-monotonic spatial reasoning features that ASPMT(QS) provides beyond what is possible using other currently available systems. Table 2 presents run times obtained by Clingo – an ASP grounder and solver [18], GQR – a binary constraint calculi reasoner [17], CLP(QS) – a declarative spatial reasoning system [8] and our ASPMT(QS) implementation. Tests were performed on an Intel Core 2 Duo 2.00 GHZ CPU with 4 GB RAM running Ubuntu 14.04. The polynomial encodings of the topological relations have not been included here for space considerations.
Table 2: Cumulative results of performed tests. “—” indicates that the problem can not be formalised, “I” indicates that indirect effects can not be formalised, “D” indicates that default rules can not be formalised.
t iti l tr i t t t t tr f i l ft f t t t t tr t .
ASPMT(QS) determines that this is inconsistent, i.e., the centres must be collinear (Figure 1b).
ASPMT(QS): Non-Monotonic Spatial Reasoning 9
ab c a b c
a b c a b c
(a)
ab c a b c
a b c a b c
(b)
Fig. 1: Reasoning about consistent and refinement by combining topology and relative orientation.
Given this basic qualitative information, ASPMT(QS) is able to refine the topological relations to infer that (Figure 1a): i) a must be a tangential proper part of b ii) both a and b must be externally connected to c.
r(a) = 1.0 r(b) = 2.0 r(c) = 1.0 x(a) = 1.0 x(b) = 0.0 x(c) = 3.0 y(a) = 0.0 y(b) = 0.0 y(c) = 0.0 rccTPP(a,b) = true rccEC(a,c) = true rccEC(b,c) = true
We then add an additional constraint that the centre of a is left of the segment between the centres b to c. ... left_of(center(a),center(b),center(c)). ( ) eter i es t at t is is i c siste t, i.e., t e ce tres st e c lli e r ( i re ). UNSATISFIABLE;
The sequential minimal optimization (SMO) algorithm [13] is an iterative decomposition algorithm [12] using minimal working sets of size two. This size is minimal to keep the current solution feasible. The algorithm explicitly exploits the special structure of the constraints of problem (1) and shows very good performance in practice. For each feasible point α ∈ R we define the index sets
Iup(α) ={i ∈ {1, . . . , ℓ} | αi < Ui}
Idown(α) ={i ∈ {1, . . . , ℓ} | αi > Li} .
The canonical form of the SMO algorithm (using the common Karush-KuhnTucker (KKT) violation stopping condition) can be stated as follows:
Algorithm 1: General SMO Algorithm
Input: feasible initial point α(0), accuracy ε ≥ 0 compute the initial gradient G(0) ← ∇f(α(0)) = y −Kα(0) set t ← 1 do
1 select a working set B(t) 2 solve the sub-problem induced by B(t) and α(t−1), resulting in α(t) 3 compute the gradient
G(t) ← ∇f(α(t)) = G(t−1) −K ( α(t) − α(t−1) )
4 stop if( max { G
(t) i ∣∣ i ∈ Iup(α(t)) } −min { G (t) j ∣∣ j ∈ Idown(α(t)) }) ≤ ε
set t ← t+ 1 loop;
If no additional information are available the initial solution is chosen to be α(0) = (0, . . . , 0)T resulting in the initial gradient G(0) = ∇f(α(0)) = y which can be computed without any kernel evaluations.
It is widely agreed that the working set selection policy is crucial for the overall performance of the algorithm. This is because starting from the initial solution the SMO algorithm generates a sequence (α(t))t∈N of solutions which is determined by the sequence of working sets (B(t))t∈N. We will briefly discuss some concrete working set selection policies later on.
First we will fix our notation. In each iteration the algorithm selects a working set of size two. In this work we will consider (ordered) tuples instead of sets for a number of reasons. Of course we want our tuples to correspond to sets of cardinality two. Therefore a working set B is of the form (i, j) with i 6= j. Due to its wide spread we will stick to the term working set
instead of tuple as long as there is no ambiguity. Whenever we need to refer
to the corresponding set, we will use the notation B̂ = (̂i, j) := {i, j}. For a tuple B = (i, j) we define the direction vB = ei − ej where en is the n-th unit vector of Rℓ. This direction has a positive component for αi and a negative component for αj . We will restrict the possible choices such that the current point α can be moved in the corresponding direction vB without immediately leaving the feasible region. This is equivalent to restricting i to Iup(α) and j to Idown(α). We collect the allowed working sets in a point α in the set B(α) = Iup(α)×Idown(α)\{(n, n) | 1 ≤ n ≤ ℓ}. With this notation a working set selection policy returns some B(t) ∈ B(α(t−1)).
The sub-problem induced by the working set B(t) solved in step 2 in iteration t is defined as
maximize f(α(t)) = yTα(t) − 1
2 (α(t))TKα(t)
s.t. ℓ∑
i=1
α (t) i = 0 (equality constraint)
Li ≤ α (t) i ≤ Ui for i ∈ B̂ (t) (box constraint)
and α (t) i = α (t−1) i for i 6∈ B̂ (t) .
That is, we solve the quadratic program as good as possible while keeping all variables outside the current working set constant. We can incorporate the equality constraint into the parameterization α(t) = α(t−1) + µ(t)vB(t) and arrive at the equivalent problem
maximize ltµ (t) −
1 2 Qtt(µ (t))2
s.t. L̃t ≤ µ (t) ≤ Ũt
for µ(t) ∈ R with
Qtt =Kii − 2Kij +Kjj = v T B(t) KvB(t)
lt = ∂f
∂αi (α(t−1))−
∂f
∂αj (α(t−1)) = vT B(t) ∇f(α(t−1))
L̃t =max{Li − α (t−1) i , α (t−1) j − Uj} Ũt =min{Ui − α (t−1) i , α (t−1) j − Lj}
and the notation B(t) = (i, j). This problem is solved by clipping the Newton step µ∗ = lt/Qtt to the bounds:
µ(t) = max { min { lt Qtt , Ũt } , L̃t } . (2)
For µ(t) = lt/Qtt we call the SMO step free. In this case the SMO step coincides with the Newton step in direction vB(t) . Otherwise the step is said to hit the box boundary.
Recently it has been observed that the SMO step itself can be used for working set selection resulting in so called second order algorithms [2, 5].
We can formally define the gain of a SMO step as the function gB(α) which computes the difference f(α′) − f(α) of the objective function before and after a SMO step with starting point α on the working set B, resulting in α′. For each working set B this function is continuous and piecewise quadratic (see [5]). Then these algorithms greedily choose a working set B(t) promising the largest functional gain gB(t)(α
(t−1)) = f(α(t))−f(α(t−1)) by heuristically evaluating a subset of size O(ℓ) of the possible working sets B(α(t−1)).
Fan et al. [2] propose to choose the working set according to
i = argmax
{ ∂f
∂αn (α)
∣∣∣∣ n ∈ Iup(α) }
j = argmax { g̃(i,n)(α) ∣∣∣ n ∈ Idown(α) \ {i} }
(3)
with g̃B(α) = 1
2
(vTB∇f(α)) 2
vTBKvB ∈ R≥0 ∪ {∞}
where g̃B(α) is an upper bound on the gain which is exact if and only if the step starting from α with working set B is not constrained by the box.1 Note that in this case the Newton step µ∗ = (vTB∇f(α))/(v T BKvB) in direction vB is finite and we get the alternative formulation
g̃B(α) = 1
2 (vTBKvB)(µ ∗)2 . (4)
This formula can be used to explicitly compute the exact SMO gain gB(α) by plugging in the clipped step size (2) instead of the Newton step µ∗.
The stopping condition in step 4 checks if the Karush-Kuhn-Tucker (KKT) conditions of problem (1) are fulfilled with the predefined accuracy ε. List et al. [9] have shown that this is a meaningful stopping criterion. The accuracy ε is usually set to 0.001 in practice.
SMO is a specialized version of the more general decomposition algorithm which imposes the weaker condition |B(t)| ≤ q ≪ ℓ on the working set size. The main motivation for decomposition is that in each step only the rows of the kernel matrix K which correspond to the working set indices are needed. Therefore the algorithm works well even if the whole matrix K does not fit into the available working memory. The SMO algorithm has the advantage over decomposition with larger working sets that the sub-problems in step 2 can be solved very easily. Because of its minimal working set size the algorithm makes less progress in a single iteration compared to larger working sets. On the other hand single iterations are faster. Thus, there is a trade-off between the time per iteration and the number of iterations needed to come close enough to the optimum. The decisive advantage of SMO in this context is that it can take its decisions which working set B (corresponding to the optimization direction vB) to choose more frequently between its very fast iterations. This strategy has proven beneficial in practice.
In elaborate implementations the algorithm is accompanied by a kernel cache and a shrinking heuristic [7]. The caching technique exploits the
1The software LIBSVM [2] sets the denominator of g̃B(α) to τ = 10 −12
> 0 whenever it vanishes. This way the infinite value is avoided. However, this trick was originally designed to tackle indefinite problems.
fact that the SMO algorithm needs the rows of the kernel matrix which correspond to the indices in the current working set B(t). The kernel cache uses a predefined amount of working memory to store rows of the kernel matrix which have already been computed. Therefore the algorithm needs to recompute only those rows from the training data evaluating the possibly costly kernel function which have not been used recently. The shrinking heuristic removes examples from the problem that are likely to end up at the box boundaries in the final solution. These techniques perfectly cooperate and result in an enormous speed up of the training process. We will later use the fact that the most recently used rows of the kernel matrix K are available from the cache.
The steps 1, 3, and 4 of the SMO optimization loop take O(ℓ) operations, while the update 2 of the current solution is done in constant time.
There has not been any work on the improvement of step 2 of Algorithm 1. Of course, it is not possible to considerably speed up a computation taking O(1) operations, but we will see in the following how we may replace the optimal (greedy) truncated Newton step with other approaches.
All the models receive as input ‘frozen’ visual and linguistic representations, obtained as follows.
Visual input For each bounding box in each scenario, we extract a visual representation using a Convolutional Neural Network (Simonyan and Zisserman, 2014). We use the VGG-19 model pre-trained on the ImageNet ILSVRC data (Russakovsky et al., 2015) and the MatConvNet (Vedaldi and Lenc, 2015) toolbox for features extraction. Each bounding box is represented by a 4096-dimension vector extracted from the 7th fully connected layer (fc7). For computational efficiency, we subsequently reduce the vectors to 400 dimensions by applying Singular Value Decomposition (SVD).
Linguistic input Similarly, each word in a query is represented by a 400-dimension vector built with the Word2Vec CBOW architecture (Mikolov et al., 2013), using the parameters that were shown to perform best in (Baroni et al., 2014). The corpus used for building the semantic space is a 2.8 billion tokens concatenation of the web-based UKWaC, a mid-2009 dump of the English Wikipedia, and the British National Corpus (BNC).
Gödel’s Diagonalization Lemma states that there is a sentence Ψ such that Ψ ↔ F (o#(Ψ)) is provable in Q where o#(Ψ) is the Gödel number for Ψ and F is some well formed formula provable in a formal system Q.
It is clear to see that the statement concerning Ψ is a tautological impredicative, as Gödel derives Ψ by the following proof.
Zachary C. Lipton is supported by the Division of Biomedical Informatics at the University of California, San Diego, via training grant (T15LM011271) from the NIH/NLM. Thanks to NVIDIA Corporation for their generous hardware donations. Thanks to John Berkowitz, Anima Anandkumar, Charles Elkan, and Julian McAuley.
The PDDL description is given as input to the planner. The advantage of using the PDDL language is that we can experiment with different planners and determine which best fits our particular problem. We have evaluated our model using both SGPlan [CWH06] and Metric-FF [Hof02].
The planner is run from inside the pentesting framework, as a pluggable module of the framework that we call PlannerRunner. The output of the planner is a plan, a sequence of actions that lead to the completion of the goal, if all the actions are successful. We make this distinction because even with well-tested exploit code, not all exploits launched are successful. The plan is given as feedback to the pentesting framework, and executed against the real target network.
In this study, we evaluated two ways of defining the input examples and target labels that the ConvNet is trained on. First, a trial-wise strategy that uses whole trials as input and per-trial labels as targets. Second, a cropped training strategy that uses crops, i.e., sliding time windows within the trial as input and per-crop labels as targets (where the label of a crop is identical to the label of the trial the crop was extracted from).

Conference Chair: Ole Lehrmann Madsen (Århus University, DK) Program Chair: Walter Olthoff (DFKI GmbH, Germany) Organizing Chair: Jørgen Lindskov Knudsen (Århus University, DK) Tutorials: Birger Møller-Pedersen (Norwegian Computing Center, Norway) Workshops: Eric Jul (University of Kopenhagen, Denmark) Panels: Boris Magnusson (Lund University, Sweden) Exhibition: Elmer Sandvad (Århus University, DK) Demonstrations: Kurt Nørdmark (Århus University, DK)
In order to investigate the unitarily invariant norm, we first present the notion of symmetric gauge functions.
Definition 7.3. A real function φ : Rn → R is called a symmetric gauge function if it satisfies the following four conditions:
(1) φ(u) > 0 for all nonzero u ∈ Rn.
(2) φ(αu) = |α|φ(u) for any constant α ∈ R.
(3) φ(u+ v) ≤ φ(u) + φ(v) for all u,v ∈ Rn.
(4) φ(Duπ) = φ(u) where uπ = (uπ1 , . . . , uπn) with π as a permutation of
[n] and D is an n× n diagonal matrix with ±1 diagonal elements.
Furthermore, the gauge function is called normalized if it satisfies the condition:
(5) φ(1, 0, . . . , 0) = 1.
Conditions (1)-(3) show that that the gauge function is a vector norm. Thus, it is convex and continuous. Condition (4) says that the gauge function is symmetric.
Lemma 7.3. [Schatten, 1950] Let u,v ∈ Rn. If |u| ≤ |v|, then φ(u) ≤ φ(v) for every symmetric gauge function φ.
Proof. In terms of Condition (4), we can directly assume that u ≥ 0 and v ≥ 0. Currently, the argument is equivalent to
φ(ω1v1, . . . , ωnvn) ≤ φ(v1, . . . , vn)
for ωi ∈ [0, 1]. Thus, by induction, it suffices to prove
φ(v1, . . . , vn−1, ωvn) ≤ φ(v1, . . . , vn)
where ω ∈ [0, 1] for every symmetric gauge function φ. It follows from
In this section, we introduce the formal description of the pratical algorithm that trains an DEFE ensemble. We first give a few definitions needed to describe the DEFE algorithm:
Definition 1. Given a classifier g : Rd → {s, b}, we call Ĝ = {x ∈ Rd, g(x) = s}the approximate selection region of classifierg. Let G = {xi, yi = s}, then GT = G ⋂
Ĝ is called the hit selection region.
Definition 2. Given a classifier g, the approximate rejection region is defined as Ĥ = {xi, g(xi) = b}. Let H = {xi, yi = b}, then HT = H ⋂ Ĥ is the hit rejection region.
Definition 3. Given the classifier g, we call T = GT ⋃
HT the hit region, and F = X \ T the anomalous region. Then, we can define the discriminative partition of the training example space as the tuples {T, F, Ĝ, Ĥ}.
From the definition above, it’s easy to see that the hit region and anomalous region is exactly the correctly classified and miss-classified samples, respectively. The reason that separate treatment of samples that counts for the fictious knowledge (i.e., {Ĝ, Ĥ}) of the weak classifier is that we want to further characterize the decision boundary trained by a first and quick ‘glance’ at the data. We can further perform discriminative partition over the resulting regions {T, F, Ĝ, Ĥ} respectively. By doing this procedure recursively for n iterations, we can obtain 4n partition of the sample space. In this paper, we consider the case that n is sufficiently small.
Hit region and anomalous region characterize the two different region of the sample space that exhibit potentially different patterns and distributions of high-level features, therefore a single classifier might fail to capture such information. To balance the number of samples of the partition, we normally set classifier to be either a weak classifier (e.g. Decision trees) or a neural network that is not fully trained. Furthermore, ‘weak’ discriminative partition obtained via such weak classifier is in fact the decision boundary trained by a first and quick ‘glance’ at the data, thus information containing the partition of {Ĝ, Ĥ} represents the subspace with principal different the structures hidden in the data. In contradiction to cluster analysis, discriminative partition tries to make use the information of the labels. The problems of overfitting might exist both due to the partition itself and the random errors from the weak classifier. To avoid this, we propose an additional procedure of
random interchange, i.e. randomly select the samples from both hit region and anomalous region according to a preset ratio and switch these selected samples. This additional procedure will not only balance the partition, but also enhance the robustness.
Now, we consider the partition against the feature space, i.e. the set containing every input attributes. In our work, we partition the feature set according to its physical interpretations. Note that overlapping of the partition is allowed. Given the partition S = ⋃
Si, we are now able to define the following procedures.
Definition 4. Let X = 4 n ⋃
i=1
Xi be a discriminative partition of the sample space, and S = m ⋃
i=1
Si
a given partition of the feature space; Then we call X ⊗ F = ( ⋃ Xi) ⊗ ( ⋃
Si) a partition of the sample-feature space. Every resulting subsets forms a new set of U = {Xi} ⊗ {Sj} = {(Xi, Sj)}, where ⊗ is the direct product.
Definition 5. From very subset Dh ∈ U = {Xi} ⊗ {Sj}, h = 1, ..., 4n ×m of the sample-feature space, we choose/train the corresponding classifier gh and its approximate selection region Ĝh. Then, we define ĜE = ⋃
h Ĝh, as the extreme selection region. Similarly, we can define as the extreme hit region GET = G ⋂
ĜE . The process of generating and constructing the extreme hit region based on the classifier chosen is called the expansion of the selection region. Similarly we can define the process of the expansion of H .
It’s trivial to see that the process of expanding selection region always increases the number of samples that can be possibly covered by a set of multiple classifiers, i.e. GT ⊂ GET . However, one primal concern might be that since discriminative partition and expansion of selection region closely rely on the label of the data, how can one guarantee that the selection region is still expanded without the prior knowledge of labels of the testing data? The key fact to solve this question lies in the fact that apart from the training data (including labels), the definition of selection region only depends of the resulting decision boundaries that can be well described and parameterized by classifiers g and gh(even with simple rules in the case of decision tree based discriminative partitions). As a result, information regarding these regions are compressed by a limited number of classifiers rather than the raw sample-feature space X ⊗ S = ( ⋃ Xi) ⊗ ( ⋃
Si). Thus, although the previously described expansion of selection region technique cannot be directly used for deriving a divideand-conquer mixture of classifier model, with the help of the resulting selection regions as stepping stones, ‘extreme’ information can then be unfolded and approximated by a single strong classifier.
In conclusion, the problem of improving the performance of deep learning can now be converted to the problem of approximating the expanded the selection region by merely a single classifier. In previous work of ensemble learning [5,6,10,11,24] tries to unify every sub-classifier gh by an ensemble procedure of linear weighting, voting or winner-take-all, and achieves a fairly good result compared to single classifier. As stated above, nevertheless, in the task of recognition of Higgs Bosons, this class of ensemble algorithms (Boosted Decision Tree for example) failed to significantly improve the performance of classification. The reason might be two folds: firstly, when applying divide-andconquer principle to the sample-feature space, only the shallow and presentational are exploited, thus missing local high-level information; secondly, only the weak classifiers’ final output is considered, therefore in intrinsic structures and learned representational features are ignored. Also, it’s too computational expensive to apply directly ensemble learning algorithms to deep learning algorithms.
With the strategy for finding labeled paths through the labeled predecessor graphs, the extension of the pruning algorithm from the fast STN compilation algorithm is straightforward. Tsamardinos, et al. (1998) provide two theorems relating dominance of edges to paths in the predecessor graph, adjusted slightly for our notation. In the following, A is assumed to be the source vertex of the SSSP, and B and C are other vertices.
Theorem 7.8 A negative edge (A,C) is lower-dominated by a negative edge (A,B) if and only if there is a path from B to C in the predecessor graph for A.
Theorem 7.9 A non-negative edge (A,C) is upper-dominated if and only if there is a vertex B, distinct from A and C, such that d[B] ≤ d[C] and there is a path from B to C in the predecessor graph for A.
Example 7.10 Figure 7.5 shows two simple examples on which we can apply these two theorems. First, Figure 7.5a is a weighted distance graph and Figure 7.5c shows its predecessor graph for source vertex A, on which we apply Theorem 7.8. Edge (A,C) is lower dominated because the weight −5 on B implies an edge (A,B) with weight −5, which is negative as the theorem requires. Furthermore, the predecessor graph has a path from B to C in the predecessor graph for A. Therefore, edge (A,C) is dominated and needed in the dispatchable form.
Figures 7.5a and 7.5c similarly exhibit Theorem 7.9. There is a path A → B → C in the graph, and d[B] = 2 ≤ d[C] = 5, so edge (A,C) is upper-dominated. Thus, edge (A,C) is not needed in the dispatchable form.
In both these examples, this step derives every possible edge weight for edges in the dispatchable form with A as their source, namely edges (A,B) and (A,C), and determines that only (A,B) is actually needed.
In the labeled case, particular labeled edge weights are dominated if the above conditions hold under all the environments the weight holds in.
Theorem 7.11 A negative labeled edge weight (dC , eC) in d[C] is lower-dominated by a negative labeled edge weight (dB, eB) in d[B] if and only if eB = eC and there is a path from B to C with environment eP in the predecessor graph for A such that eP = eC .
Theorem 7.12 A non-negative labeled edge weight (dC , eC) in edge d[C] is upper-dominated if and only if there is a labeled edge weight (dB, eB) in d[B], such that B is distinct from A and C, eB subsumes eC , dB ≤ d,C, and there is a path from B to C with environment eP in the predecessor graph for A such that eP = eC .
In practice, Drake searches over every path in the labeled predecessor graph with the source vertex as its start, and applies these theorems to find dominated edges. Specifically, during the traversal, it records the smallest vertex weight of any vertices along the path, not counting the source. That value is compared to other vertex weights of extensions of the path to apply the domination theorems. Every time a vertex weight is found to be dominated with some path, it is recorded in a list. After all traversals are done, every labeled value in the vertex weights not present in the list of dominated values is converted into an edge in the output dispatchable graph.
These two theorems require that eP = eC because the path must hold under all environments where the value dC does, but we also do not want eP to be tighter. Recall that the vertex weights we might prune also specify the paths. If eC is tighter than eP , then it must have a lower path length than the one implied by eP , or else it would not be in the labeled value set d[C]. Thus, we cannot guarantee that our path is the shortest path from the source to C, so this path is not suitable to prune it.
Example 7.13 To demonstrate the application of these ideas, reconsider Figure 7.4a. Example 7.6 gave the possible paths in the graph. The first path is A → B with path environment {x = 1, y = 1}. After reaching B, the minimal vertex weight is 2, but there are no extension of this path, so nothing can be pruned. In general, the first step from the source vertex cannot be pruned.
The next step of the traversal reaches C with environment {x = 1}, and the minimal path length is 1. C cannot be pruned. This path cannot be extended to B, but it does have an extension to D, using the vertex weight ((6, C), {x = 1, y = 1}). This path length is strictly longer than the path length of 1, and the environment of the value is equal to the environment of the path, so we add it to the pruned list.
Next consider path A → C under environment {} and path length 3. The extension to B using ((7, C), {y = 1}) is also prunable. Note that this path could never be used to prune the shorter path length ((2, A), {x = 1, y = 1}) because {x = 1, y = 1} 6= {y = 1}. Likewise, the extension to D prunes ((8, C), {y = 1}).
Collecting the non-pruned edges means that the algorithm adds edge (A,B) to the output dispatchable graph with weight W (A,B) = {(2, {x = 1, y = 1})}, and adds edge (A,C) with weight W (A,C) = {(1, {x = 1}), (3, {})}. We drop infinite weights, allowing them to be implicitly specified, and both finite weights of D were pruned, so it does not add the edge (A,D) at all.
Essentially, the pruning algorithm has the same structure as the unlabeled fast compilation algorithm. The major difference is that the values to prune have environments and that paths in the predecessor graph only exist under particular environments. Thus, the pruning step must satisfy the pruning requirement with the identical environment to prune a labeled value.
In recent years, there has been a flurry of research into identifying tractable classes of classic CSP instances based on restrictions on the hypergraphs of CSP instances, known as structural restrictions. Below, we present and discuss a few representative examples. To present the various structural restrictions, we will use the framework of width functions, introduced by Adler [1].
Definition 6 (Hypergraph). A hypergraph 〈V,H〉 is a set of vertices V together with a set of hyperedges H ⊆ P(V ).
Given a CSP instance P = 〈V,C〉, the hypergraph of P , denoted hyp(P ), has vertex set V together with a hyperedge V(δ) for every e[δ] ∈ C.
Definition 7 (Tree decomposition). A tree decomposition of a hypergraph 〈V,H〉 is a pair 〈T, λ〉 where T is a tree and λ is a labelling function from nodes of T to subsets of V , such that
1. for every v ∈ V , there exists a node t of T such that v ∈ λ(t), 2. for every hyperedge h ∈ H, there exists a node t of T such that h ⊆ λ(t),
and 3. for every v ∈ V , the set of nodes {t | v ∈ λ(t)} induces a connected subtree
of T .
Definition 8 (Width function). Let G = 〈V,H〉 be a hypergraph. A width function on G is a function f : P(V )→ R+ that assigns a positive real number to every nonempty subset of vertices of G. A width function f is monotone if f(X) ≤ f(Y ) whenever X ⊆ Y .
Let 〈T, λ〉 be a tree decomposition of G, and f a width function on G. The f - width of 〈T, λ〉 is max({f(λ(t)) | t node of T}). The f -width of G is the minimal f -width over all its tree decompositions.
In other words, a width function on a hypergraph G tells us how to assign weights to nodes of tree decompositions of G.
Definition 9 (Treewidth). Let f(X) = |X| − 1. The treewidth tw(G) of a hypergraph G is the f -width of G.
Let G = 〈V,H〉 be a hypergraph, and X ⊆ V . An edge cover for X is any set of hyperedges H ′ ⊆ H that satisfies X ⊆ ⋃ H ′. The edge cover number ρ(X) of X is the size of the smallest edge cover for X. It is clear that ρ is a width function.
Definition 10 ( [1, Chapter 2]). The generalized hypertree width hw(G) of a hypergraph G is the ρ-width of G.
Next, we define a relaxation of hypertree width known as fractional hypertree width, introduced by Grohe and Marx [20].
Definition 11 (Fractional edge cover). Let G = 〈V,H〉 be a hypergraph, and X ⊆ V . A fractional edge cover for X is a function γ : H → [0, 1] such that∑ v∈h∈H γ(h) ≥ 1 for every v ∈ X. We call ∑ h∈H γ(h) the weight of γ. The fractional edge cover number ρ∗(X) of X is the minimum weight over all fractional edge covers for X. It is known that this minimum is always rational [20].
Definition 12. The fractional hypertree width fhw(G) of a hypergraph G is the ρ∗-width of G.
For a class of hypergraphs H and a notion of width α, we write α(H) for the maximal α-width over the hypergraphs in H. If this is unbounded we write α(H) =∞; otherwise α(H) <∞.
All the above restrictions can be used to guarantee tractability for classes of CSP instances where all constraints are table constraints.
Theorem 1 ( [10, 17, 20]). Let H be a class of hypergraphs. For every α ∈ {hw, fhw}, any class of classic CSP instances whose hypergraphs are in H is tractable if α(H) <∞.
To go beyond fractional hypertree width, Marx [24, 25] recently introduced the concept of submodular width. This concept uses a set of width functions satisfying a condition (submodularity), and considers the f -width of a hypergraph for every such function f .
Definition 13 (Submodular width function). Let G = 〈V,H〉 be a hypergraph. A width function f on G is submodular if for every set X,Y ⊆ V , we have f(X) + f(Y ) ≥ f(X ∩ Y ) + f(X ∪ Y ).
Definition 14 (Submodular width). Let G be a hypergraph. The submodular width subw(G) of G is the maximum f -width of G taken over all monotone submodular width functions f on G.
For a class of hypergraphs H, we write subw(H) for the maximal submodular width over the hypergraphs in H. If this is unbounded we write subw(H) = ∞; otherwise subw(H) <∞.
Unlike for fractional hypertree width and every other structural restriction discussed so far, the running time of the algorithm given by Marx for classic CSP instances with bounded submodular width has an exponential dependence on the number of vertices in the hypergraph of the instance. The class of classic CSP instances with bounded submodular width is therefore not tractable. However, this class is what is called fixed-parameter tractable [11,12].
Definition 15 (Fixed-parameter tractable). A parameterized problem instance is a pair 〈k, P 〉, where P is a problem instance, such as a CSP instance, and k ∈ N a parameter.
Let S be a class of parameterized problem instances. We say that S is fixedparameter tractable (in FPT) if there is a function f of one argument, as well as a constant c, such that every problem 〈k, P 〉 ∈ S can be solved in time O(f(k)× |P |c).
The function f can be arbitrary, but must only depend on the parameter k. For CSP instances, a natural parameterization is by the size of the hypergraph of an instance, measured by the number of vertices. Since the hypergraph of an instance has a vertex for every variable, for every CSP instance P = 〈V,C〉 we consider the parameterized instance 〈|V |, P 〉.
Theorem 2 ( [24]). Let H be a class of hypergraphs. If subw(H) <∞, then a class of classic CSP instances whose hypergraphs are in H is in FPT.
The three structural restrictions that we have just presented form a hierarchy [20,24]: For every hypergraph G, subw(G) ≤ fhw(G) ≤ hw(G).
As the example below demonstrates, Theorem 1 does not hold for CSP instances with arbitrary global constraints, even if we have a fixed, finite domain.
Example 4. The NP-complete problem of 3-colourability [13] is to decide, given a graph 〈V,E〉, whether the vertices V can be coloured with three colours such that no two adjacent vertices have the same colour.
We may reduce this problem to a CSP with EGC constraints (cf. Example 1) as follows: Let V be the set of variables for our CSP instance, each with domain {r, g, b}. For every edge 〈v, w〉 ∈ E, we post an EGC constraint with scope {v, w}, parameterized by the function K such that K(r) = K(g) = K(b) = {0, 1}. Finally, we make the hypergraph of this CSP instance have low width by adding an EGC constraint with scope V parameterized by the function K ′ such that K ′(r) = K ′(g) = K ′(b) = {0, . . . , |V |}. This reduction clearly takes polynomial time, and the hypergraph G of the resulting instance has hw(G) = fhw(G) = subw(G) = 1.
As the constraint with scope V allows all possible assignments, any solution to this CSP is also a solution to the 3-colourability problem, and vice versa.
Likewise, Theorem 2 does not hold for CSP instances with arbitrary global constraints if we allow the variables unbounded domain size, that is, change the above example to k-colourability. With that in mind, in the rest of the paper we will identify properties of extensionally represented constraints that these structural restrictions exploit to guarantee tractability. Then, we are going to look for restricted classes of global constraints that possess these properties. To do so, we will use the following definitions.
Definition 16 (Constraint catalogue). A constraint catalogue is a set of global constraints. A CSP instance 〈V,C〉 is said to be over a constraint catalogue Γ if for every e[δ] ∈ C we have e[δ] ∈ Γ .
Definition 17 (Restricted CSP class). Let Γ be a constraint catalogue, and let H be a class of hypergraphs. We define CSP(H, Γ ) to be the class of CSP instances over Γ whose hypergraphs are in H.
Definition 17 allows us to discuss classic CSP instances alongside instances with global constraints. Let Ext be the constraint catalogue containing all table global constraints. The classic CSP instances are then precisely those that are over Ext. In particular, we can now restate Theorems 1 and 2 as follows.
Theorem 3. Let H be a class of hypergraphs. For every α ∈ {hw, fhw}, the class of CSP instances CSP(H,Ext) is tractable if α(H) < ∞. Furthermore, if subw(H) <∞ then CSP(H,Ext) is in FPT.
Lastly we test the efficacy of Problem 2 on the task of unsupervised image segmentation. We evaluate on the Grab-Cut data set, which consists of 30 color images. Each image has ground truth foreground/background labels. By “unsupervised”, we mean that no labeled data at any time in supervised or semi-supervised training, nor any kind of interactive segmentation, was used in forming or optimizing the objective. In our experiments, the image segmentation task is solved as unsupervised clustering of the pixels, where the goal is to obtain a partitioning of the pixels such that the majority of the pixels in each block share either the same foreground or the background labels.
Let V be the ground set of pixels of an image, π be an m-partition of the image, and {yv}v∈V as the pixel-wise ground truth label (yv = {0, 1} with 0 being background and 1
Number of iterations 5 10 15 20
T es
t a cc
ur ac
y (%
)
98.3
98.4
98.5
98.6
98.7
98.8
98.9
99
99.1
5-Partition on MNIST with Distributed NN
Submodular partition Random partition
Number of iterations 5 10 15 20
T es
t a cc
ur ac
y (%
)
97.8
98
98.2
98.4
98.6
98.8
99
99.2 10-Partition on MNIST with Distributed NN
Submodular partition Random partition
Figure 5: MNIST
Number of iterations 5 10 15 20 25 30 35 40 45 50 55
T es
t a cc
ur ac
y (%
)
15
20
25
30
35
40
45
50
30-Partition on TIMIT
Submodular partition Random partition
Number of iterations 5 10 15 20 25 30 35 40 45 50 55
T es
t a cc
ur ac
y (%
)
10
15
20
25
30
35
40
45
50
40-Block Partition on TIMIT with Distributed NN
Submodular partition Random partition
Figure 6: TIMIT
being foreground). We measure the performance of the partition π in the following two steps: (1) for each block i, predict ŷv for all the pixels v ∈ Aπi in the block as either 0 or 1 having larger intersection with the ground truth labels, i.e., predict ŷv = 1,∀v ∈ Aπi , if∑
v∈Aπi 1{yv = 1} ≥ ∑ v∈Aπi
1{yv = 0}, and predict ŷv = 0, ∀v ∈ Aπi otherwise. (2) report the performance of the partition π as the F-measure of the predicted labels {ŷv}v∈V relative to the ground truth label {yv}v∈V .
In the experiment we first preprocess the data by downsampling each image by a factor 0.25 for testing efficiency. We represent each pixel v as 5-dimensional features xv ∈ R5, including the RGB values and pixel positions. We normalize each feature within [0, 1]. To obtain a segmentation of each image we solve an instance of Problem 2 (0 < λ < 1) under the homogeneous setting using GeneralGreedMin (Alg. 9). We use the facility location function ffac as the objective for Problem 2. The similarity sv,a between the pixels v and a is computed as sv,a = C − ‖xv − xa‖2 with C = maxv,v′∈V ‖xv − x′v‖2 being the maximum pairwise Euclidean distance. Since the facility location function ffac is defined on a pairwise similarity graph, which requires O(|V |2) memory complexity. It becomes computationally infeasible for medium sized images. Fortunately a facility location function that is defined on a sparse k-nearest neighbor similarity graph performs just as well with k being very
sparse Wei et al. (2014a). In the experiment, we instantiate ffac by a sparse 10-nearest neighbor sparse graph, where each item v is connected only to its 10 closest neighbors.
A number of unsupervised methods are tested as baselines in the experiment, including k-means, k-medoids, graph cuts Boykov and Kolmogorov (2004) and spectral clustering Von Luxburg (2007). We use the RBF kernel sparse similarity matrix as the input for spectral clustering. The sparsity of the similarity matrix is k and the width parameter of the RBF kernel σ. We test with various choices of σ and k and find that the setting of σ = 1 and k = 20 performs the best, with which we report the results. For graph cuts, we use the MATLAB implementation Bagon (2006), which has a smoothness parameter α. We tune α = 0.3 to achieve the best performance and report the result of graph cuts using this choice.
lambda
0 0.2 0.4 0.6 0.8 1
a v e ra
g e d F
-m e a s u re
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95 Comparison with various lambda
m=5 m=10 m=15 m=20 m=25
Figure 7: m 5 10 15 20 25
a v e ra
g e d F
-m e a s u re
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95 Comparison with various m
Submodular partition K-means K-medoids Spectral clustering Graph cut
Figure 8
The proposed image segmentation method involves a hyperparameter λ, which controls the trade-off between the worst-case objective and the average-case objective. First we examine how the performance of our method varies with different choices of λ in Figure 7. The performance is measured as the averaged F -measure of a partitioning method over all images in the data set. Interestingly we observe that the performance smoothly varies as λ increases from 0 to 1. In particular the best performance is achieved when λ is within the range [0.7, 0.9]. It suggests that using only the worst-case or the average-case objective does not suffice for the unsupervised image segmentation / clustering task, and an improved result is achieved by mixing these two extreme cases. In the subsequent experiments we show only the result of our method with λ = 0.2. Next we compare the proposed approach with baseline methods on various m in Figure 8. In general, each method improves as m increases. Submodular partitioning method performs the best on almost all cases of m. Lastly we show in Figure 9 example segmentation results on several example images as well as averaged F-measure in the case of m = 15. We observe that submodular partitioning, in general, leads to less noisy and more coherent segmentation in comparison to the baselines.
Validation is the main relationship defined in ASHACL, usually producing a validation report containing the results of the validation. There are generally many different possible validation reports for a particular validation. Conformance checking is a simplified version of validation, usually producing a boolean result.
Validation and conformance checking can result in a failure. For example, a particular ASHACL processor might allow recursive shapes but result in a failure if it detects a loop. Failure can also result from resource exhaustion. Failures are reported through implementation-specific channels.
Validating a data graph against a shapes graph: Given G a data graph and S a shapes graph, a results structure for the validation of G against S is a combination of some multiset {{R1, . . . , Rn}} where {s1, . . . , sn} is the set of shapes in S and Ri is a results structure for the validation of G against si in S. A validation report for the validation of G against S is a validation report for some results structure for the validation of G against S. A data graph D conforms to a shapes graph S if and only if there is a results structure for the validation of D against S that contains no top-level validation results.
Validating a data graph against a shape in a shapes graph: Given G a data graph and s a shape in S a shapes graph, a results structure for the validation of G against s in S is a combination of some multiset {{R1, . . . , Rn}} where {t1, . . . , tn} is the complete targets from G for s in S and Ri is a results structure for the validation of ti using G against s in S. A data graph D conforms to a shape s in a shapes graph S if and only if there is a results structure for the validation of D against s in S that contains no top-level validation results.
Validating an RDF term using a data graph against a shape in a shapes graph: Given f an RDF term, D a data graph, and s a shape in a shapes graph S a results structure for the validation of f using G against s in S is a combination of some multiset {{R1, . . . , Rn}} where {c1, . . . , cn} is the constraints of s in S, Ri is an element of results(f, V,D, ci, s, S), and v is the value nodes of f with D for s in S, provided that s does not have a value for sh:deactivated in S that is a literal with datatype xsd:boolean and whose literal value is true. If s does have such a value for sh:deactivated in S then any results structure for the validation of f using G against s in S has no toplevel validation results. An RDF term f and a data graph D conform to a shape s in a shapes graph S if and only if there is a results structure for the validation of f using D against s in S that contains no top-level validation results.
Note: Although there can be multiple possible results structures for a particular validation, if any results structure for the validation has no top-level validation results they will all have no top-level validation results.
UI-nets were trained and evaluated with different seeding approaches. As depicted in Fig. 5 (a, b), the number of given seed points correlates with the overall segmentation quality (experiments (1, 2)). For an evaluation with the actual user model, the interactive user input version of the UI-net performs best as depicted in Fig. 5 (c) (experiment (3)). The UI-net trained with an interacting user model consistently performs better with each additional input provided by the user, continuously improving its segmentation results. Training an FCN with a user model which reacts on deficiencies in current segmentation results during training can therefore improve the overall segmentation result. As visualized in Fig. 5 (c,d), UI-net yields superior segmentation results w. r. t. the interactive and non-learning based GrowCut approach. We found a 6% average improvement in Dice score given the same images and active user model.
Bonilla-Silva, E. (2002). The linguistics of color blind
racism: How to talk nasty about blacks without sounding “racist”. Critical Sociology, 28(1-2):41–64. Cohen, J. (1968). Weighted Kappa: Nominal scale agreement provision for scaled disagreement or partial credit. Psychological bulletin, 70(4):213. De Smedt, T. and Daelemans, W. (2012). Pattern for Python. The Journal of Machine Learning Research, 13(1):2063–2067. Greevy, E. and Smeaton, S. (2004a). Text categorization of racist texts using a support vector machine. 7 es Journées internationales d’Analyse statistique des Données Textuelles.
Greevy, E. and Smeaton, A. F. (2004b). Classifying racist texts using a support vector machine. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 468–469. ACM. Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. Noreen, E. (1989). Computer-intensive methods for testing hypotheses: an introduction. Orrù, P. (2015). Racist discourse on social networks: A discourse analysis of Facebook posts in Italy. Rhesis, 5(1):113–133. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830. Pennebaker, J. W., Francis, M. E., and Booth, R. J. (2001). Linguistic inquiry and word count: LIWC 2001. Mahway: Lawrence Erlbaum Associates, 71:2001. Quasthoff, U. (1989). Social prejudice as a resource of power: Towards the functional ambivalence of stereotypes. Wodak, R.(éd.), Language, Power and Ideology. Amsterdam: Benjamins, pages 181–196. Razavi, A. H., Inkpen, D., Uritsky, S., and Matwin, S. (2010). Offensive language detection using multi-level classification. In Advances in Artificial Intelligence, pages 16–27. Springer. Reisigl, M. and Wodak, R. (2005). Discourse and discrimination: Rhetorics of racism and antisemitism. Routledge. Tulkens, S., Emmery, C., and Daelemans, W. (2016). Evaluating unsupervised Dutch word embeddings as a linguistic resource. In Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC). European Language Resources Association (ELRA). Van Dijk, T. A. (2002). Discourse and racism. The Blackwell companion to racial and ethnic studies, pages 145– 159. Zijlstra, H., Van Meerveld, T., Van Middendorp, H., Pennebaker, J. W., and Geenen, R. (2004). De Nederlandse versie van de ‘linguistic inquiry and word count’(LIWC). Gedrag & gezondheid, 32:271–281.
In this section we introduce basic concepts used throughout the paper.
We use the usual sequence definition, and denote a sequence by listing its elements, e.g. we write 〈a1, a2, . . . , an〉 for a (finite) sequence s : {1, . . . , n} → S of elements from some alphabet S, where s(i) = ai for any i ∈ {1, . . . , n}.
A. XES Event Logs
We use the XES standard definition of event logs, an overview of which is shown in Figure 3. XES defines an event log as a set of traces, which in itself is a sequence of events. The log, traces and events can all contain one or more attributes, which consist of a key and a value of a certain type. Event or trace attributes may be global, which indicates that the attribute needs to be defined for each event or trace respectively. A log contains one or more classifiers, which can be seen as labeling functions on the events of a log, defined on global event attributes. Extensions define a set of attributes on log, trace, or event level, in such a way that the semantics of these attributes are clearly defined. One can view XES extensions as a specification of attributes that events, traces, or event logs themselves frequently contain. XES defines the following standard extensions:
Concept Specifies the generally understood name of the event/trace/log (attribute ’Concept:name’). Lifecycle Specifies the lifecycle phase (attribute ’Lifecycle:transition’) that the event represents in a transactional model of their generating activity. The Lifecycle extension also specifies a standard transactional model for activities. Organizational Specifies three attributes for events, which identify the actor having caused the event (attribute ’Organizational:resource’), his role in the organization (attribute ’Organizational:role’), and the group or department within the organization
where he is located (attribute ’Organizational:group’).
Time Specifies the date and time at which an event occurred (attribute ’Time:timestamp’). Semantic Allows definition of an activity metamodel that specifies higher-level aggregate views on events (attribute ’Semantic:modelReference’).
We introduce a special attribute of type String with key label, which represents a high-level version of the generally understood name of an event. The concept name of a event is then considered to be a low-level name of an event. The Semantic extension closely resembles the label attribute, however, by specifying relations between low-level and high-level events in a meta-model, the Semantic extension assumes that all instances of a low-level event type belong to the same highlevel event type. The label attribute specifies the high-level label for each event individually, allowing for example one low-level event of low-level type Dishes & cups cabinet to be of high-level type Taking medicine, and another low-level event of the same type to be of high-level type Eating. Note that for some traces high-level annotations might be available, in which case its events contain the label attribute, while other traces might not be annotated. High-level interpretations of unannotated traces, by inferring the label attribute based on information that is present in the annotated traces, allow the use of unannotated traces for process discovery and conformance checking on a high level.
This section discusses the background details of the techniques utilized in this study and related previous studies carried out by others in various areas relevant to this research. The following subsections given below cover important key areas of this study.
In this paper, we introduced a novel end-to-end neural architecture for navigation in novel environments. Our architecture learns to map from first-person viewpoints and uses a planner with the learned map to plan actions for navigating to different goals in the environment. Our experiments demonstrate that such an approach outperforms other direct methods which do not use explicit mapping and planning modules. While our work represents exciting progress towards problems which have not been looked at from a learning perspective, a lot more needs to be done for solving the problem of goal oriented visual navigation in novel environments.
A central limitations in our work is the assumption of perfect odometry. Robots operating in the real world do not have perfect odometry and a model that factors in uncer-
tainty in movement is essential before such a model can be deployed in the real world.
A related limitation is that of building and maintaining metric representations of space. This does not scale well for large environments. We overcome this by using a multi-scale representation for space. Though this allows us to study larger environments, in general it makes planning more approximate given lower resolution in the coarser scales which could lead to loss in connectivity information. Investigating representations for spaces which do not suffer from such limitations is important future work.
In this work we have exclusively used DAGGER for training our agents. Though this resulted in good results, it suffers from the issue that the optimal policy under an expert may be unfeasible under the information that the agent currently has. Incorporating this in learning through guided policy search or reinforcement learning may lead to better performance specially for the case when the goal is not specified geometrically.
The proof is by induction on the formula structure of φ. Let M = 〈I, J,K〉, M |= φ, and M ′ = 〈I, J ′,K〉 for some J ⊆ J ′ ⊆ K . For the base case, consider an atomic sentence φ. If φ is of the form p(t1, . . . , tn), then p(ctI
1 , . . . , ctIn) ∈ J because M |= φ. By the fact
that J ′ ⊇ J we conclude that p(ctI 1 , . . . , ctIn) ∈ J ′ and hence M ′ |= φ. If φ is of the form t1 = t2 then M |= φ implies tI1 = t I 2, and thus M
′ |= φ. Note also that M |= φ implies φ 6= ⊥. This proves the claim for atomic formulas.
For the induction step, assume that M |= φ implies M ′ |= φ, for any sentence of depth n−1, and let φ be a sentence of depth n. We show that M |= φ implies M ′ |= φ. Suppose φ is the conjunction or disjunction of two sentences φ1 and φ2. Then φ1 and φ2 are sentences of depth n − 1. Hence, M |= φ1 implies M ′ |= φ1, and the same for φ2. Therefore, if M models both or one of the sentences then so does M ′, which implies M |= φ implies M ′ |= φ if φ is the conjunction or disjunction of two sentences. As for implication, since φ is factual we just need to consider the case where φ is of the form φ1 → ⊥, i.e., ¬φ1. Then, M |= ¬φ1 iff 〈I,K〉 |= ¬φ1 iff M ′ |= ¬φ1. This proves M |= φ implies M ′ |= φ if φ is an implication with ⊥ as its consequence. Eventually, consider a quantified sentence φ, i.e., φ is of the form ∀xφ1(x) or ∃xφ1(x). In this case, M |= φ implies M |= φ1(cε) and 〈I,K〉 |= φ1(cε), for all ε ∈ U , respectively M |= φ1(cε), for some ε ∈ U , in case of existential quantification. Since each of the sentences φ1(cε) is of depth n − 1, the same is true for M ′ by assumption, i.e., M ′ |= φ1(cε) and 〈I,K〉 |= φ1(cε), for all ε ∈ U , respectively M ′ |= φ1(cε), for some ε ∈ U . It follows that M |= φ implies M ′ |= φ also for quantified sentences φ of depth n, and therefore, for any sentence φ of depth n. This proves the claim.
The different notions of closure naturally extend to (sets of) QHT-interpretations. In particular, a total QHT-interpretation M = 〈I,K,K〉 is called total-closed in a set S of QHT-interpretations if 〈I, J,K〉 ∈ S for every J ⊆ K . A QHT-interpretation 〈I, J,K〉 is closed in a set S of QHT-interpretations if 〈I, J ′,K〉 ∈ S for every J ⊆ J ′ ⊆ K , and it is there-closed in S if 〈I,K,K〉 6∈ S and 〈I, J ′,K〉 ∈ S for every J ⊆ J ′ ⊂ K .
The first main result lifts the characterization of uniform equivalence for theories by HT-countermodels to the first-order case.
Theorem 5
Two first-order theories are uniformly equivalent iff they have the same sets of there-closed QHT-countermodels.
The proof idea is the same as in the propositional case, thus for space reasons the proof is skipped. The same applies to Theorem 6 and Proposition 7 (cf. (Fink 2009) for full proofs).
We next turn to an alternative characterization by a mixture of QHT-models and QHTcountermodels as in the propositional case. A QHT-countermodel 〈I, J,K〉 of a theory Γ is called QHT here-countermodel of Γ if 〈I,K〉 |= Γ. A QHT-interpretation 〈I, J,K〉 is an QHT equivalence-interpretation of a theory Γ, if it is a total QHT-model of Γ or a QHT here-countermodel of Γ. In slight abuse of notation, we reuse the notation Se, S ∈ {C,E} and e ∈ {c, a, s, u}, for respective sets of QHT-interpretations, and arrive at the following formal result:
Theorem 6 Two theories coincide on their QHT-countermodels iff they have the same QHT equivalenceinterpretations, in symbols Cs(Γ1) = Cs(Γ2) iff Es(Γ1) = Es(Γ2).
As a consequence of these two main results, we obtain an elegant, unified formal characterization of the different notions of equivalence for first-order theories under generalized answer-set semantics.
Corollary 5 Given two first-order theories Γ1 and Γ2, the following propositions are equivalent for e ∈ {c, a, s, u}: Γ1 ≡e Γ2; Ce(Γ1) = Ce(Γ2); Ee(Γ1) = Ee(Γ2).
Moreover, lifting the characterization of HT-countermodels provided in Proposition 2 to the first-order setting, allows us to prove a property, which simplifies the treatment of extended signatures.
Proposition 7 Let M be a QHT-interpretation over L on U . Then, M ∈ Es(Γ) for a theory Γ iff M |= Γφ(M) for some φ ∈ Γ, where Γφ(M) = {¬¬ψ | ψ ∈ Γ} ∪ {φ → (¬¬a → a) | a ∈ BP,CU}.
For QHT-models it is known that M |= Γ implies M |L |= Γ (cf. e.g., Proposition 3 in (de Bruijn et al. 2007)), hence M |L 6|= Γ implies M 6|= Γ, i.e., M |L ∈ Cs(Γ) implies M ∈ Cs(Γ). The converse direction holds for totality preserving restrictions (the proof appeared in (Fink 2008) and can also be found in (Fink 2009)):
Theorem 7 Let Γ be a theory over L, let L′ ⊃ L, and let M a QHT-interpretation over L′ such that M |L is totality preserving. Then, M ∈ Cs(Γ) implies M |L ∈ Cs(Γ).
Note that this property carries over to QHT-models, i.e., M |L |= Γ implies M |= Γ, if M |L is the restriction of M to L and this restriction is totality preserving. Otherwise, by the above result M 6|= Γ would imply M |L 6|= Γ. We remark that in (Fink 2008) it is erroneously stated informally that this property does not hold for QHT-models, however the counter-example given there is flawed (Example 5 in (Fink 2008)).
Identifying and overcoming issues with TIMEX2/TIMEX3 conversion, we have created a robust tool for converting TIMEX2 resources to TimeML/TIMEX3. Using this, we have generated a TIMEX3 resource with an order of magnitude more annotations than all previous resources put together. The resource contains new information about temporal expressions, and is helpful for training automatic timex annotation systems. We have made both the transduction tool and the TIMEX3 annotated results available, as part of a public repository. Version 1.0 of the data is packaged as a single release available on the project web page (distribution licenses apply).
The presented methods for survey modeling and object recognition rely on hand-coded knowledge about the domain. Concepts like “Signals are vertical” and “Signals intersect with the ground” are encoded within the algorithms either explicitly, through sets of rules, or implicitly, through the design of the algorithm. Such hard-coded, rule based approaches tend to be brittle and break down when tested in
new and slightly different environments. Furthermore, it can be difficult to extend an algorithm with new rule or to modify the rules to work in new environments. Based on these observations, we predict that more standard and flexible representations of facility objects and more sophisticated guidance based algorithms for object detection instead of a standard one will open the way to significant improvement in facility modeling capability and generality.
III. WIDOP PROTOTYPE
WiDOP platform is a Java platform presenting a knowledge based detection of objects in point clouds based on OWL ontology language, Semantic Web Rule Language, and 3D processing algorithms. It aims at combining geometrical analysis of 3D point clouds and specialist’s knowledge to get a more reliable facility model. In fact, this combination allows the detection and the annotation of objects contained in point clouds. WiDOP prototype takes in consideration the adjustment of the old methods and, in the meantime, profit from the advantages of the emerging cutting edge technology. From the principal point of view, our system still retains the storing mechanism within the existent 3D processing algorithms, in addition, suggest a new field of detection and annotation, where we are getting a real-time support from the target scene knowledge. Add to that, we suggest a collaborative Java Platform based on semantic web technology (OWL, RDF, and SWRL) and knowledge engineering in order to handle the information provided from the knowledge base and the 3D packages results.
The field of the Deutsch Bahn railway scene is treated for object detection. The objective of the system consists in creating, from a set of point cloud files, from an ontology that contains knowledge about the DB railway objects and 3D processing algorithms, an automatic process that produces as output a set of tagged elements contained in the point clouds.
The process enriches and populates the ontology with new individuals and relationships between them. In order to graphically represent these objects within the scene point clouds, a VRML model file [5] is generated and visualized within the prototype where the color of objects in the VRML file represents its semantic definition. The resulting ontology contains enough knowledge to feed a GIS system, and to generate IFC file [6] for CAD software. As seen in Figure 1, the created system is composed of three parts.
 Generation of a set of geometries from a point could file based on the target object characteristics  Computation of business rules with geometry, semantic and topological constrains in order to annotate the different detected geometries.  Generation of a VRML model related to the scene within the detected and annotated elements
To rich such a target, three main steps aim at detecting and identifying objects are established:
 From 3D point clouds to geometric elements.
 From geometry to topological relations.
 From geometric and/or topological relations to semantic elements annotation.
As a first impression, the system responds to the target requirement since it would take a point cloud of a facility as input and produce a fully annotated as-built model of the facility as output. In the next, we focus on the core of the WiDOP prototype which is materialized via an ontology base structure to guide the 3D scene reconstruction process.
IV. ONTOLOGY BASED PROTOTYPE
In recent years, formal ontology has been suggested as a solution to the problem of 3D objects reconstruction from 3D point clouds [21]. In this area, ontology structure was defined as a formal representation of knowledge by a set of concepts within a domain, and the relationships between those concepts. It is used to reason about the entities within that domain, and may be used to describe the domain. Conventionally, ontology presents a "formal, explicit specification of a shared conceptualization" [22]. Well-made ontology owns a number of positive aspects like the ability to define a precise vocabulary of terms, the ability to inherit and extends exiting ones, the ability to declare relationships between defined concepts and finally the ability to infer new relationship by reasoning on existing ones. Through the scientific community, the basic strength
of formal ontology is their ability to reason in a logical way based on Description Logics DL. The last one presents a form of logic to reason on objects. In fact, despite the richness of OWL's set of relational properties, the axiom does not cover the full range of expressive possibilities for object relationships that we might find. For that, it is useful to declare a relationship in term of conditions or even rules. Some of the evolved languages are related to the semantic web rule language (SWRL) and advanced Jena rules [23]. SWRL is a proposal as a Semantic Web rules language, combining sublanguages of the OWL Web Ontology Language with the Rule Markup Language [24].
In this section, we show that hinge loss minimization is not consistent in our setup, that is, that it does not lead to arbitrarily small excess error. We let Bd1 denote the unit ball in R
d. In this section, we will only work with d = 2, thus we set B1 = B21 .
Recall that the τ -hinge loss of a vector w ∈ <d on an example (x, y) ∈ <d × {−1, 1} is defined as follows:
`τ (w, x, y) = max
{ 0, 1− y(w · x)
τ } For a distribution D̃ over <d × {−1, 1}, we let LD̃τ denote the expected hinge loss over D, that is
LD̃τ (w) = E(x,y)∼D̃`τ (w, x, y).
If clear from context, we omit the superscript and write Lτ (w) for LD̃τ (w). Let Aτ be the algorithm that minimizes the empirical τ -hinge loss over a sample. That is, for W = {(x1, y1), . . . , (xm, ym)}, we have
Aτ (W ) ∈ argminw∈B1 1 |W | ∑
(x,y)∈W
`τ (w, x, y).
Hinge loss minimization over halfspaces converges to the optimal hinge loss over all halfspace (it is “hinge loss consistent”). That is, for all > 0 there is a sample size m( ) such that for all distributions D̃, we have
EW∼D̃m [L D̃ τ (Aτ (W ))] ≤ min w∈B1 LD̃τ (w) + .
In this section, we show that this does not translate into an agnostic learning guarantee for halfspaces with respect to the 0/1-loss. Moreover, hinge loss minimization is not even consistent with respect to the 0/1-loss even when restricted to a rather benign classes of distributions P . Let Pβ be the class of distributions D̃ with uniform marginal over the unit ball in <2, the Bayes classifier being a halfspace w, and satisfying the Massart noise condition with parameter β. We show that there is a distribution D̃ ∈ Pβ and an ≥ 0 and a sample size m0 such that hinge loss minimization will output a classifier of excess error larger than on expectation over samples of size larger than m0. More precisely, for all m ≥ m0:
EW∼D̃m [L D̃ τ (Aτ (W ))] > min
w∈B1 errD̃(w) + .
Formally, our lower bound for hinge loss minimization is stated as follows.
Theorem 3 (Restated). For every hinge-loss parameter τ ≥ 0 and every Massart noise parameter 0 ≤ β < 1, there exists a distribution D̃τ,β ∈ Pβ (that is, a distribution overB1×{−1, 1} with uniform marginal over B1 ⊆ <2 satisfying the β-Massart condition) such that τ -hinge loss minimization is not consistent on Pτ,β with respect to the class of halfspaces. That is, there exists an ≥ 0 and a sample size m( ) such that hinge loss minimization will output a classifier of excess error larger than (with high probability over samples of size at least m( )).
In the section, we use the notation hw for the classifier associated with a vector w ∈ B1, that is hw(x) = sign(w · x), since for our geometric construction it is convenient to differentiate between the two. The rest of this section is devoted to proving the above theorem.
A class of distributions
Let η = 1−β2 . We define a family Pα,η ⊆ Pβ of distributions D̃α,η, indexed by an angle α and a noise parameter η as follows. We let the marginal be uniform over the unit ball B1 ⊆ <2 and let the Bayes optimal classifier be linear h∗ = hw∗ for a unit vector w∗. Let hw be the classifier that is defined by the unit vector w at angle α from w∗. We partition the unit ball into areas A, B and D as in the Figure 2. That is A consists of the two wedges of disagreement between hw and hw∗ and the wedge where the two classifiers agree is divided in B (points that are closer to hw than to hw∗) and D (points that are closer to hw∗ than to hw). We now “add noise η” at all points in areas A and B and leave the labels deterministic according to hw∗ in the area D.
More formally, points at angle between α/2 and π/2 and points at angle between π + α/2 and −π/2 from w∗ are labeled with hw∗(x) with (conditional) probability 1. All other points are labeled −hw∗(x) with probability η and hw∗(x) with probability (1− η).
Useful lemmas
The following lemma relates the τ -hinge loss of unit length vectors to the hinge loss of arbitrary vectors in the unit ball. It will allow us to focus our attention to comparing the τ -hinge loss of unit vectors for τ > τ0, instead of having to argue about the τ0 hinge loss of vectors of arbitrary norms in B1.
Lemma 7. Let τ > 0 and 0 < λ ≤ 1. Letw andw∗ be two vectors of unit length. ThenLτ (λw) < Lτ (λw∗) if and only if Lτ/λ(w) < Lτ/λ(w∗).
Proof. By the definition of the hinge loss, we have
`τ (λw, x, y) = max
( 0, 1− y(λw · x)
τ
) = max ( 0, 1− y(w · x)
τ/λ
) = `τ/λ(w, x, y).
Lemma 8. Let τ > 0, for any D̃ ∈ Pα,η let wτ denote the halfspace that minimizes the τ -hinge loss with respect to D̃. If θ(w∗, wτ ) > 0, then hinge loss minimization is not consistent for the 0/1-loss.
Proof. First we show that the hinge loss minimizer is never the vector 0. Note that LD̃τ (0) = 1 (for all τ > 0). Consider the case τ ≥ 1, we show that w∗ has τ -hinge loss strictly smaller than 1. Integrating the hinge loss over the unit ball using polar coordinates, we get
LD̃τ (w∗) < 2
π
( (1− η) ∫ 1 0 ∫ π 0 (1− z τ sin(ϕ)) z dϕ dz + η ∫ 1 0 ∫ π 0 (1 + z τ sin(ϕ)) z dϕ dz ) = 2
π
( (1− η) ∫ 1 0 ∫ π 0 z − z 2 τ sin(ϕ) dϕ dz + η ∫ 1 0 ∫ π 0 z + z2 τ sin(ϕ) dϕ dz ) = 1 + 2
π
( (1− 2η) ∫ 1 0 ∫ π 0 −z 2 τ sin(ϕ) dϕ dz ) = 1− 2
π
( (1− 2η) ∫ 1 0 ∫ π 0 z2 τ sin(ϕ) dϕ dz ) < 1.
For the case of τ < 1, we have Lτ (τw∗) = L1(w∗) < 1.
Thus, (0, 0) is not the hinge-minimizer. Then, by the assumption of the lemma wτ has some positive angle γ to the w∗. Furthermore, for all 0 ≤ λ ≤ 1, LD̃τ (wτ ) < LD̃τ (λw∗). Since w 7→ LD̃τ (w) is a continuous function we can choose an > 0 such that
LD̃τ (wτ ) + /2 < LD̃τ (λw∗)− /2.
for all 0 ≤ λ ≤ 1 (note that the set {λw∗ | 0 ≤ λ ≤ 1} is compact). Now, we can choose an angle µ < γ such that for all vectors v at angle at most µ from w∗, we have
LD̃τ (v) ≥ min 0≤λ≤1 LD̃τ (λw∗)− /2
Since hinge loss minimization will eventually (in expectation over large enough samples) output classifiers of hinge loss strictly smaller than LD̃τ (wτ ) + /2, it will then not output classifiers of angle smaller than µ to w∗. By Equation 2, for all w, errD̃(w)− errD̃(w ∗) > β θ(w,w ∗)
π , therefore, the excess error of a the classfier returned by hinge loss minimization is lower bounded by a constant β µπ . Thus, hinge loss minimization is not consistent with respect to the 0/1-loss.
Proof of Theorem 3
We will show that, for every bound on the noise η0 and for every every τ0 ≥ 0 there is an α0 > 0, such that the unit length vector w has strictly lower τ -hinge loss than the unit length vector w∗ for all τ ≥ τ0. By Lemma 7, this implies that for every bound on the noise η0 and for every τ0 there is an α0 > 0 such that for all 0 < λ ≤ 1 we have Lτ0(λw) < Lτ0(λw∗). This implies that the hinge minimizer is not a multiple of w∗ and so is at a positive angle to w∗. Now Lemma 8 tells us that hinge loss minimization is not consistent for the 0/1-loss.
w*
w
⍺
⍺
A
A
⍺/2
hw hw*
B
BD
D
In the sequel, we will now focus on the unit length vectors w and w∗ and show how to choose α0 as a function of τ0 and η0. We let cA denote the hinge loss of hw∗ on one wedge (one half of) area A when the labels are correct and dA that hinge loss on that same area when the labels are not correct. Analogously, we define cB,dB, cD and dD. For example, for τ ≥ 1, we have (integrating the hinge loss over the unit ball using polar coordinates)
Now we can express the hinge loss of both hw∗ and hw in terms of these quantities. For hw∗ we have
There have been a lot of works aimed at finding a stable way to train GANs. Radford et al. [11] proposed a stable family of architectures called deep convolutional generative adversarial networks (DCGANs). We show that such constraints on architectures can be relaxed while still being able to achieve stability in the training process. In an alternate direction, a number of works have focused on developing specific objective functions that improve stability and performance of GANs. Salimans et al. [16] introduced a variety of techniques to improve the quality of samples. Che et.al [12] proposed a family of regularizers to address the missing modes problem in GANs. Zhao et.al [17] introduced energy based GAN framework which is more stable to train. Metz et al. [14] developed unrolled GANs taking inspiration from game theory literature. However, it suffers from slow performance due to the requirement of multiple unrolling steps in each iteration.
Recently, we have seen a series of works based on imposing a Lipschitz constraint on the discriminator function. Guo-Jun Qi [20] introduced LS-GAN with the idea of maintaining a margin between losses assigned to real and fake samples. Specifically, they enforce the following condition (the discriminator is used as the loss function in this setting) -
Dθ(Gφ(z))−Dθ(x) ≥ ∆(x, Gφ(z)) where the ∆(.) is some distance metric. Further, they impose a Lipschitz constraint on both G and D using weight decay. This leads to D having non-vanishing gradients everywhere between all real and fake sample pairs in the limit. Arjovsky et al. [18] proposed Wasserstein GAN which uses Earth-Mover distance as the objective to address problems with the vanilla objective. Their procedure requires the discriminator to be a Lipschitz function and they use weight clipping to achieve that. Gulrajani et al. [19] proposed an extension to address various shortcomings of the original WGAN and they impose the following condition on D -
||∇x̂Dθ(x̂)||2 ≈ 1
where x̂ = (ǫ)x + (1 − ǫ)Gφ(z) is some point between randomly chosen real and fake sample pairs. This leads to D having norm 1 gradients everywhere between all real and fake sample pairs in the limit. Notice that this behavior is very similar to that of LSGAN’s discriminator function. We argue that such constraints are too restrictive and can encourage poor generator functions. This is demonstrated in section 4 where improved WGAN fails to accurately capture a simple data distribution. Additionally, the requirement of multiple updates to D at each step makes it slow. Our proposed method address a fundamental game theoretic issue in the GAN training process and hence it can be used on top of a variety of objective functions. We show that it is possible to get excellent results using just the vanilla objective function and a single update to D.
We have proposed a deep conflation model for matching two text fields in business data analytics, with two different variants of feature extractors, namely, long-short-term memory (LSTM) and convolutional neural networks (CNN). The model encodes the input text from raw character-level into finite dimensional feature vectors, which are used for computing the corresponding relevance scores. The model is learned in an end-to-end manner by back propagation and stochastic gradient descent. Since both LSTM and CNN feature extractors retain the order information in the text, the deep conflation model achieve superior performance compared to the bag-ofcharacter (BoC) baseline.
The depth-only dataset of Mian et al. [34] includes 3D mesh models of 5 objects and 50 test depth images acquired with an industrial range scanner. The test scenes contain only the modeled objects that occlude each other. A similar dataset is provided by Taati et al. [46]. The Desk3D dataset [3] comprises of 3D mesh models for 6 objects which are captured in over 850 test depth images with occlusion, clutter and similarly looking distractor objects. The dataset was obtained with an RGB-D sensor, however only the depth images are publicly available.
The IKEA dataset by Lim et al. [30] provides RGB images with objects being aligned with their exactly matched 3D models. Crivellaro et al. [12] supply 3D CAD models and annotated RGB sequences with 3 highly occluded and texture-less objects. Muñoz et al. [36] provide RGB sequences of 6 texture-less objects that are each imaged in isolation against a clean background and without occlusion. Further to the above, there exist RGB datasets such as [13, 50, 38, 25], for which the ground truth is provided only in the form of 2D bounding boxes.
As mentioned in Section 1, recent studies [52, 11, 43, 42] show that learning submodular functions can be easier or harder depending on the learning setting.
A general outline of various learning settings is given in [76, 43] — here, we give only a very brief overview. To start, learning may involve several families of functions F , H, and T members of which are mappings from 2V to R. There is some true function f ∈ F to be learnt based on information obtained via samples of the form (A, f(A)) for A ⊆ V . One wishes to produce an approximation f̃ ∈ H to f that is good in some way. Learning submodular functions has been studied under a number of possible variants. For example, there is typically a probability distribution Pr over subsets of V (i.e., Pr(S = A) ≥ 1 and ∑A⊆V Pr(S = A) = 1 where S is a random variable). A set of samples D = {(Ai, f(Ai)}i is obtained via this distribution. The distribution Pr might be unknown [11], or might be known (and in such case, might be assumed to be uniform [43, 42]). The quality of learning could be judged over all 2n points or over some fraction, say 1−β, of the points, for β ∈ [0, 1]. In general, there is no specificity on the particular set of points, or the particular kind of points, that should be learnt as long as at least a (probability distribution measured) fraction 1− β of them are learnt. Learning itself happens with some probability 1− δ. I.e., there is some probability δ that the learning will not succeed. While learning asks for a function in f̃ ∈ H that is good, we might judge f̃ relative only to the best function f̂ ∈ T (the touchstone class). For example, in agnostic learning [76], we acknowledge that it might be difficult to show that learning is good relative to all of F (say due to noise) but still feasible to show that learning is good relative to the best within T . Also, there are a variety of ways to judge goodness. In [11], goodness is judged multiplicatively, meaning for a set A ⊆ V we wish that f̃(A) ≤ f(A) ≤ g(n)f(A) for some function g(n), and this is typically a probabilistic condition (i.e., measured by distribution Pr, goodness, or f̃(A) ≤ f(A) ≤ g(n)f(A), should happen on a fraction at least 1− β of the points). Alternatively, goodness may also be measured by an additive approximation error, say by a norm. I.e., defining errp(f, f̃) = ‖f − f̃‖p = (EA∼Pr[|f(A)− f̃(A)| p ])1/p, we may wish errp(f, f̃) < for p = 1 or p = 2. In the PAC (probably approximately correct) model, we probably (δ > 0) approximately ( > 0 or g(n) > 1) learn (β = 0) with a sample or algorithmic complexity that depends on δ and g(n). In the PMAC (probably mostly approximately correct) model [11], we also “mostly” β > 0 learn. In agnostic learning, F ⊇ H = T . Let Cn be the space of all submodular functions. In some cases F ⊇ Cn = H so we wish to learn the best submodular approximation to a non-submodular function. In other cases, F = Cn ⊆ T ⊆ H meaning we are allowed to deviate from submodularity as long as the error is small.
In the machine learning community, H may be a parametric family of submodular functions. For example, given a fixed set of component submodular functions, say {fi}`i=1 one may with to learn only the weights of a mixture {wi}i to produce f = ∑ i wifi where wi ≥ 0 for all i to ensure submodularity is preserved. What is learnt is only the coefficients of the mixture, not the components, so the flexibility of the family is determined by the diverseness and quantity of components used. Empirically, experiments that learn submodularity for various data science applications [140, 92], has been more successful than simply hand-designing a fixed submodular function. This is true both for image [150] and document [92] summarization tasks. There also has been some initial work on learnability bounds in [92]. Learning just the mixture coefficients of a mixture of submodular functions, while keeping the component functions themselves fixed, is only as flexible as the set of component functions allows, however. Given a small (or indiscriminately selected and hence potentially redundant) number of components, the family over which one can learn might be limited. As a result, one might need add a very large number of components before one obtains a sufficiently powerful family.
An alternative approach to learning a mixture that alleviates to some extent the above problem is to learn over a richer parametric family, and this is where DSFs hold promise. An approach to learning DSFs, therefore, is to learn within its parametric family, so H = DSFk for some finite k and where fw ∈ DSF is parameterized by the vector w that determines the topology (e.g., number and width of layers) of the network, the numeric parameters (set of matrices) within that topology, and the set of concave functions {φu}u. As shown in the present paper, DSFs represent a strictly larger family than SCMMs. Therefore, even in the mixture case above where the components may also be learnt, there are DSFs that are unachievable by SCMMs. In addition, by Theorem 5.12, a DSF rather than a mixture can be applied to a fixed set of input submodular components (e.g., some of which might be simple indicators of the form gu(A) = 1u∈A and others could be cycle matroid rank functions in order to reduce any chance of the unachievability mentioned
in Theorem 6.26). Even in cases where a DSF can be represented by an SCMM, DSFs may be a far more parsimonious representation of classes of submodular functions and hence a more efficient family over which to learn, analogous to results in DNNs showing the need for exponentially many hidden units for shallow networks to implement a network with more layers [40].
Suppose f ∈ Cn is a target submodular function, fw ∈ DSFk is a parameterized k-layer DSF, D = {(Si, yi)}i is a training set consisting of subsets Si ⊆ V and valuations yi = f(Si) for the target function and that is drawn from distribution Pr. An empirical risk minimization (ERM), or regression, style of learning is obtained a standard way:
min w∈W J(w) = ∑ i L(yi, fw(Si)) + ‖w‖ (107)
where L(·, ·) is a loss function and ‖w‖ is a norm on the parameters. Obvious candidates for the loss would be squared loss, or L1 loss, and the norm can also be chosen to prefer smaller values for w. Given the objective J(w) one may proceed using, for example, projected stochastic gradient descent, where at each step we project the weights w into W which corresponds to the non-negative orthant for parameters other than m± to ensure submodularity is retained. Under this approach, and with an appropriate regularizer, it may be feasible to obtain generalization bounds in some form [135] as is often found in statistical machine learning settings. Note that, depending on the loss L used, this approach may be tolerant of noisy estimates of the function, where, say, yi = fw(Si) + and where is noise, somewhat analogous to how it is possible to optimize a noisy submodular function [59]. Alternatively, one could analyze it under an agnostic learning setting.
Under many distribution assumptions, such as when Pr is the uniform distribution over 2V , then as the training set gets larger, we approach the case where there are O(2|V |) distinct samples, and the goal is to learn the function at all points. For large ground sets, certain learning settings might become infeasible in practice due to the curse of dimensionality. As mentioned above, there are learning settings that ask only for a fraction 1− β of the points to be learnt, but without a mechanism to specify which fraction.
In many practical learning situations, however, access to an oracle function h(A), or training data that utilizes h’s evaluations, might not be available. Even if h available, such a learning setting might be overkill for certain applications, as we might not need a submodular function fw to be accurate at all points A ⊆ V . One example is in summarization applications [92, 150] where we wish to learn a submodular function fw that, when maximized subject to a cardinality constraint, produces a set that is valuated highly by the true submodular function relative to other sets of that size. Such a set should be diverse and high quality. In this case, one does not need fw to be an accurate surrogate for f except on sets A for which f is large. More precisely, instead of trying to learn f everywhere, we seek only to learn the parameters w of a function so that if B ∈ argmaxA⊆V :|A|≤k fw(A), then h(B) ≥ αh(A∗) for some α ∈ [0, 1] where A∗ ∈ argmaxA⊆V :|A|≤k h(A). This setting puts fewer constraints on what is needing to be learnt than the regression approach and hence should correspondingly be easier. This is somewhat analogous to discriminative learning where the entire distribution over input and output variables is not needed and instead only a conditional distribution (or a deterministic mapping from input to output) is required.
The max-margin approach [140, 92, 150] is appropriate to this problem and is applicable to learning DSFs. Given an unknown but desired non-negative submodular function f ∈ Cn, we are given a set of representative sets S = {S1, S2, . . .}, with Si ⊆ V and where each S ∈ S is scored highly by f(·). Unlike the regression approach, we do not need the actual evaluations f(Si). It might be, for example, that the sets are selected summaries chosen by a human annotator from a larger set. A matroid analogy is to learn a matroid using a set of independent sets of a particular size, say `. If M ′ = (V, I ′) is a matroid of rank `′ > `, then M = (V, I) is also a matroid where I = {I ∈ I ′ : |I| ≤ `}.
In max-margin approach, we learn the parameters w of fw in an attempt to make, for all S ∈ S, fw(S) high, while for A ∈ 2V , fw(A) is lower by a given loss. More precisely, we ask that for S ∈ S and A ∈ 2V , fw(S) ≥ fw(A) + `S(A). The loss is chosen so that `S(S) = 0, so that `S(A) is very small whenever A is close to S (e.g., if A is also a good summary), and so that `S(A) is large when A is considered much worse (e.g. if A is a poor summary). Achieving the above is done by maximizing the loss-dependent margin, and reduces to finding parameters so that fw(S) ≥ maxA∈2V [fw(A) + `S(A)] is satisfied for S ∈ S. The task of finding the maximizing set is known as loss-augmented inference (LAI) [146, 160], which for general `(A) is
NP-hard. With regularization, the optimization becomes:
min w∈W ∑ S∈S L ( max A∈2V [fw(A) + `S(A)]− fw(S) ) + λ 2 ||w||22. (108)
where L is a classification loss function such as the logistic (L(x) = log(1 + exp(−x))) or hinge (L(x) = max(0, x)) loss. If it is the case that fw(S) is linear in w (such as when w are mixture parameters in an SCMM as was done in [140, 92, 150]), and if the maximization can is done exactly, then this constitutes a convex minimization procedure. In general, however, there are several complications.
Firstly, the LAI problem maxA∈2V [fw(A) + `S(A)] may be hard. Given a submodular function for the loss, as was done in [92], then the greedy algorithm offers the standard 1 − 1/e approximation guarantee for LAI. On the other hand, a submodular function is not always natural for the loss. Recall above that `S(A) should be large when A is considered a poor set relative to S (e.g. if A is a poor summary). If it is the case that one may get an assessment of A, say via a surrogate f̃ of the ground truth function f , then one may use `S(A) = κ− f̃(A) but this, to the extent that f̃ needs to represent f , approaches the labeling needs of the ERM/regression approach above. If f̃ is submodular, then κ − f̃ is supermodular, and in this case solving maxA∈2V \S [f(A) + `(A)] involves maximizing the difference between two submodular functions, and the submodular-supermodular procedure [110, 64] can be used although this procedure does not have guarantees in general.
Secondly, when fw is not linear in w, the above problem is not convex. Given the enormous success of deep neural networks in addressing non-convex optimization problems, however, this should not be daunting. Indeed, given an estimation to Ã ∈ argmaxA∈2V [fw(A) + `S(A)], we can easily obtain an approximate subgradient of weights dw ∈ ∂w(fw(Ã)− fw(S) + λ/2‖w‖22) to be used in a projected stochastic subgradient descent procedure. For a DSF, this subgradient can be easily computed using backpropagation, similar to the approach of [121]. Like in the mixtures case, we must use projected descent to ensure w ∈ W and submodularity is preserved. Recall, however that the weights corresponding to m±(A) may be left negative if they so choose. Preliminary experiments in learning DSFs in this fashion were reported in [37] and show encouraging results.
As an additional benefit, many of the concave functions mentioned in Section 3.1 are parameterized themselves, and these parameters may also be the target of stochastic gradient based learning. In such case, not only the weights but also the concave functions of a DSF may be learnt.
Given the ongoing research on the non-convex learning of DNNs, which have achieved remarkable results on a plethora of machine learning tasks [87, 54], and given the similarity between DSFs and DNNs, we may leverage the same DNN learning techniques to learn DSFs. This includes stochastic gradient descent, convolutional linear maps, momentum, dropout, batch normalization, unsupervised pre-training, learning rate scheduling such as AdaGrad/Adam, convolutional matrix patterns, mini-batching, and so on. In some cases these methods might need to be modified (e.g., stochastic projected gradient descent to ensure the function remains submodular). Moreover, the suitability of fast GPU computing to the matrix-matrix multiplications necessary to evaluate DSFs should also be a benefit. Lastly, the many toolkits that support DNN training (such as Tensorflow, Theano, Torch, Caffe, CNTK, and so on), and that include automatic symbolic differentiation and semi-differentiation (for non-differentiable functions) for backpropagation-style parameter learning can easily be used to train DSF. All of these techniques and software may be leveraged to DSF’s benefit, and is true both for the regression and max-margin setting.
We define the DGW to be the solutions of Eq. (6), for different α = √ s. In addition, we require two other properties. Firstly, we want the wave to be causal, i.e. to have an initial starting point in time. Secondly, in many applications, the wave propagation is affected by an attenuation over time. We thus introduce a damping term. The DGW defined in the graph spectral domain is thus
W̃s(λℓ, t) = H(t) e−βt cos ( t arccos ( 1− sλℓ
2
)) , (12)
where H(t) is the Heaviside function and e−βt is the damped decaying exponential function in time.
The damping term has two remarkable effects. Firstly, it lower the importance of the chosen boundary conditions in time (e.g. periodic or reflective) as the wave vanishes before touching them. Secondly, it favors the construction of a frame of DGW: we will see in the following that β is involved in the lower frame bound of the DGW.
Firstly, we initialize our word embedding with Skip-gram or Glove on the unannotated corpus. We get the number of word senses with the WordNet (Miller, 1992) and we initialize the embeddings of all the senses to be their word embeddings.
In this trial, we started the robot near the ocean floor, which was sparsely populated with sea plants and corals. We see the robot manages to keep its focus on sea life, while not wasting time over sand.
Analysis of other CGH datasets. We report the differences between the reconstructed trees also based on datasets of gastrointestinal and oral cancer ([69, 148] respectively). In the case of gastrointestinal stromal cancer, among the 13 CGH events considered in [69] (gains on 5p, 5q and 8q, losses on 14q, 1p, 15q, 13q, 21q, 22q, 9p, 9q, 10q and 6q), oncotrees identify the path progression
1p− → 15q− → 13q− → 21q−
while CAPRESE reconstructs the branch
1p− → 15q− 1p− → 13q− → 21q − .
In the case of oral cancer, among the 12 CGH events considered in [148] (gains on 8q, 9q, 11q, 20q, 17p, 7p, 5p, 20p and 18p, losses on 3p, 8p and 18q), the reconstructed trees differ since oncotrees identifies the path
8q+→ 20q+→ 20p+
while our algorithm reconstructs the path
3p− → 7p+→ 20q+→ 20p+ .
These examples show that CAPRESE provides important differences in the reconstruction compared to oncotrees.
This technique removes common words (articles, prepositions, determiners,…) such as “ھک”,”ھب”,”زا”. These words sometimes have very little influence in meaning but sometimes play somewhat important role in the text.
Two kind of deep and shallow stop-word-removal tables tested to find the true impact of this technic.
There have been several attempts to use heuristics for estimating a good learning rate at each iteration of gradient descent. These either attempt to speed up learning when suitable or to slow down learning near a local minima. Here we consider the latter.
ar X
iv :1
21 2.
57 01
v1 [
cs .L
G ]
2 2
D ec
2 01
2
When gradient descent nears a minima in the cost surface, the parameter values can oscillate back and forth around the minima. One method to prevent this is to slow down the parameter updates by decreasing the learning rate. This can be done manually when the validation accuracy appears to plateau. Alternatively, learning rate schedules have been proposed [1] to automatically anneal the learning rate based on how many epochs through the data have been done. These approaches typically add additional hyperparameters to control how quickly the learning rate decays.
We convert the discrete pixel intensities from {0, . . . , 255}D to continuous noisy images from [0, 256]D by adding a uniform noise u ∈ [0, 1]D to each pixel intensity. The computed log-probability density will be then a lower bound for the log-probability mass (Theis et al., 2015):
∫ [0,1]D log p(x+ u) du ≤ log ∫ [0,1]D p(x+ u) du (3)
Finally, before feeding the pixel intensities to a network, we divide the pixel intensities by 256. This is a simple nonvolume preserving transformation. If the noisy image is z2 = x+u and the image with scaled intensities is z1 = z2256
then the log-probability density of the noisy image is:
log pZ2(z2) = log pZ1(z1)− log |det ∂z2 ∂z1 | (4)
= log pZ1(z1)−D log 256 (5)
where D is the number of dimensions in the image (e.g., D = 32 × 32 × 3). For example, if an uninformed model assigns uniform probability density pZ1(z1) = 1 to all z1 ∈ [0, 1]D, the model would require log2(256) = 8 bits/dim to describe an image. The reported negative logprobability density then corresponds to an upper bound on compression loss in bits/dim.
The details of our conversational speech recognition system are described elsewhere [20], so we only give a brief summary here. The system employs independent decodings by diverse acoustic models, including convolutional neural net (CNN) and bidirectional long short-term memory (BLSTM) models that differ by model architecture, number of senones, amount of training data, and other metaparameters. Decoding uses a pruned 4- gram N-gram language model (LM) to generate lattices, which are then expanded into 500-best lists using a larger N-gram
LM. The N-best lists are rescored with multiple LSTM-LMs operating in forward and backward directions. Model scores are combined log-linearly at the utterance level and converted to posterior probabilities represented as word confusion networks. The various subsystems making up the final system are selected in a greedy search, and their weights are optimized via an expectation-maximization algorithm, on development data. The acoustic training data comprises all the publicly available CTS data (about 2000 hours), while the LMs are additionally trained on Broadcast News and Web data from U. Washington. The individual subsystems (based on different acoustic models) achieve word error rates between 6.4% and 7.7% on the Switchboard evaluation set, and between 12.2% and 17.0% on the CallHome portion. Combined, the system achieves 5.8% and 11.0% WER, respectively.
As described in Section 2.2 various syntax features were used namely, Part-of-Speech tags, brown clusters of TweetNLP project. However, these didn’t perform well in cross validation. Hence, they were dropped from the final system. While performing grid-search as mentioned in Section 2.4, keeping all the lexicon based features same, choice of combination of emoji vector and word vectors are varied to minimize cross validation metric. Table 1 describes the results for experiments conducted with different combinations of word vectors. Emoji embeddings (Eisner et al., 2016) give better results than using plain GloVe and Edinburgh embeddings. Edinburgh embeddings outperform GloVe embeddings in Joy and Sadness category but lag behind in Anger and Fear category. The official submission comprised of the top-performing model for each emotion category. This system ranked 3rd for the entire test dataset and 2nd for the subset of the test data formed by taking every instance with a gold emo-
tion intensity score greater than or equal to 0.5. Post competition, experiments were performed on ensembling diverse models for improving the accuracy. An ensemble obtained by averaging the results of the top 2 performing models outperforms all the individual models.
Given ǫ0,1,2 and α, select δ0,1,2 > 0 such that 1− α = δ0 + (Nt − 1)δ1 + (Nt − 1)δ2, and let us pick values for N ,M , M0 such that
δ0 ≤ 2|A|e−2M0(ǫ0) 2 , δ1 ≤ 2|A|Ne−2M(ǫ1) 2 , δ2 ≤ 4e(d + 1)
(32e
ǫp2
)d e−
Nǫ 2p 2
128 .
Note that the first two inequalities are approximated with first order approximation for which we know that 1− (1−2e−2M0(ǫ0)2)|A| ≤ 2|A|e−2M0(ǫ0)2 and 1− (1−2e−2M(ǫ1)2)|A|N ≤ 2|A|Ne−2M(ǫ1)2 . The obtained integer values for N ,M , M0 are given as 
    
    
N = ⌈ 128 (ln(4e(d + 1)) + d ln(32e)) (
1 ǫ2
)2p + 128dp (
1 ǫ2
)2p ln (
1 ǫ2
) + 128 (
1 ǫ2
)2p ln (
1 δ2
)⌉
,
M = ⌈
1 2
(
1 ǫ1
)2 (
ln(2|A|) + ln( 1δ1 ) + ln(N) ) ⌉ ,
M0 = ⌈ 1 2 ( 1 ǫ0
)2 (
ln(2|A|) + ln( 1δ0 ) )⌉ ,
.
The use of the obtained M,M0, N in (17) leads to a confidence of at least α.
The word sense disambiguation (WSD) task has been widely studied in the field of Natural Language Processing (NLP) [1]. This task is defined as the ability to computationally detect which sense is being conveyed in a particular context [2]. Although humans solve ambiguities in an effortlessly manner, this matter remains an open problem in computer science, owing to the complexity associated with the representation of human knowledge in computer-based systems [3]. The importance of the WSD task stems from its essential role in a variety of real world applications, such as machine translation [4], word processing [5], information retrieval and extraction [6, 7, 8, 9, 10, 11]. In addition, the resolution of ambiguities plays a pivotal role in the development of the so-called semantic web [12].
Many approaches devised to solve ambiguities in texts employ machine learning methods to automatically extract the best features in specific contexts [2]. Automatic methods commonly use texts as a source of information, and these texts need to be transformed into a structured format. Popular representations are vectors of features, trees and graphs of relations between words [1]. All such representations attempt to grasp, in a particular way, the semantical features related to the context surrounding ambiguous (target) words. Then, the information extracted from the context is used in the learning process. Although graphs have been employed in general pattern recognition methods [13, 14] and, particularly in the analysis of the semantical properties of texts in several ways [15, 16, 17, 18, 19, 20, 21], the use of network models in the learning process has been restricted to a few works (see e.g. [22, 23]). In addition, most of the current network models emphasise the relationship between all words of the document. As a consequence, a minor relevance has been given Preprint submitted to Elsevier June 28, 2016
to the relationships between feature and target words. In this paper, we propose a different network representation which does not consider the relationship between all words, as described e.g. in [17, 22]. We rather model texts using a bipartite network representation which focus on the relevant information arising from the relationship between feature and target words. This representation is then used as an underlying structure on which the proposed learning algorithm is applied. As we shall show, the combination of this textual representation and the proposed learning technique may improve the classification process when compared with well-known supervised algorithms hinging on traditional text representations. Remarkably, we have also found that our method retains its discriminative power even when a considerable small amount of training instances is available.
The remainder of this paper is organized as follows. Section 2 presents a brief review of basic concepts employed in this paper and related works. Section 3 presents the details of the proposed representation and algorithm to undertake the word sense disambiguation task. In Section 4, we discuss the details of the experiments and the results concerning the accuracy and robustness of the proposed method. Finally, we present some perspectives for further works.
Poseidon builds upon Petuum, a distributed big machine learning framework that provides a generic interface to a broad spectrum of ML programs [28]. Its design philosophy is rooted in iterative-convergent solutions to loss
function minimization. A number of ML algorithms are formulated in this manner, which involves repeatedly executing update equations that decrease some error functions. Some notable examples include stochastic gradient descent in optimization programs, MCMC and variational methods for graphical models, and proximal optimization for structured sparsity problems, among others.
In a mathematical form, the iterative-convergent algorithm can be represented as follows. Given data D and a loss function `, a typical ML problem can be solved by iteratively executing the update equation until the model parameters A reaches some stopping criteria.
A(t) = F(A(t−1),∆`(A(t−1),D)) (1)
where t denotes the iteration. The update function ∆` performs computation on data D with model parameters A to improve the loss `. The intermediate results are aggregated by function F .
In large-scale machine learning, both data D and model A can be very large. In data-parallelism, the data D is partitioned and assigned to computational worker machines (indexed by p = 1, · · · ,P), whereas in modelparallelism, the model A is partitioned and assigned to workers. Since we are interested in data-parallelism, we partition the data D into a set of Dp denoting the p-th data partition (i.e. often called mini-batch), as shown in Figure 1. Then, the update equation becomes
A(t) = F(A(t−1), P
∑ p=1 ∆`(A(t−1),Dp)) (2)
In each iteration, parameter updates ∆` produced by each partition of data are locally computed on each worker, and then are communicated to each other.
Note that the leader sends back the ‘leave’ agreement to follower3 if, and only if, it received an acknowledgement from follower3 showing that spacing has been increased.
2 (G f3 leave_platoon & ¬B f3 leave_agr (f3)) → 2 ¬D f3 perf(speed_controller(0))
(7)
It is important to recall that perceptions and communications coming in to the agent are represented as internal beliefs. Hence the proliferation of belief operators. The AJPF program model checker explores all possible combinations of shared beliefs and messages and so, even with the relatively low number of perceptions above, the combinatorial explosion associated with exploring all possibilities is very significant. Therefore, verifying the whole multi-agent platooning system using AJPF is infeasible.
To verify the global properties of multi-agent platooning, we use a complementary approach. We manually generate a model of the whole system as timed-automata and use the Uppaal model checker to establish the (timed) correctness of multi-agent platooning. In the following, we review the relevant timed-automata and highlight some of the global safety properties of vehicle platooning that have been verified using Uppaal.
As mentioned in the above section, fine tuning uses backpropagation, which is a common learning algorithm used in neural networks. Unlike one-step backpropagation used in training classifier of RBM, this one is a thorough one going through each layer. Backprogation algorithm is:
I. Perform a pass through all layers of the network, computing total input of each layer {z(1), ..., z(N)} and activations {a(1), ..., a(N)} of each layer. a(i) is the row vector that represents the activation of layer i.
II. For the last layer, compute δ(N)i as
δ (N) i =
∂L
∂z (N) i
(52)
where L is the classification error. Acquire a row vector δ(N).
III. For l = N − 1, ..., 1, compute
δ(l) = ( δ (W (l))T ) • g′(z(l)) (53)
where g is the activation function.
IV. Compute the gradients in each layer. For l = N, ..., 1, compute
∇W (l)L = (a(l−1))T δ(l), (54) ∇b(l)L = δ(l). (55)
where a(0) is the training data.
V. Update the weights and biases of each layer using gradient descent.
In fine tuning, the training data should be used repeatedly to refine the model parameters.
The majority class is under-sampled by randomly removing samples from the majority class population until the minority class becomes some specified percentage of the majority class. This forces the learner to experience varying degrees of under-sampling and at higher degrees of under-sampling the minority class has a larger presence in the training set. In describing our experiments, our terminology will be such that if we under-sample the majority class at 200%, it would mean that the modified dataset will contain twice as many elements from the minority class as from the majority class; that is, if the minority class had 50 samples and the majority class had 200 samples and we under-sample majority at 200%, the majority class would end up having 25 samples. By applying a combination of under-sampling and over-sampling, the initial bias of the learner towards the negative (majority) class is reversed in the favor of the positive (minority) class. Classifiers are learned on the dataset perturbed by “SMOTING” the minority class and under-sampling the majority class.
RNNs are a class of neural networks used to map sequences to sequences. This is possible because of the feedback connections between hidden nodes. In a bi-
directional RNN, the hidden layer has two components each corresponding to forward(past) and backward(future) connections. For a given input sequence O = (O1, O2, ..., OT ), the output of the network is calculated as follows: forward pass through forward component of the hidden layer at a given instant t is given by
h f t = g(W f hoOt + W f hhh f t−1 + b f h) (1)
where Wfho is the input-to-hidden weights for forward component, W f hh corresponds to hidden-to-hidden weights between forward components, and b f
h is the forward component bias. g is a non-linearity depending on the choice of the hidden layer unit. Similarly, forward pass through the backward component of the hidden layer is given by
h b
t = g(W b hoOt + W b hhh
b t−1 + b b h) (2)
where Wbho, W b hh, b
b h are the corresponding parameters for the backward com-
ponent. The input to next layer is the concatenated vector [hft ,h b t ]. In a deep RNN multiple such bidirectional hidden layers are stacked. RNNs are trained using Back-Propagation Through Time (BPTT) algorithm. The training algorithm suffers from vanishing gradients problem which is overcome by using a special unit in hidden layer called the Long Short Term Memory(LSTM)[11][6].
Theorem 1. Mechanism (π∗, xPEV ) is ex-post truthful if for all i ∈ N , vi is multilinear in PoS.
Proof. According to the characterization of truthful mechanisms given by Proposition 9.27 from (Nisan et al. 2007), we need to prove that for all i ∈ N , for all θ ∈ Θ:
1. xPEVi (θ) does not depend on i’s report, but only on the task allocation alternatives;
2. i’s utility is maximized by reporting θi truthfully if the others report θ−i truthfully.
From the definition of xPEVi in (2), we can see that given the allocation π∗(θ), agent i cannot change V 1−i(θ, π
∗) and V 0−i(θ, π
∗) without changing the allocation π∗(θ). Therefore, xPEVi does not depend on i’s report, but only on the task allocation outcome π∗(θ).
In what follows, we show that for each agent i, if the others report types truthfully, then i’s utility is maximized by reporting her type truthfully.
Given an agent i’ of type θi and the others’ true type profile θ−i, assume that i reported θ̂i 6= θi. For the allocation τ = π∗(θ̂i, θ−i), according to xPEVi , when i finally completes her tasks, i’s utility is u1i = vi(τ, (1, p τ −i)) − hi(θ−i) + V 1 −i((θ̂i, θ−i), π ∗) and her utility if she fails is u0i = vi(τ, (0, p τ −i)) − hi(θ−i) + V 0 −i((θ̂i, θ−i), π
∗). Note that i’s expected valuation depends on her true valuation vi and all agents’ true PoS. Therefore, i’s expected utility is:
pτi×u 1 i + (1− p τ i )× u 0 i =
pτi × vi(τ, (1, p τ −i)) (3)
+ (1− pτi )× vi(τ, (0, p τ −i)) (4) + pτi ∑
j∈N\{i}
vj(τ, (1, p τ −i)) (5)
+ (1− pτi ) ∑
j∈N\{i}
vj(τ, (0, p τ −i)) (6)
− hi(θ−i).
Since all valuations are multilinear in PoS, the sum of (3) and (4) is equal to vi(τ, pτ ), and the sum of (5) and (6) is ∑
j∈N\{i} vj(τ, p τ ). Thus, the sum of (3), (4), (5) and (6) is the social welfare under allocation π∗(θ̂i, θ−i). The social welfare is maximized when i reports truthfully because π∗ maximizes social welfare (note that this is not the case when θ−i is not truthfully reported). Moreover, hi(θ−i) is independent of i’s report and is the maximum social welfare that the others can achieve without i. Therefore, by reporting θi truthfully, i’s utility is maximized.
Theorem 1 shows that multilinearity in PoS is sufficient to truthfully implement (π∗, xPEV ) in an expost equilibrium (ex-post truthful), but not in a dominant strategy (truthful). It has been shown in similar settings that ex-post truthfulness is the best we can achieve here (Porter et al. 2008; Ramchurn et al. 2009; Stein et al. 2011; Conitzer and Vidali 2014).
This work is partially supported by the Dragon 3 programme, a co-operation between the European Space Agency and the Ministry of Science and Technology of China. The authors would also like to acknowledge the Chinese partners at the Academy of Opto-Electronics, Chinese Academy of Sciences for making this data available for our research.

We use the pdsat program described in Section 3 to plan computational experiments in the volunteer computing project SAT@home. SAT@home was launched on the 29th of September 2011 [31]. It uses computing resources provided by volunteer PCs to solve hard combinatorial problems that can be effectively reduced to SAT. The project was implemented using the BOINC platform. An experiment that consisted in solving 10 inversion problems of the generator A5/1 was successfully finished in SAT@home on the 7th of May 2012. It should be noted that we considered only instances that cannot be solved using the known rainbow tables [26].
In that experiment we used the decomposition set from [28]. The computing application was based on a modified version of MiniSat-C 1.14.1 (see [28]).
First 114 bits of the keystream that correspond to one keystream burst of the GSM protocol were analyzed. On average in order to solve one problem of logical cryptanalysis of A5/1 SAT@home processed about 1 billion SAT problems.
Since May 2012 SAT@home is occupied in searching for systems of orthogonal Latin squares. During this time we found several pairs of orthogonal diagonal Latin squares of order 10 that are different from the ones published in [6].
Characteristics of the SAT@home project as of 8 of February 2013 are (according to BOINCstats1):
– 2367 active PCs (active PC in volunteer computing is a PC that sent at least one result in last 30 days) about 80% of them use Microsoft Windows OSes; – 1299 active users (active user is a user that has at least one active PC); – versions of the client application: Windows/x86, Linux/x86, Linux/x64; – average real performance: 2,9 teraflops, maximal performance: 6,3 teraflops.
The dynamics of the real performance of SAT@home can be seen at the SAT@home performance page2.
It should be noted that the estimation for the A5/1 cryptanalysis (see Section 3) obtained with the use of pdsat is close to the average real time spent by SAT@home to solve corresponding SAT problems. With respect to the estimation from Section 3 logical cryptanalysis of Bivium cipher would take about 6 years in SAT@home with its current performance.
This section mainly discusses some applications of our multi-granular perspectives on covering. we firstly recall relationship between quasi-discrete closure operator and binary relation, and then discuss relational interpretations as well as axiomizations of four types of covering rough approximation operators.
The trend today on recommendation systems is to recommend relevant information to users, using supervised machine learning techniques. In that type of techniques, the recommender system has to pass by two steps: (1) The learning step, where examples are presented to the system; which "learns" from examples and gradually adjusts its parameters to the desired output. (2)Exploitation step: new examples never seen before are presented to the system and ask it for generalizing [10]. These approaches have good results. However, they need an amount of experience provided by an expert. They cannot start from scratch and they are slow. Moreover, the user’s interest can change with the time, and the techniques cannot really follow this. Some works found in literature try to solve those problems, as explained in what follows.
-Starting from scratch: to avoid this problem, which is commune to machine learning algorithms, in [7] authors use collaborative filtering to consider demographic information about users for providing them more accurate prediction, but their system does not follow the user’s interest evolution. - Avoiding the intervention of experts: To avoid the intervention of an expert, in [9] the authors use Reinforcement Learning (RL), which is a good alternative because it does not need a previous experience to start work. However, a major difficulty when applying RL techniques to real world problems is their slow convergence. -Accelerate the learning process: In [9], the author proposes to accelerate RL by using indirect Q-learning. However, their recommendation system starts with a set of actions which are predefined by them. -The evolution of the user’s interest: The authors on [19] propose to follow the interest of the user by using an exploration strategy on the q-learning algorithm. But they don’t care about the others problems cited above. We can observe that each work cited above tries to solve only one of those problems and none of them proposes to solve all of them at the same time. To create a system avoiding all the problems, we propose to use the Q-learning algorithm with an exploration strategy to solve the problem of intervention of an expert and follow the user’s interest evolution. For the starting from scratch problem, we give Qlearning algorithm the ability to explore the knowledge of other users by using collaborative filtering. To accelerate the Q-learning process, we mix it with case base reasoning techniques to allow the reuse of the case-base and satisfy the user more quickly. We were inspired by case base reasoning to accelerate reinforcement learning techniques introduced and implemented by [11] in robotic.
The text generation process in CBR-METEO begins with the retrieval of a case from the casebase whose weather data is most similar to the input (query). Defining how the similarity metric is therefore very important for the retrieval component. The best form of similarity minimizes the work done by the succeeding components of reuse and revise.
Our similarity computation ensures that a retrieved similar weather data must have the same number of states as the input. This is because the number of states usually determines the number of phrases in the forecast text. Time attributes are compared using the differences between time stamps in aligned wind states. We then define similarity between weather data (i.e. input and each case in the casebase) mainly in terms of patterns across wind states. The patterns for a scalar attribute (e.g. wind speed) are increasing, decreasing or constant as we move from one state to another while veering (clockwise), backing (anticlockwise) or stable patterns are applicable to vectorial attributes (e.g. wind direction). The input and weather data in each case in the casebase are transformed into a representation showing the pattern transition across wind states for each scalar and vectorial attribute.
Retrieval is done in a step-wise or hierarchical manner in which all previous cases having the same number of wind states as the input data are first retrieved. Cases within this retrieval set whose average time stamp differences from the input are within a specified threshold are then selected. Similarly, cases with the same weather patterns (for wind speeds, directions and gusts) as the input across states are then selected iteratively from preceding subset of cases. If more than one case is retrieved at the end of this iterative process, the most similar case is chosen as the one with the shortest average distance between its wind states and the input‟s. The distance between wind states is computed by first converting wind directions into their numeric angular values (in degrees). Each pair of wind speed and angular direction is taken as a vector quantity which represents a wind state. Cosine rule is then used to compute the distance between vectors and an average taken across the number of wind states in the input. However, if no case is retrieved as the end of the iterative process, the system gives no forecast text. The stepwise retrieval ensures that retrieved cases are semantically similar to the input data and therefore minimal modifications are carried out by the reuse and revision components.
Table 2 shows the best case retrieved for the input data in Table 1. The retrieved case is most similar to the query not only because they have the same number of states and time stamps but the wind speed and direction patterns are also similar. The wind direction as we move from state 1 to 2 in both the input data (n → ne) and retrieved case (nnw → nne) are veering (i.e. clockwise). The increasing wind speed pattern is also common to both; an average speed of 8/9 knots in the early morning (6a.m.) to 17/14 knots by midnight in the input/retrieved case respectively. An example of how to compute the distance between two wind states using cosine rule as the last step in retrieval process is shown in Figure 3. The values shown in the example are those from the first wind states for the input and the retrieved similar wind data. The wind speed shown for each wind state is an average of the minimum and maximum wind speeds.
We are grateful to Sujan Perera and Monireh Ebrahimi for thought-provoking discussions on the topic. We acknowledge partial support from the National Science Foundation (NSF) award: CNS1513721: “Context-Aware Harassment Detection on Social Media” and National Institutes of Health (NIH) award: MH105384-01A1: “Modeling Social Behavior for Healthcare Utilization in Depression”. Any opinions, findings, and conclusions/recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the NSF or NIH.
For this report, only a small subset of the papers was selected to cover as much ground as possible. The following list may be valuable to the interested readers.
• Mining evidences for named entity disambiguation The authors discuss a modified LDA model for gathering more words that are important to disambiguate an entity. Li, Yang, et al. ”Mining evidences for named entity disambiguation.” Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2013.
• We have emphasized on Wikipedia as the catalog. The following work presents a general approach Sil, Avirup, et al. ”Linking named entities to any database.” Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics, 2012.
• Large scale named entity disambiguation. Cucerzan, Silviu. “Large-Scale Named Entity Disambiguation Based on Wikipedia Data.” EMNLP-CoNLL. Vol. 7. 2007.
• One of the initial works on NED Bunescu, Razvan C., and Marius Pasca. ”Using Encyclopedic Knowledge for Named entity Disambiguation.” EACL. Vol. 6. 2006.
• Quick entity annotations for short text Suchanek, Fabian M., Gjergji Kasneci, and Gerhard Weikum. ”Yago: a core of semantic knowledge.” Proceedings of the 16th international conference on World Wide Web. ACM, 2007.
We augmented our system by introducing a primitive automatic weight generation technique [17]. We further improve our primitive automatic weight generation technique with our proposed metric of automatic weight generation [32]. The overall augmented instance matching system is depicted in Fig. 2.
Considering weight factor assigned to each of the property automatically, we define the affinity between two SLCs by modified affinity measurement metric as follows:
(14)
whereγ represents the factors for missing property values.
We next focus on two so-called targeted algorithms, which focus on playing against particular classes of opponents. Both these algorithms are based around identifying what the opponent is doing (with particular attention paid to stationarity and Nash equilibrium), and then updating their behavior based on this assessment.
AWESOME (Conitzer and Sandholm, 2003; Conitzer and Sandholm, 2007) tracks the opponent’s behavior in different periods of play and tries to maintain hypotheses about its play. For example, AWESOME attempts to determine whether the other algorithm is playing a particular stage-game Nash equilibrium. If it is, AWESOME responds with its own component of that special equilibrium. This special equilibrium is known in advance by all implementations of AWESOME to avoid equilibrium selection issues in self play. There are other situations where it acts in a similar fashion to fictitious play, and there are still other discrete modes of play that it engages in depending on its beliefs.
Meta (Powers and Shoham, 2005) switches between three simpler strategies: a strategy similar to fictitious play, a determined-style algorithm that stubbornly plays a Nash equilibrium, and the maxmin strategy. Strategy selection depends on the recorded history of average reward and the empirical distribution of the opponent’s actions across different periods of play.
2 We can imagine variations on the determined idea that do not play an action from a Nash equilibrium. For example, a variant could instead choose the action whose best response yields the algorithm the highest payoff. Note that this differs from a stage-game Nash equilibrium because this determined-like algorithm need not itself play a best response. Such an outcome amounts to an equilibrium of the Stackelberg version of the stage game. That is, we can change the game so that instead of the two players moving simultaneously, the determined-like agent moves first.
5 Meta was shown both theoretically and empirically to be nearly optimal against itself, close to the best response against stationary agents, and to approach (or exceed) the security level of the game in all cases.
Because both of these algorithms switch between simpler strategies depending on the situation, they can be viewed as portfolio algorithms. Note that both manage similar portfolios that include a determined-style algorithm and a fictitious play algorithm.
Evaluation Metrics. We conduct colocalization experiments on PASCAL VOC 2007 [35]. We use two evaluation metrics to compare with state of the art colocalization techniques:
1) The standard Intersection over union (IoU) metric for object detection(intersection of predicted bounding box area and groundtruth bounding box area divided by the area of their union)
2) Correct Localization (CorLoc) metric, an evaluation metric used in related work [9], [30], and defined as the percentage of images correctly localized according to the criterion: IoU > .5.
Given the domain description partially explained above, we can solve planning problems using the reasoning system CCalc [4] by means of “queries” like the following:
:- query
maxstep :: 0..3; % Initial State 0: is_at(sco2)=loc_0x0, ori_is(sco2)=vert,
is_at(obj1)=loc_2x1, ori_is(obj1)=vert;
% Goal maxstep: is_below(loc_0x0,obj1).
This query asks for a shortest plan whose length is at most 3, for a planning problem with:
– Initial state: the object sco2 is placed on the work place at location loc_0x0 with a vertical orientation and the object obj1 is placed at location loc_2x1 with a vertical orientation, and – Goal: a configuration of objects such that obj1 is above location loc_0x0.
This work is supported by National High Technology Research and Development Program of China (2015AA016305), National Natural Science Foundation of China (NSFC) (61375027, 61433018 and 61370023), joint fund of NSFCRGC (Research Grant Council of Hong Kong) (61531166002, N CUHK404/15) and Major Program for National Social Science Foundation of China (13&ZD189).
1git@github.com : mxmaxi007/Emotion Recognition.git
Finally, we turn to hyperparamater tuning to show that DFO-TR can also outperform state-of-the-art methods on this problem. We consider tuning parameters of an RBFkernel, cost-sensitive, SVM, with ℓ2 regularization parameter λ, kernel width γ, and positive class cost cp. Thus, in this setting, we compare the performance of DFO-TR,
Table 6. Comparing DFO-TR vs. random search algorithm.
Data DFO-TR Random Search Random Search
AUC num. fevals AUC num. fevals AUC num. fevals
fourclass 0.835±0.019 100 0.836±0.017 100 0.839±0.021 200 svmguide1 0.988±0.004 100 0.965±0.024 100 0.977±0.009 200 diabetes 0.829±0.041 100 0.783±0.038 100 0.801±0.045 200 shuttle 0.990±0.001 100 0.982±0.006 100 0.988±0.001 200 vowel 0.975±0.027 100 0.944±0.040 100 0.961±0.031 200
magic04 0.842±0.006 100 0.815±0.009 100 0.817±0.011 200 letter 0.987±0.003 200 0.920±0.026 200 0.925±0.018 400 segment 0.992±0.007 300 0.903±0.041 300 0.908±0.036 600 ijcnn1 0.913±0.005 300 0.618±0.010 300 0.629±0.013 600 svmguide3 0.776±0.046 300 0.690±0.038 300 0.693±0.039 600 german 0.795±0.024 300 0.726±0.028 300 0.739±0.021 600 satimage 0.757±0.013 300 0.743±0.029 300 0.750±0.020 600
10 20 30 40 50 60 70 80 90 100 110
Number of accessed data points (scale 1e+4)
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
A U
C v
al ue
DFO-TR, letter (d=16, N=20000)
0 50 100 150 200 250
Number of accessed data points (scale 1e+4)
0.9
0.91
0.92
0.93
0.94
0.95
0.96
0.97
0.98
0.99
A U
C v
al ue
DFO-TR, shuttle (d=9, N=43500)
0 5 10 15 20 25
Number of accessed data points (scale 1e+4)
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
A U
C v
al ue
S-DFO-TR, letter (d=16, N=20000)
0 5 10 15 20 25 30 35 40
Number of accessed data points (scale 1e+4)
0.9
0.91
0.92
0.93
0.94
0.95
0.96
0.97
0.98
0.99
A U
C v
al ue
S-DFO-TR, shuttle (d=9, N=43500)
Figure 1. Comparison of stochastic DFO-TR and deterministic one in optimizing AUC function.
random search, and Bayesian optimization algorithms, in tuning a three-dimensional hyperparameterw = (λ, γ, cp), in order to achieve a high test accuracy.
For the random search algorithm, as well as the Bayesian optimization algorithms, the search space is chosen as λ ∈ [10−6, 100], γ ∈ [100, 103], as is done in (Jamieson & Talwalkar, 2015), and cp ∈ [10−2, 102]. The setting of Algorithm 1 is as described in §5.1, while w0 = (λ0, γ0, cp0) is a three-dimensional vector randomly drawn from the search space defined above.
We have used the five-fold cross-validation with the trainvalidate-test framework as follows: we used two folds as the training set for the SVM model, other two folds as the validation set to compute and maximize the validation accuracy, and the remaining one as the test set to report the test accuracy.
Figure 2 illustrates the performance of DFO-TR versus random search and Bayesian optimization algorithms, in terms of the average test accuracy over the number of function
evaluations. As we can see, DFO-TR constantly surpasses random search and Bayesian optimization algorithms. It is worth mentioning that random search is competitive with the BO methods and in contrast to §5.1 and §5.2, SMAC performs the best among the Bayesian optimization algorithms.
ar X
iv :1
40 9.
51 89
v1 [
cs .A
I] 1
8 Se
p 20
14
In this section, we propose a novel method to train word embedding on unlabeled data with BLSTM RNN. In this approach, BLSTM RNN is also used to do a tagging task, but only has two types of tags to predict: incorrect/correct. The input is a sequence of words which is a normal sentence with some words replaced by randomly chosen words. For those replaced words, their tags are 0 (incorrect) and for those that are not replaced, their tags are 1 (correct). Although it is possible that some replaced words are also reasonable in the sentence, they are still considered “incorrect”. Then BLSTM RNN is trained to minimize the binary classification error on the training corpus. The neural network structure is the same as that in Figure 1. When the neural network is trained, W1 contains all trained word embeddings.
Traditional logic programming [1,2] is based upon query answering. Unlike this, logic programs under the stable model semantics [3] are implemented by model generation based systems, viz. answer set solvers [4]. Although the latter also allows for checking whether a query is entailed by some stable model, there is so far no way to explore a domain at hand by posing consecutive queries without relaunching the solver. The same applies to the interactive addition and/or deletion of temporary program parts that come in handy during theory exploration, for instance, when dealing with hypotheses.
An exemplary area where such exploration capacities would be of great benefit is bio-informatics (cf. [5,6,7,8,9,10]). Here, we usually encounter problems with large amounts of data, resulting in runs having substantial grounding and solving times. Furthermore, problems are often under-constrained, thus yielding numerous alternative solutions. In such a setting, it would be highly beneficial to explore a domain via successive queries and/or under certain hypotheses. For instance, for determining nutritional requirements for sustaining maintenance or growth of an organism, it is important to indicate seed compounds needed for the synthesis of other compounds. Now, rather than continuously analyzing several thousand stable models (or their intersection or union), a biologist may rather perform interactive “in-silico” experiments by temporarily adding compounds and subsequently exploring the resulting models by posing successive queries.
We address this shortcoming and show how recently developed systems for reactive answer set programming (ASP) [11,12] can be harnessed to provide query answering ? Affiliated with the School of Computing Science at Simon Fraser University, Burnaby, Canada,
and the Institute for Integrated and Intelligent Systems at Griffith University, Brisbane, Australia.
ar X
iv :1
31 2.
61 43
v1 [
cs .A
I] 2
0 D
ec 2
01 3
and theory exploration capacities. In fact, reactive ASP was conceived for incorporating online information into operative ASP solving processes. Although this technology was originally devised for dealing with data streams in dynamic environments, like assisted living and cognitive robotics, it can likewise be used to incorporate facts, rules, or queries provided by a user. As a result, we present the design and implementation of a system for interactive query answering and theory exploration with ASP. Our system quontroller1 is based on the reactive answer set solver oclingo and implemented as a dedicated front-end. We describe its functionality and implementation, and we illustrate its features on a running example.
Content-based bibliographic studies is a discipline that attempts to detect main topics in a research domain. By making use of associations or distinctions, researchers might track changes, advances and novelties on big textual datasets. Under this perspective, counting words for topic identification makes sense because it is inferred that documents using the same words are closely related on topic and field (Benzécri, 1981). At this respect, correspondence analysis (CA) techniques (Benzécri, 1973, 1981; Lebart, L., Salem, A., & Berry, 1998; Murtagh, 2005) have proved to be a suitable tool for seeking similarities among documents by identifying words co-occurrences. The above makes the CA a useful tool to be applied on text mining (Bansard et al., 2007; Kerbaol, Bansard, & Coatrieux, 2006; Kerbaol & Bansard, 2000; Morin & Rennes, 2006; Rouillier, Bansard, & Kerbaol, 2002; Šilić, Morin, Chauchat, & DalbeloBašić, 2012).
When doing textual mining, it is identified that progress on science takes place by establishing a link among pioneer words of the first publications, which later are complemented or enriched by further authors. This progress is expressed on how the vocabulary evolves over the time, and how the introduced words are adopted on further research. To track the evolution, a time-oriented viewpoint is adopted, but the word co-occurrence approach is maintained. Multiple factor analysis for contingency tables (MFACT; (Bécue-Bertaut & Pagès, 2004, 2008) are applied on these purposes. Maps of words and abstracts, which takes into account both chronology and co-occurrences, are utilized for disclosing the evolutions on vocabulary by the passing of the years. In other words, by taking advantage of the features of MFACT for comparing chronology and vocabulary on the abstracts, novelties are spotted. MFACT is also a powerful tool for identifying abrupt changes on vocabulary or periods characterized by a flat use of words.
In this subsection, we consider the case where the desired data is generated by a piecewise linear model whose partitions do not match with the initial partitioning of the proposed algorithms. This experiment mainly focuses on to demonstrate how the proposed algorithms learn the underlying data structure. We also aim to emphasize the importance of adaptive structure.
We use the following piecewise linear model to generate the data sequence,
ŷt =  wT1 xt + υt ,x T t n0 ≥ 0.5 and xTt n1 ≥ −0.5 wT2 xt + υt ,x T t n0 ≥ 0.5 and xTt n1 < −0.5 wT2 xt + υt ,x T t n0 < 0.5 and x T t n2 ≥ −0.5
wT1 xt + υt ,x T t n0 < 0.5 and x T t n2 < −0.5
(33)
where w1 = [1, 1] T , w2 = [1,−1]T , n0 = [2,−1]T , n1 = [−1, 1]T and n2 = [2, 1]T . The feature vector xt = [xt,1, xt,2] T is composed of two jointly Gaussian processes with [0, 0]T mean and I2 variance. υt is a sample taken from a Gaus-
sian process with zero mean and 0.1 variance. The generated data sequence is represented by ŷt. The learning rates are set to 0.04 for the FMP, 0.025 for the SP, 0.005 for the S-DAT, the CTW and the FNF, 0.025 for the EMFNF and the VF, 0.5 for the GKR.
In Fig. 8, we demonstrate the normalized time accumulated error performance of the proposed algorithms. Different from the matched partition scenario, we emphasize that the CTW algorithm performs even worse than the VF, the FNF and the EMFNF algorithms, which are not based on piecewise linear modeling. The reason is that the CTW algorithm has fixed regions that are mismatched with the underlying partitions. Besides, the adaptive algorithms, FMP, SP, S-DAT and DAT achieve considerably better performance, since these algorithms update their partitions in accordance with the data distribution. Comparing these four algorithms, Fig. 8 exhibits that the FMP notably outperforms its competitors, since this algorithm exactly matches its partitioning to the partitions of the piecewise linear model given in (33).
We illustrate how the FMP and the DAT algorithms update their region
boundaries in Fig. 9. Both algorithms initially partition the regression space into 4 equal quadrant, i.e., the cases shown in t = 0. We emphasize that when the number of iterations reaches 10000, i.e., t = 10000, the FMP algorithm trains its region boundaries such that its partitions substantially match the partitioning of the piecewise linear model. However, the DAT algorithm cannot capture the data distribution yet, when t = 10000. Therefore, the FMP algorithm, which uses the second order methods for training, has a faster convergence rate compared to the DAT algorithm, which updates its region boundaries using first order methods.
The performance of spoofing detection using the DNN-FBCC feature is evaluated on the ASVspoof 2015 database [17]. As shown in TABLE I, the database includes three sub datasets without target speaker overlap: the training set, the development set and the evaluation set. We used the training set for FBNN and human/spoof classifier training. The development set and evaluation set were used for testing.
Training set and development set are attacked by the same five spoofing methods, where S1, S2 and S5 belong to VC method and S3, S4 belong to SS method. Regarding the evaluation set, besides the five known spoofing methods, there are another five unknown methods, where S6-S9 are VC methods and S10 is an SS method.
The speech signals were segmented into frames with 20ms length and 10ms step size. Pre-emphasis and a hamming window were applied on the frames before the spectrum computation. Paper [16] showed that all the frames of speech are useful for spoofing detection, so we did not apply any voice activity detection method.